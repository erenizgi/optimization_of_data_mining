{
    "author": "zucchini-nlp",
    "message": "ðŸš¨ [v5] Rendundant code in nested configs (#41314)\n\n* batch update models\n\n* delete even more\n\n* fix modular super init location\n\n* fix\n\n* fix copies\n\n* fix again, these have force-set values in configs\n\n* fix copies",
    "sha": "d1c6310d6a02481d48d81607cba7840be04580d1",
    "files": [
        {
            "sha": "dbb11ae0ab369a1df6dcea7cca43226d0eaa2d08",
            "filename": "docs/source/en/model_doc/align.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -154,7 +154,6 @@ for label, score in zip(candidate_labels, probs):\n ## AlignConfig\n \n [[autodoc]] AlignConfig\n-    - from_text_vision_configs\n \n ## AlignTextConfig\n "
        },
        {
            "sha": "5f5d5efd7a1565d8187f4bb73a661b1007b0018b",
            "filename": "docs/source/en/model_doc/blip-2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -60,7 +60,6 @@ If you're interested in submitting a resource to be included here, please feel f\n ## Blip2Config\n \n [[autodoc]] Blip2Config\n-    - from_vision_qformer_text_configs\n \n ## Blip2VisionConfig\n "
        },
        {
            "sha": "9c30c29ee5a1c3756949b4f95e9a78288e792372",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -87,7 +87,6 @@ Refer to this [notebook](https://github.com/huggingface/notebooks/blob/main/exam\n ## BlipConfig\n \n [[autodoc]] BlipConfig\n-    - from_text_vision_configs\n \n ## BlipTextConfig\n "
        },
        {
            "sha": "c804ce3f04d73ef2896198085fef00fa4983fb1a",
            "filename": "docs/source/en/model_doc/chinese_clip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -76,7 +76,6 @@ Currently, following scales of pretrained Chinese-CLIP models are available on \n ## ChineseCLIPConfig\n \n [[autodoc]] ChineseCLIPConfig\n-    - from_text_vision_configs\n \n ## ChineseCLIPTextConfig\n "
        },
        {
            "sha": "a1fe7753feb21ca71e3a725db4212601a06f073e",
            "filename": "docs/source/en/model_doc/clap.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -63,7 +63,6 @@ print(f\"Text embeddings: {text_features}\")\n ## ClapConfig\n \n [[autodoc]] ClapConfig\n-    - from_text_audio_configs\n \n ## ClapTextConfig\n "
        },
        {
            "sha": "529194d32a37c2b723308955104f61a6e5951f8c",
            "filename": "docs/source/en/model_doc/clip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -87,7 +87,6 @@ print(f\"Most likely label: {most_likely_label} with probability: {probs[0][most_\n ## CLIPConfig\n \n [[autodoc]] CLIPConfig\n-    - from_text_vision_configs\n \n ## CLIPTextConfig\n "
        },
        {
            "sha": "6af0bb754de4bdefff5bda85c5c564fc5f0348ea",
            "filename": "docs/source/en/model_doc/clipseg.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -72,7 +72,6 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n ## CLIPSegConfig\n \n [[autodoc]] CLIPSegConfig\n-    - from_text_vision_configs\n \n ## CLIPSegTextConfig\n "
        },
        {
            "sha": "ad3bc51161cd01d8b1317ee0618b430e04fad696",
            "filename": "docs/source/en/model_doc/clvp.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -73,7 +73,6 @@ Example :\n ## ClvpConfig\n \n [[autodoc]] ClvpConfig\n-    - from_sub_model_configs\n \n ## ClvpEncoderConfig\n "
        },
        {
            "sha": "646da0fa4ab762d1dd9d7ff822986ae97f9e1163",
            "filename": "docs/source/en/model_doc/groupvit.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -46,7 +46,6 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n ## GroupViTConfig\n \n [[autodoc]] GroupViTConfig\n-    - from_text_vision_configs\n \n ## GroupViTTextConfig\n "
        },
        {
            "sha": "7cbab82b287e8b794e50ac34c187b253ae124541",
            "filename": "docs/source/en/model_doc/instructblip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -45,7 +45,6 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n ## InstructBlipConfig\n \n [[autodoc]] InstructBlipConfig\n-    - from_vision_qformer_text_configs\n \n ## InstructBlipVisionConfig\n "
        },
        {
            "sha": "3a6ba29d243fa3c6f34b92a0315e5812bde031ee",
            "filename": "docs/source/en/model_doc/instructblipvideo.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -45,7 +45,6 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n ## InstructBlipVideoConfig\n \n [[autodoc]] InstructBlipVideoConfig\n-    - from_vision_qformer_text_configs\n \n ## InstructBlipVideoVisionConfig\n "
        },
        {
            "sha": "ce17459b8d8577ecc0e8c0f5fae427102b7f3ae3",
            "filename": "docs/source/en/model_doc/metaclip_2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -88,7 +88,6 @@ print(f\"Most likely label: {most_likely_label} with probability: {probs[0][most_\n ## MetaClip2Config\n \n [[autodoc]] MetaClip2Config\n-    - from_text_vision_configs\n \n ## MetaClip2TextConfig\n "
        },
        {
            "sha": "88ca524dd4c4ed5d244f3a50b56e955b4ca3d93f",
            "filename": "docs/source/en/model_doc/owlv2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -90,7 +90,6 @@ Usage of OWLv2 is identical to [OWL-ViT](owlvit) with a new, updated image proce\n ## Owlv2Config\n \n [[autodoc]] Owlv2Config\n-    - from_text_vision_configs\n \n ## Owlv2TextConfig\n "
        },
        {
            "sha": "06d88fdf98b5696aa2ae39d115544f5d19f83fbc",
            "filename": "docs/source/en/model_doc/owlvit.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -81,7 +81,6 @@ A demo notebook on using OWL-ViT for zero- and one-shot (image-guided) object de\n ## OwlViTConfig\n \n [[autodoc]] OwlViTConfig\n-    - from_text_vision_configs\n \n ## OwlViTTextConfig\n "
        },
        {
            "sha": "6894ba7bb593c8200a8c92f76915d2818adfffeb",
            "filename": "docs/source/en/model_doc/pix2struct.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -47,7 +47,6 @@ The original code can be found [here](https://github.com/google-research/pix2str\n ## Pix2StructConfig\n \n [[autodoc]] Pix2StructConfig\n-    - from_text_vision_configs\n \n ## Pix2StructTextConfig\n "
        },
        {
            "sha": "28def85a8b038533f2fb6ddad9d0ad161562efe5",
            "filename": "docs/source/en/model_doc/siglip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -130,7 +130,6 @@ print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n ## SiglipConfig\n \n [[autodoc]] SiglipConfig\n-    - from_text_vision_configs\n \n ## SiglipTextConfig\n "
        },
        {
            "sha": "529879c7bcb36fe28c01ab11bbb9dc2db2cb36c2",
            "filename": "docs/source/en/model_doc/xclip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fxclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fen%2Fmodel_doc%2Fxclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxclip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -57,7 +57,6 @@ If you're interested in submitting a resource to be included here, please feel f\n ## XCLIPConfig\n \n [[autodoc]] XCLIPConfig\n-    - from_text_vision_configs\n \n ## XCLIPTextConfig\n "
        },
        {
            "sha": "efa380dcc08779a89633b0e9bfaf22969e062955",
            "filename": "docs/source/ja/model_doc/align.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Falign.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Falign.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Falign.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -72,7 +72,6 @@ ALIGNã®ä½¿ç”¨ã‚’é–‹å§‹ã™ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ã®Hugging Faceã¨ã‚³ãƒŸãƒ¥ãƒ‹\n ## AlignConfig\n \n [[autodoc]] AlignConfig\n-    - from_text_vision_configs\n \n ## AlignTextConfig\n "
        },
        {
            "sha": "108fc55c955adb4061b5a1b7a0fd5a23753e3fe6",
            "filename": "docs/source/ja/model_doc/altclip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Faltclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Faltclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Faltclip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -65,7 +65,6 @@ Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã«ç”»åƒã‚’ä¸Žãˆã‚‹ã«ã¯ã€å„ç”»åƒã‚’å›ºå®šã‚µ\n ## AltCLIPConfig\n \n [[autodoc]] AltCLIPConfig\n-    - from_text_vision_configs\n \n ## AltCLIPTextConfig\n "
        },
        {
            "sha": "594631b3e233a46b98dde6eb930a7b83a39908d6",
            "filename": "docs/source/ja/model_doc/blip-2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip-2.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -51,7 +51,6 @@ BLIP-2 ã®ä½¿ç”¨ã‚’é–‹å§‹ã™ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³\n ## Blip2Config\n \n [[autodoc]] Blip2Config\n-    - from_vision_qformer_text_configs\n \n ## Blip2VisionConfig\n "
        },
        {
            "sha": "f55d4edf17e05f87b2b53324c1878a0e65f91ebf",
            "filename": "docs/source/ja/model_doc/blip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -42,7 +42,6 @@ BLIP ã¯ã€æ¬¡ã®ã‚ˆã†ãªã•ã¾ã–ã¾ãªãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« ã‚¿ã‚¹ã‚¯ã‚’å®Ÿ\n ## BlipConfig\n \n [[autodoc]] BlipConfig\n-    - from_text_vision_configs\n \n ## BlipTextConfig\n "
        },
        {
            "sha": "979c2b798458a69405fb5ac0b18e59ea7b051071",
            "filename": "docs/source/ja/model_doc/chinese_clip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fchinese_clip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fchinese_clip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fchinese_clip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -71,7 +71,6 @@ Chinese-CLIP ãƒ¢ãƒ‡ãƒ«ã¯ã€[OFA-Sys](https://huggingface.co/OFA-Sys) ã«ã‚ˆã£\n ## ChineseCLIPConfig\n \n [[autodoc]] ChineseCLIPConfig\n-    - from_text_vision_configs\n \n ## ChineseCLIPTextConfig\n "
        },
        {
            "sha": "ee8befe3e28e858ed6d8fb7245ca0f1035bd7d35",
            "filename": "docs/source/ja/model_doc/clap.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fclap.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fclap.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fclap.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -33,7 +33,6 @@ CLAP (Contrastive Language-Audio Pretraining) ã¯ã€ã•ã¾ã–ã¾ãª (éŸ³å£°ã€\n ## ClapConfig\n \n [[autodoc]] ClapConfig\n-    - from_text_audio_configs\n \n ## ClapTextConfig\n "
        },
        {
            "sha": "3594fbd8b216ddc98863a9ea308c0f4ff7ee9d19",
            "filename": "docs/source/ja/model_doc/clip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fclip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -106,7 +106,6 @@ CLIP ã‚’ä½¿ã„å§‹ã‚ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹\n ## CLIPConfig\n \n [[autodoc]] CLIPConfig\n-    - from_text_vision_configs\n \n ## CLIPTextConfig\n "
        },
        {
            "sha": "3ad91c8d7ad474c7b733a20d60bf036bd2960aff",
            "filename": "docs/source/ja/model_doc/clipseg.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fclipseg.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fclipseg.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fclipseg.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -67,7 +67,6 @@ CLIPSeg ã®ä½¿ç”¨ã‚’é–‹å§‹ã™ã‚‹ã®ã«å½¹ç«‹ã¤ã€å…¬å¼ Hugging Face ãŠã‚ˆã³\n ## CLIPSegConfig\n \n [[autodoc]] CLIPSegConfig\n-    - from_text_vision_configs\n \n ## CLIPSegTextConfig\n "
        },
        {
            "sha": "654addc180ddcf8dc91a41403d1567223c672b82",
            "filename": "docs/source/ja/model_doc/clvp.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fclvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fja%2Fmodel_doc%2Fclvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fclvp.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -70,7 +70,6 @@ CLVP (Contrastive Language-Voice Pretrained Transformer) ãƒ¢ãƒ‡ãƒ«ã¯ã€James Be\n ## ClvpConfig\n \n [[autodoc]] ClvpConfig\n-    - from_sub_model_configs\n \n ## ClvpEncoderConfig\n "
        },
        {
            "sha": "117a18c76774a7478d331e028bd97dbb6fc0b372",
            "filename": "docs/source/ko/model_doc/altclip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Faltclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Faltclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Faltclip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -46,7 +46,6 @@ AltCLIPì€ ë©€í‹°ëª¨ë‹¬ ë¹„ì „ ë° ì–¸ì–´ ëª¨ë¸ìž…ë‹ˆë‹¤. ì´ë¯¸ì§€ì™€ í…ìŠ¤\n ## AltCLIPConfig\n \n [[autodoc]] AltCLIPConfig\n-    - from_text_vision_configs\n \n ## AltCLIPTextConfig\n "
        },
        {
            "sha": "648a20b16c959334ad24726d66ec092b75b28288",
            "filename": "docs/source/ko/model_doc/blip-2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fblip-2.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -46,7 +46,6 @@ BLIP-2ë¥¼ ì‹œìž‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê³µì‹ Hugging Face ë° ì»¤ë®¤ë‹ˆí‹°\n ## Blip2Config[[transformers.Blip2Config]]\n \n [[autodoc]] Blip2Config\n-    - from_vision_qformer_text_configs\n \n ## Blip2VisionConfig[[transformers.Blip2VisionConfig]]\n "
        },
        {
            "sha": "3342decf902e0e65ff0c7eba664860c71d2b9dde",
            "filename": "docs/source/ko/model_doc/blip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fblip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -42,7 +42,6 @@ BLIPì€ ì—¬ëŸ¬ ë©€í‹°ëª¨ë‹¬ ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆëŠ” ëª¨ë¸ìž…ë‹ˆë‹¤:\n ## BlipConfig[[transformers.BlipConfig]]\n \n [[autodoc]] BlipConfig\n-    - from_text_vision_configs\n \n ## BlipTextConfig[[transformers.BlipTextConfig]]\n "
        },
        {
            "sha": "62df5ed03bf8c4724294ffff70939d9e85e876ad",
            "filename": "docs/source/ko/model_doc/clip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -199,7 +199,6 @@ CLIPì„ ì‹œìž‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Hugging Faceì™€ community ìžë£Œ ëª©\n ## CLIPConfig[[transformers.CLIPConfig]]\n \n [[autodoc]] CLIPConfig\n-    - from_text_vision_configs\n \n ## CLIPTextConfig[[transformers.CLIPTextConfig]]\n "
        },
        {
            "sha": "12846635b05526f74437d8e7874e297a6608d5c1",
            "filename": "docs/source/ko/model_doc/clipseg.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fclipseg.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fclipseg.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fclipseg.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -55,7 +55,6 @@ CLIPSegë¥¼ ì‹œìž‘í•˜ëŠ” ë° ë„ì›€ì´ ë  Hugging Face ê³µì‹ ìžë£Œì™€ ì»¤ë®¤\n ## CLIPSegConfig[[transformers.CLIPSegConfig]]\n \n [[autodoc]] CLIPSegConfig\n-    - from_text_vision_configs\n \n ## CLIPSegTextConfig[[transformers.CLIPSegTextConfig]]\n "
        },
        {
            "sha": "f1221be16d9729d4c5107bd80bc19608c57b5ab1",
            "filename": "docs/source/ko/model_doc/siglip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fsiglip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -197,7 +197,6 @@ PyTorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ ìŠ¤ì¼€ì¼ëœ ì ê³± ì–´í…ì…˜(SDPA\n ## SiglipConfig\n \n [[autodoc]] SiglipConfig\n-    - from_text_vision_configs\n \n ## SiglipTextConfig\n "
        },
        {
            "sha": "52882b25c33cead044b27802e52d8feeed94166d",
            "filename": "docs/source/ko/model_doc/xclip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fxclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/docs%2Fsource%2Fko%2Fmodel_doc%2Fxclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fxclip.md?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -56,7 +56,6 @@ X-CLIPì„ ì‹œìž‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê³µì‹ Hugging Face ë° ì»¤ë®¤ë‹ˆí‹°\n ## XCLIPConfig[[xclipconfig]]\n \n [[autodoc]] XCLIPConfig\n-    - from_text_vision_configs\n \n ## XCLIPTextConfig[[xcliptextconfig]]\n "
        },
        {
            "sha": "7f186a32437b4123d1746a5a4086741bcf201fba",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 39,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -1131,11 +1131,11 @@ def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n         non_default_generation_parameters = {}\n         decoder_attribute_name = None\n \n-        # Composite models don't have a default config, use their decoder config as a fallback for default values\n+        # Some composite models don't have a default config, use their decoder config as a fallback for default values\n         # If no known pattern is matched, then `default_config = None` -> check against the global generation defaults\n-        try:\n+        if not self.has_no_defaults_at_init:\n             default_config = self.__class__()\n-        except ValueError:\n+        else:\n             decoder_config = self.get_text_config(decoder=True)\n             if decoder_config is not self:\n                 default_config = decoder_config.__class__()\n@@ -1246,42 +1246,6 @@ def get_text_config(self, decoder=None, encoder=None) -> \"PreTrainedConfig\":\n \n         return config_to_return\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config, vision_config, **kwargs):\n-        r\"\"\"\n-        Instantiate a model config (or a derived class) from text model configuration and vision model\n-        configuration.\n-\n-        Returns:\n-            [`PreTrainedConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        warnings.warn(\n-            \"The `from_text_vision_configs` method is deprecated and will be removed in v4.60 of Transformers. Please instantiate \"\n-            \"the config class directly with `MyConfig(text_config=text_config, vision_config=vision_config, **kwargs)` instead.\",\n-            FutureWarning,\n-        )\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n-    @classmethod\n-    def from_text_audio_configs(cls, text_config, audio_config, **kwargs):\n-        r\"\"\"\n-        Instantiate a model config (or a derived class) from text model configuration and audio model\n-        configuration.\n-\n-        Returns:\n-            [`PreTrainedConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        warnings.warn(\n-            \"The `from_text_audio_configs` method is deprecated and will be removed in v4.60 of Transformers. Please instantiate \"\n-            \"the config class directly with `MyConfig(text_config=text_config, audio_config=audio_config, **kwargs)` instead.\",\n-            FutureWarning,\n-        )\n-\n-        return cls(text_config=text_config.to_dict(), audio_config=audio_config.to_dict(), **kwargs)\n-\n \n def get_configuration_file(configuration_files: list[str]) -> str:\n     \"\"\""
        },
        {
            "sha": "49abf63e6c14f4b0e784cd536f18ddccea910033",
            "filename": "src/transformers/models/aimv2/configuration_aimv2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -264,21 +264,25 @@ class Aimv2Config(PreTrainedConfig):\n     def __init__(\n         self, text_config=None, vision_config=None, projection_dim=512, logit_scale_init_value=2.6592, **kwargs\n     ):\n-        super().__init__(**kwargs)\n-\n+        self.projection_dim = projection_dim\n+        self.logit_scale_init_value = logit_scale_init_value\n+        self.max_logit_scale = 100.0\n         if text_config is None:\n-            text_config = {}\n+            text_config = Aimv2TextConfig()\n             logger.info(\"`text_config` is `None`. Initializing the `Aimv2TextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = Aimv2TextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = Aimv2VisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `Aimv2VisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = Aimv2VisionConfig(**vision_config)\n \n-        self.text_config = Aimv2TextConfig(**text_config)\n-        self.vision_config = Aimv2VisionConfig(**vision_config)\n-        self.projection_dim = projection_dim\n-        self.logit_scale_init_value = logit_scale_init_value\n-        self.max_logit_scale = 100.0\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"Aimv2Config\", \"Aimv2VisionConfig\", \"Aimv2TextConfig\"]"
        },
        {
            "sha": "1320ac8c64aceb516bf9c75c452163281a5469e5",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -280,10 +280,10 @@ class Aimv2Config(SiglipConfig):\n     def __init__(\n         self, text_config=None, vision_config=None, projection_dim=512, logit_scale_init_value=2.6592, **kwargs\n     ):\n-        super().__init__(text_config, vision_config, **kwargs)\n         self.projection_dim = projection_dim\n         self.logit_scale_init_value = logit_scale_init_value\n         self.max_logit_scale = 100.0\n+        super().__init__(text_config, vision_config, **kwargs)\n \n         del self.initializer_factor\n "
        },
        {
            "sha": "570fb8346b3766b061a9be88e3c65c4dc1915a4a",
            "filename": "src/transformers/models/align/configuration_align.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -287,7 +287,7 @@ class AlignConfig(PreTrainedConfig):\n     >>> config_text = AlignTextConfig()\n     >>> config_vision = AlignVisionConfig()\n \n-    >>> config = AlignConfig.from_text_vision_configs(config_text, config_vision)\n+    >>> config = AlignConfig(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"align\"\n@@ -302,22 +302,25 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the AlignTextConfig with default values.\")\n+            text_config = AlignTextConfig()\n+            logger.info(\"`text_config` is `None`. Initializing the `AlignTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = AlignTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"vision_config is None. Initializing the AlignVisionConfig with default values.\")\n+            vision_config = AlignVisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `AlignVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = AlignVisionConfig(**vision_config)\n \n-        self.text_config = AlignTextConfig(**text_config)\n-        self.vision_config = AlignVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.temperature_init_value = temperature_init_value\n         self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"AlignTextConfig\", \"AlignVisionConfig\", \"AlignConfig\"]"
        },
        {
            "sha": "874befda5277b2710a51fca194ac328013f2caa8",
            "filename": "src/transformers/models/align/convert_align_tf_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Falign%2Fconvert_align_tf_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Falign%2Fconvert_align_tf_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconvert_align_tf_to_hf.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -56,9 +56,7 @@ def get_align_config():\n     vision_config.depthwise_padding = []\n \n     text_config = BertConfig()\n-    config = AlignConfig.from_text_vision_configs(\n-        text_config=text_config, vision_config=vision_config, projection_dim=640\n-    )\n+    config = AlignConfig(text_config=text_config, vision_config=vision_config, projection_dim=640)\n     return config\n \n "
        },
        {
            "sha": "7d950d0cf5de807bfe84678faff4182094d71663",
            "filename": "src/transformers/models/altclip/configuration_altclip.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -266,7 +266,7 @@ class AltCLIPConfig(PreTrainedConfig):\n     >>> config_text = AltCLIPTextConfig()\n     >>> config_vision = AltCLIPVisionConfig()\n \n-    >>> config = AltCLIPConfig.from_text_vision_configs(config_text, config_vision)\n+    >>> config = AltCLIPConfig(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"altclip\"\n@@ -281,8 +281,6 @@ def __init__(\n         text_config_dict = kwargs.pop(\"text_config_dict\", None)\n         vision_config_dict = kwargs.pop(\"vision_config_dict\", None)\n \n-        super().__init__(**kwargs)\n-\n         # Instead of simply assigning `[text|vision]_config_dict` to `[text|vision]_config`, we use the values in\n         # `[text|vision]_config_dict` to update the values in `[text|vision]_config`. The values should be same in most\n         # cases, but we don't want to break anything regarding `_config_dict` that existed before commit `8827e1b2`.\n@@ -346,19 +344,24 @@ def __init__(\n             vision_config.update(_vision_config_dict)\n \n         if text_config is None:\n-            text_config = {}\n+            text_config = AltCLIPTextConfig()\n             logger.info(\"`text_config` is `None`. Initializing the `AltCLIPTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = AltCLIPTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = AltCLIPVisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `AltCLIPVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = AltCLIPVisionConfig(**vision_config)\n \n-        self.text_config = AltCLIPTextConfig(**text_config)\n-        self.vision_config = AltCLIPVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = 1.0\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"AltCLIPTextConfig\", \"AltCLIPVisionConfig\", \"AltCLIPConfig\"]"
        },
        {
            "sha": "8aed0d4a812d15a37884d127709e1fd6512016c0",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -180,13 +180,10 @@ def __init__(\n         moe_num_shared_experts: int = 2,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n+        self.intermediate_size = intermediate_size\n+        self.moe_num_experts = moe_num_experts\n+        self.moe_topk = moe_topk\n+        self.moe_num_shared_experts = moe_num_shared_experts\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -215,9 +212,14 @@ def __init__(\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n-        self.moe_num_experts = moe_num_experts\n-        self.moe_topk = moe_topk\n-        self.moe_num_shared_experts = moe_num_shared_experts\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n class AriaConfig(PreTrainedConfig):"
        },
        {
            "sha": "4853ef361eb85d1d851248dd20bccb1187817a5b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -214,11 +214,11 @@ def __init__(\n         pad_token_id=2,\n         **super_kwargs,\n     ):\n-        super().__init__(pad_token_id=pad_token_id, **super_kwargs)\n         self.intermediate_size = intermediate_size\n         self.moe_num_experts = moe_num_experts\n         self.moe_topk = moe_topk\n         self.moe_num_shared_experts = moe_num_shared_experts\n+        super().__init__(pad_token_id=pad_token_id, **super_kwargs)\n \n \n class AriaConfig(PreTrainedConfig):"
        },
        {
            "sha": "7355356b90dbb4ccb45c3b4745515e86b99975e8",
            "filename": "src/transformers/models/bark/configuration_bark.py",
            "status": "modified",
            "additions": 24,
            "deletions": 37,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -221,7 +221,7 @@ class BarkConfig(PreTrainedConfig):\n \n \n     >>> # Initializing a Bark module style configuration\n-    >>> configuration = BarkConfig.from_sub_model_configs(\n+    >>> configuration = BarkConfig(\n     ...     semantic_config, coarse_acoustics_config, fine_acoustics_config, codec_config\n     ... )\n \n@@ -251,53 +251,40 @@ def __init__(\n         **kwargs,\n     ):\n         if semantic_config is None:\n-            semantic_config = {}\n-            logger.info(\"semantic_config is None. initializing the semantic model with default values.\")\n+            semantic_config = BarkSemanticConfig()\n+            logger.info(\"`semantic_config` is `None`. Initializing the `BarkSemanticConfig` with default values.\")\n+        elif isinstance(semantic_config, dict):\n+            semantic_config = BarkSemanticConfig(**semantic_config)\n \n         if coarse_acoustics_config is None:\n-            coarse_acoustics_config = {}\n-            logger.info(\"coarse_acoustics_config is None. initializing the coarse model with default values.\")\n+            coarse_acoustics_config = BarkCoarseConfig()\n+            logger.info(\n+                \"`coarse_acoustics_config` is `None`. Initializing the `BarkCoarseConfig` with default values.\"\n+            )\n+        elif isinstance(coarse_acoustics_config, dict):\n+            coarse_acoustics_config = BarkCoarseConfig(**coarse_acoustics_config)\n \n         if fine_acoustics_config is None:\n-            fine_acoustics_config = {}\n-            logger.info(\"fine_acoustics_config is None. initializing the fine model with default values.\")\n+            fine_acoustics_config = BarkFineConfig()\n+            logger.info(\"`fine_acoustics_config` is `None`. Initializing the `BarkFineConfig` with default values.\")\n+        elif isinstance(fine_acoustics_config, dict):\n+            fine_acoustics_config = BarkFineConfig(**fine_acoustics_config)\n \n         if codec_config is None:\n-            codec_config = {}\n-            logger.info(\"codec_config is None. initializing the codec model with default values.\")\n+            codec_config = CONFIG_MAPPING[\"encodec\"]()\n+            logger.info(\"`codec_config` is `None`. Initializing the `codec_config` with default values.\")\n+        elif isinstance(codec_config, dict):\n+            codec_model_type = codec_config.get(\"model_type\", \"encodec\")\n+            codec_config = CONFIG_MAPPING[codec_model_type](**codec_config)\n \n-        self.semantic_config = BarkSemanticConfig(**semantic_config)\n-        self.coarse_acoustics_config = BarkCoarseConfig(**coarse_acoustics_config)\n-        self.fine_acoustics_config = BarkFineConfig(**fine_acoustics_config)\n-        codec_model_type = codec_config.get(\"model_type\", \"encodec\")\n-        self.codec_config = CONFIG_MAPPING[codec_model_type](**codec_config)\n+        self.semantic_config = semantic_config\n+        self.coarse_acoustics_config = coarse_acoustics_config\n+        self.fine_acoustics_config = fine_acoustics_config\n+        self.codec_config = codec_config\n \n         self.initializer_range = initializer_range\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_sub_model_configs(\n-        cls,\n-        semantic_config: BarkSemanticConfig,\n-        coarse_acoustics_config: BarkCoarseConfig,\n-        fine_acoustics_config: BarkFineConfig,\n-        codec_config: PreTrainedConfig,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a [`BarkConfig`] (or a derived class) from bark sub-models configuration.\n-\n-        Returns:\n-            [`BarkConfig`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(\n-            semantic_config=semantic_config.to_dict(),\n-            coarse_acoustics_config=coarse_acoustics_config.to_dict(),\n-            fine_acoustics_config=fine_acoustics_config.to_dict(),\n-            codec_config=codec_config.to_dict(),\n-            **kwargs,\n-        )\n-\n \n __all__ = [\"BarkCoarseConfig\", \"BarkConfig\", \"BarkFineConfig\", \"BarkSemanticConfig\"]"
        },
        {
            "sha": "d1c2f85a3c7b6f2191bf522100560f200b7ddf8f",
            "filename": "src/transformers/models/bark/convert_suno_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -229,11 +229,9 @@ def load_whole_bark_model(\n     fineAcoustic = BarkFineModel.from_pretrained(fine_path)\n     codec = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n \n-    bark_config = BarkConfig.from_sub_model_configs(\n-        semanticConfig, coarseAcousticConfig, fineAcousticConfig, codecConfig\n-    )\n+    bark_config = BarkConfig(semanticConfig, coarseAcousticConfig, fineAcousticConfig, codecConfig)\n \n-    bark_generation_config = BarkGenerationConfig.from_sub_model_configs(\n+    bark_generation_config = BarkGenerationConfig(\n         semantic.generation_config, coarseAcoustic.generation_config, fineAcoustic.generation_config\n     )\n "
        },
        {
            "sha": "e03c0fe8dbd46461e18f228c07836d48cc67f7c1",
            "filename": "src/transformers/models/blip/configuration_blip.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -275,7 +275,7 @@ class BlipConfig(PreTrainedConfig):\n     >>> config_text = BlipTextConfig()\n     >>> config_vision = BlipVisionConfig()\n \n-    >>> config = BlipConfig.from_text_vision_configs(config_text, config_vision)\n+    >>> config = BlipConfig(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"blip\"\n@@ -291,18 +291,20 @@ def __init__(\n         label_smoothing=0.0,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n+            text_config = BlipTextConfig()\n             logger.info(\"`text_config` is `None`. Initializing the `BlipTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = BlipTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"`vision_config` is `None`. Initializing the `BlipVisionConfig` with default values.\")\n+            vision_config = BlipVisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `BlipVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = BlipVisionConfig(**vision_config)\n \n-        self.text_config = BlipTextConfig(**text_config)\n-        self.vision_config = BlipVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.text_config.encoder_hidden_size = self.vision_config.hidden_size\n \n@@ -312,6 +314,7 @@ def __init__(\n         self.initializer_range = 0.02\n         self.image_text_hidden_size = image_text_hidden_size\n         self.label_smoothing = label_smoothing\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"BlipConfig\", \"BlipTextConfig\", \"BlipVisionConfig\"]"
        },
        {
            "sha": "2694fdeb1085d1e61d9ea2f51f3d4317469e6efb",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 47,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -14,8 +14,6 @@\n # limitations under the License.\n \"\"\"BLIP-2 model configuration\"\"\"\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...utils import logging\n@@ -261,7 +259,7 @@ class Blip2Config(PreTrainedConfig):\n     >>> qformer_config = Blip2QFormerConfig()\n     >>> text_config = OPTConfig()\n \n-    >>> config = Blip2Config.from_text_vision_configs(vision_config, qformer_config, text_config)\n+    >>> config = Blip2Config(vision_config=vision_config, qformer_config=qformer_config, text_config=text_config)\n     ```\"\"\"\n \n     model_type = \"blip-2\"\n@@ -280,64 +278,39 @@ def __init__(\n         image_token_index=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n-        if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"vision_config is None. initializing the Blip2VisionConfig with default values.\")\n+        if text_config is None:\n+            text_config = CONFIG_MAPPING[\"opt\"]()\n+            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n+        elif isinstance(text_config, dict):\n+            text_model_type = text_config.get(\"model_type\", \"opt\")\n+            text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n         if qformer_config is None:\n-            qformer_config = {}\n+            qformer_config = Blip2QFormerConfig()\n             logger.info(\"qformer_config is None. Initializing the Blip2QFormerConfig with default values.\")\n+        elif isinstance(qformer_config, dict):\n+            qformer_config = Blip2QFormerConfig(**qformer_config)\n \n-        if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n+        if vision_config is None:\n+            vision_config = Blip2VisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `Blip2VisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = Blip2VisionConfig(**vision_config)\n \n-        self.vision_config = Blip2VisionConfig(**vision_config)\n-        self.qformer_config = Blip2QFormerConfig(**qformer_config)\n-        text_model_type = text_config.get(\"model_type\", \"opt\")\n-        self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.qformer_config = qformer_config\n \n         self.num_query_tokens = num_query_tokens\n         self.image_text_hidden_size = image_text_hidden_size\n         self.image_token_index = image_token_index\n         self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size\n         self.use_decoder_only_language_model = self.text_config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n-        self.is_encoder_decoder = self.text_config.is_encoder_decoder\n         self.initializer_factor = 1.0\n         self.initializer_range = 0.02\n \n-    @classmethod\n-    def from_vision_qformer_text_configs(\n-        cls,\n-        vision_config: Blip2VisionConfig,\n-        qformer_config: Blip2QFormerConfig,\n-        text_config: Optional[PreTrainedConfig] = None,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a [`Blip2Config`] (or a derived class) from a BLIP-2 vision model, Q-Former and language model\n-        configurations.\n-\n-        Args:\n-            vision_config (`dict`):\n-                Dictionary of configuration options used to initialize [`Blip2VisionConfig`].\n-            qformer_config (`dict`):\n-                Dictionary of configuration options used to initialize [`Blip2QFormerConfig`].\n-            text_config (`dict`, *optional*):\n-                Dictionary of configuration options used to initialize any [`PreTrainedConfig`].\n-\n-        Returns:\n-            [`Blip2Config`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(\n-            vision_config=vision_config.to_dict(),\n-            qformer_config=qformer_config.to_dict(),\n-            text_config=text_config.to_dict() if text_config is not None else None,\n-            **kwargs,\n-        )\n+        kwargs[\"is_encoder_decoder\"] = self.text_config.is_encoder_decoder\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"Blip2Config\", \"Blip2QFormerConfig\", \"Blip2VisionConfig\"]"
        },
        {
            "sha": "7a0dcf754711a96525e8987d736251fbade25364",
            "filename": "src/transformers/models/bridgetower/configuration_bridgetower.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -272,7 +272,6 @@ def __init__(\n         _ = kwargs.pop(\"text_config_dict\", None)\n         _ = kwargs.pop(\"vision_config_dict\", None)\n \n-        super().__init__(**kwargs)\n         self.share_cross_modal_transformer_layers = share_cross_modal_transformer_layers\n         self.hidden_act = hidden_act\n         self.hidden_size = hidden_size\n@@ -286,15 +285,20 @@ def __init__(\n         self.init_layernorm_from_vision_encoder = init_layernorm_from_vision_encoder\n \n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `BridgeTowerTextConfig` with default values.\")\n+            text_config = BridgeTowerTextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `BridgeTowerTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = BridgeTowerTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"`vision_config` is `None`. Initializing the `BridgeTowerVisionConfig` with default values.\")\n+            vision_config = BridgeTowerVisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `BridgeTowerVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = BridgeTowerVisionConfig(**vision_config)\n \n-        self.text_config = BridgeTowerTextConfig(**text_config)\n-        self.vision_config = BridgeTowerVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"BridgeTowerConfig\", \"BridgeTowerTextConfig\", \"BridgeTowerVisionConfig\"]"
        },
        {
            "sha": "72f37d5f58a8aa6c5d28dd7af26f95a55bd8ba2b",
            "filename": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -269,7 +269,7 @@ class ChineseCLIPConfig(PreTrainedConfig):\n     >>> config_text = ChineseCLIPTextConfig()\n     >>> config_vision = ChineseCLIPVisionConfig()\n \n-    >>> config = ChineseCLIPConfig.from_text_vision_configs(config_text, config_vision)\n+    >>> config = ChineseCLIPConfig(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"chinese_clip\"\n@@ -284,8 +284,6 @@ def __init__(\n         text_config_dict = kwargs.pop(\"text_config_dict\", None)\n         vision_config_dict = kwargs.pop(\"vision_config_dict\", None)\n \n-        super().__init__(**kwargs)\n-\n         # Instead of simply assigning `[text|vision]_config_dict` to `[text|vision]_config`, we use the values in\n         # `[text|vision]_config_dict` to update the values in `[text|vision]_config`. The values should be same in most\n         # cases, but we don't want to break anything regarding `_config_dict` that existed before commit `8827e1b2`.\n@@ -349,20 +347,25 @@ def __init__(\n             vision_config.update(_vision_config_dict)\n \n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `ChineseCLIPTextConfig` with default values.\")\n+            text_config = ChineseCLIPTextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `ChineseCLIPTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = ChineseCLIPTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = ChineseCLIPVisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `ChineseCLIPVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = ChineseCLIPVisionConfig(**vision_config)\n \n-        self.text_config = ChineseCLIPTextConfig(**text_config)\n-        self.vision_config = ChineseCLIPVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = 1.0\n         self.initializer_range = 0.02\n+        super().__init__(**kwargs)\n \n \n class ChineseCLIPOnnxConfig(OnnxConfig):"
        },
        {
            "sha": "0e45b6e4e244662e57db614e50fc33d28df7a47c",
            "filename": "src/transformers/models/clap/configuration_clap.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -328,7 +328,7 @@ class ClapConfig(PreTrainedConfig):\n     >>> config_text = ClapTextConfig()\n     >>> config_audio = ClapAudioConfig()\n \n-    >>> config = ClapConfig.from_text_audio_configs(config_text, config_audio)\n+    >>> config = ClapConfig(text_config=config_text, audio_config=config_audio)\n     ```\"\"\"\n \n     model_type = \"clap\"\n@@ -344,18 +344,21 @@ def __init__(\n         initializer_factor=1.0,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the ClapTextConfig with default values.\")\n+            text_config = ClapTextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `ClapTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = ClapTextConfig(**text_config)\n \n         if audio_config is None:\n-            audio_config = {}\n-            logger.info(\"audio_config is None. initializing the ClapAudioConfig with default values.\")\n+            audio_config = ClapAudioConfig()\n+            logger.info(\"`audio_config` is `None`. initializing the `ClapAudioConfig` with default values.\")\n+        elif isinstance(audio_config, dict):\n+            audio_config = ClapAudioConfig(**audio_config)\n+\n+        self.text_config = text_config\n+        self.audio_config = audio_config\n \n-        self.text_config = ClapTextConfig(**text_config)\n-        self.audio_config = ClapAudioConfig(**audio_config)\n         self.text_config.projection_dim = projection_dim\n         self.audio_config.projection_dim = projection_dim\n \n@@ -369,6 +372,7 @@ def __init__(\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = initializer_factor\n         self.num_hidden_layers = self.text_config.num_hidden_layers + len(self.audio_config.depths)\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"ClapAudioConfig\", \"ClapConfig\", \"ClapTextConfig\"]"
        },
        {
            "sha": "3f4681482cddebb0b5ec3b01d40cfa36ae17b4ea",
            "filename": "src/transformers/models/clip/configuration_clip.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -266,7 +266,7 @@ class CLIPConfig(PreTrainedConfig):\n     >>> config_text = CLIPTextConfig()\n     >>> config_vision = CLIPVisionConfig()\n \n-    >>> config = CLIPConfig.from_text_vision_configs(config_text, config_vision)\n+    >>> config = CLIPConfig(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"clip\"\n@@ -281,8 +281,6 @@ def __init__(\n         text_config_dict = kwargs.pop(\"text_config_dict\", None)\n         vision_config_dict = kwargs.pop(\"vision_config_dict\", None)\n \n-        super().__init__(**kwargs)\n-\n         # Instead of simply assigning `[text|vision]_config_dict` to `[text|vision]_config`, we use the values in\n         # `[text|vision]_config_dict` to update the values in `[text|vision]_config`. The values should be same in most\n         # cases, but we don't want to break anything regarding `_config_dict` that existed before commit `8827e1b2`.\n@@ -346,19 +344,24 @@ def __init__(\n             vision_config.update(_vision_config_dict)\n \n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `CLIPTextConfig` with default values.\")\n+            text_config = CLIPTextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `CLIPTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = CLIPTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = CLIPVisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `CLIPVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = CLIPVisionConfig(**vision_config)\n \n-        self.text_config = CLIPTextConfig(**text_config)\n-        self.vision_config = CLIPVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = 1.0\n+        super().__init__(**kwargs)\n \n \n class CLIPOnnxConfig(OnnxConfig):"
        },
        {
            "sha": "e8f0057912a43b581e4b912737dcd3dde8a029a7",
            "filename": "src/transformers/models/clipseg/configuration_clipseg.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -265,7 +265,7 @@ class CLIPSegConfig(PreTrainedConfig):\n     >>> config_text = CLIPSegTextConfig()\n     >>> config_vision = CLIPSegVisionConfig()\n \n-    >>> config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)\n+    >>> config = CLIPSegConfig(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"clipseg\"\n@@ -293,8 +293,6 @@ def __init__(\n         text_config_dict = kwargs.pop(\"text_config_dict\", None)\n         vision_config_dict = kwargs.pop(\"vision_config_dict\", None)\n \n-        super().__init__(**kwargs)\n-\n         # Instead of simply assigning `[text|vision]_config_dict` to `[text|vision]_config`, we use the values in\n         # `[text|vision]_config_dict` to update the values in `[text|vision]_config`. The values should be same in most\n         # cases, but we don't want to break anything regarding `_config_dict` that existed before commit `8827e1b2`.\n@@ -358,15 +356,19 @@ def __init__(\n             vision_config.update(_vision_config_dict)\n \n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `CLIPSegTextConfig` with default values.\")\n+            text_config = CLIPSegTextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `CLIPSegTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = CLIPSegTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = CLIPSegVisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `CLIPSegVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = CLIPSegVisionConfig(**vision_config)\n \n-        self.text_config = CLIPSegTextConfig(**text_config)\n-        self.vision_config = CLIPSegVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.logit_scale_init_value = logit_scale_init_value\n@@ -379,6 +381,7 @@ def __init__(\n         self.conditional_layer = conditional_layer\n         self.initializer_factor = 1.0\n         self.use_complex_transposed_convolution = use_complex_transposed_convolution\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"CLIPSegConfig\", \"CLIPSegTextConfig\", \"CLIPSegVisionConfig\"]"
        },
        {
            "sha": "01f6d8e472fa70b2802ba441d4db4c93184922dc",
            "filename": "src/transformers/models/clipseg/convert_clipseg_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -39,9 +39,9 @@ def get_clipseg_config(model_name):\n     use_complex_transposed_convolution = \"refined\" in model_name\n     reduce_dim = 16 if \"rd16\" in model_name else 64\n \n-    config = CLIPSegConfig.from_text_vision_configs(\n-        text_config,\n-        vision_config,\n+    config = CLIPSegConfig(\n+        text_config=text_config,\n+        vision_config=vision_config,\n         use_complex_transposed_convolution=use_complex_transposed_convolution,\n         reduce_dim=reduce_dim,\n     )"
        },
        {
            "sha": "a3b5633fb58826a843122a58a64c54a58beb860d",
            "filename": "src/transformers/models/clvp/configuration_clvp.py",
            "status": "modified",
            "additions": 16,
            "deletions": 42,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -362,7 +362,7 @@ class ClvpConfig(PreTrainedConfig):\n     >>> config_speech = ClvpEncoderConfig()\n     >>> decoder_config = ClvpDecoderConfig()\n \n-    >>> config = ClvpConfig.from_sub_model_configs(config_text, config_speech, decoder_config)\n+    >>> config = ClvpConfig(config_text, config_speech, decoder_config)\n     ```\"\"\"\n \n     model_type = \"clvp\"\n@@ -382,58 +382,32 @@ def __init__(\n         initializer_factor=1.0,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `ClvpEncoderConfig` with default values.\")\n+            text_config = ClvpEncoderConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `ClvpEncoderConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = ClvpEncoderConfig(**text_config)\n \n         if speech_config is None:\n-            speech_config = {}\n+            speech_config = ClvpEncoderConfig()\n             logger.info(\"`speech_config` is `None`. initializing the `ClvpEncoderConfig` with default values.\")\n+        elif isinstance(speech_config, dict):\n+            speech_config = ClvpEncoderConfig(**speech_config)\n \n         if decoder_config is None:\n-            decoder_config = {}\n-            logger.info(\"`decoder_config` is `None`. initializing the `ClvpDecoderConfig` with default values.\")\n+            decoder_config = ClvpDecoderConfig()\n+            logger.info(\"`image_config` is `None`. initializing the `ClvpDecoderConfig` with default values.\")\n+        elif isinstance(decoder_config, dict):\n+            decoder_config = ClvpDecoderConfig(**decoder_config)\n \n-        self.text_config = ClvpEncoderConfig(**text_config)\n-        self.speech_config = ClvpEncoderConfig(**speech_config)\n-        self.decoder_config = ClvpDecoderConfig(**decoder_config)\n+        self.text_config = text_config\n+        self.speech_config = speech_config\n+        self.decoder_config = decoder_config\n \n         self.projection_dim = projection_dim\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = initializer_factor\n-\n-    @classmethod\n-    def from_sub_model_configs(\n-        cls,\n-        text_config: ClvpEncoderConfig,\n-        speech_config: ClvpEncoderConfig,\n-        decoder_config: ClvpDecoderConfig,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a [`ClvpConfig`] (or a derived class) from CLVP text model configuration, CLVP speech model\n-        configuration and CLVP decoder model configuration.\n-\n-        Args:\n-            text_config (`ClvpEncoderConfig`):\n-                Text model configuration of type [`ClvpEncoderConfig`].\n-            speech_config (`ClvpEncoderConfig`):\n-                Speech model configuration of type [`ClvpEncoderConfig`].\n-            decoder_config (`ClvpDecoderConfig`):\n-                Decoder model configuration of type [`ClvpDecoderConfig`].\n-\n-        Returns:\n-            [`ClvpConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(\n-            text_config=text_config.to_dict(),\n-            speech_config=speech_config.to_dict(),\n-            decoder_config=decoder_config.to_dict(),\n-            **kwargs,\n-        )\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"ClvpConfig\", \"ClvpDecoderConfig\", \"ClvpEncoderConfig\"]"
        },
        {
            "sha": "911a25266105b03f6fa286643a56d124f655d138",
            "filename": "src/transformers/models/cohere2_vision/configuration_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -51,7 +51,6 @@ def __init__(\n         alignment_intermediate_size=36864,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         self.downsample_factor = downsample_factor\n         self.image_token_id = image_token_id\n         self.alignment_intermediate_size = alignment_intermediate_size\n@@ -77,6 +76,7 @@ def __init__(\n             text_config = CONFIG_MAPPING[\"cohere2\"](tie_word_embeddings=True)\n \n         self.text_config = text_config\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"Cohere2VisionConfig\"]"
        },
        {
            "sha": "fa0198ef4962a0c20125b42e3f084348b5cfb83f",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -73,9 +73,9 @@ def __init__(\n         visual_prompt_prefix: str = \"Describe the image.\",\n         query_prefix: str = \"Question: \",\n     ):\n-        super().__init__(image_processor=image_processor, tokenizer=tokenizer, chat_template=chat_template)\n         self.visual_prompt_prefix = visual_prompt_prefix\n         self.query_prefix = query_prefix\n+        super().__init__(image_processor=image_processor, tokenizer=tokenizer, chat_template=chat_template)\n \n     @property\n     def query_augmentation_token(self) -> str:"
        },
        {
            "sha": "cd33607a35fd290853a159d1c2a96da62084916f",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -105,7 +105,8 @@ def __init__(\n         visual_prompt_prefix: str = \"Describe the image.\",\n         query_prefix: str = \"Question: \",\n     ):\n-        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+        self.visual_prompt_prefix = visual_prompt_prefix\n+        self.query_prefix = query_prefix\n         if not hasattr(image_processor, \"image_seq_length\"):\n             raise ValueError(\"Image processor is missing an `image_seq_length` attribute.\")\n \n@@ -124,8 +125,8 @@ def __init__(\n         tokenizer.add_tokens(EXTRA_TOKENS)\n         tokenizer.add_bos_token = False\n         tokenizer.add_eos_token = False\n-        self.visual_prompt_prefix = visual_prompt_prefix\n-        self.query_prefix = query_prefix\n+\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__(\n         self,"
        },
        {
            "sha": "8604951436c2e914addaaefb57ae0536a941a77f",
            "filename": "src/transformers/models/csm/configuration_csm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -373,14 +373,6 @@ def __init__(\n         if kwargs.pop(\"tie_word_embeddings\", False):\n             raise ValueError(\"`tie_word_embeddings=True` is not supported for CsmConfig\")\n \n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=False,\n-            **kwargs,\n-        )\n-\n         if depth_decoder_config is None:\n             self.depth_decoder_config = CsmDepthDecoderConfig()\n             logger.info(\"depth_decoder_config is None, using default depth decoder config.\")\n@@ -433,6 +425,14 @@ def __init__(\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n \n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=False,\n+            **kwargs,\n+        )\n+\n \n __all__ = [\n     \"CsmDepthDecoderConfig\","
        },
        {
            "sha": "722888d5022fa8c65c499edd2717cca7ac020d2d",
            "filename": "src/transformers/models/d_fine/configuration_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -397,22 +397,5 @@ def __init__(\n             )\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @classmethod\n-    def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n-        \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n-        configuration.\n-\n-            Args:\n-                backbone_config ([`PreTrainedConfig`]):\n-                    The backbone configuration.\n-\n-            Returns:\n-                [`DFineConfig`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(\n-            backbone_config=backbone_config,\n-            **kwargs,\n-        )\n-\n \n __all__ = [\"DFineConfig\"]"
        },
        {
            "sha": "01d59e238acb4f6928e2834df871b3ed7d43961b",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -416,23 +416,6 @@ def __init__(\n             )\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @classmethod\n-    def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n-        \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n-        configuration.\n-\n-            Args:\n-                backbone_config ([`PreTrainedConfig`]):\n-                    The backbone configuration.\n-\n-            Returns:\n-                [`DFineConfig`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(\n-            backbone_config=backbone_config,\n-            **kwargs,\n-        )\n-\n \n class DFineMultiscaleDeformableAttention(nn.Module):\n     def __init__(self, config: DFineConfig):"
        },
        {
            "sha": "1d803891d12820bbd70975ede3286badef617329",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 23,
            "deletions": 21,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -174,13 +174,20 @@ def __init__(\n         moe_intermediate_size=1407,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n+        self.first_k_dense_replace = first_k_dense_replace\n+        self.kv_lora_rank = kv_lora_rank\n+        self.q_lora_rank = q_lora_rank\n+        self.n_group = n_group\n+        self.n_routed_experts = n_routed_experts\n+        self.n_shared_experts = n_shared_experts\n+        self.qk_nope_head_dim = qk_nope_head_dim\n+        self.qk_rope_head_dim = qk_rope_head_dim\n+        self.routed_scaling_factor = routed_scaling_factor\n+        self.topk_group = topk_group\n+        self.topk_method = topk_method\n+        self.v_head_dim = v_head_dim\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.moe_intermediate_size = moe_intermediate_size\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -202,26 +209,21 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n+\n         self.head_dim = qk_rope_head_dim\n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, copy it it to 'rope_type'.\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n-        self.first_k_dense_replace = first_k_dense_replace\n-        self.kv_lora_rank = kv_lora_rank\n-        self.q_lora_rank = q_lora_rank\n-        self.n_group = n_group\n-        self.n_routed_experts = n_routed_experts\n-        self.n_shared_experts = n_shared_experts\n-        self.qk_nope_head_dim = qk_nope_head_dim\n-        self.qk_rope_head_dim = qk_rope_head_dim\n-        self.routed_scaling_factor = routed_scaling_factor\n-        self.topk_group = topk_group\n-        self.topk_method = topk_method\n-        self.v_head_dim = v_head_dim\n-        self.num_experts_per_tok = num_experts_per_tok\n-        self.moe_intermediate_size = moe_intermediate_size\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"DeepseekV2Config\"]"
        },
        {
            "sha": "8bfe11345393064c088346271e15b231ba929c49",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -188,9 +188,6 @@ def __init__(\n         moe_intermediate_size=1407,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n-        del self.pretraining_tp\n         self.first_k_dense_replace = first_k_dense_replace\n         self.kv_lora_rank = kv_lora_rank\n         self.q_lora_rank = q_lora_rank\n@@ -205,7 +202,11 @@ def __init__(\n         self.v_head_dim = v_head_dim\n         self.num_experts_per_tok = num_experts_per_tok\n         self.moe_intermediate_size = moe_intermediate_size\n+\n+        super().__init__(**kwargs)\n+\n         self.head_dim = qk_rope_head_dim\n+        del self.pretraining_tp\n \n \n def apply_rotary_emb("
        },
        {
            "sha": "64dcf84d4a35c16b04412f35905f5c3f47d4ce3f",
            "filename": "src/transformers/models/deepseek_vl/configuration_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -71,8 +71,6 @@ def __init__(\n         image_token_id: int = 100015,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n             text_config = {}\n             logger.info(\"`text_config` is `None`. Initializing the `LlamaConfig` with default values.\")\n@@ -92,6 +90,7 @@ def __init__(\n         self.text_config = text_config\n         self.vision_config = vision_config\n         self.image_token_id = image_token_id\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"DeepseekVLConfig\"]"
        },
        {
            "sha": "36c5aee2569f1bd8188c41142043d9d257aaa813",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -82,8 +82,6 @@ def __init__(\n         image_token_id: int = 100015,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n             text_config = {}\n             logger.info(\"`text_config` is `None`. Initializing the `LlamaConfig` with default values.\")\n@@ -103,6 +101,7 @@ def __init__(\n         self.text_config = text_config\n         self.vision_config = vision_config\n         self.image_token_id = image_token_id\n+        super().__init__(**kwargs)\n \n \n class DeepseekVLBaseModelOutputWithPast(IdeficsBaseModelOutputWithPast):"
        },
        {
            "sha": "cbda990152e72e9162a9ef92a434d69f9edfa3ea",
            "filename": "src/transformers/models/deepseek_vl_hybrid/configuration_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 9,
            "deletions": 11,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -74,8 +74,15 @@ def __init__(\n         image_token_id: int = 100015,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n+        if high_res_vision_config is None:\n+            high_res_vision_config = {}\n+            logger.info(\"`high_res_vision_config` is `None`. Initializing the `SamVisionConfig` with default values.\")\n+\n+        if isinstance(high_res_vision_config, dict):\n+            high_res_vision_config[\"model_type\"] = high_res_vision_config.get(\"model_type\", \"sam_vision_model\")\n+            high_res_vision_config = CONFIG_MAPPING[high_res_vision_config[\"model_type\"]](**high_res_vision_config)\n \n+        self.high_res_vision_config = high_res_vision_config\n         if text_config is None:\n             text_config = {}\n             logger.info(\"`text_config` is `None`. Initializing the `LlamaConfig` with default values.\")\n@@ -95,16 +102,7 @@ def __init__(\n         self.text_config = text_config\n         self.vision_config = vision_config\n         self.image_token_id = image_token_id\n-\n-        if high_res_vision_config is None:\n-            high_res_vision_config = {}\n-            logger.info(\"`high_res_vision_config` is `None`. Initializing the `SamVisionConfig` with default values.\")\n-\n-        if isinstance(high_res_vision_config, dict):\n-            high_res_vision_config[\"model_type\"] = high_res_vision_config.get(\"model_type\", \"sam_vision_model\")\n-            high_res_vision_config = CONFIG_MAPPING[high_res_vision_config[\"model_type\"]](**high_res_vision_config)\n-\n-        self.high_res_vision_config = high_res_vision_config\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"DeepseekVLHybridConfig\"]"
        },
        {
            "sha": "27062cfd06b2a4fab62fffc319ccddd9e7eb5cfb",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -125,13 +125,6 @@ def __init__(\n         image_token_id: int = 100015,\n         **kwargs,\n     ):\n-        super().__init__(\n-            text_config=text_config,\n-            vision_config=vision_config,\n-            image_token_id=image_token_id,\n-            **kwargs,\n-        )\n-\n         if high_res_vision_config is None:\n             high_res_vision_config = {}\n             logger.info(\"`high_res_vision_config` is `None`. Initializing the `SamVisionConfig` with default values.\")\n@@ -142,6 +135,13 @@ def __init__(\n \n         self.high_res_vision_config = high_res_vision_config\n \n+        super().__init__(\n+            text_config=text_config,\n+            vision_config=vision_config,\n+            image_token_id=image_token_id,\n+            **kwargs,\n+        )\n+\n \n class DeepseekVLHybridBaseModelOutputWithPast(IdeficsBaseModelOutputWithPast):\n     pass"
        },
        {
            "sha": "a4ce05d67d3ca3fdf8c59cfdbc0624f26ffdcd54",
            "filename": "src/transformers/models/deprecated/jukebox/configuration_jukebox.py",
            "status": "modified",
            "additions": 11,
            "deletions": 21,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -559,14 +559,16 @@ def __init__(\n         **kwargs,\n     ):\n         if vqvae_config is None:\n-            vqvae_config = {}\n+            vqvae_config = JukeboxVQVAEConfig()\n             logger.info(\"vqvae_config is None. initializing the JukeboxVQVAE with default values.\")\n-\n-        self.vqvae_config = JukeboxVQVAEConfig(**vqvae_config)\n-        if prior_config_list is not None:\n-            self.prior_configs = [JukeboxPriorConfig(**prior_config) for prior_config in prior_config_list]\n-        else:\n-            self.prior_configs = []\n+        elif isinstance(vqvae_config, dict):\n+            vqvae_config = JukeboxVQVAEConfig(**vqvae_config)\n+        self.vqvae_config = vqvae_config\n+\n+        if prior_config_list is not None and isinstance(prior_config_list[0], dict):\n+            prior_configs = [JukeboxPriorConfig(**prior_config) for prior_config in prior_config_list]\n+        elif prior_config_list is None:\n+            prior_configs = []\n             for prior_idx in range(nb_priors):\n                 prior_config = kwargs.pop(f\"prior_{prior_idx}\", None)\n                 if prior_config is None:\n@@ -575,10 +577,10 @@ def __init__(\n                         f\"prior_{prior_idx}'s  config is None. Initializing the JukeboxPriorConfig list with default\"\n                         \" values.\"\n                     )\n-                self.prior_configs.append(JukeboxPriorConfig(**prior_config))\n+                prior_configs.append(JukeboxPriorConfig(**prior_config))\n+        self.prior_configs = prior_configs\n \n         self.hop_fraction = self.vqvae_config.hop_fraction\n-\n         self.nb_priors = nb_priors\n \n         # Metadata conditioning\n@@ -591,18 +593,6 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_configs(cls, prior_configs: list[JukeboxPriorConfig], vqvae_config: JukeboxVQVAEConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`JukeboxConfig`] (or a derived class) from clip text model configuration and clip vision model\n-        configuration.\n-\n-        Returns:\n-            [`JukeboxConfig`]: An instance of a configuration object\n-        \"\"\"\n-        prior_config_list = [config.to_dict() for config in prior_configs]\n-        return cls(prior_config_list=prior_config_list, vqvae_config_dict=vqvae_config.to_dict(), **kwargs)\n-\n     def to_dict(self):\n         # Override the default to_dict to apply to_dict to the list of prior configs.\n         result = super().to_dict()"
        },
        {
            "sha": "4ffd9c4c58bcd1bd34e47fcd39b30c91093b2fef",
            "filename": "src/transformers/models/depth_anything/configuration_depth_anything.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -108,7 +108,6 @@ def __init__(\n         max_depth=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if backbone_config is None and backbone is None:\n             logger.info(\"`backbone_config` is `None`. Initializing the config with the default `Dinov2` backbone.\")\n             backbone_config = CONFIG_MAPPING[\"dinov2\"](\n@@ -150,5 +149,7 @@ def __init__(\n         self.depth_estimation_type = depth_estimation_type\n         self.max_depth = max_depth if max_depth else 1\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"DepthAnythingConfig\"]"
        },
        {
            "sha": "0740ec1769b59e883fdcf3c0b83098d9d40a30f4",
            "filename": "src/transformers/models/depth_pro/configuration_depth_pro.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -109,8 +109,6 @@ def __init__(\n         fov_model_config=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         # scaled_images_ratios is sorted\n         if scaled_images_ratios != sorted(scaled_images_ratios):\n             raise ValueError(\n@@ -200,5 +198,7 @@ def __init__(\n \n             setattr(self, sub_config_key, sub_config)\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"DepthProConfig\"]"
        },
        {
            "sha": "5536734d2cb0f259e79fb76640ff09c89d20d562",
            "filename": "src/transformers/models/detr/configuration_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -245,18 +245,6 @@ def __init__(\n         self.eos_coefficient = eos_coefficient\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @classmethod\n-    def from_backbone_config(cls, backbone_config: PreTrainedConfig, **kwargs):\n-        \"\"\"Instantiate a [`DetrConfig`] (or a derived class) from a pre-trained backbone model configuration.\n-\n-        Args:\n-            backbone_config ([`PreTrainedConfig`]):\n-                The backbone configuration.\n-        Returns:\n-            [`DetrConfig`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(backbone_config=backbone_config, **kwargs)\n-\n \n class DetrOnnxConfig(OnnxConfig):\n     torch_onnx_minimum_version = version.parse(\"1.11\")"
        },
        {
            "sha": "509c214645989bcd5f4f628a4bb2ae85d93f310d",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -180,8 +180,6 @@ def __init__(\n         pooler_act=\"tanh\",\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         self.hidden_size = hidden_size\n         self.is_hybrid = is_hybrid\n \n@@ -273,6 +271,7 @@ def __init__(\n         self.semantic_classifier_dropout = semantic_classifier_dropout\n         self.pooler_output_size = pooler_output_size if pooler_output_size else hidden_size\n         self.pooler_act = pooler_act\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"DPTConfig\"]"
        },
        {
            "sha": "2c4ef6e1d4336afdfe33ef2af21e1eaa42436387",
            "filename": "src/transformers/models/edgetam/configuration_edgetam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -84,8 +84,6 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         backbone_channel_list = [384, 192, 96, 48] if backbone_channel_list is None else backbone_channel_list\n         backbone_feature_sizes = (\n             [[256, 256], [128, 128], [64, 64]] if backbone_feature_sizes is None else backbone_feature_sizes\n@@ -118,6 +116,7 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.layer_norm_eps = layer_norm_eps\n         self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n \n \n class EdgeTamPromptEncoderConfig(PreTrainedConfig):\n@@ -309,7 +308,6 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         vision_config = vision_config if vision_config is not None else {}\n         prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n         mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n@@ -327,6 +325,7 @@ def __init__(\n         self.mask_decoder_config = EdgeTamMaskDecoderConfig(**mask_decoder_config)\n \n         self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"EdgeTamConfig\", \"EdgeTamVisionConfig\", \"EdgeTamPromptEncoderConfig\", \"EdgeTamMaskDecoderConfig\"]"
        },
        {
            "sha": "b32d71ec0a5aef6f3c5a062318e0682dd93c1d5d",
            "filename": "src/transformers/models/edgetam/modular_edgetam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -107,8 +107,6 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         backbone_channel_list = [384, 192, 96, 48] if backbone_channel_list is None else backbone_channel_list\n         backbone_feature_sizes = (\n             [[256, 256], [128, 128], [64, 64]] if backbone_feature_sizes is None else backbone_feature_sizes\n@@ -141,6 +139,7 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.layer_norm_eps = layer_norm_eps\n         self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n \n \n class EdgeTamPromptEncoderConfig(Sam2PromptEncoderConfig):"
        },
        {
            "sha": "face8009a6868bccf96f2cf55384a8531912d4c2",
            "filename": "src/transformers/models/falcon_mamba/configuration_falcon_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -133,7 +133,6 @@ def __init__(\n         mixer_rms_eps=1e-6,\n         **kwargs,\n     ):\n-        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, pad_token_id=pad_token_id, **kwargs)\n         self.vocab_size = vocab_size\n         self.hidden_size = hidden_size\n         self.state_size = state_size\n@@ -164,6 +163,8 @@ def __init__(\n         self.residual_in_fp32 = residual_in_fp32\n         self.use_cache = use_cache\n         self.use_falcon_mambapy = use_falcon_mambapy\n+\n+        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, pad_token_id=pad_token_id, **kwargs)\n         self.mixer_rms_eps = mixer_rms_eps\n \n "
        },
        {
            "sha": "b3818246dce01d353f0e4091efa3a7eaa9be1f39",
            "filename": "src/transformers/models/fastspeech2_conformer/convert_model_with_hifigan.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_model_with_hifigan.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_model_with_hifigan.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconvert_model_with_hifigan.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -63,7 +63,7 @@ def convert_FastSpeech2ConformerWithHifiGan_checkpoint(\n     load_weights(espnet_checkpoint, vocoder, vocoder_config)\n \n     # Prepare the model + vocoder\n-    config = FastSpeech2ConformerWithHifiGanConfig.from_sub_model_configs(model_config, vocoder_config)\n+    config = FastSpeech2ConformerWithHifiGanConfig(model_config, vocoder_config)\n     with_hifigan_model = FastSpeech2ConformerWithHifiGan(config)\n     with_hifigan_model.model = model\n     with_hifigan_model.vocoder = vocoder"
        },
        {
            "sha": "75dcae413c212c8cbf7aa87b23c04f0690e14645",
            "filename": "src/transformers/models/flava/configuration_flava.py",
            "status": "modified",
            "additions": 24,
            "deletions": 43,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -494,8 +494,6 @@ def __init__(\n         multimodal_config_dict = kwargs.pop(\"multimodal_config_dict\", None)\n         image_codebook_config_dict = kwargs.pop(\"image_codebook_config_dict\", None)\n \n-        super().__init__(**kwargs)\n-\n         # Instead of simply assigning `[text|vision]_config_dict` to `[text|vision]_config`, we use the values in\n         # `[text|vision]_config_dict` to update the values in `[text|vision]_config`. The values should be same in most\n         # cases, but we don't want to break anything regarding `_config_dict` that existed before commit `8827e1b2`.\n@@ -619,28 +617,35 @@ def __init__(\n             # Update all values in `image_codebook_config` with the ones in `_image_codebook_config_dict`.\n             image_codebook_config.update(_image_codebook_config_dict)\n \n+        if text_config is None:\n+            text_config = FlavaTextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `FlavaTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = FlavaTextConfig(**text_config)\n+\n         if image_config is None:\n-            image_config = {}\n+            image_config = FlavaImageConfig()\n             logger.info(\"`image_config` is `None`. initializing the `FlavaImageConfig` with default values.\")\n-\n-        if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `FlavaTextConfig` with default values.\")\n+        elif isinstance(image_config, dict):\n+            image_config = FlavaImageConfig(**image_config)\n \n         if multimodal_config is None:\n-            multimodal_config = {}\n-            logger.info(\"`multimodal_config` is `None`. initializing the `FlavaMultimodalConfig` with default values.\")\n+            multimodal_config = FlavaMultimodalConfig()\n+            logger.info(\"`image_config` is `None`. initializing the `FlavaMultimodalConfig` with default values.\")\n+        elif isinstance(multimodal_config, dict):\n+            multimodal_config = FlavaMultimodalConfig(**multimodal_config)\n \n         if image_codebook_config is None:\n-            image_codebook_config = {}\n-            logger.info(\n-                \"`image_codebook_config` is `None`. initializing the `FlavaImageCodebookConfig` with default values.\"\n-            )\n-\n-        self.image_config = FlavaImageConfig(**image_config)\n-        self.text_config = FlavaTextConfig(**text_config)\n-        self.multimodal_config = FlavaMultimodalConfig(**multimodal_config)\n-        self.image_codebook_config = FlavaImageCodebookConfig(**image_codebook_config)\n+            image_codebook_config = FlavaImageCodebookConfig()\n+            logger.info(\"`image_config` is `None`. initializing the `FlavaImageCodebookConfig` with default values.\")\n+        elif isinstance(image_codebook_config, dict):\n+            image_codebook_config = FlavaImageCodebookConfig(**image_codebook_config)\n+\n+        self.text_config = text_config\n+        self.image_config = image_config\n+        self.multimodal_config = multimodal_config\n+        self.image_codebook_config = image_codebook_config\n+\n         self.projection_dim = projection_dim\n         self.init_codebook = init_codebook\n \n@@ -659,31 +664,7 @@ def __init__(\n         self.global_backprop_contrastive = global_backprop_contrastive\n         self.skip_unmasked_multimodal_encoder = skip_unmasked_multimodal_encoder\n         self.return_loss = return_loss\n-\n-    @classmethod\n-    def from_configs(\n-        cls,\n-        image_config: FlavaImageConfig,\n-        text_config: FlavaTextConfig,\n-        multimodal_config: FlavaMultimodalConfig,\n-        image_codebook_config: FlavaImageCodebookConfig,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a [`FlavaConfig`] (or a derived class) from flava text model configuration, flava image model\n-        configuration, flava multimodal model and flava codebook model configuration.\n-\n-        Returns:\n-            [`FlavaConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(\n-            image_config=image_config.to_dict(),\n-            text_config=text_config.to_dict(),\n-            multimodal_config=multimodal_config.to_dict(),\n-            image_codebook_config=image_codebook_config.to_dict(),\n-            **kwargs,\n-        )\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"FlavaConfig\", \"FlavaImageCodebookConfig\", \"FlavaImageConfig\", \"FlavaMultimodalConfig\", \"FlavaTextConfig\"]"
        },
        {
            "sha": "e3b36bab6128481bc715d3256a7c79a5369d269d",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -499,15 +499,18 @@ def __init__(\n         model_args: Optional[dict] = None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         self.architecture = architecture\n         self.initializer_range = initializer_range\n         self.do_pooling = do_pooling\n-        self.model_args = model_args  # named \"model_args\" for BC with timm\n         self.hidden_size = hidden_size\n         self.vocab_size = vocab_size\n         self.vocab_offset = vocab_offset\n         self.rms_norm_eps = rms_norm_eps\n+        self.architecture = architecture\n+        self.initializer_range = initializer_range\n+        self.do_pooling = do_pooling\n+        self.model_args = model_args  # named \"model_args\" for BC with timm\n+        super().__init__(**kwargs)\n \n     @classmethod\n     def from_dict(cls, config_dict: dict[str, Any], **kwargs):"
        },
        {
            "sha": "edf5fd4e3db4d4ee1fd903c57a5b43c2294ad44f",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -511,14 +511,14 @@ def __init__(\n         model_args: Optional[dict] = None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         self.architecture = architecture\n         self.initializer_range = initializer_range\n         self.do_pooling = do_pooling\n         self.hidden_size = hidden_size\n         self.vocab_size = vocab_size\n         self.vocab_offset = vocab_offset\n         self.rms_norm_eps = rms_norm_eps\n+        super().__init__(**kwargs)\n \n \n class Gemma3nConfig(PreTrainedConfig):"
        },
        {
            "sha": "f22fd8e8d54750e573321e86bc6933c052daf1e3",
            "filename": "src/transformers/models/git/configuration_git.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -185,8 +185,6 @@ def __init__(\n         num_image_with_embedding=None,\n         **kwargs,\n     ):\n-        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, pad_token_id=pad_token_id, **kwargs)\n-\n         if vision_config is None:\n             vision_config = {}\n             logger.info(\"vision_config is None. initializing the GitVisionConfig with default values.\")\n@@ -204,11 +202,15 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n         self.use_cache = use_cache\n-        self.tie_word_embeddings = tie_word_embeddings\n         self.num_image_with_embedding = num_image_with_embedding\n \n-        self.bos_token_id = bos_token_id\n-        self.eos_token_id = eos_token_id\n+        super().__init__(\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            pad_token_id=pad_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"GitConfig\", \"GitVisionConfig\"]"
        },
        {
            "sha": "e8f9c948c66df6bcb2ea372f362dd8265b3a58b8",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -66,7 +66,6 @@ class Glm4vProcessor(ProcessorMixin):\n     tokenizer_class = (\"PreTrainedTokenizer\", \"PreTrainedTokenizerFast\")\n \n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n-        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n         self.image_token = \"<|image|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n         self.image_token_id = (\n@@ -79,6 +78,7 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n             if getattr(tokenizer, \"video_token_id\", None)\n             else tokenizer.convert_tokens_to_ids(self.video_token)\n         )\n+        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n \n     def __call__(\n         self,"
        },
        {
            "sha": "1c495b403d48d889e9c2bcce8d3eb3bc0014df7e",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -365,7 +365,6 @@ def __init__(\n         video_end_token_id=151342,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if isinstance(vision_config, dict):\n             self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n         elif vision_config is None:\n@@ -383,5 +382,7 @@ def __init__(\n         self.image_start_token_id = image_start_token_id\n         self.image_end_token_id = image_end_token_id\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"Glm4vMoeConfig\", \"Glm4vMoeTextConfig\"]"
        },
        {
            "sha": "45adf3606578cb9652c79e097ae5682e8a3e547d",
            "filename": "src/transformers/models/groupvit/configuration_groupvit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -274,8 +274,6 @@ def __init__(\n         text_config_dict = kwargs.pop(\"text_config_dict\", None)\n         vision_config_dict = kwargs.pop(\"vision_config_dict\", None)\n \n-        super().__init__(**kwargs)\n-\n         # Instead of simply assigning `[text|vision]_config_dict` to `[text|vision]_config`, we use the values in\n         # `[text|vision]_config_dict` to update the values in `[text|vision]_config`. The values should be same in most\n         # cases, but we don't want to break anything regarding `_config_dict` that existed before commit `8827e1b2`.\n@@ -339,22 +337,27 @@ def __init__(\n             vision_config.update(_vision_config_dict)\n \n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `GroupViTTextConfig` with default values.\")\n+            text_config = GroupViTTextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `GroupViTTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = GroupViTTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = GroupViTVisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `GroupViTVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = GroupViTVisionConfig(**vision_config)\n \n-        self.text_config = GroupViTTextConfig(**text_config)\n-        self.vision_config = GroupViTVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.projection_intermediate_dim = projection_intermediate_dim\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_range = 0.02\n         self.initializer_factor = 1.0\n         self.output_segmentation = False\n+        super().__init__(**kwargs)\n \n \n class GroupViTOnnxConfig(OnnxConfig):"
        },
        {
            "sha": "c8ba3203ac6f8e3e236240e5b06e489abb2af7c4",
            "filename": "src/transformers/models/instructblip/configuration_instructblip.py",
            "status": "modified",
            "additions": 19,
            "deletions": 37,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -256,7 +256,7 @@ class InstructBlipConfig(PreTrainedConfig):\n     >>> qformer_config = InstructBlipQFormerConfig()\n     >>> text_config = OPTConfig()\n \n-    >>> config = InstructBlipConfig.from_text_vision_configs(vision_config, qformer_config, text_config)\n+    >>> config = InstructBlipConfig(vision_config=vision_config, qformer_config=qformer_config, text_config=text_config)\n     ```\"\"\"\n \n     model_type = \"instructblip\"\n@@ -278,54 +278,36 @@ def __init__(\n         image_token_index=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n-        if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"vision_config is None. initializing the InstructBlipVisionConfig with default values.\")\n+        if text_config is None:\n+            text_config = CONFIG_MAPPING[\"opt\"]()\n+            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n+        elif isinstance(text_config, dict):\n+            text_model_type = text_config.get(\"model_type\", \"opt\")\n+            text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n         if qformer_config is None:\n-            qformer_config = {}\n+            qformer_config = InstructBlipQFormerConfig()\n             logger.info(\"qformer_config is None. Initializing the InstructBlipQFormerConfig with default values.\")\n+        elif isinstance(qformer_config, dict):\n+            qformer_config = InstructBlipQFormerConfig(**qformer_config)\n \n-        if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n+        if vision_config is None:\n+            vision_config = InstructBlipVisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `InstructBlipVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = InstructBlipVisionConfig(**vision_config)\n \n-        self.vision_config = InstructBlipVisionConfig(**vision_config)\n-        self.qformer_config = InstructBlipQFormerConfig(**qformer_config)\n-        text_model_type = text_config.get(\"model_type\", \"opt\")\n-        self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.qformer_config = qformer_config\n \n         self.num_query_tokens = num_query_tokens\n         self.image_token_index = image_token_index\n         self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size\n         self.use_decoder_only_language_model = self.text_config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n         self.initializer_factor = 1.0\n         self.initializer_range = 0.02\n-\n-    @classmethod\n-    def from_vision_qformer_text_configs(\n-        cls,\n-        vision_config: InstructBlipVisionConfig,\n-        qformer_config: InstructBlipQFormerConfig,\n-        text_config: PreTrainedConfig,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a [`InstructBlipConfig`] (or a derived class) from a InstructBLIP vision model, Q-Former and\n-        language model configurations.\n-\n-        Returns:\n-            [`InstructBlipConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(\n-            vision_config=vision_config.to_dict(),\n-            qformer_config=qformer_config.to_dict(),\n-            text_config=text_config.to_dict(),\n-            **kwargs,\n-        )\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"InstructBlipConfig\", \"InstructBlipQFormerConfig\", \"InstructBlipVisionConfig\"]"
        },
        {
            "sha": "f39d23f23860f7ceb7a425e2b949da7768eaa431",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 21,
            "deletions": 37,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -262,7 +262,7 @@ class InstructBlipVideoConfig(PreTrainedConfig):\n     >>> qformer_config = InstructBlipVideoQFormerConfig()\n     >>> text_config = OPTConfig()\n \n-    >>> config = InstructBlipVideoConfig.from_text_vision_configs(vision_config, qformer_config, text_config)\n+    >>> config = InstructBlipVideoConfig(vision_config=vision_config, qformer_config=qformer_config, text_config=text_config)\n     ```\"\"\"\n \n     model_type = \"instructblipvideo\"\n@@ -284,54 +284,38 @@ def __init__(\n         video_token_index=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n-        if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"vision_config is None. initializing the InstructBlipVideoVisionConfig with default values.\")\n+        if text_config is None:\n+            text_config = CONFIG_MAPPING[\"opt\"]()\n+            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n+        elif isinstance(text_config, dict):\n+            text_model_type = text_config.get(\"model_type\", \"opt\")\n+            text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n         if qformer_config is None:\n-            qformer_config = {}\n+            qformer_config = InstructBlipVideoQFormerConfig()\n             logger.info(\"qformer_config is None. Initializing the InstructBlipVideoQFormerConfig with default values.\")\n+        elif isinstance(qformer_config, dict):\n+            qformer_config = InstructBlipVideoQFormerConfig(**qformer_config)\n \n-        if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n+        if vision_config is None:\n+            vision_config = InstructBlipVideoVisionConfig()\n+            logger.info(\n+                \"`vision_config` is `None`. initializing the `InstructBlipVideoVisionConfig` with default values.\"\n+            )\n+        elif isinstance(vision_config, dict):\n+            vision_config = InstructBlipVideoVisionConfig(**vision_config)\n \n-        self.vision_config = InstructBlipVideoVisionConfig(**vision_config)\n-        self.qformer_config = InstructBlipVideoQFormerConfig(**qformer_config)\n-        text_model_type = text_config.get(\"model_type\", \"opt\")\n-        self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.qformer_config = qformer_config\n \n         self.num_query_tokens = num_query_tokens\n         self.video_token_index = video_token_index\n         self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size\n         self.use_decoder_only_language_model = self.text_config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n         self.initializer_factor = 1.0\n         self.initializer_range = 0.02\n-\n-    @classmethod\n-    def from_vision_qformer_text_configs(\n-        cls,\n-        vision_config: InstructBlipVideoVisionConfig,\n-        qformer_config: InstructBlipVideoQFormerConfig,\n-        text_config: PreTrainedConfig,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a [`InstructBlipVideoConfig`] (or a derived class) from a InstructBlipVideo vision model, Q-Former and\n-        language model configurations.\n-\n-        Returns:\n-            [`InstructBlipVideoConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(\n-            vision_config=vision_config.to_dict(),\n-            qformer_config=qformer_config.to_dict(),\n-            text_config=text_config.to_dict(),\n-            **kwargs,\n-        )\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"InstructBlipVideoConfig\", \"InstructBlipVideoQFormerConfig\", \"InstructBlipVideoVisionConfig\"]"
        },
        {
            "sha": "f03df415e4677ab28ee967a445f57c0528af7555",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 21,
            "deletions": 37,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -103,7 +103,7 @@ class InstructBlipVideoConfig(PreTrainedConfig):\n     >>> qformer_config = InstructBlipVideoQFormerConfig()\n     >>> text_config = OPTConfig()\n \n-    >>> config = InstructBlipVideoConfig.from_text_vision_configs(vision_config, qformer_config, text_config)\n+    >>> config = InstructBlipVideoConfig(vision_config=vision_config, qformer_config=qformer_config, text_config=text_config)\n     ```\"\"\"\n \n     model_type = \"instructblipvideo\"\n@@ -125,54 +125,38 @@ def __init__(\n         video_token_index=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n-        if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"vision_config is None. initializing the InstructBlipVideoVisionConfig with default values.\")\n+        if text_config is None:\n+            text_config = CONFIG_MAPPING[\"opt\"]()\n+            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n+        elif isinstance(text_config, dict):\n+            text_model_type = text_config.get(\"model_type\", \"opt\")\n+            text_config = CONFIG_MAPPING[text_model_type](**text_config)\n \n         if qformer_config is None:\n-            qformer_config = {}\n+            qformer_config = InstructBlipVideoQFormerConfig()\n             logger.info(\"qformer_config is None. Initializing the InstructBlipVideoQFormerConfig with default values.\")\n+        elif isinstance(qformer_config, dict):\n+            qformer_config = InstructBlipVideoQFormerConfig(**qformer_config)\n \n-        if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the text config with default values (`OPTConfig`).\")\n+        if vision_config is None:\n+            vision_config = InstructBlipVideoVisionConfig()\n+            logger.info(\n+                \"`vision_config` is `None`. initializing the `InstructBlipVideoVisionConfig` with default values.\"\n+            )\n+        elif isinstance(vision_config, dict):\n+            vision_config = InstructBlipVideoVisionConfig(**vision_config)\n \n-        self.vision_config = InstructBlipVideoVisionConfig(**vision_config)\n-        self.qformer_config = InstructBlipVideoQFormerConfig(**qformer_config)\n-        text_model_type = text_config.get(\"model_type\", \"opt\")\n-        self.text_config = CONFIG_MAPPING[text_model_type](**text_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.qformer_config = qformer_config\n \n         self.num_query_tokens = num_query_tokens\n         self.video_token_index = video_token_index\n         self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size\n         self.use_decoder_only_language_model = self.text_config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n         self.initializer_factor = 1.0\n         self.initializer_range = 0.02\n-\n-    @classmethod\n-    def from_vision_qformer_text_configs(\n-        cls,\n-        vision_config: InstructBlipVideoVisionConfig,\n-        qformer_config: InstructBlipVideoQFormerConfig,\n-        text_config: PreTrainedConfig,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a [`InstructBlipVideoConfig`] (or a derived class) from a InstructBlipVideo vision model, Q-Former and\n-        language model configurations.\n-\n-        Returns:\n-            [`InstructBlipVideoConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(\n-            vision_config=vision_config.to_dict(),\n-            qformer_config=qformer_config.to_dict(),\n-            text_config=text_config.to_dict(),\n-            **kwargs,\n-        )\n+        super().__init__(**kwargs)\n \n \n class InstructBlipVideoPreTrainedModel(InstructBlipPreTrainedModel):"
        },
        {
            "sha": "b8e032786bf4bf7b623a4898a10d8dcbcaaad63e",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -54,11 +54,11 @@ class JanusImageProcessorFast(BaseImageProcessorFast):\n     valid_kwargs = JanusImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[JanusImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n         if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:\n             background_color = tuple(int(x * 255) for x in kwargs.get(\"image_mean\"))\n-        super().__init__(**kwargs)\n         self.background_color = tuple(background_color)\n \n     def resize("
        },
        {
            "sha": "1b9dff5aabf9942e85de406932c62991b634f1be",
            "filename": "src/transformers/models/kosmos2/configuration_kosmos2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fconfiguration_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fconfiguration_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fconfiguration_kosmos2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -245,20 +245,22 @@ def __init__(\n         latent_query_num=64,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `Kosmos2TextConfig` with default values.\")\n+            text_config = Kosmos2TextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `Kosmos2TextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = Kosmos2TextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"`vision_config` is `None`. Initializing the `Kosmos2VisionConfig` with default values.\")\n-\n-        self.text_config = Kosmos2TextConfig(**text_config)\n-        self.vision_config = Kosmos2VisionConfig(**vision_config)\n+            vision_config = Kosmos2VisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `Kosmos2VisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = Kosmos2VisionConfig(**vision_config)\n \n+        self.text_config = text_config\n+        self.vision_config = vision_config\n         self.latent_query_num = latent_query_num\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"Kosmos2Config\"]"
        },
        {
            "sha": "b366194194963f9734f998840a543dacb596aa0b",
            "filename": "src/transformers/models/kosmos2_5/configuration_kosmos2_5.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fconfiguration_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fconfiguration_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fconfiguration_kosmos2_5.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -237,18 +237,22 @@ def __init__(\n         latent_query_num=2048,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the Kosmos2_5TextConfig with default values.\")\n-        if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"vision_config is None. Initializing the Kosmos2_5VisionConfig with default values.\")\n+            text_config = Kosmos2_5TextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `Kosmos2_5TextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = Kosmos2_5TextConfig(**text_config)\n \n-        self.text_config = Kosmos2_5TextConfig(**text_config)\n-        self.vision_config = Kosmos2_5VisionConfig(**vision_config)\n+        if vision_config is None:\n+            vision_config = Kosmos2_5VisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `Kosmos2_5VisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = Kosmos2_5VisionConfig(**vision_config)\n \n+        self.text_config = text_config\n+        self.vision_config = vision_config\n         self.latent_query_num = latent_query_num\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"Kosmos2_5Config\"]"
        },
        {
            "sha": "6618b8573aade673b68a4f34b1295684dd7a79dd",
            "filename": "src/transformers/models/kyutai_speech_to_text/configuration_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -142,10 +142,6 @@ def __init__(\n         codec_config=None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id, bos_token_id=bos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n-        )\n-\n         if codec_config is None:\n             self.codec_config = AutoConfig.for_model(\"mimi\")\n             logger.info(\"codec_config is None, using default audio encoder config.\")\n@@ -184,5 +180,9 @@ def __init__(\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n         self.sliding_window = sliding_window\n \n+        super().__init__(\n+            pad_token_id=pad_token_id, bos_token_id=bos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n+        )\n+\n \n __all__ = [\"KyutaiSpeechToTextConfig\"]"
        },
        {
            "sha": "0b39d0a01a43283dba3345f24aebf2cd04e7fbc8",
            "filename": "src/transformers/models/mask2former/configuration_mask2former.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -237,21 +237,5 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_backbone_config(cls, backbone_config: PreTrainedConfig, **kwargs):\n-        \"\"\"Instantiate a [`Mask2FormerConfig`] (or a derived class) from a pre-trained backbone model configuration.\n-\n-        Args:\n-            backbone_config ([`PreTrainedConfig`]):\n-                The backbone configuration.\n-\n-        Returns:\n-            [`Mask2FormerConfig`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(\n-            backbone_config=backbone_config,\n-            **kwargs,\n-        )\n-\n \n __all__ = [\"Mask2FormerConfig\"]"
        },
        {
            "sha": "cd6d08a7c0032aa60e5288e503172be174d07ced",
            "filename": "src/transformers/models/maskformer/configuration_maskformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -201,27 +201,5 @@ def __init__(\n         self.backbone_kwargs = backbone_kwargs\n         super().__init__(**kwargs)\n \n-    @classmethod\n-    def from_backbone_and_decoder_configs(\n-        cls, backbone_config: PreTrainedConfig, decoder_config: PreTrainedConfig, **kwargs\n-    ):\n-        \"\"\"Instantiate a [`MaskFormerConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n-        configuration.\n-\n-            Args:\n-                backbone_config ([`PreTrainedConfig`]):\n-                    The backbone configuration.\n-                decoder_config ([`PreTrainedConfig`]):\n-                    The transformer decoder configuration to use.\n-\n-            Returns:\n-                [`MaskFormerConfig`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(\n-            backbone_config=backbone_config,\n-            decoder_config=decoder_config,\n-            **kwargs,\n-        )\n-\n \n __all__ = [\"MaskFormerConfig\"]"
        },
        {
            "sha": "1bf50e8bd1c187e741a8de9731659c40b9cd65f0",
            "filename": "src/transformers/models/metaclip_2/configuration_metaclip_2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -248,7 +248,7 @@ class MetaClip2Config(PreTrainedConfig):\n     >>> config_text = MetaClip2TextConfig()\n     >>> config_vision = MetaClip2VisionConfig()\n \n-    >>> config = MetaClip2Config.from_text_vision_configs(config_text, config_vision)\n+    >>> config = MetaClip2Config(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"metaclip_2\"\n@@ -263,8 +263,6 @@ def __init__(\n         text_config_dict = kwargs.pop(\"text_config_dict\", None)\n         vision_config_dict = kwargs.pop(\"vision_config_dict\", None)\n \n-        super().__init__(**kwargs)\n-\n         # Instead of simply assigning `[text|vision]_config_dict` to `[text|vision]_config`, we use the values in\n         # `[text|vision]_config_dict` to update the values in `[text|vision]_config`. The values should be same in most\n         # cases, but we don't want to break anything regarding `_config_dict` that existed before commit `8827e1b2`.\n@@ -328,19 +326,24 @@ def __init__(\n             vision_config.update(_vision_config_dict)\n \n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `MetaClip2TextConfig` with default values.\")\n+            text_config = MetaClip2TextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `MetaClip2TextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = MetaClip2TextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = MetaClip2VisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `MetaClip2VisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = MetaClip2VisionConfig(**vision_config)\n \n-        self.text_config = MetaClip2TextConfig(**text_config)\n-        self.vision_config = MetaClip2VisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = 1.0\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"MetaClip2Config\", \"MetaClip2TextConfig\", \"MetaClip2VisionConfig\"]"
        },
        {
            "sha": "98252c7fbc680f4f392aa8188a57584c8b689193",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -197,7 +197,7 @@ class MetaClip2Config(CLIPConfig):\n     >>> config_text = MetaClip2TextConfig()\n     >>> config_vision = MetaClip2VisionConfig()\n \n-    >>> config = MetaClip2Config.from_text_vision_configs(config_text, config_vision)\n+    >>> config = MetaClip2Config(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     pass"
        },
        {
            "sha": "d12264e2ae4958b1fd8ff4a33f1cfd0512ba891b",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 15,
            "deletions": 16,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -181,13 +181,14 @@ def __init__(\n         mlp_beta_factor=1,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n+        self.layer_types = layer_types\n+        self.block_size = block_size\n+        self.full_attn_alpha_factor = full_attn_alpha_factor\n+        self.full_attn_beta_factor = full_attn_beta_factor\n+        self.linear_attn_alpha_factor = linear_attn_alpha_factor\n+        self.linear_attn_beta_factor = linear_attn_beta_factor\n+        self.mlp_alpha_factor = mlp_alpha_factor\n+        self.mlp_beta_factor = mlp_beta_factor\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -214,15 +215,13 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.router_jitter_noise = router_jitter_noise\n-        self.layer_types = layer_types\n-        self.block_size = block_size\n-        self.full_attn_alpha_factor = full_attn_alpha_factor\n-        self.full_attn_beta_factor = full_attn_beta_factor\n-        self.linear_attn_alpha_factor = linear_attn_alpha_factor\n-        self.linear_attn_beta_factor = linear_attn_beta_factor\n-        self.mlp_alpha_factor = mlp_alpha_factor\n-        self.mlp_beta_factor = mlp_beta_factor\n-\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n         if self.layer_types is None:\n             self.layer_types = [\n                 \"full_attention\" if bool((i + 1) % 2) else \"linear_attention\" for i in range(self.num_hidden_layers)"
        },
        {
            "sha": "4afe2b57bf8351f4b0cd120dd0c9fbc0986ea399",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -163,7 +163,6 @@ def __init__(\n         mlp_beta_factor=1,\n         **super_kwargs,\n     ):\n-        super().__init__(**super_kwargs)\n         self.layer_types = layer_types\n         self.block_size = block_size\n         self.full_attn_alpha_factor = full_attn_alpha_factor\n@@ -173,6 +172,7 @@ def __init__(\n         self.mlp_alpha_factor = mlp_alpha_factor\n         self.mlp_beta_factor = mlp_beta_factor\n \n+        super().__init__(**super_kwargs)\n         if self.layer_types is None:\n             self.layer_types = [\n                 \"full_attention\" if bool((i + 1) % 2) else \"linear_attention\" for i in range(self.num_hidden_layers)"
        },
        {
            "sha": "7a257591b514b52734ddee926b60c60b1fb8246a",
            "filename": "src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -198,7 +198,6 @@ def __init__(\n         layer_norm_eps=1e-5,\n         **kwargs,\n     ):\n-        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n         if backbone_config is None and backbone is None:\n             logger.info(\"`backbone_config` is `None`. Initializing the config with the default `Swin` backbone.\")\n             backbone_config = CONFIG_MAPPING[\"swin\"](\n@@ -281,5 +280,7 @@ def __init__(\n         self.init_std = init_std\n         self.layer_norm_eps = layer_norm_eps\n \n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+\n \n __all__ = [\"MMGroundingDinoConfig\"]"
        },
        {
            "sha": "4aed0c1a9b64befb1da99237d539e6c43cff4a80",
            "filename": "src/transformers/models/mm_grounding_dino/modular_mm_grounding_dino.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -20,9 +20,8 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n from ..auto.modeling_auto import AutoModel\n-from ..grounding_dino.configuration_grounding_dino import GroundingDinoConfig\n from ..grounding_dino.modeling_grounding_dino import (\n     GroundingDinoContrastiveEmbedding,\n     GroundingDinoConvEncoder,\n@@ -40,7 +39,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class MMGroundingDinoConfig(GroundingDinoConfig, PreTrainedConfig):\n+class MMGroundingDinoConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`MMGroundingDinoModel`]. It is used to instantiate a\n     MM Grounding DINO model according to the specified arguments, defining the model architecture. Instantiating a\n@@ -158,6 +157,11 @@ class MMGroundingDinoConfig(GroundingDinoConfig, PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"mm-grounding-dino\"\n+    sub_configs = {\"backbone_config\": AutoConfig, \"text_config\": AutoConfig}\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"encoder_attention_heads\",\n+    }\n \n     def __init__(\n         self,\n@@ -205,7 +209,6 @@ def __init__(\n         layer_norm_eps=1e-5,\n         **kwargs,\n     ):\n-        PreTrainedConfig.__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n         if backbone_config is None and backbone is None:\n             logger.info(\"`backbone_config` is `None`. Initializing the config with the default `Swin` backbone.\")\n             backbone_config = CONFIG_MAPPING[\"swin\"](\n@@ -288,6 +291,8 @@ def __init__(\n         self.init_std = init_std\n         self.layer_norm_eps = layer_norm_eps\n \n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+\n \n class MMGroundingDinoContrastiveEmbedding(GroundingDinoContrastiveEmbedding):\n     def __init__(self, config):"
        },
        {
            "sha": "76c951668f46198d884f14bfcf1784ed619d4714",
            "filename": "src/transformers/models/musicgen/configuration_musicgen.py",
            "status": "modified",
            "additions": 24,
            "deletions": 47,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconfiguration_musicgen.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -142,15 +142,12 @@ class MusicgenConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        kwargs (*optional*):\n-            Dictionary of keyword arguments. Notably:\n-\n-                - **text_encoder** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that\n-                  defines the text encoder config.\n-                - **audio_encoder** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that\n-                  defines the audio encoder config.\n-                - **decoder** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that defines\n-                  the decoder config.\n+        text_encoder (`Union[dict, `PretrainedConfig`]`):\n+            An instance of a configuration object that defines the text encoder config.\n+        audio_encoder (`Union[dict, `PretrainedConfig`]`):\n+            An instance of a configuration object that defines the audio encoder config.\n+        decoder (`Union[dict, `PretrainedConfig`]`):\n+            An instance of a configuration object that defines the decoder config.\n \n     Example:\n \n@@ -168,8 +165,10 @@ class MusicgenConfig(PreTrainedConfig):\n     >>> audio_encoder_config = EncodecConfig()\n     >>> decoder_config = MusicgenDecoderConfig()\n \n-    >>> configuration = MusicgenConfig.from_sub_models_config(\n-    ...     text_encoder_config, audio_encoder_config, decoder_config\n+    >>> configuration = MusicgenConfig(\n+    ...     text_encoder=text_encoder_config,\n+    ...     audio_encoder=audio_encoder_config,\n+    ...     decoder=decoder_config,\n     ... )\n \n     >>> # Initializing a MusicgenForConditionalGeneration (with random weights) from the facebook/musicgen-small style configuration\n@@ -197,47 +196,25 @@ class MusicgenConfig(PreTrainedConfig):\n     }\n     has_no_defaults_at_init = True\n \n-    def __init__(self, **kwargs):\n-        super().__init__(**kwargs)\n-        if \"text_encoder\" not in kwargs or \"audio_encoder\" not in kwargs or \"decoder\" not in kwargs:\n-            raise ValueError(\"Config has to be initialized with text_encoder, audio_encoder and decoder config\")\n-\n-        text_encoder_config = kwargs.pop(\"text_encoder\")\n-        text_encoder_model_type = text_encoder_config.pop(\"model_type\")\n+    def __init__(self, text_encoder, audio_encoder, decoder, **kwargs):\n+        if isinstance(text_encoder, dict):\n+            text_encoder_model_type = text_encoder.pop(\"model_type\")\n+            text_encoder = AutoConfig.for_model(text_encoder_model_type, **text_encoder)\n \n-        audio_encoder_config = kwargs.pop(\"audio_encoder\")\n-        audio_encoder_model_type = audio_encoder_config.pop(\"model_type\")\n+        if isinstance(audio_encoder, dict):\n+            audio_encoder_model_type = audio_encoder.pop(\"model_type\")\n+            audio_encoder = AutoConfig.for_model(audio_encoder_model_type, **audio_encoder)\n \n-        decoder_config = kwargs.pop(\"decoder\")\n+        if isinstance(decoder, dict):\n+            decoder = MusicgenDecoderConfig(**decoder)\n \n-        self.text_encoder = AutoConfig.for_model(text_encoder_model_type, **text_encoder_config)\n-        self.audio_encoder = AutoConfig.for_model(audio_encoder_model_type, **audio_encoder_config)\n-        self.decoder = MusicgenDecoderConfig(**decoder_config)\n-        self.is_encoder_decoder = True\n+        self.text_encoder = text_encoder\n+        self.audio_encoder = audio_encoder\n+        self.decoder = decoder\n         self.initializer_factor = self.decoder.initializer_factor\n \n-    @classmethod\n-    def from_sub_models_config(\n-        cls,\n-        text_encoder_config: PreTrainedConfig,\n-        audio_encoder_config: PreTrainedConfig,\n-        decoder_config: MusicgenDecoderConfig,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a [`MusicgenConfig`] (or a derived class) from text encoder, audio encoder and decoder\n-        configurations.\n-\n-        Returns:\n-            [`MusicgenConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(\n-            text_encoder=text_encoder_config.to_dict(),\n-            audio_encoder=audio_encoder_config.to_dict(),\n-            decoder=decoder_config.to_dict(),\n-            **kwargs,\n-        )\n+        kwargs[\"is_encoder_decoder\"] = True\n+        super().__init__(**kwargs)\n \n     @property\n     # This is a property because you might want to change the codec model on the fly"
        },
        {
            "sha": "77c2a4ee1fb3a4bf8d09cc731618a4afc46b28c2",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -1308,7 +1308,9 @@ def __init__(\n                 \"Either a configuration has to be provided, or all three of text encoder, audio encoder and MusicGen decoder.\"\n             )\n         if config is None:\n-            config = MusicgenConfig.from_sub_models_config(text_encoder.config, audio_encoder.config, decoder.config)\n+            config = MusicgenConfig(\n+                text_encoder=text_encoder.config, audio_encoder=audio_encoder.config, decoder=decoder.config\n+            )\n         else:\n             if not isinstance(config, self.config_class):\n                 raise ValueError(f\"Config: {config} has to be of type {self.config_class}\")\n@@ -1616,8 +1618,8 @@ def from_sub_models_pretrained(\n             decoder = MusicgenForCausalLM.from_pretrained(decoder_pretrained_model_name_or_path, **kwargs_decoder)\n \n         # instantiate config with corresponding kwargs\n-        config = MusicgenConfig.from_sub_models_config(\n-            text_encoder.config, audio_encoder.config, decoder.config, **kwargs\n+        config = MusicgenConfig(\n+            text_encoder=text_encoder.config, audio_encoder=audio_encoder.config, decoder=decoder.config, **kwargs\n         )\n         return cls(text_encoder=text_encoder, audio_encoder=audio_encoder, decoder=decoder, config=config)\n "
        },
        {
            "sha": "a4ec8528590a6b8b0d1c8ce39fa8e36ea5d96eb9",
            "filename": "src/transformers/models/musicgen_melody/configuration_musicgen_melody.py",
            "status": "modified",
            "additions": 26,
            "deletions": 49,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fconfiguration_musicgen_melody.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -145,18 +145,16 @@ class MusicgenMelodyConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        num_chroma (`int`, *optional*, defaults to 12): Number of chroma bins to use.\n+        text_encoder (`Union[dict, `PretrainedConfig`]`):\n+            An instance of a configuration object that defines the text encoder config.\n+        audio_encoder (`Union[dict, `PretrainedConfig`]`):\n+            An instance of a configuration object that defines the audio encoder config.\n+        decoder (`Union[dict, `PretrainedConfig`]`):\n+            An instance of a configuration object that defines the decoder config.\n+        num_chroma (`int`, *optional*, defaults to 12):\n+            Number of chroma bins to use.\n         chroma_length (`int`, *optional*, defaults to 235):\n             Maximum chroma duration if audio is used to condition the model. Corresponds to the maximum duration used during training.\n-        kwargs (*optional*):\n-            Dictionary of keyword arguments. Notably:\n-\n-                - **text_encoder** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that\n-                  defines the text encoder config.\n-                - **audio_encoder** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that\n-                  defines the audio encoder config.\n-                - **decoder** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that defines\n-                  the decoder config.\n \n     Example:\n \n@@ -174,8 +172,8 @@ class MusicgenMelodyConfig(PreTrainedConfig):\n     >>> audio_encoder_config = EncodecConfig()\n     >>> decoder_config = MusicgenMelodyDecoderConfig()\n \n-    >>> configuration = MusicgenMelodyConfig.from_sub_models_config(\n-    ...     text_encoder_config, audio_encoder_config, decoder_config\n+    >>> configuration = MusicgenMelodyConfig(\n+    ...     text_encoder=text_encoder_config, audio_encoder=audio_encoder_config, decoder=decoder_config\n     ... )\n \n     >>> # Initializing a MusicgenMelodyForConditionalGeneration (with random weights) from the facebook/musicgen-melody style configuration\n@@ -205,52 +203,31 @@ class MusicgenMelodyConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n+        text_encoder,\n+        audio_encoder,\n+        decoder,\n         num_chroma=12,\n         chroma_length=235,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-        if \"text_encoder\" not in kwargs or \"audio_encoder\" not in kwargs or \"decoder\" not in kwargs:\n-            raise ValueError(\"Config has to be initialized with text_encoder, audio_encoder and decoder config\")\n-\n-        text_encoder_config = kwargs.pop(\"text_encoder\")\n-        text_encoder_model_type = text_encoder_config.pop(\"model_type\")\n+        if isinstance(text_encoder, dict):\n+            text_encoder_model_type = text_encoder.pop(\"model_type\")\n+            text_encoder = AutoConfig.for_model(text_encoder_model_type, **text_encoder)\n \n-        audio_encoder_config = kwargs.pop(\"audio_encoder\")\n-        audio_encoder_model_type = audio_encoder_config.pop(\"model_type\")\n+        if isinstance(audio_encoder, dict):\n+            audio_encoder_model_type = audio_encoder.pop(\"model_type\")\n+            audio_encoder = AutoConfig.for_model(audio_encoder_model_type, **audio_encoder)\n \n-        decoder_config = kwargs.pop(\"decoder\")\n-\n-        self.text_encoder = AutoConfig.for_model(text_encoder_model_type, **text_encoder_config)\n-        self.audio_encoder = AutoConfig.for_model(audio_encoder_model_type, **audio_encoder_config)\n-        self.decoder = MusicgenMelodyDecoderConfig(**decoder_config)\n-        self.is_encoder_decoder = False\n+        if isinstance(decoder, dict):\n+            decoder = MusicgenMelodyDecoderConfig(**decoder)\n \n+        self.text_encoder = text_encoder\n+        self.audio_encoder = audio_encoder\n+        self.decoder = decoder\n         self.num_chroma = num_chroma\n         self.chroma_length = chroma_length\n-\n-    @classmethod\n-    def from_sub_models_config(\n-        cls,\n-        text_encoder_config: PreTrainedConfig,\n-        audio_encoder_config: PreTrainedConfig,\n-        decoder_config: MusicgenMelodyDecoderConfig,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        Instantiate a [`MusicgenMelodyConfig`] (or a derived class) from text encoder, audio encoder and decoder\n-        configurations.\n-\n-        Returns:\n-            [`MusicgenMelodyConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(\n-            text_encoder=text_encoder_config.to_dict(),\n-            audio_encoder=audio_encoder_config.to_dict(),\n-            decoder=decoder_config.to_dict(),\n-            **kwargs,\n-        )\n+        kwargs[\"is_encoder_decoder\"] = False\n+        super().__init__(**kwargs)\n \n     @property\n     # This is a property because you might want to change the codec model on the fly"
        },
        {
            "sha": "a918c847e1064707e755ae360e345e2b17caaa9c",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -1253,8 +1253,8 @@ def __init__(\n                 \"Either a configuration has to be provided, or all three of text encoder, audio encoder and Musicgen Melody decoder.\"\n             )\n         if config is None:\n-            config = MusicgenMelodyConfig.from_sub_models_config(\n-                text_encoder.config, audio_encoder.config, decoder.config\n+            config = MusicgenMelodyConfig(\n+                text_encoder=text_encoder.config, audio_encoder=audio_encoder.config, decoder=decoder.config\n             )\n         else:\n             if not isinstance(config, self.config_class):\n@@ -1537,8 +1537,8 @@ def from_sub_models_pretrained(\n             )\n \n         # instantiate config with corresponding kwargs\n-        config = MusicgenMelodyConfig.from_sub_models_config(\n-            text_encoder.config, audio_encoder.config, decoder.config, **kwargs\n+        config = MusicgenMelodyConfig(\n+            text_encoder=text_encoder.config, audio_encoder=audio_encoder.config, decoder=decoder.config, **kwargs\n         )\n         return cls(text_encoder=text_encoder, audio_encoder=audio_encoder, decoder=decoder, config=config)\n "
        },
        {
            "sha": "7b2190d6bae48507fccbec9858cba22d70b5b557",
            "filename": "src/transformers/models/owlv2/configuration_owlv2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 23,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fowlv2%2Fconfiguration_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fowlv2%2Fconfiguration_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fconfiguration_owlv2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -246,38 +246,26 @@ def __init__(\n         return_dict=True,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the Owlv2TextConfig with default values.\")\n+            text_config = Owlv2TextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `Owlv2TextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = Owlv2TextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"vision_config is None. initializing the Owlv2VisionConfig with default values.\")\n+            vision_config = Owlv2VisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `Owlv2VisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = Owlv2VisionConfig(**vision_config)\n \n-        self.text_config = Owlv2TextConfig(**text_config)\n-        self.vision_config = Owlv2VisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.logit_scale_init_value = logit_scale_init_value\n         self.return_dict = return_dict\n         self.initializer_factor = 1.0\n-\n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: dict, vision_config: dict, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`Owlv2Config`] (or a derived class) from owlv2 text model configuration and owlv2 vision\n-        model configuration.\n-\n-        Returns:\n-            [`Owlv2Config`]: An instance of a configuration object\n-        \"\"\"\n-        config_dict = {}\n-        config_dict[\"text_config\"] = text_config\n-        config_dict[\"vision_config\"] = vision_config\n-\n-        return cls.from_dict(config_dict, **kwargs)\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"Owlv2Config\", \"Owlv2TextConfig\", \"Owlv2VisionConfig\"]"
        },
        {
            "sha": "851dc077a47c43eaf38f24c1935e4dc1b635e824",
            "filename": "src/transformers/models/owlvit/configuration_owlvit.py",
            "status": "modified",
            "additions": 11,
            "deletions": 23,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconfiguration_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconfiguration_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconfiguration_owlvit.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -252,38 +252,26 @@ def __init__(\n         return_dict=True,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the OwlViTTextConfig with default values.\")\n+            text_config = OwlViTTextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `OwlViTTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = OwlViTTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"vision_config is None. initializing the OwlViTVisionConfig with default values.\")\n+            vision_config = OwlViTVisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `OwlViTVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = OwlViTVisionConfig(**vision_config)\n \n-        self.text_config = OwlViTTextConfig(**text_config)\n-        self.vision_config = OwlViTVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.logit_scale_init_value = logit_scale_init_value\n         self.return_dict = return_dict\n         self.initializer_factor = 1.0\n-\n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: dict, vision_config: dict, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`OwlViTConfig`] (or a derived class) from owlvit text model configuration and owlvit vision\n-        model configuration.\n-\n-        Returns:\n-            [`OwlViTConfig`]: An instance of a configuration object\n-        \"\"\"\n-        config_dict = {}\n-        config_dict[\"text_config\"] = text_config\n-        config_dict[\"vision_config\"] = vision_config\n-\n-        return cls.from_dict(config_dict, **kwargs)\n+        super().__init__(**kwargs)\n \n \n class OwlViTOnnxConfig(OnnxConfig):"
        },
        {
            "sha": "bed51d1639fcc307d958231b2e6429cc7be4025a",
            "filename": "src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -379,13 +379,17 @@ def __init__(\n         audio_config=None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            pad_token_id=pad_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n+        if isinstance(vision_config, dict):\n+            vision_config = Phi4MultimodalVisionConfig(**vision_config)\n+        elif vision_config is None:\n+            Phi4MultimodalVisionConfig()\n+        self.vision_config = vision_config\n+\n+        if isinstance(audio_config, dict):\n+            audio_config = Phi4MultimodalAudioConfig(**audio_config)\n+        elif vision_config is None:\n+            audio_config = Phi4MultimodalAudioConfig()\n+        self.audio_config = audio_config\n         self.vocab_size = vocab_size\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size\n@@ -412,17 +416,13 @@ def __init__(\n         self._rope_scaling_validation()\n         self.sliding_window = sliding_window\n \n-        if isinstance(vision_config, dict):\n-            vision_config = Phi4MultimodalVisionConfig(**vision_config)\n-        elif vision_config is None:\n-            Phi4MultimodalVisionConfig()\n-        self.vision_config = vision_config\n-\n-        if isinstance(audio_config, dict):\n-            audio_config = Phi4MultimodalAudioConfig(**audio_config)\n-        elif vision_config is None:\n-            audio_config = Phi4MultimodalAudioConfig()\n-        self.audio_config = audio_config\n+        super().__init__(\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            pad_token_id=pad_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n     def _rope_scaling_adjustment(self):\n         \"\"\""
        },
        {
            "sha": "6b132126bceba0853bf2e58541d5dcfb7f2ef27d",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -402,6 +402,18 @@ def __init__(\n         audio_config=None,\n         **kwargs,\n     ):\n+        if isinstance(vision_config, dict):\n+            vision_config = Phi4MultimodalVisionConfig(**vision_config)\n+        elif vision_config is None:\n+            Phi4MultimodalVisionConfig()\n+        self.vision_config = vision_config\n+\n+        if isinstance(audio_config, dict):\n+            audio_config = Phi4MultimodalAudioConfig(**audio_config)\n+        elif vision_config is None:\n+            audio_config = Phi4MultimodalAudioConfig()\n+        self.audio_config = audio_config\n+\n         super().__init__(\n             vocab_size=vocab_size,\n             hidden_size=hidden_size,\n@@ -429,18 +441,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        if isinstance(vision_config, dict):\n-            vision_config = Phi4MultimodalVisionConfig(**vision_config)\n-        elif vision_config is None:\n-            Phi4MultimodalVisionConfig()\n-        self.vision_config = vision_config\n-\n-        if isinstance(audio_config, dict):\n-            audio_config = Phi4MultimodalAudioConfig(**audio_config)\n-        elif vision_config is None:\n-            audio_config = Phi4MultimodalAudioConfig()\n-        self.audio_config = audio_config\n-\n \n class Phi4MultimodalVisionMLP(SiglipMLP):\n     pass"
        },
        {
            "sha": "a31f5751ad5d369f3cba89929cba8874436686ce",
            "filename": "src/transformers/models/pix2struct/configuration_pix2struct.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -289,7 +289,7 @@ class Pix2StructConfig(PreTrainedConfig):\n     >>> config_text = Pix2StructTextConfig()\n     >>> config_vision = Pix2StructVisionConfig()\n \n-    >>> config = Pix2StructConfig.from_text_vision_configs(config_text, config_vision)\n+    >>> config = Pix2StructConfig(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"pix2struct\"\n@@ -306,20 +306,24 @@ def __init__(\n         is_encoder_decoder=True,\n         **kwargs,\n     ):\n-        super().__init__(tie_word_embeddings=tie_word_embeddings, is_encoder_decoder=is_encoder_decoder, **kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"text_config is None. Initializing the Pix2StructTextConfig with default values.\")\n+            text_config = Pix2StructTextConfig(\n+                {\"is_encoder_decoder\": is_encoder_decoder, \"tie_word_embeddings\": tie_word_embeddings}\n+            )\n+            logger.info(\"`text_config` is `None`. initializing the `Pix2StructTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config[\"is_encoder_decoder\"] = is_encoder_decoder\n+            text_config[\"tie_word_embeddings\"] = tie_word_embeddings\n+            text_config = Pix2StructTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n-            logger.info(\"vision_config is None. Initializing the Pix2StructVisionConfig with default values.\")\n+            vision_config = Pix2StructVisionConfig()\n+            logger.info(\"`vision_config` is `None`. initializing the `Pix2StructVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = Pix2StructVisionConfig(**vision_config)\n \n-        text_config[\"is_encoder_decoder\"] = is_encoder_decoder\n-        text_config[\"tie_word_embeddings\"] = tie_word_embeddings\n-        self.text_config = Pix2StructTextConfig(**text_config)\n-        self.vision_config = Pix2StructVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.decoder_start_token_id = self.text_config.decoder_start_token_id\n         self.pad_token_id = self.text_config.pad_token_id\n@@ -332,6 +336,7 @@ def __init__(\n         self.vision_config.initializer_range = self.initializer_range\n \n         self.is_vqa = is_vqa\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n \n __all__ = [\"Pix2StructConfig\", \"Pix2StructTextConfig\", \"Pix2StructVisionConfig\"]"
        },
        {
            "sha": "53653abd952145df616ee631b797ac9f5c95cfef",
            "filename": "src/transformers/models/prompt_depth_anything/configuration_prompt_depth_anything.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -111,7 +111,6 @@ def __init__(\n         max_depth=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if backbone_config is None and backbone is None:\n             logger.info(\"`backbone_config` is `None`. Initializing the config with the default `Dinov2` backbone.\")\n             backbone_config = CONFIG_MAPPING[\"dinov2\"](\n@@ -153,5 +152,7 @@ def __init__(\n         self.depth_estimation_type = depth_estimation_type\n         self.max_depth = max_depth if max_depth else 1\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"PromptDepthAnythingConfig\"]"
        },
        {
            "sha": "74281ab88f97c6e2f08f04f78bceefb77dc717ae",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -1027,7 +1027,7 @@ class Qwen2_5OmniConfig(PreTrainedConfig):\n \n \n     >>> # Initializing a module style configuration\n-    >>> configuration = Qwen2_5OmniConfig.from_sub_model_configs(\n+    >>> configuration = Qwen2_5OmniConfig(\n     ...     thinker_config, talker_config, token2wav_config\n     ... )\n "
        },
        {
            "sha": "d4aa1af34486d24c8d02cb9a0445aa52dd2b9823",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -1061,7 +1061,7 @@ class Qwen2_5OmniConfig(PreTrainedConfig):\n \n \n     >>> # Initializing a module style configuration\n-    >>> configuration = Qwen2_5OmniConfig.from_sub_model_configs(\n+    >>> configuration = Qwen2_5OmniConfig(\n     ...     thinker_config, talker_config, token2wav_config\n     ... )\n "
        },
        {
            "sha": "a96b7699c7b5a6a8b7da147c003286ec8202aca4",
            "filename": "src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 19,
            "deletions": 15,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -343,10 +343,6 @@ def __init__(\n         mlp_only_layers=None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -380,6 +376,11 @@ def __init__(\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n \n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n \n class Qwen3OmniMoeThinkerConfig(PreTrainedConfig):\n     r\"\"\"\n@@ -453,7 +454,6 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         self.user_token_id = user_token_id\n         self.position_id_per_seconds = position_id_per_seconds\n         self.audio_start_token_id = audio_start_token_id\n@@ -476,6 +476,8 @@ def __init__(\n         elif text_config is None:\n             text_config = Qwen3OmniMoeTextConfig()\n         self.text_config = text_config\n+\n+        super().__init__(**kwargs)\n         self.audio_token_id = audio_token_id\n         self.image_token_id = image_token_id\n         self.video_token_id = video_token_id\n@@ -635,10 +637,6 @@ def __init__(\n         num_code_groups=32,\n         **kwargs,\n     ):\n-        super().__init__(\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -676,6 +674,11 @@ def __init__(\n                 for i in range(self.num_hidden_layers)\n             ]\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n+\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n         self.num_code_groups = num_code_groups\n \n \n@@ -853,10 +856,6 @@ def __init__(\n         mlp_only_layers=None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -890,6 +889,11 @@ def __init__(\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n \n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n \n class Qwen3OmniMoeTalkerConfig(PreTrainedConfig):\n     r\"\"\"\n@@ -991,7 +995,6 @@ def __init__(\n         speaker_id=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if code_predictor_config is None:\n             code_predictor_config = {}\n             self.code_predictor_config = Qwen3OmniMoeTalkerCodePredictorConfig()\n@@ -1025,6 +1028,7 @@ def __init__(\n         self.audio_start_token_id = audio_start_token_id\n         self.vision_start_token_id = vision_start_token_id\n         self.speaker_id = speaker_id\n+        super().__init__(**kwargs)\n \n \n class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n@@ -1203,7 +1207,6 @@ def __init__(\n         assistant_token_id=77091,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if thinker_config is None:\n             thinker_config = {}\n             logger.info(\"thinker_config is None. Initializing thinker model with default values\")\n@@ -1228,6 +1231,7 @@ def __init__(\n         self.system_token_id = system_token_id\n         self.user_token_id = user_token_id\n         self.assistant_token_id = assistant_token_id\n+        super().__init__(**kwargs)\n \n     def get_text_config(self, decoder=False) -> \"PreTrainedConfig\":\n         \"\"\""
        },
        {
            "sha": "a3d3f5ecb9aee200d9e287b23b7f27886d38e90d",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -526,7 +526,6 @@ def __init__(\n         speaker_id=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if code_predictor_config is None:\n             code_predictor_config = {}\n             self.code_predictor_config = Qwen3OmniMoeTalkerCodePredictorConfig()\n@@ -560,6 +559,7 @@ def __init__(\n         self.audio_start_token_id = audio_start_token_id\n         self.vision_start_token_id = vision_start_token_id\n         self.speaker_id = speaker_id\n+        super().__init__(**kwargs)\n \n \n class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n@@ -738,7 +738,6 @@ def __init__(\n         assistant_token_id=77091,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if thinker_config is None:\n             thinker_config = {}\n             logger.info(\"thinker_config is None. Initializing thinker model with default values\")\n@@ -763,6 +762,7 @@ def __init__(\n         self.system_token_id = system_token_id\n         self.user_token_id = user_token_id\n         self.assistant_token_id = assistant_token_id\n+        super().__init__(**kwargs)\n \n     def get_text_config(self, decoder=False) -> \"PreTrainedConfig\":\n         \"\"\""
        },
        {
            "sha": "d367a184f01fdfad6c6999e014de8ae28c83936e",
            "filename": "src/transformers/models/qwen3_vl/processing_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -67,7 +67,6 @@ class Qwen3VLProcessor(ProcessorMixin):\n     tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n \n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n-        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n         self.image_token_id = (\n@@ -80,6 +79,7 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n             if getattr(tokenizer, \"video_token_id\", None)\n             else tokenizer.convert_tokens_to_ids(self.video_token)\n         )\n+        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n         self.vision_start_token = (\n             \"<|vision_start|>\" if not hasattr(tokenizer, \"vision_start_token\") else tokenizer.vision_start_token\n         )"
        },
        {
            "sha": "565a6e18091b2d0e19819327581f93a609a69db1",
            "filename": "src/transformers/models/rt_detr/configuration_rt_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -336,22 +336,5 @@ def __init__(\n         self.eos_coefficient = eos_coefficient\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @classmethod\n-    def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n-        \"\"\"Instantiate a [`RTDetrConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n-        configuration.\n-\n-            Args:\n-                backbone_config ([`PreTrainedConfig`]):\n-                    The backbone configuration.\n-\n-            Returns:\n-                [`RTDetrConfig`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(\n-            backbone_config=backbone_config,\n-            **kwargs,\n-        )\n-\n \n __all__ = [\"RTDetrConfig\"]"
        },
        {
            "sha": "b40ee12ea43a7bd02c7117370fe60d3f5d2fb257",
            "filename": "src/transformers/models/rt_detr_v2/configuration_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -258,7 +258,6 @@ def __init__(\n         decoder_method=\"default\",\n         **kwargs,\n     ):\n-        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n         self.initializer_range = initializer_range\n         self.initializer_bias_prior_prob = initializer_bias_prior_prob\n         self.layer_norm_eps = layer_norm_eps\n@@ -358,23 +357,7 @@ def __init__(\n         self.decoder_n_levels = decoder_n_levels\n         self.decoder_offset_scale = decoder_offset_scale\n         self.decoder_method = decoder_method\n-\n-    @classmethod\n-    def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n-        \"\"\"Instantiate a [`RTDetrV2Config`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n-        configuration.\n-\n-            Args:\n-                backbone_config ([`PreTrainedConfig`]):\n-                    The backbone configuration.\n-\n-            Returns:\n-                [`RTDetrV2Config`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(\n-            backbone_config=backbone_config,\n-            **kwargs,\n-        )\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n \n __all__ = [\"RTDetrV2Config\"]"
        },
        {
            "sha": "e5e243e1e7f855bbc137713bb8007e0650c6419d",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -269,7 +269,6 @@ def __init__(\n         decoder_method=\"default\",\n         **kwargs,\n     ):\n-        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n         self.initializer_range = initializer_range\n         self.initializer_bias_prior_prob = initializer_bias_prior_prob\n         self.layer_norm_eps = layer_norm_eps\n@@ -369,23 +368,7 @@ def __init__(\n         self.decoder_n_levels = decoder_n_levels\n         self.decoder_offset_scale = decoder_offset_scale\n         self.decoder_method = decoder_method\n-\n-    @classmethod\n-    def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n-        \"\"\"Instantiate a [`RTDetrV2Config`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n-        configuration.\n-\n-            Args:\n-                backbone_config ([`PreTrainedConfig`]):\n-                    The backbone configuration.\n-\n-            Returns:\n-                [`RTDetrV2Config`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(\n-            backbone_config=backbone_config,\n-            **kwargs,\n-        )\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n \n def multi_scale_deformable_attention_v2("
        },
        {
            "sha": "0229cf40d8cb04b30986a7442d0988da6bde41db",
            "filename": "src/transformers/models/sam/configuration_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -316,7 +316,6 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         vision_config = vision_config if vision_config is not None else {}\n         prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n         mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n@@ -332,6 +331,7 @@ def __init__(\n         self.prompt_encoder_config = SamPromptEncoderConfig(**prompt_encoder_config)\n         self.mask_decoder_config = SamMaskDecoderConfig(**mask_decoder_config)\n         self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"SamConfig\", \"SamMaskDecoderConfig\", \"SamPromptEncoderConfig\", \"SamVisionConfig\"]"
        },
        {
            "sha": "a0aa5e97e96a4d44f52c0fc60ef809bbbbe3af8e",
            "filename": "src/transformers/models/sam2/configuration_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -202,8 +202,6 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         backbone_channel_list = [768, 384, 192, 96] if backbone_channel_list is None else backbone_channel_list\n         backbone_feature_sizes = (\n             [[256, 256], [128, 128], [64, 64]] if backbone_feature_sizes is None else backbone_feature_sizes\n@@ -233,6 +231,7 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.layer_norm_eps = layer_norm_eps\n         self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n \n \n class Sam2PromptEncoderConfig(PreTrainedConfig):\n@@ -424,7 +423,6 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         vision_config = vision_config if vision_config is not None else {}\n         prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n         mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n@@ -442,6 +440,7 @@ def __init__(\n         self.mask_decoder_config = Sam2MaskDecoderConfig(**mask_decoder_config)\n \n         self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n \n \n __all__ = ["
        },
        {
            "sha": "f80d10704fcf5340526027565466b92f5f4e4c86",
            "filename": "src/transformers/models/sam_hq/configuration_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -292,7 +292,6 @@ def __init__(\n         initializer_range=0.02,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         vision_config = vision_config if vision_config is not None else {}\n         prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n         mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n@@ -308,6 +307,7 @@ def __init__(\n         self.prompt_encoder_config = SamHQPromptEncoderConfig(**prompt_encoder_config)\n         self.mask_decoder_config = SamHQMaskDecoderConfig(**mask_decoder_config)\n         self.initializer_range = initializer_range\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"SamHQVisionConfig\", \"SamHQMaskDecoderConfig\", \"SamHQPromptEncoderConfig\", \"SamHQConfig\"]"
        },
        {
            "sha": "102d63512f5427f8be8dee222008ee957d8b0775",
            "filename": "src/transformers/models/siglip/configuration_siglip.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -231,27 +231,30 @@ class SiglipConfig(PreTrainedConfig):\n     >>> config_text = SiglipTextConfig()\n     >>> config_vision = SiglipVisionConfig()\n \n-    >>> config = SiglipConfig.from_text_vision_configs(config_text, config_vision)\n+    >>> config = SiglipConfig(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"siglip\"\n     sub_configs = {\"text_config\": SiglipTextConfig, \"vision_config\": SiglipVisionConfig}\n \n     def __init__(self, text_config=None, vision_config=None, **kwargs):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n+            text_config = SiglipTextConfig()\n             logger.info(\"`text_config` is `None`. Initializing the `SiglipTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = SiglipTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = SiglipVisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `SiglipVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = SiglipVisionConfig(**vision_config)\n \n-        self.text_config = SiglipTextConfig(**text_config)\n-        self.vision_config = SiglipVisionConfig(**vision_config)\n-\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n         self.initializer_factor = 1.0\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"SiglipConfig\", \"SiglipTextConfig\", \"SiglipVisionConfig\"]"
        },
        {
            "sha": "8d29bea4e73a7f5118bb16211ce26d676cfae1ae",
            "filename": "src/transformers/models/siglip2/configuration_siglip2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconfiguration_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconfiguration_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconfiguration_siglip2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -239,27 +239,30 @@ class Siglip2Config(PreTrainedConfig):\n     >>> config_text = Siglip2TextConfig()\n     >>> config_vision = Siglip2VisionConfig()\n \n-    >>> config = Siglip2Config.from_text_vision_configs(config_text, config_vision)\n+    >>> config = Siglip2Config(text_config=config_text, vision_config=config_vision)\n     ```\"\"\"\n \n     model_type = \"siglip2\"\n     sub_configs = {\"text_config\": Siglip2TextConfig, \"vision_config\": Siglip2VisionConfig}\n \n     def __init__(self, text_config=None, vision_config=None, **kwargs):\n-        super().__init__(**kwargs)\n-\n         if text_config is None:\n-            text_config = {}\n+            text_config = Siglip2TextConfig()\n             logger.info(\"`text_config` is `None`. Initializing the `Siglip2TextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = Siglip2TextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = Siglip2VisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `Siglip2VisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = Siglip2VisionConfig(**vision_config)\n \n-        self.text_config = Siglip2TextConfig(**text_config)\n-        self.vision_config = Siglip2VisionConfig(**vision_config)\n-\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n         self.initializer_factor = 1.0\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"Siglip2Config\", \"Siglip2TextConfig\", \"Siglip2VisionConfig\"]"
        },
        {
            "sha": "83ea976296cd9f6d6f316dac5fe811fda9853ab4",
            "filename": "src/transformers/models/timesfm/configuration_timesfm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconfiguration_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconfiguration_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconfiguration_timesfm.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -118,10 +118,8 @@ def __init__(\n         self.min_timescale = min_timescale\n         self.max_timescale = max_timescale\n \n-        super().__init__(\n-            is_encoder_decoder=self.is_encoder_decoder,\n-            **kwargs,\n-        )\n+        kwargs[\"is_encoder_decoder\"] = self.is_encoder_decoder\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"TimesFmConfig\"]"
        },
        {
            "sha": "eed6a435d94079f20ee7ff12012f11a7a1f96453",
            "filename": "src/transformers/models/tvp/configuration_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -128,7 +128,6 @@ def __init__(\n         attention_probs_dropout_prob=0.1,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if backbone_config is None and backbone is None:\n             logger.info(\"`backbone_config` is `None`. Initializing the config with the default `ResNet` backbone.\")\n             backbone_config = CONFIG_MAPPING[\"resnet\"](out_features=[\"stage4\"])\n@@ -171,17 +170,7 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.attention_probs_dropout_prob = attention_probs_dropout_prob\n \n-    @classmethod\n-    def from_backbone_config(cls, backbone_config: PreTrainedConfig, **kwargs):\n-        \"\"\"Instantiate a [`TvpConfig`] (or a derived class) from a pre-trained backbone model configuration.\n-\n-        Args:\n-            backbone_config ([`PreTrainedConfig`]):\n-                The backbone configuration.\n-        Returns:\n-            [`TvpConfig`]: An instance of a configuration object\n-        \"\"\"\n-        return cls(backbone_config=backbone_config, **kwargs)\n+        super().__init__(**kwargs)\n \n \n __all__ = [\"TvpConfig\"]"
        },
        {
            "sha": "ec7a564ef55aed89aff8f9512054369d525fc177",
            "filename": "src/transformers/models/upernet/configuration_upernet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fupernet%2Fconfiguration_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fupernet%2Fconfiguration_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fconfiguration_upernet.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -104,7 +104,6 @@ def __init__(\n         loss_ignore_index=255,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n         if backbone_config is None and backbone is None:\n             logger.info(\"`backbone_config` is `None`. Initializing the config with the default `ResNet` backbone.\")\n             backbone_config = CONFIG_MAPPING[\"resnet\"](out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n@@ -137,5 +136,7 @@ def __init__(\n         self.auxiliary_concat_input = auxiliary_concat_input\n         self.loss_ignore_index = loss_ignore_index\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"UperNetConfig\"]"
        },
        {
            "sha": "ae465f53c22b1af770a575e1a50c19a9b526f518",
            "filename": "src/transformers/models/vitmatte/configuration_vitmatte.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -93,8 +93,6 @@ def __init__(\n         fusion_hidden_sizes: list[int] = [256, 128, 64, 32],\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if backbone_config is None and backbone is None:\n             logger.info(\"`backbone_config` is `None`. Initializing the config with the default `VitDet` backbone.\")\n             backbone_config = CONFIG_MAPPING[\"vitdet\"](out_features=[\"stage4\"])\n@@ -122,5 +120,7 @@ def __init__(\n         self.convstream_hidden_sizes = convstream_hidden_sizes\n         self.fusion_hidden_sizes = fusion_hidden_sizes\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"VitMatteConfig\"]"
        },
        {
            "sha": "f2a50d561153c60c062979d2e4c1728ec22f3188",
            "filename": "src/transformers/models/vitpose/configuration_vitpose.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -88,8 +88,6 @@ def __init__(\n         use_simple_decoder: bool = True,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if use_pretrained_backbone:\n             logger.info(\n                 \"`use_pretrained_backbone` is `True`. For the pure inference purpose of VitPose weight do not set this value.\"\n@@ -123,5 +121,7 @@ def __init__(\n         self.scale_factor = scale_factor\n         self.use_simple_decoder = use_simple_decoder\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"VitPoseConfig\"]"
        },
        {
            "sha": "25dcc37e3de87980e97a3f72e428d1b780e96837",
            "filename": "src/transformers/models/x_clip/configuration_x_clip.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -280,8 +280,6 @@ def __init__(\n         text_config_dict = kwargs.pop(\"text_config_dict\", None)\n         vision_config_dict = kwargs.pop(\"vision_config_dict\", None)\n \n-        super().__init__(**kwargs)\n-\n         # Instead of simply assigning `[text|vision]_config_dict` to `[text|vision]_config`, we use the values in\n         # `[text|vision]_config_dict` to update the values in `[text|vision]_config`. The values should be same in most\n         # cases, but we don't want to break anything regarding `_config_dict` that existed before commit `8827e1b2`.\n@@ -345,15 +343,19 @@ def __init__(\n             vision_config.update(_vision_config_dict)\n \n         if text_config is None:\n-            text_config = {}\n-            logger.info(\"`text_config` is `None`. Initializing the `XCLIPTextConfig` with default values.\")\n+            text_config = XCLIPTextConfig()\n+            logger.info(\"`text_config` is `None`. initializing the `XCLIPTextConfig` with default values.\")\n+        elif isinstance(text_config, dict):\n+            text_config = XCLIPTextConfig(**text_config)\n \n         if vision_config is None:\n-            vision_config = {}\n+            vision_config = XCLIPVisionConfig()\n             logger.info(\"`vision_config` is `None`. initializing the `XCLIPVisionConfig` with default values.\")\n+        elif isinstance(vision_config, dict):\n+            vision_config = XCLIPVisionConfig(**vision_config)\n \n-        self.text_config = XCLIPTextConfig(**text_config)\n-        self.vision_config = XCLIPVisionConfig(**vision_config)\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n \n         self.projection_dim = projection_dim\n         self.prompt_layers = prompt_layers\n@@ -365,5 +367,7 @@ def __init__(\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = 1.0\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"XCLIPConfig\", \"XCLIPTextConfig\", \"XCLIPVisionConfig\"]"
        },
        {
            "sha": "99890fca81c56d2605a406ec4d81e314ee5c4155",
            "filename": "src/transformers/models/x_clip/convert_x_clip_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -55,7 +55,7 @@ def get_xclip_config(model_name, num_frames):\n     if model_name == \"xclip-large-patch14-16-frames\":\n         vision_config.image_size = 336\n \n-    config = XCLIPConfig.from_text_vision_configs(text_config, vision_config)\n+    config = XCLIPConfig(text_config=text_config, vision_config=vision_config)\n \n     if \"large\" in model_name:\n         config.projection_dim = 768"
        },
        {
            "sha": "bf91c02912ca1431ece45b506c1e97d88c1609b6",
            "filename": "src/transformers/models/xcodec/configuration_xcodec.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconfiguration_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconfiguration_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fconfiguration_xcodec.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -102,8 +102,6 @@ def __init__(\n         semantic_model_config: Union[dict, HubertConfig] = None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if acoustic_model_config is None:\n             self.acoustic_model_config = DacConfig(\n                 encoder_hidden_size=64,\n@@ -158,6 +156,8 @@ def __init__(\n             codebook_dim = self.acoustic_model_config.hidden_size + self.semantic_model_config.hidden_size\n         self.codebook_dim = codebook_dim\n \n+        super().__init__(**kwargs)\n+\n     @property\n     def frame_rate(self) -> int:\n         return math.ceil(self.sample_rate / self.hop_length)"
        },
        {
            "sha": "c193ad5310d8c40459751e54d70a619db5a85319",
            "filename": "src/transformers/models/zoedepth/configuration_zoedepth.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -169,8 +169,6 @@ def __init__(\n         patch_transformer_num_attention_heads=None,\n         **kwargs,\n     ):\n-        super().__init__(**kwargs)\n-\n         if readout_type not in [\"ignore\", \"add\", \"project\"]:\n             raise ValueError(\"Readout_type must be one of ['ignore', 'add', 'project']\")\n \n@@ -234,5 +232,7 @@ def __init__(\n         self.patch_transformer_intermediate_size = patch_transformer_intermediate_size\n         self.patch_transformer_num_attention_heads = patch_transformer_num_attention_heads\n \n+        super().__init__(**kwargs)\n+\n \n __all__ = [\"ZOEDEPTH_PRETRAINED_CONFIG_ARCHIVE_MAP\", \"ZoeDepthConfig\"]"
        },
        {
            "sha": "25d0ac4a5ea3df7e68f4caae2bed241c28c1518c",
            "filename": "tests/models/aimv2/test_modeling_aimv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -352,8 +352,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return Aimv2Config.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return Aimv2Config(\n+            text_config=self.text_model_tester.get_config(),\n+            vision_config=self.vision_model_tester.get_config(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):"
        },
        {
            "sha": "909c9b22208e5300f31211c020944a54a00d7bc7",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -490,7 +490,7 @@ def __init__(\n         self.is_training = is_training\n \n     def get_config(self):\n-        return BarkConfig.from_sub_model_configs(\n+        return BarkConfig(\n             self.semantic_model_tester.get_config(),\n             self.coarse_acoustics_model_tester.get_config(),\n             self.fine_acoustics_model_tester.get_config(),"
        },
        {
            "sha": "73715e160022af77155d103f706e48edf5f24115",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -427,7 +427,7 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return Blip2Config.from_vision_qformer_text_configs(\n+        return Blip2Config(\n             vision_config=self.vision_model_tester.get_config(),\n             qformer_config=self.qformer_model_tester.get_config(),\n             text_config=self.text_model_tester.get_config(),\n@@ -733,7 +733,7 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values, decoder_input_ids, decoder_attention_mask, lm_labels\n \n     def get_config(self):\n-        return Blip2Config.from_vision_qformer_text_configs(\n+        return Blip2Config(\n             vision_config=self.vision_model_tester.get_config(),\n             qformer_config=self.qformer_model_tester.get_config(),\n             text_config=self.text_model_tester.get_config(),\n@@ -1010,7 +1010,7 @@ def __init__(self, parent, vision_kwargs=None, qformer_kwargs=None, is_training=\n         self.batch_size = self.vision_model_tester.batch_size  # need bs for batching_equivalence test\n \n     def get_config(self):\n-        return Blip2Config.from_vision_qformer_text_configs(\n+        return Blip2Config(\n             vision_config=self.vision_model_tester.get_config(),\n             qformer_config=self.qformer_model_tester.get_config(),\n         )\n@@ -1168,7 +1168,7 @@ def __init__(self, parent, vision_kwargs=None, qformer_kwargs=None, is_training=\n         self.batch_size = self.vision_model_tester.batch_size  # need bs for batching_equivalence test\n \n     def get_config(self):\n-        return Blip2Config.from_vision_qformer_text_configs(\n+        return Blip2Config(\n             vision_config=self.vision_model_tester.get_config(),\n             qformer_config=self.qformer_model_tester.get_config(),\n         )\n@@ -1330,7 +1330,7 @@ def __init__(self, parent, vision_kwargs=None, qformer_kwargs=None, is_training=\n         self.batch_size = self.vision_model_tester.batch_size  # need bs for batching_equivalence test\n \n     def get_config(self):\n-        return Blip2Config.from_vision_qformer_text_configs(\n+        return Blip2Config(\n             vision_config=self.vision_model_tester.get_config(),\n             qformer_config=self.qformer_model_tester.get_config(),\n         )"
        },
        {
            "sha": "9f635afbfc753fbd3383a134086ed6e805e80a68",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -357,10 +357,10 @@ def get_config(self):\n         speech_config = self.clvp_encoder_tester.get_config()\n         speech_config.vocab_size = 300\n \n-        return ClvpConfig.from_sub_model_configs(\n-            text_config,\n-            speech_config,\n-            decoder_config,\n+        return ClvpConfig(\n+            text_config=text_config,\n+            speech_config=speech_config,\n+            decoder_config=decoder_config,\n             projection_dim=16,\n         )\n "
        },
        {
            "sha": "ee554a0d6c805a387374ab75a721b3c1f1e38715",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -206,7 +206,7 @@ def get_config(self):\n             stem_channels=[3, 16, 16],\n             use_lab=True,\n         )\n-        return DFineConfig.from_backbone_configs(\n+        return DFineConfig(\n             backbone_config=backbone_config,\n             encoder_hidden_dim=self.encoder_hidden_dim,\n             encoder_in_channels=self.encoder_in_channels,"
        },
        {
            "sha": "2c9656771a13bee927303511b08abbbaeee92e42",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -126,12 +126,6 @@ def test_model_rope_scaling_frequencies(self):\n     def test_model_rope_scaling_from_config(self, scaling_type):\n         pass\n \n-    #\n-    # @unittest.skip(reason=\"Not that big not that slow offload\")\n-    # def test_model_is_small(self):\n-    #     pass\n-    #\n-\n \n @require_torch\n class DbrxModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "8042b917ed6f8bc236680a79743178cb490c40d4",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -814,11 +814,11 @@ def prepare_config_and_inputs_for_common(self):\n         }\n \n     def get_config(self):\n-        return FlavaConfig.from_configs(\n-            self.image_model_tester.get_config(),\n-            self.text_model_tester.get_config(),\n-            self.multimodal_model_tester.get_config(),\n-            self.image_codebook_tester.get_config(),\n+        return FlavaConfig(\n+            image_config=self.image_model_tester.get_config(),\n+            text_config=self.text_model_tester.get_config(),\n+            multimodal_config=self.multimodal_model_tester.get_config(),\n+            image_codebook_config=self.image_codebook_tester.get_config(),\n             hidden_size=self.hidden_size,\n             projection_dim=self.projection_dim,\n             initializer_range=self.initializer_range,"
        },
        {
            "sha": "ee06e2e85da65611bbaf1f0c6c16a99f52baefde",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -422,7 +422,7 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, qformer_input_ids, qformer_attention_mask, pixel_values\n \n     def get_config(self):\n-        return InstructBlipConfig.from_vision_qformer_text_configs(\n+        return InstructBlipConfig(\n             vision_config=self.vision_model_tester.get_config(),\n             qformer_config=self.qformer_model_tester.get_config(),\n             text_config=self.text_model_tester.get_config(),"
        },
        {
            "sha": "b7acc2a5389ff12b70546e7d0bb7ea66ccb2a1c7",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -439,7 +439,7 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, qformer_input_ids, qformer_attention_mask, pixel_values\n \n     def get_config(self):\n-        return InstructBlipVideoConfig.from_vision_qformer_text_configs(\n+        return InstructBlipVideoConfig(\n             vision_config=self.vision_model_tester.get_config(),\n             qformer_config=self.qformer_model_tester.get_config(),\n             text_config=self.text_model_tester.get_config(),"
        },
        {
            "sha": "f1d11545dfe2049e65cd01b439fb4ebd90224a46",
            "filename": "tests/models/maskformer/test_modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -97,7 +97,7 @@ def prepare_config_and_inputs(self):\n         return config, pixel_values, pixel_mask, mask_labels, class_labels\n \n     def get_config(self):\n-        return MaskFormerConfig.from_backbone_and_decoder_configs(\n+        return MaskFormerConfig(\n             backbone_config=SwinConfig(\n                 depths=[1, 1, 1, 1],\n                 embed_dim=16,"
        },
        {
            "sha": "d0bf11c33132eb7aac5c848a27457279fd257535",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -555,7 +555,7 @@ def get_config(self):\n             tie_word_embeddings=False,\n             audio_channels=self.audio_channels,\n         )\n-        config = MusicgenConfig.from_sub_models_config(text_encoder_config, audio_encoder_config, decoder_config)\n+        config = MusicgenConfig(text_encoder_config, audio_encoder_config, decoder_config)\n         return config\n \n     def prepare_config_and_inputs_for_common(self):"
        },
        {
            "sha": "12ba4c7d134529a4c2a718038c4f0fc4080d7b58",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -573,7 +573,7 @@ def get_config(self):\n             tie_word_embeddings=False,\n             audio_channels=self.audio_channels,\n         )\n-        config = MusicgenMelodyConfig.from_sub_models_config(\n+        config = MusicgenMelodyConfig(\n             text_encoder_config, audio_encoder_config, decoder_config, chroma_length=self.chroma_length\n         )\n         return config"
        },
        {
            "sha": "bb66366f789d0921c435d04234b784a3a80a8cef",
            "filename": "tests/models/rt_detr/test_modeling_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -180,7 +180,7 @@ def get_config(self):\n             out_features=[\"stage2\", \"stage3\", \"stage4\"],\n             out_indices=[2, 3, 4],\n         )\n-        return RTDetrConfig.from_backbone_configs(\n+        return RTDetrConfig(\n             backbone_config=backbone_config,\n             encoder_hidden_dim=self.encoder_hidden_dim,\n             encoder_in_channels=hidden_sizes[1:],"
        },
        {
            "sha": "5aeb3d6043bd2b3dd480901c2d4ab250e8d0d5d3",
            "filename": "tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -183,7 +183,7 @@ def get_config(self):\n             out_features=[\"stage2\", \"stage3\", \"stage4\"],\n             out_indices=[2, 3, 4],\n         )\n-        return RTDetrV2Config.from_backbone_configs(\n+        return RTDetrV2Config(\n             backbone_config=backbone_config,\n             encoder_hidden_dim=self.encoder_hidden_dim,\n             encoder_in_channels=hidden_sizes[1:],"
        },
        {
            "sha": "e0e8f02759614ea38e73b5b3ab41bdcca17e7be6",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -4103,12 +4103,21 @@ def check_attn_implementation_setter(config: PreTrainedConfig, attn_implementati\n                 if isinstance(attribute_value, PreTrainedConfig):\n                     check_attn_implementation_setter(attribute_value, attn_implementation)\n \n-        config._attn_implementation = \"eager\"\n+        # Check that attention implementation can be passed with init args\n+        config_dict = config.to_diff_dict()\n+        config_dict.pop(\"_attn_implementation_internal\", None)\n+        config_dict.pop(\"_attn_implementation\", None)\n+        config_dict[\"attn_implementation\"] = \"eager\"\n+        config = type(config)(**config_dict)\n         check_attn_implementation_setter(config, \"eager\")\n \n+        # Check that attention implementation can be set to different value\n         config._attn_implementation = \"sdpa\"\n         check_attn_implementation_setter(config, \"sdpa\")\n \n+        config._attn_implementation = \"eager\"\n+        check_attn_implementation_setter(config, \"eager\")\n+\n     def test_internal_model_config_and_subconfig_are_same(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         subconfig_keys = list(config.sub_configs.keys())"
        },
        {
            "sha": "e56afce2184adc1ec25037f6aa27c070c707cf00",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1c6310d6a02481d48d81607cba7840be04580d1/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1c6310d6a02481d48d81607cba7840be04580d1/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=d1c6310d6a02481d48d81607cba7840be04580d1",
            "patch": "@@ -312,8 +312,23 @@ def _fix_post_init_location(self, new_body: list[cst.CSTNode]):\n                 break\n         return new_body\n \n-    def _fix_init_location(self, new_body):\n-        \"\"\"Fix the location of the `super().__init__()` in the new body, if we had new statements before it.\"\"\"\n+    def _fix_init_location(self, new_body, original_body):\n+        \"\"\"\n+        Fix the location of the `super().__init__()` in the new body, if we had new statements before it.\n+        If the original class' `super().__init__()` is not in the beginning, do not fix it and leave where it is.\n+        In some cases we do not want to call super() at the very beginning.\n+        \"\"\"\n+        start_index = 0\n+        for i, node in enumerate(original_body):\n+            if m.matches(node, DOCSTRING_NODE) and i == start_index:\n+                start_index += 1\n+                continue\n+            code = self.python_module.code_for_node(node)\n+            comment_less_code = re.sub(r\"#.*\", \"\", code).strip()\n+            comment_less_code = re.sub(r\"\\ *\\n\", \"\\n\", comment_less_code).strip()\n+            if \"super().__init__\" in comment_less_code and i > start_index:\n+                return new_body\n+\n         start_index = 0\n         for i, node in enumerate(new_body):\n             if m.matches(node, DOCSTRING_NODE) and i == start_index:\n@@ -344,7 +359,7 @@ def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.Fu\n                 if self.is_call_to_super(base_statement_node, func_name):\n                     original_modeling_method_body = self.original_modeling_methods[func_name].body.body\n                     new_body.extend(self.update_body(original_modeling_method_body, actual_body[i + 1 :]))\n-                    new_body = self._fix_init_location(new_body)\n+                    new_body = self._fix_init_location(new_body, original_modeling_method_body)\n                     # Break here as all future statement were already accounted for in `update_body`\n                     break\n                 # If not a call to super, this will replace all calls of the form `module.Class.func(...)` by a\n@@ -1039,6 +1054,7 @@ def replace_class_node(\n     # Recreate the whole new class body\n     new_class_body = new_class_docstring + new_class_attributes + new_class_methods\n \n+    # if renamed_super_class == \"Aimv2Config\":\n     # Replace the calls to `super()` of the redefined modular methods with the unrolled code\n     result_node = original_modeling_node.with_changes(body=cst.IndentedBlock(body=new_class_body))\n     temp_module = cst.Module(body=[result_node])"
        }
    ],
    "stats": {
        "total": 1753,
        "additions": 707,
        "deletions": 1046
    }
}