{
    "author": "Cyrilvallez",
    "message": "Add Mistral3 (#36790)\n\n* initial start\n\n* style and dummies\n\n* Create convert_mistral3_weights_to_hf.py\n\n* update\n\n* typo\n\n* typo\n\n* Update convert_mistral3_weights_to_hf.py\n\n* Update convert_mistral3_weights_to_hf.py\n\n* Update convert_mistral3_weights_to_hf.py\n\n* Update convert_mistral3_weights_to_hf.py\n\n* up\n\n* Update convert_mistral3_weights_to_hf.py\n\n* Update convert_mistral3_weights_to_hf.py\n\n* update\n\n* update\n\n* Update image_processing_mistral3.py\n\n* Update convert_mistral3_weights_to_hf.py\n\n* fix patch merger\n\n* Update convert_mistral3_weights_to_hf.py\n\n* Update convert_mistral3_weights_to_hf.py\n\n* up\n\n* update modular to fit\n\n* style\n\n* Update convert_mistral3_weights_to_hf.py\n\n* typo\n\n* Update modular_mistral3.py\n\n* simplify a lot all shape shenanigans\n\n* simplify\n\n* add working test processor\n\n* Add partially working common modeling tests\n\n* All tests working and remove mistral3 image processors\n\n* add docs and fixup\n\n* fix inference with image size >1540\n\n* ðŸš¨fix test image proc pixtral\n\n* Remove vision_feature_select_strategy\n\n* Update convert_mistral3_weights_to_hf.py\n\n* Update convert_mistral3_weights_to_hf.py\n\n* Update convert_mistral3_weights_to_hf.py\n\n* Update convert_mistral3_weights_to_hf.py\n\n* clean\n\n* fix test checkpoints\n\n* Update test_modeling_mistral3.py\n\n* Update test_modeling_mistral3.py\n\n* style\n\n* Use Pixtral processor\n\n* up\n\n* finish cleaning processor to use pixtral directly\n\n* Update __init__.py\n\n* Update processing_pixtral.py\n\n* doc\n\n* Update __init__.py\n\n* Update mistral3.md\n\n* Update _toctree.yml\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: yonigozlan <yoni.gozlan10@gmail.com>",
    "sha": "e959530b8f0011098246572e1777cac06e4bfe73",
    "files": [
        {
            "sha": "79f8eb3d490d71b0ad8df0dfed9abb769dc984c4",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -529,6 +529,8 @@\n         title: MegatronGPT2\n       - local: model_doc/mistral\n         title: Mistral\n+      - local: model_doc/mistral3\n+        title: Mistral3\n       - local: model_doc/mixtral\n         title: Mixtral\n       - local: model_doc/mluke"
        },
        {
            "sha": "5b607294f659b42ee1ae2209998f26c843c1b33c",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "added",
            "additions": 234,
            "deletions": 0,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -0,0 +1,234 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Mistral3\n+\n+## Overview\n+\n+Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.\n+\n+It is ideal for:\n+- Fast-response conversational agents.\n+- Low-latency function calling.\n+- Subject matter experts via fine-tuning.\n+- Local inference for hobbyists and organizations handling sensitive data.\n+- Programming and math reasoning.\n+- Long document understanding.\n+- Visual understanding.\n+\n+This model was contributed by [cyrilvallez](https://huggingface.co/cyrilvallez) and [yonigozlan](https://huggingface.co/yonigozlan).\n+\n+The original code can be found [here](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/pixtral.py) and [here](https://github.com/mistralai/mistral-common).\n+\n+## Usage example\n+\n+### Inference with Pipeline\n+\n+Here is how you can use the `image-text-to-text` pipeline to perform inference with the `Mistral3` models in just a few lines of code:\n+```python\n+>>> from transformers import pipeline\n+\n+>>> messages = [\n+...     {\n+...         \"role\": \"user\",\n+...         \"content\": [\n+...             {\n+...                 \"type\": \"image\",\n+...                 \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\n+...             },\n+...             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+...         ],\n+...     },\n+... ]\n+\n+>>> pipe = pipeline(\"image-text-to-text\", model=\"../mistral3_weights\", torch_dtype=torch.bfloat16)\n+>>> outputs = pipe(text=messages, max_new_tokens=50, return_full_text=False)\n+>>> outputs[0][\"generated_text\"]\n+'The image depicts a vibrant and lush garden scene featuring a variety of wildflowers and plants. The central focus is on a large, pinkish-purple flower, likely a Greater Celandine (Chelidonium majus), with a'\n+```\n+### Inference on a single image\n+\n+This example demonstrates how to perform inference on a single image with the Mistral3 models using chat templates.\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n+\n+>>> torch_device = \"cuda\"\n+>>> model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+\n+>>> messages = [\n+...     {\n+...         \"role\": \"user\",\n+...         \"content\": [\n+...             {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+...             {\"type\": \"text\", \"text\": \"Describe this image\"},\n+...         ],\n+...     }\n+... ]\n+\n+>>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+>>> generate_ids = model.generate(**inputs, max_new_tokens=20)\n+>>> decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+\n+>>> decoded_output\n+\"The image depicts two cats lying on a pink blanket. The larger cat, which appears to be an\"...\n+```\n+\n+### Text-only generation\n+This example shows how to generate text using the Mistral3 model without providing any image input.\n+\n+\n+````python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n+\n+>>> torch_device = \"cuda\"\n+>>> model_checkpoint = \".mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+\n+>>> SYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n+>>> user_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n+\n+>>> messages = [\n+...    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n+...    {\"role\": \"user\", \"content\": user_prompt},\n+... ]\n+\n+>>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+>>> inputs = processor(text=text, return_tensors=\"pt\").to(0, dtype=torch.float16)\n+>>> generate_ids = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n+>>> decoded_output = processor.batch_decode(generate_ids[:, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)[0]\n+\n+>>> print(decoded_output)\n+\"1. Ã€ plus tard!\n+2. Salut, Ã  plus!\n+3. Ã€ toute!\n+4. Ã€ la prochaine!\n+5. Je me casse, Ã  plus!\n+\n+```\n+ /\\_/\\\n+( o.o )\n+ > ^ <\n+```\"\n+````\n+\n+### Batched image and text inputs\n+Mistral3 models also support batched image and text inputs.\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n+\n+>>> torch_device = \"cuda\"\n+>>> model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+\n+>>> messages = [\n+...     [\n+...         {\n+...             \"role\": \"user\",\n+...             \"content\": [\n+...                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+...                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+...             ],\n+...         },\n+...     ],\n+...     [\n+...         {\n+...             \"role\": \"user\",\n+...             \"content\": [\n+...                 {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+...                 {\"type\": \"text\", \"text\": \"Describe this image\"},\n+...             ],\n+...         },\n+...     ],\n+... ]\n+\n+\n+>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+>>> output = model.generate(**inputs, max_new_tokens=25)\n+\n+>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n+>>> decoded_outputs\n+[\"Write a haiku for this imageCalm waters reflect\\nWhispers of the forest's breath\\nPeace on wooden path\"\n+, \"Describe this imageThe image depicts a vibrant street scene in what appears to be a Chinatown district. The focal point is a traditional Chinese\"]\n+```\n+\n+### Batched multi-image input and quantization with BitsAndBytes\n+This implementation of the Mistral3 models supports batched text-images inputs with different number of images for each text.\n+This example also how to use `BitsAndBytes` to load the model in 4bit quantization.\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n+>>> import torch\n+\n+>>> torch_device = \"cuda\"\n+>>> model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+>>> model = AutoModelForImageTextToText.from_pretrained(\n+...     model_checkpoint, quantization_config=quantization_config\n+... )\n+\n+>>> messages = [\n+... Â  Â  [\n+... Â  Â  Â  Â  {\n+... Â  Â  Â  Â  Â  Â  \"role\": \"user\",\n+... Â  Â  Â  Â  Â  Â  \"content\": [\n+... Â  Â  Â  Â  Â  Â  Â  Â  {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+... Â  Â  Â  Â  Â  Â  Â  Â  {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+... Â  Â  Â  Â  Â  Â  ],\n+... Â  Â  Â  Â  },\n+... Â  Â  ],\n+... Â  Â  [\n+... Â  Â  Â  Â  {\n+... Â  Â  Â  Â  Â  Â  \"role\": \"user\",\n+... Â  Â  Â  Â  Â  Â  \"content\": [\n+... Â  Â  Â  Â  Â  Â  Â  Â  {\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"},\n+... Â  Â  Â  Â  Â  Â  Â  Â  {\"type\": \"image\", \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"},\n+... Â  Â  Â  Â  Â  Â  Â  Â  {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n+... Â  Â  Â  Â  Â  Â  ],\n+... Â  Â  Â  Â  },\n+... Â  Â  ],\n+>>> ]\n+\n+>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+>>> output = model.generate(**inputs, max_new_tokens=25)\n+\n+>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n+>>> decoded_outputs\n+[\"Write a haiku for this imageSure, here is a haiku inspired by the image:\\n\\nCalm lake's wooden path\\nSilent forest stands guard\\n\", \"These images depict two different landmarks. Can you identify them? Certainly! The images depict two iconic landmarks:\\n\\n1. The first image shows the Statue of Liberty in New York City.\"]\n+```\n+\n+\n+## Mistral3Config\n+\n+[[autodoc]] Mistral3Config\n+\n+\n+## Mistral3ForConditionalGeneration\n+\n+[[autodoc]] Mistral3ForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "4c6ca0fe1e924615d246484a0ff7c633346de214",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -613,6 +613,7 @@\n     ],\n     \"models.mimi\": [\"MimiConfig\"],\n     \"models.mistral\": [\"MistralConfig\"],\n+    \"models.mistral3\": [\"Mistral3Config\"],\n     \"models.mixtral\": [\"MixtralConfig\"],\n     \"models.mllama\": [\n         \"MllamaConfig\",\n@@ -2940,6 +2941,12 @@\n             \"MistralPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.mistral3\"].extend(\n+        [\n+            \"Mistral3ForConditionalGeneration\",\n+            \"Mistral3PreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.mixtral\"].extend(\n         [\n             \"MixtralForCausalLM\",\n@@ -5788,6 +5795,7 @@\n         MimiConfig,\n     )\n     from .models.mistral import MistralConfig\n+    from .models.mistral3 import Mistral3Config\n     from .models.mixtral import MixtralConfig\n     from .models.mllama import (\n         MllamaConfig,\n@@ -7844,6 +7852,10 @@\n             MistralModel,\n             MistralPreTrainedModel,\n         )\n+        from .models.mistral3 import (\n+            Mistral3ForConditionalGeneration,\n+            Mistral3PreTrainedModel,\n+        )\n         from .models.mixtral import (\n             MixtralForCausalLM,\n             MixtralForQuestionAnswering,"
        },
        {
            "sha": "3a72fca91e91291cb3f607f016f494b35c6d8bb2",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -169,6 +169,7 @@\n     mgp_str,\n     mimi,\n     mistral,\n+    mistral3,\n     mixtral,\n     mllama,\n     mluke,"
        },
        {
            "sha": "0969976937f17e81c9ef02198df9e06e407023ea",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -192,6 +192,7 @@\n         (\"mgp-str\", \"MgpstrConfig\"),\n         (\"mimi\", \"MimiConfig\"),\n         (\"mistral\", \"MistralConfig\"),\n+        (\"mistral3\", \"Mistral3Config\"),\n         (\"mixtral\", \"MixtralConfig\"),\n         (\"mllama\", \"MllamaConfig\"),\n         (\"mobilebert\", \"MobileBertConfig\"),\n@@ -537,6 +538,7 @@\n         (\"mgp-str\", \"MGP-STR\"),\n         (\"mimi\", \"Mimi\"),\n         (\"mistral\", \"Mistral\"),\n+        (\"mistral3\", \"Mistral3\"),\n         (\"mixtral\", \"Mixtral\"),\n         (\"mllama\", \"Mllama\"),\n         (\"mluke\", \"mLUKE\"),"
        },
        {
            "sha": "b1db2458397c745b61634c5c27451f57a04f251d",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -111,6 +111,7 @@\n             (\"mask2former\", (\"Mask2FormerImageProcessor\",)),\n             (\"maskformer\", (\"MaskFormerImageProcessor\",)),\n             (\"mgp-str\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n+            (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"mllama\", (\"MllamaImageProcessor\",)),\n             (\"mobilenet_v1\", (\"MobileNetV1ImageProcessor\",)),\n             (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\",)),"
        },
        {
            "sha": "90dee0eb5805aa7c62a62bea27e76740a98fbacd",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -361,6 +361,7 @@\n         (\"mamba2\", \"Mamba2ForCausalLM\"),\n         (\"mega\", \"MegaForMaskedLM\"),\n         (\"megatron-bert\", \"MegatronBertForPreTraining\"),\n+        (\"mistral3\", \"Mistral3ForConditionalGeneration\"),\n         (\"mllama\", \"MllamaForConditionalGeneration\"),\n         (\"mobilebert\", \"MobileBertForPreTraining\"),\n         (\"mpnet\", \"MPNetForMaskedLM\"),\n@@ -802,6 +803,7 @@\n         (\"llava_next\", \"LlavaNextForConditionalGeneration\"),\n         (\"llava_next_video\", \"LlavaNextVideoForConditionalGeneration\"),\n         (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),\n+        (\"mistral3\", \"Mistral3ForConditionalGeneration\"),\n         (\"mllama\", \"MllamaForConditionalGeneration\"),\n         (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n         (\"pix2struct\", \"Pix2StructForConditionalGeneration\"),\n@@ -839,6 +841,7 @@\n         (\"llava\", \"LlavaForConditionalGeneration\"),\n         (\"llava_next\", \"LlavaNextForConditionalGeneration\"),\n         (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),\n+        (\"mistral3\", \"Mistral3ForConditionalGeneration\"),\n         (\"mllama\", \"MllamaForConditionalGeneration\"),\n         (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n         (\"pix2struct\", \"Pix2StructForConditionalGeneration\"),"
        },
        {
            "sha": "a318d443fb16f7ef3d2f41f0b785e6b52fbd8ac2",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -84,6 +84,7 @@\n         (\"markuplm\", \"MarkupLMProcessor\"),\n         (\"mctct\", \"MCTCTProcessor\"),\n         (\"mgp-str\", \"MgpstrProcessor\"),\n+        (\"mistral3\", \"PixtralProcessor\"),\n         (\"mllama\", \"MllamaProcessor\"),\n         (\"moonshine\", \"Wav2Vec2Processor\"),\n         (\"oneformer\", \"OneFormerProcessor\"),"
        },
        {
            "sha": "11a9fcbdc4ed3935dc549e3cd1992b7550d9bbad",
            "filename": "src/transformers/models/mistral3/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2F__init__.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_mistral3 import *\n+    from .modeling_mistral3 import *\n+    from .processing_mistral3 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e7b27d57220e5e13edd657aa6fe87ef75ef40089",
            "filename": "src/transformers/models/mistral3/configuration_mistral3.py",
            "status": "added",
            "additions": 137,
            "deletions": 0,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconfiguration_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconfiguration_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconfiguration_mistral3.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -0,0 +1,137 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class Mistral3Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Mistral3ForConditionalGeneration`]. It is used to instantiate an\n+    Mistral3 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    [mistralai/Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `PixtralVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `MistralConfig`):\n+            The config object or dictionary of the text backbone.\n+        image_token_index (`int`, *optional*, defaults to 10):\n+            The image token index to encode the image prompt.\n+        projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function used by the multimodal projector.\n+        vision_feature_layer (`Union[int, List[int]]`, *optional*, defaults to -1):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n+        multimodal_projector_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the multimodal projector.\n+        spatial_merge_size (`int`, *optional*, defaults to 2):\n+            The downsampling factor for the spatial merge operation.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Mistral3ForConditionalGeneration, Mistral3Config, PixtralVisionConfig, MistralConfig\n+\n+    >>> # Initializing a Pixtral-vision config\n+    >>> vision_config = PixtralVisionConfig()\n+\n+    >>> # Initializing a Mistral config\n+    >>> text_config = MistralConfig()\n+\n+    >>> # Initializing a Mistral3 configuration\n+    >>> configuration = Mistral3Config(vision_config, text_config)\n+\n+    >>> # Initializing a model from the mistral3.1 configuration\n+    >>> model = Mistral3ForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"mistral3\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n+    is_composition = True\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        image_token_index=10,\n+        projector_hidden_act=\"gelu\",\n+        vision_feature_layer=-1,\n+        multimodal_projector_bias=False,\n+        spatial_merge_size=2,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.image_token_index = image_token_index\n+        self.projector_hidden_act = projector_hidden_act\n+\n+        self.vision_feature_layer = vision_feature_layer\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config[\"model_type\"] if \"model_type\" in vision_config else \"pixtral\"\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif vision_config is None:\n+            vision_config = CONFIG_MAPPING[\"pixtral\"](\n+                intermediate_size=4096,\n+                hidden_size=1024,\n+                patch_size=14,\n+                image_size=1540,\n+                num_hidden_layers=24,\n+                num_attention_heads=16,\n+                vocab_size=32000,\n+                head_dim=64,\n+                hidden_act=\"gelu\",\n+            )\n+\n+        self.vision_config = vision_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"mistral\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"mistral\"](\n+                attention_dropout=0.0,\n+                head_dim=128,\n+                hidden_act=\"silu\",\n+                hidden_size=5120,\n+                initializer_range=0.02,\n+                intermediate_size=32768,\n+                max_position_embeddings=131072,\n+                model_type=\"mistral\",\n+                num_attention_heads=32,\n+                num_hidden_layers=40,\n+                num_key_value_heads=8,\n+                rms_norm_eps=1e-05,\n+                rope_theta=1000000000.0,\n+                sliding_window=None,\n+                use_cache=True,\n+                vocab_size=131072,\n+            )\n+\n+        self.text_config = text_config\n+        self.multimodal_projector_bias = multimodal_projector_bias\n+        self.spatial_merge_size = spatial_merge_size\n+\n+\n+__all__ = [\"Mistral3Config\"]"
        },
        {
            "sha": "11b2d18f04d6f8318c32085dd60496f0f060a410",
            "filename": "src/transformers/models/mistral3/convert_mistral3_weights_to_hf.py",
            "status": "added",
            "additions": 241,
            "deletions": 0,
            "changes": 241,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconvert_mistral3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconvert_mistral3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconvert_mistral3_weights_to_hf.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -0,0 +1,241 @@\n+# Copyright 2023 Mistral AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import json\n+import os\n+import re\n+\n+import torch\n+from safetensors.torch import load_file\n+\n+from transformers import (\n+    Mistral3Config,\n+    Mistral3ForConditionalGeneration,\n+    MistralConfig,\n+    PixtralImageProcessorFast,\n+    PixtralProcessor,\n+    PixtralVisionConfig,\n+)\n+from transformers.integrations.mistral import convert_tekken_tokenizer\n+\n+\n+# fmt: off\n+STATE_DICT_MAPPING = {\n+    # Text model keys\n+    r\"^output.weight\":                            r\"language_model.lm_head.weight\",\n+    r\"^norm.weight\":                              r\"language_model.model.norm.weight\",\n+    r\"^tok_embeddings.weight\":                    r\"language_model.model.embed_tokens.weight\",\n+    r\"^layers.(\\d+).attention_norm.weight\":       r\"language_model.model.layers.\\1.input_layernorm.weight\",\n+    r\"^layers.(\\d+).ffn_norm.weight\":             r\"language_model.model.layers.\\1.post_attention_layernorm.weight\",\n+    r\"^layers.(\\d+).attention.w(q|k|v|o).weight\": r\"language_model.model.layers.\\1.self_attn.\\2_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w1.weight\":      r\"language_model.model.layers.\\1.mlp.gate_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w2.weight\":      r\"language_model.model.layers.\\1.mlp.down_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w3.weight\":      r\"language_model.model.layers.\\1.mlp.up_proj.weight\",\n+\n+    # Vision model keys\n+    r\"vision_encoder.transformer.layers.(\\d+).attention_norm.weight\": r\"vision_tower.transformer.layers.\\1.attention_norm.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).ffn_norm.weight\": r\"vision_tower.transformer.layers.\\1.ffn_norm.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).attention.w(q|k|v|o).weight\": r\"vision_tower.transformer.layers.\\1.attention.\\2_proj.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w1.weight\": r\"vision_tower.transformer.layers.\\1.feed_forward.gate_proj.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w2.weight\": r\"vision_tower.transformer.layers.\\1.feed_forward.down_proj.weight\",\n+    r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w3.weight\": r\"vision_tower.transformer.layers.\\1.feed_forward.up_proj.weight\",\n+    r\"^vision_language_adapter.w_in\": r\"multi_modal_projector.linear_1\",\n+    r\"^vision_language_adapter.w_out\": r\"multi_modal_projector.linear_2\",\n+    r\"^vision_encoder.ln_pre.weight\": r\"vision_tower.ln_pre.weight\",\n+    r\"^vision_encoder.patch_conv.weight\": r\"vision_tower.patch_conv.weight\",\n+    r\"^patch_merger.merging_layer.weight\": r\"multi_modal_projector.patch_merger.merging_layer.weight\",\n+    r\"^pre_mm_projector_norm.weight\": r\"multi_modal_projector.norm.weight\",\n+}\n+# fmt: on\n+\n+\n+def map_old_key_to_new(old_key):\n+    \"\"\"Map of a key of the original state dict to the equivalent key in HF format\"\"\"\n+    for pattern, replacement in STATE_DICT_MAPPING.items():\n+        new_key, n_replace = re.subn(pattern, replacement, old_key)\n+        # Early exit of the loop\n+        if n_replace > 0:\n+            return new_key\n+\n+    raise ValueError(f\"Key: {old_key} could not be mapped (check the mapping).\")\n+\n+\n+def read_json(path):\n+    with open(path, \"r\") as f:\n+        return json.load(f)\n+\n+\n+def permute_for_rope(tensor, n_heads, dim1, dim2):\n+    \"\"\"Permute the weights for the ROPE formulation.\"\"\"\n+    tensor = tensor.view(n_heads, dim1 // n_heads // 2, 2, dim2)\n+    tensor = tensor.transpose(1, 2)\n+    tensor = tensor.reshape(dim1, dim2)\n+    return tensor\n+\n+\n+def convert_state_dict(original_state_dict: dict, config: MistralConfig):\n+    \"\"\"Convert a state dict file, when a single `nn.Module` is never sharded in different files (usual case).\"\"\"\n+    new_dict = {}\n+\n+    for old_key, tensor in original_state_dict.items():\n+        new_key = map_old_key_to_new(old_key)\n+\n+        if \"vision\" in old_key:\n+            num_attention_heads = config.vision_config.num_attention_heads\n+            num_key_value_heads = num_attention_heads\n+            hidden_size = config.vision_config.hidden_size\n+            head_dim = config.vision_config.head_dim\n+            key_value_dim = head_dim * num_attention_heads\n+            query_dim = head_dim * num_attention_heads\n+        else:\n+            num_attention_heads = config.text_config.num_attention_heads\n+            hidden_size = config.text_config.hidden_size\n+            head_dim = config.text_config.head_dim\n+            num_key_value_heads = config.text_config.num_key_value_heads\n+            key_value_dim = head_dim * num_key_value_heads\n+            query_dim = head_dim * num_attention_heads\n+\n+        if \"q_proj\" in new_key:\n+            tensor = permute_for_rope(tensor, num_attention_heads, query_dim, hidden_size)\n+        elif \"k_proj\" in new_key:\n+            tensor = permute_for_rope(tensor, num_key_value_heads, key_value_dim, hidden_size)\n+\n+        new_dict[new_key] = tensor\n+    return new_dict\n+\n+\n+def convert_config(original_config: dict, max_position_embeddings: int = 131072):\n+    original_vision_config = original_config.pop(\"vision_encoder\")\n+    original_text_config = original_config\n+\n+    # Text config\n+    text_key_mapping = {\n+        \"hidden_size\": \"dim\",\n+        \"num_hidden_layers\": \"n_layers\",\n+        \"intermediate_size\": \"hidden_dim\",\n+        \"num_attention_heads\": \"n_heads\",\n+        \"num_key_value_heads\": \"n_kv_heads\",\n+        \"rms_norm_eps\": \"norm_eps\",\n+    }\n+    similar_text_keys_to_keep = [\n+        \"head_dim\",\n+        \"vocab_size\",\n+        \"rope_theta\",\n+    ]\n+    new_text_config_kwargs = {k: original_text_config[v] for k, v in text_key_mapping.items()}\n+    new_text_config_kwargs.update({k: v for k, v in original_text_config.items() if k in similar_text_keys_to_keep})\n+    # These are not always defined depending on `params.json`\n+    new_text_config_kwargs[\"sliding_window\"] = original_text_config.get(\"sliding_window\", None)\n+    new_text_config_kwargs[\"max_position_embeddings\"] = original_text_config.get(\n+        \"max_seq_len\", max_position_embeddings\n+    )\n+    # This may sometimes be a string in `params.json`\n+    if new_text_config_kwargs[\"sliding_window\"] is not None:\n+        new_text_config_kwargs[\"sliding_window\"] = int(new_text_config_kwargs[\"sliding_window\"])\n+    new_text_config = MistralConfig(**new_text_config_kwargs)\n+\n+    # Vision config\n+    new_vision_config = original_vision_config\n+    adapter_bias = new_vision_config.pop(\"adapter_bias\", False)\n+    _ = new_vision_config.pop(\"mm_projector_id\", None)\n+    _ = new_vision_config.pop(\"add_pre_mm_projector_layer_norm\", None)\n+    spatial_merge_size = new_vision_config.pop(\"spatial_merge_size\")\n+    image_token_id = new_vision_config.pop(\"image_token_id\", 10)\n+    _ = new_vision_config.pop(\"image_break_token_id\", 12)\n+    _ = new_vision_config.pop(\"image_end_token_id\", 13)\n+    _ = new_vision_config.pop(\"max_image_size\")\n+    new_vision_config = PixtralVisionConfig(**new_vision_config)\n+\n+    new_config = Mistral3Config(\n+        vision_config=new_vision_config,\n+        text_config=new_text_config,\n+        multimodal_projector_bias=adapter_bias,\n+        image_token_index=image_token_id,\n+        spatial_merge_size=spatial_merge_size,\n+        vision_feature_layer=-1,\n+    )\n+    return new_config\n+\n+\n+def convert_and_write_model(input_dir: str, output_dir: str, max_position_embeddings: int):\n+    \"\"\"Convert the model and save it (this implicitly save the config as well).\"\"\"\n+    params = read_json(os.path.join(input_dir, \"params.json\"))\n+    config = convert_config(params, max_position_embeddings)\n+\n+    full_state_dict = {}\n+    # The model may be split between different files, but a single nn.Module is always fully present in a single file\n+    shards = [file for file in os.listdir(input_dir) if file.endswith(\".safetensors\")]\n+    for shard_file in shards:\n+        original_state_dict = load_file(os.path.join(input_dir, shard_file))\n+        new_dict = convert_state_dict(original_state_dict, config)\n+        full_state_dict.update(new_dict)\n+\n+    # Load weights into model and resave them\n+    with torch.device(\"meta\"):\n+        model = Mistral3ForConditionalGeneration(config)\n+    model.load_state_dict(full_state_dict, strict=True, assign=True)\n+    model.save_pretrained(output_dir)\n+\n+\n+def convert_and_write_processor(input_dir: str, output_dir: str):\n+    \"\"\"Convert the tokenizer and save it.\"\"\"\n+    tokenizer_file = os.path.join(input_dir, \"tekken.json\")\n+    tokenizer = convert_tekken_tokenizer(tokenizer_file)\n+    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n+    chat_template = '{%- if messages[0][\"role\"] == \"system\" %}{%- set system_message = messages[0][\"content\"] %}{%- set loop_messages = messages[1:] %}\\n{%- else %}{%- set loop_messages = messages %}{%- endif %}{{- bos_token }}{%- for message in loop_messages %}{%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) %}{{- raise_exception(\\'After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\\') }}{%- endif %}{%- if message[\"role\"] == \"user\" %}{%- if loop.last and system_message is defined %}{{- \"[INST]\" + system_message + \"\\n\\n\" }}{%- else %}{{ \"[INST]\" }}{%- endif %}{%- endif %}{%- if message[\"content\"] is not string %}{%- for chunk in message[\"content\"] %}{%- if chunk[\"type\"] == \"text\" %}{%- if \"content\" in chunk %}{{- chunk[\"content\"] }}{%- elif \"text\" in chunk %}{{- chunk[\"text\"] }}{%- endif %}{%- elif chunk[\"type\"] == \"image\" %}{{- \"[IMG]\" }}{%- else %}{{- raise_exception(\"Unrecognized content type!\") }}{%- endif %}{%- endfor %}{%- else %}{{- message[\"content\"] }}{%- endif %}{%- if message[\"role\"] == \"user\" %}{{- \"[/INST]\" }}{%- elif message[\"role\"] == \"assistant\" %}{{- eos_token}}{%- else %}{{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}{%- endif %}{%- endfor %}'\n+\n+    config = read_json(os.path.join(input_dir, \"params.json\"))\n+    patch_size = config[\"vision_encoder\"][\"patch_size\"]\n+    spatial_merge_size = config[\"vision_encoder\"][\"spatial_merge_size\"]\n+    max_image_size = config[\"vision_encoder\"][\"max_image_size\"]\n+    image_processor = PixtralImageProcessorFast(patch_size=patch_size, size={\"longest_edge\": max_image_size})\n+\n+    processor = PixtralProcessor(\n+        tokenizer=tokenizer,\n+        image_processor=image_processor,\n+        image_token=\"[IMG]\",\n+        patch_size=patch_size,\n+        chat_template=chat_template,\n+        spatial_merge_size=spatial_merge_size,\n+    )\n+\n+    # Finally save it\n+    processor.save_pretrained(output_dir)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"input_dir\",\n+        help=\"Location of Mistral weights, which contains tokenizer.model and model folders\",\n+    )\n+    parser.add_argument(\n+        \"output_dir\",\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--max_position_embeddings\",\n+        type=int,\n+        default=131072,\n+        help=\"`max_position_embeddings` field in the config. This needs to be manually passed (not present anywhere otherwise).\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    convert_and_write_model(args.input_dir, args.output_dir, args.max_position_embeddings)\n+    convert_and_write_processor(args.input_dir, args.output_dir)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "4ded5efed626b09b60f7f312e3e2eddc0fe5204a",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "added",
            "additions": 553,
            "deletions": 0,
            "changes": 553,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -0,0 +1,553 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/mistral3/modular_mistral3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_mistral3.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n+    replace_return_docstrings,\n+)\n+from ...utils.deprecation import deprecate_kwarg\n+from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_mistral3 import Mistral3Config\n+\n+\n+_CONFIG_FOR_DOC = \"Mistral3Config\"\n+\n+\n+class Mistral3RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Mistral3RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Mistral3PatchMerger(nn.Module):\n+    \"\"\"\n+    Learned merging of spatial_merge_size ** 2 patches\n+    \"\"\"\n+\n+    def __init__(self, config: Mistral3Config):\n+        super().__init__()\n+        self.config = config\n+\n+        hidden_size = config.vision_config.hidden_size\n+        self.spatial_merge_size = config.spatial_merge_size\n+        self.patch_size = self.config.vision_config.patch_size\n+        self.merging_layer = nn.Linear(hidden_size * self.spatial_merge_size**2, hidden_size, bias=False)\n+\n+    def forward(self, image_features: torch.Tensor, image_sizes: torch.Tensor) -> torch.Tensor:\n+        image_sizes = [\n+            (image_size[0] // self.patch_size, image_size[1] // self.patch_size) for image_size in image_sizes\n+        ]\n+\n+        tokens_per_image = [h * w for h, w in image_sizes]\n+        d = image_features.shape[-1]\n+\n+        permuted_tensor = []\n+        for image_index, image_tokens in enumerate(image_features.split(tokens_per_image)):\n+            # Reshape image_tokens into a 2D grid\n+            h, w = image_sizes[image_index]\n+            image_grid = image_tokens.view(h, w, d).permute(2, 0, 1).unsqueeze(0)\n+            grid = torch.nn.functional.unfold(\n+                image_grid, kernel_size=self.spatial_merge_size, stride=self.spatial_merge_size\n+            )\n+            grid = grid.view(d * self.spatial_merge_size**2, -1).t()\n+            permuted_tensor.append(grid)\n+\n+        image_features = torch.cat(permuted_tensor, dim=0)\n+        image_features = self.merging_layer(image_features)\n+        return image_features\n+\n+\n+class Mistral3MultiModalProjector(nn.Module):\n+    def __init__(self, config: Mistral3Config):\n+        super().__init__()\n+        self.norm = Mistral3RMSNorm(config.vision_config.hidden_size)\n+        self.patch_merger = Mistral3PatchMerger(config)\n+        # We have hidden_size * the number of vision feature layers\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size * num_feature_layers,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n+\n+    def forward(self, image_features: torch.Tensor, image_sizes: torch.Tensor):\n+        image_features = self.norm(image_features)\n+        image_features = self.patch_merger(image_features, image_sizes)\n+        hidden_states = self.linear_1(image_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+@dataclass\n+class Mistral3CausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for Mistral3 causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+MISTRAL3_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Mistral3Config`] or [`Mistral3VisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    MISTRAL3_START_DOCSTRING,\n+)\n+class Mistral3PreTrainedModel(PreTrainedModel):\n+    config_class = Mistral3Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Mistral3VisionAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        # important: this ported version of Mistral3 isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n+        # https://github.com/haotian-liu/Mistral3/tree/main/mistral3 should serve for that purpose\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.text_config.initializer_range\n+        )\n+\n+        if hasattr(module, \"class_embedding\"):\n+            module.class_embedding.data.normal_(mean=0.0, std=std)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+MISTRAL3_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`Mistral3Processor`] uses\n+            [`CLIPImageProcessor`] for processing images).\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The MISTRAL3 model which consists of a vision backbone and a language model.\"\"\",\n+    MISTRAL3_START_DOCSTRING,\n+)\n+class Mistral3ForConditionalGeneration(Mistral3PreTrainedModel, GenerationMixin):\n+    def __init__(self, config: Mistral3Config):\n+        super().__init__(config)\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n+\n+        self.multi_modal_projector = Mistral3MultiModalProjector(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n+        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        image_sizes: torch.Tensor,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+            image_sizes (`torch.Tensor`):\n+                Tensor containing the image sizes as returned by the processor.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n+        # this is not memory efficient at all (output_hidden_states=True) will save all the hidden states.\n+        image_outputs = self.vision_tower(pixel_values, image_sizes=image_sizes, output_hidden_states=True, **kwargs)\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [image_outputs.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_image_feature = torch.cat(hs_pool, dim=-1)\n+\n+        image_features = self.multi_modal_projector(selected_image_feature.squeeze(0), image_sizes)\n+        return image_features\n+\n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n+    @add_start_docstrings_to_model_forward(MISTRAL3_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Mistral3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        image_sizes: torch.Tensor = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, Mistral3CausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Mistral3ForConditionalGeneration\n+\n+        >>> model = Mistral3ForConditionalGeneration.from_pretrained(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\")\n+        >>> processor = AutoProcessor.from_pretrained(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\")\n+\n+        >>> prompt = \"<s>[INST][IMG]What is the image?[/INST]\"\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_new_tokens=15)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"What is the image?The image depicts two cats lying on a pink blanket.\"\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                image_sizes=image_sizes,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            if attention_mask is not None:\n+                # we use the input attention mask to shift the logits and labels, because it is 2D.\n+                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n+                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n+                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n+                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n+            else:\n+                shift_logits = logits[..., :-1, :].contiguous()\n+                shift_labels = labels[..., 1:].contiguous()\n+            # Flatten the tokens\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(\n+                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return Mistral3CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"Mistral3PreTrainedModel\", \"Mistral3ForConditionalGeneration\"]"
        },
        {
            "sha": "9d1edf97bd9f8a849aac08a4b930de93bdb63186",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "added",
            "additions": 286,
            "deletions": 0,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -0,0 +1,286 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...utils import is_torchdynamo_compiling, logging\n+from ..llava.modeling_llava import LlavaCausalLMOutputWithPast, LlavaForConditionalGeneration\n+from ..mistral.modeling_mistral import MistralRMSNorm\n+from .configuration_mistral3 import Mistral3Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Mistral3RMSNorm(MistralRMSNorm):\n+    pass\n+\n+\n+class Mistral3PatchMerger(nn.Module):\n+    \"\"\"\n+    Learned merging of spatial_merge_size ** 2 patches\n+    \"\"\"\n+\n+    def __init__(self, config: Mistral3Config):\n+        super().__init__()\n+        self.config = config\n+\n+        hidden_size = config.vision_config.hidden_size\n+        self.spatial_merge_size = config.spatial_merge_size\n+        self.patch_size = self.config.vision_config.patch_size\n+        self.merging_layer = nn.Linear(hidden_size * self.spatial_merge_size**2, hidden_size, bias=False)\n+\n+    def forward(self, image_features: torch.Tensor, image_sizes: torch.Tensor) -> torch.Tensor:\n+        image_sizes = [\n+            (image_size[0] // self.patch_size, image_size[1] // self.patch_size) for image_size in image_sizes\n+        ]\n+\n+        tokens_per_image = [h * w for h, w in image_sizes]\n+        d = image_features.shape[-1]\n+\n+        permuted_tensor = []\n+        for image_index, image_tokens in enumerate(image_features.split(tokens_per_image)):\n+            # Reshape image_tokens into a 2D grid\n+            h, w = image_sizes[image_index]\n+            image_grid = image_tokens.view(h, w, d).permute(2, 0, 1).unsqueeze(0)\n+            grid = torch.nn.functional.unfold(\n+                image_grid, kernel_size=self.spatial_merge_size, stride=self.spatial_merge_size\n+            )\n+            grid = grid.view(d * self.spatial_merge_size**2, -1).t()\n+            permuted_tensor.append(grid)\n+\n+        image_features = torch.cat(permuted_tensor, dim=0)\n+        image_features = self.merging_layer(image_features)\n+        return image_features\n+\n+\n+class Mistral3MultiModalProjector(nn.Module):\n+    def __init__(self, config: Mistral3Config):\n+        super().__init__()\n+        self.norm = Mistral3RMSNorm(config.vision_config.hidden_size)\n+        self.patch_merger = Mistral3PatchMerger(config)\n+        # We have hidden_size * the number of vision feature layers\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size * num_feature_layers,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n+\n+    def forward(self, image_features: torch.Tensor, image_sizes: torch.Tensor):\n+        image_features = self.norm(image_features)\n+        image_features = self.patch_merger(image_features, image_sizes)\n+        hidden_states = self.linear_1(image_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+class Mistral3CausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n+    pass\n+\n+\n+class Mistral3ForConditionalGeneration(LlavaForConditionalGeneration):\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        image_sizes: torch.Tensor,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+            image_sizes (`torch.Tensor`):\n+                Tensor containing the image sizes as returned by the processor.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n+        # this is not memory efficient at all (output_hidden_states=True) will save all the hidden states.\n+        image_outputs = self.vision_tower(pixel_values, image_sizes=image_sizes, output_hidden_states=True, **kwargs)\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [image_outputs.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_image_feature = torch.cat(hs_pool, dim=-1)\n+\n+        image_features = self.multi_modal_projector(selected_image_feature.squeeze(0), image_sizes)\n+        return image_features\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        image_sizes: torch.Tensor = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, Mistral3CausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Mistral3ForConditionalGeneration\n+\n+        >>> model = Mistral3ForConditionalGeneration.from_pretrained(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\")\n+        >>> processor = AutoProcessor.from_pretrained(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\")\n+\n+        >>> prompt = \"<s>[INST][IMG]What is the image?[/INST]\"\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_new_tokens=15)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"What is the image?The image depicts two cats lying on a pink blanket.\"\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                image_sizes=image_sizes,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            if attention_mask is not None:\n+                # we use the input attention mask to shift the logits and labels, because it is 2D.\n+                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n+                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n+                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n+                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n+            else:\n+                shift_logits = logits[..., :-1, :].contiguous()\n+                shift_labels = labels[..., 1:].contiguous()\n+            # Flatten the tokens\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(\n+                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return Mistral3CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+__all__ = [\n+    \"Mistral3PreTrainedModel\",  # noqa\n+    \"Mistral3ForConditionalGeneration\",\n+]"
        },
        {
            "sha": "2cb452863a76f9b368759aba9e6d878a7d654dc2",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -128,8 +128,9 @@ def get_resize_output_image_size(\n \n     if ratio > 1:\n         # Orgiginal implementation uses `round` which utilises bankers rounding, which can lead to surprising results\n-        height = int(math.ceil(height / ratio))\n-        width = int(math.ceil(width / ratio))\n+        # Here we use floor to ensure the image is always smaller than the given \"longest_edge\"\n+        height = int(math.floor(height / ratio))\n+        width = int(math.floor(width / ratio))\n \n     num_height_tokens, num_width_tokens = _num_image_tokens((height, width), (patch_height, patch_width))\n     return num_height_tokens * patch_height, num_width_tokens * patch_width"
        },
        {
            "sha": "66da1bf9f780c0138c9dea36d63217987e73230e",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -64,6 +64,8 @@ class PixtralProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n         patch_size (`int`, *optional*, defaults to 16):\n             Patch size from the vision tower.\n+        spatial_merge_size (`int`, *optional*, defaults to 1):\n+            The downsampling factor for the spatial merge operation.\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n         image_token (`str`, *optional*, defaults to `\"[IMG]\"`):\n@@ -78,6 +80,7 @@ class PixtralProcessor(ProcessorMixin):\n     valid_kwargs = [\n         \"chat_template\",\n         \"patch_size\",\n+        \"spatial_merge_size\",\n         \"image_token\",\n         \"image_break_token\",\n         \"image_end_token\",\n@@ -90,13 +93,15 @@ def __init__(\n         image_processor=None,\n         tokenizer=None,\n         patch_size: int = 16,\n+        spatial_merge_size: int = 1,\n         chat_template=None,\n         image_token=\"[IMG]\",  # set the default and let users change if they have peculiar special tokens in rare cases\n         image_break_token=\"[IMG_BREAK]\",\n         image_end_token=\"[IMG_END]\",\n         **kwargs,\n     ):\n         self.patch_size = patch_size\n+        self.spatial_merge_size = spatial_merge_size\n         self.image_token = image_token\n         self.image_break_token = image_break_token\n         self.image_end_token = image_end_token\n@@ -187,8 +192,8 @@ def __call__(\n             for sample in text:\n                 while self.image_token in sample:\n                     height, width = next(image_sizes)\n-                    num_height_tokens = height // self.patch_size\n-                    num_width_tokens = width // self.patch_size\n+                    num_height_tokens = height // (self.patch_size * self.spatial_merge_size)\n+                    num_width_tokens = width // (self.patch_size * self.spatial_merge_size)\n                     replace_tokens = [\n                         [self.image_token] * num_width_tokens + [self.image_break_token]\n                     ] * num_height_tokens"
        },
        {
            "sha": "c4532ead938c3078276e40c682303dda36f25c22",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -6392,6 +6392,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Mistral3ForConditionalGeneration(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Mistral3PreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class MixtralForCausalLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e8e14b0497e1762114790421b0f6fef3e0a7bf3c",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -125,6 +125,7 @@\n     \"qwen2_5_vl\",\n     \"ayavision\",\n     \"gemma3\",\n+    \"mistral3\",\n ]\n \n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/mistral3/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fmodels%2Fmistral3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fmodels%2Fmistral3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2F__init__.py?ref=e959530b8f0011098246572e1777cac06e4bfe73"
        },
        {
            "sha": "d6f225b56134beba9add7100077bcc8208685408",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "added",
            "additions": 482,
            "deletions": 0,
            "changes": 482,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -0,0 +1,482 @@\n+# coding=utf-8\n+# Copyright 2024 The Qwen team, Alibaba Group and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch GotOcr2 model.\"\"\"\n+\n+import unittest\n+\n+from transformers import (\n+    AutoProcessor,\n+    Mistral3Config,\n+    is_bitsandbytes_available,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_bitsandbytes,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Mistral3ForConditionalGeneration,\n+    )\n+\n+\n+if is_bitsandbytes_available():\n+    from transformers import BitsAndBytesConfig\n+\n+\n+class Mistral3VisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        seq_length=7,\n+        image_seq_length=4,\n+        vision_feature_layer=-1,\n+        ignore_index=-100,\n+        bos_token_id=0,\n+        eos_token_id=0,\n+        pad_token_id=0,\n+        image_token_index=1,\n+        num_channels=3,\n+        image_size=30,\n+        model_type=\"mistral3\",\n+        is_training=True,\n+        text_config={\n+            \"model_type\": \"mistral\",\n+            \"vocab_size\": 99,\n+            \"attention_dropout\": 0.0,\n+            \"hidden_act\": \"silu\",\n+            \"hidden_size\": 32,\n+            \"initializer_range\": 0.02,\n+            \"intermediate_size\": 37,\n+            \"max_position_embeddings\": 512,\n+            \"num_attention_heads\": 4,\n+            \"num_hidden_layers\": 2,\n+            \"num_key_value_heads\": 2,\n+            \"rms_norm_eps\": 1e-05,\n+            \"rope_theta\": 1000000000.0,\n+            \"sliding_window\": None,\n+            \"bos_token_id\": 0,\n+            \"eos_token_id\": 0,\n+            \"pad_token_id\": 0,\n+        },\n+        vision_config={\n+            \"model_type\": \"pixtral\",\n+            \"hidden_size\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"image_size\": 30,\n+            \"patch_size\": 6,\n+            \"num_channels\": 3,\n+            \"hidden_act\": \"gelu\",\n+        },\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.image_token_index = image_token_index\n+        self.model_type = model_type\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.batch_size = batch_size\n+        self.vision_feature_layer = vision_feature_layer\n+        self.is_training = is_training\n+        self.image_seq_length = image_seq_length\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.seq_length = seq_length + self.image_seq_length\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+\n+    def get_config(self):\n+        return Mistral3Config(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            model_type=self.model_type,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+            image_token_index=self.image_token_index,\n+            image_seq_length=self.image_seq_length,\n+            vision_feature_layer=self.vision_feature_layer,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+        image_sizes = torch.tensor(\n+            [[self.image_size, self.image_size]] * self.batch_size, dtype=torch.long, device=torch_device\n+        )\n+\n+        # input_ids[:, -1] = self.pad_token_id\n+        input_ids[input_ids == self.image_token_index] = self.pad_token_id\n+        input_ids[:, : self.image_seq_length] = self.image_token_index\n+\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"image_sizes\": image_sizes,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n+        model = Mistral3ForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.half()\n+        model.eval()\n+        logits = model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            pixel_values=pixel_values.to(torch.bfloat16),\n+            return_dict=True,\n+        )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+    def create_and_check_model_fp16_autocast_forward(self, config, input_ids, pixel_values, attention_mask):\n+        config.torch_dtype = torch.float16\n+        model = Mistral3ForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n+            logits = model(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                pixel_values=pixel_values.to(torch.bfloat16),\n+                return_dict=True,\n+            )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+\n+@require_torch\n+class Mistral3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (Mistral3ForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (Mistral3ForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"image-text-to-text\": Mistral3ForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    _is_composite = True\n+    test_headmasking = False\n+    test_pruning = False\n+\n+    def setUp(self):\n+        self.model_tester = Mistral3VisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Mistral3Config, has_text_modality=False)\n+\n+    def test_config(self):\n+        # overwritten from `tests/test_configuration_common.py::ConfigTester` after #36077\n+        # TODO: avoid overwritten once there is a better fix for #36077\n+        def check_config_can_be_init_without_params():\n+            config = self.config_tester.config_class()\n+            self.config_tester.parent.assertIsNotNone(config)\n+\n+        self.config_tester.check_config_can_be_init_without_params = check_config_can_be_init_without_params\n+        self.config_tester.run_common_tests()\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n+    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\"Pixtral does not support attention interfaces.\")\n+    def test_eager_matches_fa2_generate(self):\n+        pass\n+\n+    @unittest.skip(\"Pixtral does not support attention interfaces.\")\n+    def test_eager_matches_sdpa_generate(self):\n+        pass\n+\n+    @unittest.skip(\"Pixtral does not support attention interfaces.\")\n+    def test_flash_attn_2_from_config(self):\n+        pass\n+\n+    @unittest.skip(\"Pixtral does not support attention interfaces.\")\n+    def test_flash_attn_2_inference_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\"Pixtral does not support attention interfaces.\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+    @unittest.skip(\"Pixtral does not support attention interfaces.\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+\n+@slow\n+@require_torch_gpu\n+class Mistral3IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_mistral3_integration_generate_text_only(self):\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        model = Mistral3ForConditionalGeneration.from_pretrained(\n+            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"Write a haiku\"},\n+                ],\n+            }\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device, dtype=torch.bfloat16)\n+\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+        expected_output = \"Sure, here's a haiku for you:\\n\\nWhispers of the breeze,\\nCherry blossoms softly fall,\\nSpring's gentle embrace.\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    def test_mistral3_integration_generate(self):\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        model = Mistral3ForConditionalGeneration.from_pretrained(\n+            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Describe this image\"},\n+                ],\n+            }\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device, dtype=torch.bfloat16)\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+        expected_output = \"The image depicts two cats lying on a pink blanket. The larger cat, which appears to be an\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    def test_mistral3_integration_batched_generate(self):\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        model = Mistral3ForConditionalGeneration.from_pretrained(\n+            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+                        {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                        {\"type\": \"text\", \"text\": \"Describe this image\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(model.device, dtype=torch.bfloat16)\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        # Check first output\n+        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n+        expected_output = \"Write a haiku for this imageSure, here is a haiku inspired by the image:\\n\\nCalm lake's mirror gleams,\\nWhispering pines\"\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check second output\n+        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n+        expected_output = \"Describe this imageThe image depicts a vibrant street scene in what appears to be a Chinatown district. The focal point is a traditional Chinese\"\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+    @require_bitsandbytes\n+    def test_mistral3_integration_batched_generate_multi_image(self):\n+        processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n+        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+        model = Mistral3ForConditionalGeneration.from_pretrained(\n+            self.model_checkpoint, quantization_config=quantization_config\n+        )\n+\n+        # Prepare inputs\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+                        {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                        },\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+                        },\n+                        {\n+                            \"type\": \"text\",\n+                            \"text\": \"These images depict two different landmarks. Can you identify them?\",\n+                        },\n+                    ],\n+                },\n+            ],\n+        ]\n+        inputs = processor.apply_chat_template(\n+            messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(model.device, dtype=torch.float16)\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        # Check first output\n+        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n+        expected_output = \"Write a haiku for this imageSure, here is a haiku inspired by the image:\\n\\nCalm lake's wooden path\\nSilent forest stands guard\\n\"\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check second output\n+        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n+        expected_output = \"These images depict two different landmarks. Can you identify them?Certainly! The images depict two iconic landmarks:\\n\\n1. The first image shows the Statue of Liberty in New York City.\"\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )"
        },
        {
            "sha": "da9c9a759a3006be644b9f84c2d2858d36709d0f",
            "filename": "tests/models/mistral3/test_processor_mistral3.py",
            "status": "added",
            "additions": 293,
            "deletions": 0,
            "changes": 293,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -0,0 +1,293 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+import requests\n+\n+from transformers import PixtralProcessor\n+from transformers.testing_utils import require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+@require_vision\n+class Mistral3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    \"\"\"This tests Pixtral processor with the new `spatial_merge_size` argument in Mistral3.\"\"\"\n+\n+    processor_class = PixtralProcessor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.url_0 = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        cls.image_0 = Image.open(requests.get(cls.url_0, stream=True).raw)\n+        cls.url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        cls.image_1 = Image.open(requests.get(cls.url_1, stream=True).raw)\n+        cls.url_2 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n+        cls.image_2 = Image.open(requests.get(cls.url_2, stream=True).raw)\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        processor = PixtralProcessor.from_pretrained(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\")\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def test_chat_template(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        expected_prompt = \"<s>[INST][IMG]What is shown in this image?[/INST]\"\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(expected_prompt, formatted_prompt)\n+\n+    def test_image_token_filling(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        # Important to check with non square image\n+        image = torch.randint(0, 2, (3, 500, 316))\n+        expected_image_tokens = 198\n+        image_token_index = 10\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        inputs = processor(\n+            text=[processor.apply_chat_template(messages)],\n+            images=[image],\n+            return_tensors=\"pt\",\n+        )\n+        image_tokens = (inputs[\"input_ids\"] == image_token_index).sum().item()\n+        self.assertEqual(expected_image_tokens, image_tokens)\n+\n+    def test_processor_with_single_image(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        prompt_string = \"USER: [IMG]\\nWhat's the content of the image? ASSISTANT:\"\n+\n+        # Make small for checking image token expansion\n+        processor.image_processor.size = {\"longest_edge\": 30}\n+        processor.patch_size = 6\n+\n+        # Test passing in an image\n+        inputs_image = processor(text=prompt_string, images=self.image_0, return_tensors=\"pt\")\n+        self.assertIn(\"input_ids\", inputs_image)\n+        self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n+        self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 30]))\n+\n+        # fmt: off\n+        input_ids = inputs_image[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to \"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the content of the image? ASSISTANT:\"\n+            [1, 21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+        )\n+        # fmt: on\n+\n+        # Test passing in a url\n+        inputs_url = processor(text=prompt_string, images=self.url_0, return_tensors=\"pt\")\n+        self.assertIn(\"input_ids\", inputs_url)\n+        self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n+        self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 30]))\n+\n+        # fmt: off\n+        input_ids = inputs_url[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to \"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the content of the image? ASSISTANT:\"\n+            [1, 21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+        )\n+        # fmt: on\n+\n+        # Test passing inputs as a single list\n+        inputs_image = processor(text=prompt_string, images=[self.image_0], return_tensors=\"pt\")\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 30]))\n+\n+        # fmt: off\n+        self.assertEqual(\n+            inputs_image[\"input_ids\"][0].tolist(),\n+            [1, 21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+        )\n+        # fmt: on\n+\n+        # Test as nested single list\n+        inputs_image = processor(text=prompt_string, images=[[self.image_0]], return_tensors=\"pt\")\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 30]))\n+\n+        # fmt: off\n+        self.assertEqual(\n+            inputs_image[\"input_ids\"][0].tolist(),\n+            [1, 21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+        )\n+        # fmt: on\n+\n+    def test_processor_with_multiple_images_single_list(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        prompt_string = \"USER: [IMG][IMG]\\nWhat's the difference between these two images? ASSISTANT:\"\n+\n+        # Make small for checking image token expansion\n+        processor.image_processor.size = {\"longest_edge\": 30}\n+        processor.patch_size = 6\n+\n+        # Test passing in an image\n+        inputs_image = processor(text=prompt_string, images=[self.image_0, self.image_1], return_tensors=\"pt\")\n+        self.assertIn(\"input_ids\", inputs_image)\n+        self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n+        self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 24, 30]))\n+\n+        # fmt: off\n+        input_ids = inputs_image[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n+            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+                    )\n+        # fmt: on\n+\n+        # Test passing in a url\n+        inputs_url = processor(text=prompt_string, images=[self.url_0, self.url_1], return_tensors=\"pt\")\n+        self.assertIn(\"input_ids\", inputs_url)\n+        self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n+        self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 24, 30]))\n+\n+        # fmt: off\n+        input_ids = inputs_url[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n+            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n+        # Test passing in as a nested list\n+        inputs_url = processor(text=prompt_string, images=[[self.image_0, self.image_1]], return_tensors=\"pt\")\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 24, 30]))\n+\n+        # fmt: off\n+        self.assertEqual(\n+            inputs_url[\"input_ids\"][0].tolist(),\n+            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n+    def test_processor_with_multiple_images_multiple_lists(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        prompt_string = [\n+            \"USER: [IMG][IMG]\\nWhat's the difference between these two images? ASSISTANT:\",\n+            \"USER: [IMG]\\nWhat's the content of the image? ASSISTANT:\",\n+        ]\n+        processor.tokenizer.pad_token = \"</s>\"\n+        image_inputs = [[self.image_0, self.image_1], [self.image_2]]\n+\n+        # Make small for checking image token expansion\n+        processor.image_processor.size = {\"longest_edge\": 30}\n+        processor.patch_size = 6\n+\n+        # Test passing in an image\n+        inputs_image = processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n+        self.assertIn(\"input_ids\", inputs_image)\n+        self.assertTrue(len(inputs_image[\"input_ids\"]) == 2)\n+        self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 30, 30]))\n+\n+        # fmt: off\n+        input_ids = inputs_image[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n+            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n+        # Test passing in a url\n+        inputs_url = processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n+        self.assertIn(\"input_ids\", inputs_url)\n+        self.assertTrue(len(inputs_url[\"input_ids\"]) == 2)\n+        self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 30, 30]))\n+\n+        # fmt: off\n+        input_ids = inputs_url[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n+            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n+        # Test passing as a single flat list\n+        inputs_image = processor(\n+            text=prompt_string, images=[self.image_0, self.image_1, self.image_2], return_tensors=\"pt\", padding=True\n+        )\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 30, 30]))\n+\n+        # fmt: off\n+        self.assertEqual(\n+            inputs_image[\"input_ids\"][0].tolist(),\n+            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n+    def test_processor_returns_full_length_batches(self):\n+        # to avoid https://github.com/huggingface/transformers/issues/34204\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        prompt_string = [\n+            \"USER: [IMG]\\nWhat's the content of the image? ASSISTANT:\",\n+        ] * 5\n+        processor.tokenizer.pad_token = \"</s>\"\n+        image_inputs = [[self.image_0]] * 5\n+\n+        # Make small for checking image token expansion\n+        processor.image_processor.size = {\"longest_edge\": 30}\n+        processor.patch_size = 6\n+\n+        # Test passing in an image\n+        inputs_image = processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n+        self.assertIn(\"input_ids\", inputs_image)\n+        self.assertTrue(len(inputs_image[\"input_ids\"]) == 5)"
        },
        {
            "sha": "3f9deded6d8bd4f61e231b6a94e28f28efac3812",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e959530b8f0011098246572e1777cac06e4bfe73/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=e959530b8f0011098246572e1777cac06e4bfe73",
            "patch": "@@ -109,8 +109,8 @@ def expected_output_image_shape(self, images):\n \n             ratio = max(height / max_height, width / max_width)\n             if ratio > 1:\n-                height = int(np.ceil(height / ratio))\n-                width = int(np.ceil(width / ratio))\n+                height = int(np.floor(height / ratio))\n+                width = int(np.floor(width / ratio))\n \n             patch_height, patch_width = self.patch_size[\"height\"], self.patch_size[\"width\"]\n             num_height_tokens = (height - 1) // patch_height + 1"
        }
    ],
    "stats": {
        "total": 2309,
        "additions": 2303,
        "deletions": 6
    }
}