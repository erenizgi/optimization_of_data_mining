{
    "author": "rwightman",
    "message": "ðŸš¨ðŸš¨ðŸš¨ An attempt to fix #29554. Include 'LayerNorm.' in gamma/beta rename scope, optimize string search. (#35615)\n\n* An attempt to fix #29554. Include 'LayerNorm.' in gamma/beta rename scope, reduce number of characters searched on every load considerably.\r\n\r\n* Fix fix on load issue\r\n\r\n* Fix gamma/beta warning test\r\n\r\n* A style complaint\r\n\r\n* Improve efficiency of weight norm key rename. Add better comments about weight norm and layer norm renaming.\r\n\r\n* Habitual elif redunant with the return",
    "sha": "8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3",
    "files": [
        {
            "sha": "d003c02c85e79eb9a5ca2d00b503670572fc2175",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 27,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3",
            "patch": "@@ -4367,26 +4367,31 @@ def from_pretrained(\n         return model\n \n     @staticmethod\n-    def _fix_state_dict_key_on_load(key):\n+    def _fix_state_dict_key_on_load(key) -> Tuple[str, bool]:\n         \"\"\"Replace legacy parameter names with their modern equivalents. E.g. beta -> bias, gamma -> weight.\"\"\"\n \n-        if \"beta\" in key:\n-            return key.replace(\"beta\", \"bias\")\n-        if \"gamma\" in key:\n-            return key.replace(\"gamma\", \"weight\")\n+        # Rename LayerNorm beta & gamma params for some early models ported from Tensorflow (e.g. Bert)\n+        # This rename is logged.\n+        if key.endswith(\"LayerNorm.beta\"):\n+            return key.replace(\"LayerNorm.beta\", \"LayerNorm.bias\"), True\n+        if key.endswith(\"LayerNorm.gamma\"):\n+            return key.replace(\"LayerNorm.gamma\", \"LayerNorm.weight\"), True\n \n-        # to avoid logging parametrized weight norm renaming\n+        # Rename weight norm parametrizations to match changes across torch versions.\n+        # Impacts a number of speech/wav2vec models. e.g. Hubert, Wav2Vec2, and others.\n+        # This rename is not logged.\n         if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n-            if \"weight_g\" in key:\n-                return key.replace(\"weight_g\", \"parametrizations.weight.original0\")\n-            if \"weight_v\" in key:\n-                return key.replace(\"weight_v\", \"parametrizations.weight.original1\")\n+            if key.endswith(\"weight_g\"):\n+                return key.replace(\"weight_g\", \"parametrizations.weight.original0\"), True\n+            if key.endswith(\"weight_v\"):\n+                return key.replace(\"weight_v\", \"parametrizations.weight.original1\"), True\n         else:\n-            if \"parametrizations.weight.original0\" in key:\n-                return key.replace(\"parametrizations.weight.original0\", \"weight_g\")\n-            if \"parametrizations.weight.original1\" in key:\n-                return key.replace(\"parametrizations.weight.original1\", \"weight_v\")\n-        return key\n+            if key.endswith(\"parametrizations.weight.original0\"):\n+                return key.replace(\"parametrizations.weight.original0\", \"weight_g\"), True\n+            if key.endswith(\"parametrizations.weight.original1\"):\n+                return key.replace(\"parametrizations.weight.original1\", \"weight_v\"), True\n+\n+        return key, False\n \n     @classmethod\n     def _fix_state_dict_keys_on_load(cls, state_dict):\n@@ -4397,15 +4402,15 @@ def _fix_state_dict_keys_on_load(cls, state_dict):\n         renamed_keys = {}\n         state_dict_keys = list(state_dict.keys())\n         for key in state_dict_keys:\n-            new_key = cls._fix_state_dict_key_on_load(key)\n-            if new_key != key:\n+            new_key, has_changed = cls._fix_state_dict_key_on_load(key)\n+            if has_changed:\n                 state_dict[new_key] = state_dict.pop(key)\n \n-            # add it once for logging\n-            if \"gamma\" in key and \"gamma\" not in renamed_keys:\n-                renamed_keys[\"gamma\"] = (key, new_key)\n-            if \"beta\" in key and \"beta\" not in renamed_keys:\n-                renamed_keys[\"beta\"] = (key, new_key)\n+                # track gamma/beta rename for logging\n+                if key.endswith(\"LayerNorm.gamma\"):\n+                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n+                elif key.endswith(\"LayerNorm.beta\"):\n+                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n \n         if renamed_keys:\n             warning_msg = f\"A pretrained model of type `{cls.__name__}` \"\n@@ -4418,19 +4423,19 @@ def _fix_state_dict_keys_on_load(cls, state_dict):\n         return state_dict\n \n     @staticmethod\n-    def _fix_state_dict_key_on_save(key):\n+    def _fix_state_dict_key_on_save(key) -> Tuple[str, bool]:\n         \"\"\"\n         Similar to `_fix_state_dict_key_on_load` allows to define hook for state dict key renaming on model save.\n-        Do nothing by default, but can be overriden in particular models.\n+        Do nothing by default, but can be overridden in particular models.\n         \"\"\"\n-        return key\n+        return key, False\n \n     def _fix_state_dict_keys_on_save(self, state_dict):\n         \"\"\"\n         Similar to `_fix_state_dict_keys_on_load` allows to define hook for state dict key renaming on model save.\n         Apply `_fix_state_dict_key_on_save` to all keys in `state_dict`.\n         \"\"\"\n-        return {self._fix_state_dict_key_on_save(key): value for key, value in state_dict.items()}\n+        return {self._fix_state_dict_key_on_save(key)[0]: value for key, value in state_dict.items()}\n \n     @classmethod\n     def _load_pretrained_model(\n@@ -4488,7 +4493,7 @@ def _load_pretrained_model(\n             expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, loaded_keys)\n \n         original_loaded_keys = loaded_keys\n-        loaded_keys = [cls._fix_state_dict_key_on_load(key) for key in loaded_keys]\n+        loaded_keys = [cls._fix_state_dict_key_on_load(key)[0] for key in loaded_keys]\n \n         if len(prefix) > 0:\n             has_prefix_module = any(s.startswith(prefix) for s in loaded_keys)"
        },
        {
            "sha": "a74202ce5aa53c4584ad8059ff7a44736262adab",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3",
            "patch": "@@ -90,22 +90,22 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n     @staticmethod\n-    def _fix_state_dict_key_on_load(key):\n+    def _fix_state_dict_key_on_load(key) -> Tuple[str, bool]:\n         \"\"\"\n         Overrides original method that renames `gamma` and `beta` to `weight` and `bias`.\n         We don't want this behavior for timm wrapped models. Instead, this method adds a\n         \"timm_model.\" prefix to enable loading official timm Hub checkpoints.\n         \"\"\"\n         if \"timm_model.\" not in key:\n-            return f\"timm_model.{key}\"\n-        return key\n+            return f\"timm_model.{key}\", True\n+        return key, False\n \n     def _fix_state_dict_key_on_save(self, key):\n         \"\"\"\n         Overrides original method to remove \"timm_model.\" prefix from state_dict keys.\n         Makes the saved checkpoint compatible with the `timm` library.\n         \"\"\"\n-        return key.replace(\"timm_model.\", \"\")\n+        return key.replace(\"timm_model.\", \"\"), True\n \n     def load_state_dict(self, state_dict, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "84b5ebbb24cea0de8fbb54f06a5b9e3641a0db76",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 34,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=8c1b5d37827a6691fef4b2d926f2d04fb6f5a9e3",
            "patch": "@@ -1618,57 +1618,47 @@ def test_model_from_pretrained_from_mlx(self):\n             self.assertTrue(torch.allclose(outputs_from_saved[\"logits\"], outputs[\"logits\"]))\n \n     def test_warning_for_beta_gamma_parameters(self):\n-        class TestModelGamma(PreTrainedModel):\n+        class TestGammaBetaNorm(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.gamma = torch.nn.Parameter(torch.ones(1))\n+                self.beta = torch.nn.Parameter(torch.zeros(1))\n+\n+            def forward(self):\n+                return self.gamma.sum() + self.beta.sum()\n+\n+        class TestModelGammaBeta(PreTrainedModel):\n             def __init__(self, config):\n                 super().__init__(config)\n-                self.gamma_param = nn.Parameter(torch.ones(10))\n+                self.LayerNorm = TestGammaBetaNorm()\n                 self.post_init()\n \n             def forward(self):\n-                return self.gamma_param.sum()\n+                return self.LayerNorm()\n \n         logger = logging.get_logger(\"transformers.modeling_utils\")\n         config = PretrainedConfig()\n-        warning_msg_gamma = \"`gamma_param` -> `weight_param`\"\n-        model = TestModelGamma(config)\n+        warning_msg_gamma = \"`LayerNorm.gamma` -> `LayerNorm.weight`\"\n+        warning_msg_beta = \"`LayerNorm.beta` -> `LayerNorm.bias`\"\n+        model = TestModelGammaBeta(config)\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n             model.save_pretrained(tmp_dir)\n             with LoggingLevel(logging.INFO):\n                 with CaptureLogger(logger) as cl1:\n-                    _, loading_info = TestModelGamma.from_pretrained(tmp_dir, config=config, output_loading_info=True)\n+                    _, loading_info = TestModelGammaBeta.from_pretrained(\n+                        tmp_dir, config=config, output_loading_info=True\n+                    )\n \n         missing_keys = loading_info[\"missing_keys\"]\n         unexpected_keys = loading_info[\"unexpected_keys\"]\n-        self.assertIn(\"`TestModelGamma`\", cl1.out)\n+        self.assertIn(\"`TestModelGammaBeta`\", cl1.out)\n         self.assertIn(warning_msg_gamma, cl1.out)\n-        self.assertIn(\"gamma_param\", missing_keys)\n-        self.assertIn(\"weight_param\", unexpected_keys)\n-\n-        class TestModelBeta(PreTrainedModel):\n-            def __init__(self, config):\n-                super().__init__(config)\n-                self.beta_param = nn.Parameter(torch.ones(10))\n-                self.post_init()\n-\n-            def forward(self):\n-                return self.beta_param.sum()\n-\n-        warning_msg_beta = \"`beta_param` -> `bias_param`\"\n-        model = TestModelBeta(config)\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir)\n-            with LoggingLevel(logging.INFO):\n-                with CaptureLogger(logger) as cl2:\n-                    _, loading_info = TestModelBeta.from_pretrained(tmp_dir, config=config, output_loading_info=True)\n-\n-        missing_keys = loading_info[\"missing_keys\"]\n-        unexpected_keys = loading_info[\"unexpected_keys\"]\n-        self.assertIn(\"`TestModelBeta`\", cl2.out)\n-        self.assertIn(warning_msg_beta, cl2.out)\n-        self.assertIn(\"beta_param\", missing_keys)\n-        self.assertIn(\"bias_param\", unexpected_keys)\n+        self.assertIn(warning_msg_beta, cl1.out)\n+        self.assertIn(\"LayerNorm.gamma\", missing_keys)\n+        self.assertIn(\"LayerNorm.weight\", unexpected_keys)\n+        self.assertIn(\"LayerNorm.beta\", missing_keys)\n+        self.assertIn(\"LayerNorm.bias\", unexpected_keys)\n \n     def test_isin_mps_friendly(self):\n         \"\"\"tests that our custom `isin_mps_friendly` matches `torch.isin`\"\"\""
        }
    ],
    "stats": {
        "total": 125,
        "additions": 60,
        "deletions": 65
    }
}