{
    "author": "remi-or",
    "message": "Various test fixes for AMD (#39978)\n\n* Add amd expectation in internvl\n\n* Add amd expectation to llama\n\n* Added bnb decorator for a llava test that requires bnb\n\n* Added amd expectation for mistral3\n\n* Style",
    "sha": "597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5",
    "files": [
        {
            "sha": "4317cb36825e85bdcc88505274e038b33160223f",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5",
            "patch": "@@ -652,10 +652,11 @@ def test_llama_small_model_integration_forward(self):\n \n         expected_logits_all = Expectations(\n             {\n-                (\"xpu\", 3): torch.tensor([-9.8750, -0.5703, 1.4297, -10.3125, -10.3125], dtype=torch.float16),\n-                (\"cuda\", 7): torch.tensor([-9.8750,  -0.4861,   1.4648, -10.3359, -10.3359], dtype=torch.float16),\n-                (\"cuda\", 8): torch.tensor([-9.8906,  -0.4995,   1.4473, -10.3359, -10.3438], dtype=torch.float16),\n-                (\"rocm\", (9, 5)): torch.tensor([ -9.8906,  -0.4976,   1.4502, -10.3359, -10.3438], dtype=torch.float16),\n+                (\"xpu\", 3): [-9.8750, -0.5703, 1.4297, -10.3125, -10.3125],\n+                (\"cuda\", 7): [-9.8750,  -0.4861,   1.4648, -10.3359, -10.3359],\n+                (\"cuda\", 8): [-9.8906,  -0.4995,   1.4473, -10.3359, -10.3438],\n+                (\"rocm\", (9, 4)): [ -9.8750,  -0.4885,   1.4668, -10.3359, -10.3359],\n+                (\"rocm\", (9, 5)): [ -9.8906,  -0.4976,   1.4502, -10.3359, -10.3438],\n             }\n         )  # fmt: skip\n         expected_logits = torch.tensor(expected_logits_all.get_expectation(), dtype=torch.float16)"
        },
        {
            "sha": "0867a5a27068a31bdd78b4e5d0dc75879fdd7dcd",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 15,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5",
            "patch": "@@ -151,14 +151,16 @@ def test_model_7b_logits_bf16(self):\n             {\n             (\"xpu\", 3): torch.tensor([[-6.5208, -4.1218, -4.9377, -3.2536,  0.8127, -2.9811,  1.2918, -3.3848]]),\n             (\"cuda\", 7): torch.tensor([[-6.5061, -4.1147, -4.9669, -3.2038, 0.8069, -2.9694, 1.2864, -3.3786]]),\n-            (\"cuda\", 8): torch.tensor([[-6.5208, -4.1218, -4.9377, -3.2536,  0.8127, -2.9811,  1.2918, -3.3848]])\n-         })\n+            (\"cuda\", 8): torch.tensor([[-6.5208, -4.1218, -4.9377, -3.2536,  0.8127, -2.9811,  1.2918, -3.3848]]),\n+            (\"rocm\", (9, 4)): torch.tensor([[-6.5094, -4.1329, -4.9754, -3.5042,  0.8082, -2.9443,  1.2830, -3.3539]]),\n+        })\n \n-        expected_mean = expected_means.get_expectation()\n+        expected_mean = expected_means.get_expectation().to(torch_device)\n+        actual_mean = out.logits.float().mean(-1)\n         self.assertTrue(\n             torch.allclose(\n-                expected_mean.to(torch_device),\n-                out.logits.float().mean(-1),\n+                expected_mean,\n+                actual_mean,\n                 atol=1e-2,\n                 rtol=1e-2\n             )\n@@ -169,18 +171,13 @@ def test_model_7b_logits_bf16(self):\n             {\n             (\"xpu\", 3): torch.tensor([[-12.5625,  -7.1250,  -0.6289,  -7.8750,  -6.9688,  -7.8125,  -6.5000, -7.4375,  -7.6562,  -6.9688,  -6.0312,  -7.0312,  -1.8203,   1.8750, -8.5000]]),\n             (\"cuda\", 7): torch.tensor([[-12.5000, -7.0625, -0.6289, -7.8750, -6.9688, -7.8125, -6.4688, -7.4375, -7.6875, -6.9375, -6.0312, -7.0000, -1.8594, 1.8438, -8.5000]]),\n-            (\"cuda\", 8): torch.tensor([[-12.5625,  -7.1250,  -0.6289,  -7.8750,  -6.9688,  -7.8125,  -6.5000, -7.4375,  -7.6562,  -6.9688,  -6.0312,  -7.0312,  -1.8203,   1.8750, -8.5000]])\n+            (\"cuda\", 8): torch.tensor([[-12.5625,  -7.1250,  -0.6289,  -7.8750,  -6.9688,  -7.8125,  -6.5000, -7.4375,  -7.6562,  -6.9688,  -6.0312,  -7.0312,  -1.8203,   1.8750, -8.5000]]),\n+            (\"rocm\", (9, 4)): torch.tensor([[-12.5000,  -7.0625,  -0.6289,  -7.8750,  -6.9688,  -7.8125,  -6.5000, -7.4375,  -7.6562,  -6.9375,  -6.0312,  -7.0312,  -1.8594,   1.8438, -8.5000]])\n         })\n         # fmt: on\n-        expected_slice = expected_slices.get_expectation()\n-        self.assertTrue(\n-            torch.allclose(\n-                expected_slice.to(torch_device),\n-                out.logits[0, 0, :15].float(),\n-                atol=1e-2,\n-                rtol=1e-2,\n-            )\n-        )\n+        expected_slice = expected_slices.get_expectation().to(torch_device)\n+        actual_slice = out.logits[0, 0, :15].float()\n+        self.assertTrue(torch.allclose(expected_slice, actual_slice, atol=1e-2, rtol=1e-2))\n \n     @slow\n     def test_model_7b_logits(self):"
        },
        {
            "sha": "095c8aecf196e8c25fb75c3bf1d9937e24a66e00",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5",
            "patch": "@@ -476,6 +476,7 @@ def test_small_model_integration_test_llama_batched_regression(self):\n     @slow\n     @require_torch\n     @require_vision\n+    @require_bitsandbytes\n     def test_batched_generation(self):\n         model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", load_in_4bit=True)\n "
        },
        {
            "sha": "666997d4a5e11380fa435878851627d130142b9f",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=597ed1a11d9b4f8c17bf7ccea8ea2cdfab14f0b5",
            "patch": "@@ -317,6 +317,7 @@ def test_mistral3_integration_generate(self):\n             {\n                 (\"xpu\", 3): \"The image features two cats resting on a pink blanket. The cat on the left is a kitten\",\n                 (\"cuda\", 8): 'The image features two cats lying on a pink surface, which appears to be a couch or a bed',\n+                (\"rocm\", (9, 4)): \"The image features two cats lying on a pink surface, which appears to be a couch or a bed\",\n                 (\"rocm\", (9, 5)): \"The image features two tabby cats lying on a pink surface, which appears to be a cushion or\"\n             }\n         )  # fmt: skip"
        }
    ],
    "stats": {
        "total": 38,
        "additions": 19,
        "deletions": 19
    }
}