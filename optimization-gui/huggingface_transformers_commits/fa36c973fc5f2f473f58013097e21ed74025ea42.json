{
    "author": "cyyever",
    "message": "Remove unnecessary list comprehension (#41305)\n\nRemove unnecessary comprehension\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "fa36c973fc5f2f473f58013097e21ed74025ea42",
    "files": [
        {
            "sha": "d9bfe47dd0ec2d8b8ab19f3947995d40612c1f8b",
            "filename": "examples/pytorch/contrastive-image-text/run_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -387,7 +387,7 @@ def _freeze_params(module):\n         return\n \n     # 6. Get the column names for input/target.\n-    dataset_columns = dataset_name_mapping.get(data_args.dataset_name, None)\n+    dataset_columns = dataset_name_mapping.get(data_args.dataset_name)\n     if data_args.image_column is None:\n         image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n     else:"
        },
        {
            "sha": "88b203e1a276b48f07958353c8a9e3164b16f108",
            "filename": "examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -933,7 +933,7 @@ def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n             all_end_top_index.append(accelerator.gather_for_metrics(end_top_index).cpu().numpy())\n             all_cls_logits.append(accelerator.gather_for_metrics(cls_logits).cpu().numpy())\n \n-    max_len = max([x.shape[1] for x in all_end_top_log_probs])  # Get the max_length of the tensor\n+    max_len = max(x.shape[1] for x in all_end_top_log_probs)  # Get the max_length of the tensor\n \n     # concatenate all numpy arrays collected above\n     start_top_log_probs_concat = create_and_fill_np_array(all_start_top_log_probs, eval_dataset, max_len)\n@@ -993,7 +993,7 @@ def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n                 all_end_top_index.append(accelerator.gather_for_metrics(end_top_index).cpu().numpy())\n                 all_cls_logits.append(accelerator.gather_for_metrics(cls_logits).cpu().numpy())\n \n-        max_len = max([x.shape[1] for x in all_end_top_log_probs])  # Get the max_length of the tensor\n+        max_len = max(x.shape[1] for x in all_end_top_log_probs)  # Get the max_length of the tensor\n \n         # concatenate all numpy arrays collected above\n         start_top_log_probs_concat = create_and_fill_np_array(all_start_top_log_probs, predict_dataset, max_len)"
        },
        {
            "sha": "a9956159304aa36192baf5091c96779696d9938c",
            "filename": "examples/pytorch/question-answering/run_seq2seq_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -416,7 +416,7 @@ def main():\n         return\n \n     # Get the column names for input/target.\n-    dataset_columns = question_answering_column_name_mapping.get(data_args.dataset_name, None)\n+    dataset_columns = question_answering_column_name_mapping.get(data_args.dataset_name)\n     if data_args.question_column is None:\n         question_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n     else:"
        },
        {
            "sha": "7888a2b55eb7bf0ab2e9a9281ad73d5a565f1cc3",
            "filename": "examples/pytorch/summarization/run_summarization.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -531,7 +531,7 @@ def main():\n         model.config.forced_bos_token_id = forced_bos_token_id\n \n     # Get the column names for input/target.\n-    dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n+    dataset_columns = summarization_name_mapping.get(data_args.dataset_name)\n     if data_args.text_column is None:\n         text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n     else:"
        },
        {
            "sha": "bcae7c38edf39d71a74dc2ef967db745935d52da",
            "filename": "examples/pytorch/summarization/run_summarization_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -476,7 +476,7 @@ def main():\n     column_names = raw_datasets[\"train\"].column_names\n \n     # Get the column names for input/target.\n-    dataset_columns = summarization_name_mapping.get(args.dataset_name, None)\n+    dataset_columns = summarization_name_mapping.get(args.dataset_name)\n     if args.text_column is None:\n         text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n     else:"
        },
        {
            "sha": "0a1efea13cfff943e0b5694c48c33a48b9bbecea",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -65,7 +65,7 @@ def __init__(self, **kwargs: Unpack[DeepseekVLHybridImageProcessorKwargs]):\n         if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:\n-            background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n+            background_color = tuple(int(x * 255) for x in kwargs.get(\"image_mean\"))\n         if kwargs.get(\"high_res_image_mean\") is None:\n             high_res_background_color = (127, 127, 127)\n         else:"
        },
        {
            "sha": "4135623743aec60c59fbb2275881650050b42a81",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -764,7 +764,7 @@ def __init__(self, **kwargs: Unpack[DeepseekVLHybridImageProcessorKwargs]):\n         if kwargs.get(\"image_mean\") is None:\n             background_color = (127, 127, 127)\n         else:\n-            background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n+            background_color = tuple(int(x * 255) for x in kwargs.get(\"image_mean\"))\n         if kwargs.get(\"high_res_image_mean\") is None:\n             high_res_background_color = (127, 127, 127)\n         else:"
        },
        {
            "sha": "b110cba6eca64bc9e38b2244f44bb98e5f59042b",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -551,7 +551,7 @@ def forward(\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n \n         if inputs_embeds is None:\n-            inputs_embeds = sum([self.embed_tokens[codebook](input[:, codebook]) for codebook in range(num_codebooks)])\n+            inputs_embeds = sum(self.embed_tokens[codebook](input[:, codebook]) for codebook in range(num_codebooks))\n \n         attention_mask = self._update_causal_mask(\n             attention_mask,"
        },
        {
            "sha": "4c797c9e7c807a15c3879bc6df4e5094d2a352c7",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -718,7 +718,7 @@ def get_num_masks(self, class_labels: torch.Tensor, device: torch.device) -> tor\n         \"\"\"\n         Computes the average number of target masks across the batch, for normalization purposes.\n         \"\"\"\n-        num_masks = sum([len(classes) for classes in class_labels])\n+        num_masks = sum(len(classes) for classes in class_labels)\n         num_masks = torch.as_tensor([num_masks], dtype=torch.float, device=device)\n         world_size = 1\n         if is_accelerate_available():"
        },
        {
            "sha": "2bc883f95e73e3a74c3bf8d90da6916c3cc3d310",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -184,7 +184,7 @@ def get_min_tile_covering_grid(\n     for tile_grid in candidate_tile_grids:\n         tile_regions = split_image_into_grid(image_height, image_width, tile_grid)\n         tile_covering_ratio = (\n-            sum([compute_patch_covering_area(*region, target_patch_size) for region in tile_regions]) / image_area\n+            sum(compute_patch_covering_area(*region, target_patch_size) for region in tile_regions) / image_area\n         )\n \n         evaluated_grids.append((tile_grid, tile_covering_ratio))"
        },
        {
            "sha": "b12d86b66ee9f798fe4e94727dcc3ac63d3e1744",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -1542,7 +1542,7 @@ def __init__(self, config: PatchTSMixerConfig):\n                 \"normal\": NormalOutput,\n                 \"negative_binomial\": NegativeBinomialOutput,\n             }\n-            output_class = distribution_output_map.get(config.distribution_output, None)\n+            output_class = distribution_output_map.get(config.distribution_output)\n             if output_class is not None:\n                 self.distribution_output = output_class(dim=dim)\n             else:"
        },
        {
            "sha": "33b10915354f035789435dab0b31d02850b45f02",
            "filename": "src/transformers/models/phi4_multimodal/image_processing_phi4_multimodal_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -237,7 +237,7 @@ def _preprocess(\n             images_tokens.append(num_img_tokens)\n             image_sizes.append([height, width])\n             max_crops = hd_image_reshape.size(0)\n-        max_crops = max([img.size(0) for img in images_transformed])\n+        max_crops = max(img.size(0) for img in images_transformed)\n         images_transformed = [self.pad_to_max_num_crops(im, max_crops) for im in images_transformed]\n         images_transformed = torch.stack(images_transformed, dim=0)\n         masks_transformed = [self.pad_mask_to_max_num_crops(mask, max_crops) for mask in masks_transformed]"
        },
        {
            "sha": "fc6dc266bc124b9d93740ab1966f3c2f6716d6b5",
            "filename": "src/transformers/models/pop2piano/tokenization_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -265,7 +265,7 @@ def relative_tokens_ids_to_notes(\n \n         current_idx = start_idx\n         current_velocity = 0\n-        note_onsets_ready = [None for i in range(sum([k.endswith(\"NOTE\") for k in self.encoder]) + 1)]\n+        note_onsets_ready = [None for i in range(sum(k.endswith(\"NOTE\") for k in self.encoder) + 1)]\n         notes = []\n         for token_type, number in words:\n             if token_type == \"TOKEN_SPECIAL\":"
        },
        {
            "sha": "f2852b8623c4b64fc74afde3c685c51235d935a2",
            "filename": "src/transformers/models/sam_hq/processing_samhq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -171,7 +171,7 @@ def _pad_points_and_labels(self, input_points, input_labels, point_pad_value):\n         r\"\"\"\n         The method pads the 2D points and labels to the maximum number of points in the batch.\n         \"\"\"\n-        expected_nb_points = max([point.shape[0] for point in input_points])\n+        expected_nb_points = max(point.shape[0] for point in input_points)\n         processed_input_points = []\n         for i, point in enumerate(input_points):\n             if point.shape[0] != expected_nb_points:"
        },
        {
            "sha": "70aaec971b6eec1df9dbda835e8db9ea880ccfe2",
            "filename": "tests/models/mask2former/test_image_processing_mask2former.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -552,8 +552,8 @@ def test_post_process_label_fusing(self):\n                 fuse_targets = [1 for el in el_unfused if el[\"label_id\"] == 1]\n                 num_to_fuse = 0 if len(fuse_targets) == 0 else sum(fuse_targets) - 1\n                 # Expected number of segments after fusing\n-                expected_num_segments = max([el[\"id\"] for el in el_unfused]) - num_to_fuse\n-                num_segments_fused = max([el[\"id\"] for el in el_fused])\n+                expected_num_segments = max(el[\"id\"] for el in el_unfused) - num_to_fuse\n+                num_segments_fused = max(el[\"id\"] for el in el_fused)\n                 self.assertEqual(num_segments_fused, expected_num_segments)\n \n     def test_slow_fast_equivalence(self):"
        },
        {
            "sha": "bc480f244bc4a08ea614210549cf0fbf2cb76a33",
            "filename": "tests/models/maskformer/test_image_processing_maskformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -540,8 +540,8 @@ def test_post_process_label_fusing(self):\n                 fuse_targets = [1 for el in el_unfused if el[\"label_id\"] == 1]\n                 num_to_fuse = 0 if len(fuse_targets) == 0 else sum(fuse_targets) - 1\n                 # Expected number of segments after fusing\n-                expected_num_segments = max([el[\"id\"] for el in el_unfused]) - num_to_fuse\n-                num_segments_fused = max([el[\"id\"] for el in el_fused])\n+                expected_num_segments = max(el[\"id\"] for el in el_unfused) - num_to_fuse\n+                num_segments_fused = max(el[\"id\"] for el in el_fused)\n                 self.assertEqual(num_segments_fused, expected_num_segments)\n \n     def test_slow_fast_equivalence(self):"
        },
        {
            "sha": "b3aece87fae9df77f99e2ebdfa23af708e004a6b",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -242,7 +242,7 @@ def test_inputs_embeds(self):\n             input_ids = input_ids.reshape(-1, config.num_codebooks, input_ids.shape[-1])\n \n             inputs[\"inputs_embeds\"] = sum(\n-                [embed_tokens[codebook](input_ids[:, codebook]) for codebook in range(config.num_codebooks)]\n+                embed_tokens[codebook](input_ids[:, codebook]) for codebook in range(config.num_codebooks)\n             )\n \n             with torch.no_grad():"
        },
        {
            "sha": "a0214977fdb0a48cedb1608a9a7ea6c911d356a0",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -251,7 +251,7 @@ def test_inputs_embeds(self):\n             input_ids = input_ids.reshape(-1, config.num_codebooks, input_ids.shape[-1])\n \n             inputs[\"inputs_embeds\"] = sum(\n-                [embed_tokens[codebook](input_ids[:, codebook]) for codebook in range(config.num_codebooks)]\n+                embed_tokens[codebook](input_ids[:, codebook]) for codebook in range(config.num_codebooks)\n             )\n \n             with torch.no_grad():"
        },
        {
            "sha": "46a7881b786a6022160af46449d528cef1912e8c",
            "filename": "tests/models/owlvit/test_processing_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fowlvit%2Ftest_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Fmodels%2Fowlvit%2Ftest_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_processing_owlvit.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -187,7 +187,7 @@ def test_processor_with_nested_text_list(self):\n \n         seq_length = 16\n         batch_size = len(input_texts)\n-        num_max_text_queries = max([len(texts) for texts in input_texts])\n+        num_max_text_queries = max(len(texts) for texts in input_texts)\n \n         self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"attention_mask\"])\n         self.assertEqual(inputs[\"input_ids\"].shape, (batch_size * num_max_text_queries, seq_length))"
        },
        {
            "sha": "44b03e2369ce3889ad7ae55701ebcffe969b909a",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -352,9 +352,7 @@ def test_dynamic_cache_hard(self):\n         decoded = tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n         # sum of the scores for the generated tokens\n         input_length = inputs.input_ids.shape[1]\n-        score_sum = sum(\n-            [score[0][gen_out.sequences[0][input_length + idx]] for idx, score in enumerate(gen_out.scores)]\n-        )\n+        score_sum = sum(score[0][gen_out.sequences[0][input_length + idx]] for idx, score in enumerate(gen_out.scores))\n \n         EXPECTED_GENERATION = (\n             \"Here's everything I know about cats. Cats are mammals, they have four legs, they have a tail, they have \""
        },
        {
            "sha": "f666d888507848cbf559968766e353ca093cc9eb",
            "filename": "utils/check_modular_conversion.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/utils%2Fcheck_modular_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/utils%2Fcheck_modular_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_modular_conversion.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -179,7 +179,7 @@ def guaranteed_no_diff(modular_file_path, dependencies, models_in_diff):\n     # we start applying modular conversion to each list in parallel, starting from the first list\n \n     console.print(f\"[bold yellow]Number of dependency levels: {len(ordered_files)}[/bold yellow]\")\n-    console.print(f\"[bold yellow]Files per level: {tuple([len(x) for x in ordered_files])}[/bold yellow]\")\n+    console.print(f\"[bold yellow]Files per level: {tuple(len(x) for x in ordered_files)}[/bold yellow]\")\n \n     try:\n         for dependency_level_files in ordered_files:"
        },
        {
            "sha": "04932832dfe7048964aa9a320531dacef131a942",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa36c973fc5f2f473f58013097e21ed74025ea42/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa36c973fc5f2f473f58013097e21ed74025ea42/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=fa36c973fc5f2f473f58013097e21ed74025ea42",
            "patch": "@@ -876,7 +876,7 @@ def create_reverse_dependency_map() -> dict[str, list[str]]:\n     # all the modules impacted by that init.\n     for m in [f for f in all_modules if f.endswith(\"__init__.py\")]:\n         direct_deps = get_module_dependencies(m, cache=cache)\n-        deps = sum([reverse_map[d] for d in direct_deps if not d.endswith(\"__init__.py\")], direct_deps)\n+        deps = sum((reverse_map[d] for d in direct_deps if not d.endswith(\"__init__.py\")), direct_deps)\n         reverse_map[m] = list(set(deps) - {m})\n \n     return reverse_map"
        }
    ],
    "stats": {
        "total": 52,
        "additions": 25,
        "deletions": 27
    }
}