{
    "author": "ydshieh",
    "message": "Remove `@slow` for `test_eager_matches_sdpa_inference` (#34558)\n\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
    "files": [
        {
            "sha": "b0a20d6c5ccd932d8569afe2f339c9d59aef27b8",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -95,7 +95,7 @@ def __init__(self, config):\n         mode = config.spatial_pool_mode\n         stride = config.spatial_pool_stride\n         out_channels = getattr(config, \"spatial_pool_out_channels\", config.vision_config.hidden_size)\n-        self.image_size = config.vision_config.image_size // config.vision_config.patch_size**2\n+        self.image_size = (config.vision_config.image_size // config.vision_config.patch_size) ** 2\n \n         if mode == \"average\":\n             self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride)"
        },
        {
            "sha": "3d6431d7ea29bac219fb74809436544261411027",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -191,7 +191,7 @@ def __init__(self, config):\n         mode = config.spatial_pool_mode\n         stride = config.spatial_pool_stride\n         out_channels = getattr(config, \"spatial_pool_out_channels\", config.vision_config.hidden_size)\n-        self.image_size = config.vision_config.image_size // config.vision_config.patch_size**2\n+        self.image_size = (config.vision_config.image_size // config.vision_config.patch_size) ** 2\n \n         if mode == \"average\":\n             self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride)"
        },
        {
            "sha": "6ce5e77706d358ec514b10000656723f99f45deb",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -1577,7 +1577,7 @@ def forward(\n         quantized_features, codevector_perplexity = self.quantizer(extract_features)\n \n         # project quantized features twice\n-        quantized_features = self.project_q(quantized_features)\n+        quantized_features = self.project_q(quantized_features.to(self.project_q.weight.dtype))\n         quantized_features = self.project_hid(quantized_features)\n \n         prob_replace_matrix = torch.empty(transformer_features.size(0), transformer_features.size(1)).fill_("
        },
        {
            "sha": "cbe851e97e9aed51c213bb188393eaacccc1f2ff",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -185,16 +185,16 @@ def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         # This is a band-aid for VLM models, to ensure they don't generate image/video tokens which would cause them\n         # to crash. On pretrained models this isn't a risk, as they are trained to not generate these tokens.\n         if config is not None:\n-            image_token_index = (\n-                config.image_token_index\n-                if getattr(config, \"image_token_index\", None) is not None\n-                else getattr(config, \"image_token_id\", None)\n-            )\n-            video_token_index = config.video_token_index if hasattr(config, \"video_token_index\") else None\n-            if image_token_index is not None and image_token_index < config.get_text_config().vocab_size:\n-                logits_processor_kwargs[\"bad_words_ids\"].append([image_token_index])\n-            if video_token_index is not None and video_token_index < config.get_text_config().vocab_size:\n-                logits_processor_kwargs[\"bad_words_ids\"].append([video_token_index])\n+            for key in [\n+                \"image_token_index\",\n+                \"image_token_id\",\n+                \"video_token_index\",\n+                \"video_token_id\",\n+                \"vision_start_token_id\",\n+            ]:\n+                token_index = getattr(config, key, None)\n+                if token_index is not None and token_index < config.get_text_config().vocab_size:\n+                    logits_processor_kwargs[\"bad_words_ids\"].append([token_index])\n \n         return logits_processor_kwargs\n "
        },
        {
            "sha": "0a123c02ab778bbe5a2c1db92e494228e9a6f004",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -17,10 +17,11 @@\n import unittest\n \n from packaging import version\n+from parameterized import parameterized\n \n from transformers import AlbertConfig, AutoTokenizer, is_torch_available\n from transformers.models.auto import get_values\n-from transformers.testing_utils import require_torch, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torch_sdpa, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n@@ -288,6 +289,12 @@ def setUp(self):\n         self.model_tester = AlbertModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=AlbertConfig, hidden_size=37)\n \n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @require_torch_sdpa\n+    @unittest.skip(\"Albert requires `head_mask` which is currently not done in this test.\")\n+    def test_eager_matches_sdpa_inference(self):\n+        pass\n+\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "ebac3b9167ce26a119b0cf41277e2d3da5e39106",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 302,
            "changes": 302,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -14,13 +14,9 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Glm model.\"\"\"\n \n-import inspect\n-import tempfile\n import unittest\n \n-import numpy as np\n import pytest\n-from parameterized import parameterized\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, GlmConfig, is_torch_available\n from transformers.testing_utils import (\n@@ -32,7 +28,6 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_torch_bf16_available_on_device, is_torch_fp16_available_on_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -421,303 +416,6 @@ def test_custom_4d_attention_mask(self):\n \n             torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-3)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        \"\"\"Overwrite to add flakyness: some cases can sometimes fail\"\"\"\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n-        if torch_dtype == \"float16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bfloat16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"float32\":\n-            torch_dtype = torch.float32\n-\n-        atols = {\n-            (\"cpu\", False, torch.float32): 1e-6,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-6,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-6,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-6,\n-            (\"cuda\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-        rtols = {\n-            (\"cpu\", False, torch.float32): 1e-4,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-4,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-4,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-\n-        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n-            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-            # FIXME: we deactivate boolean mask for models using \"use_mask_token\" in their constructors.\n-            # These models support masking only in the case `use_mask_token=True`. Otherwise they cannot consume an input mask.\n-            # This means that the class needs to be instantiated much later, after `use_mask` is set, which means a significant refactor of the code.\n-            # However masking there is not done at any layers that matters (i.e self-attention), therefore we can safely deactivate it.\n-            deactivate_mask = \"use_mask_token\" in inspect.signature(model_class).parameters\n-\n-            is_encoder_decoder = model.config.is_encoder_decoder\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n-                # but it would be nicer to have an efficient way to use parameterized.expand\n-                fail_cases = []\n-                for padding_side in [\"left\", \"right\"]:\n-                    for use_mask in [False, True]:\n-                        for output_attentions in [True, False]:\n-                            can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                            if not (self.has_attentions and can_output_attn) and output_attentions:\n-                                continue\n-                            for batch_size in [1, 5]:\n-                                dummy_input = inputs_dict[model.main_input_name]\n-\n-                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                    dummy_input = dummy_input.to(torch_dtype)\n-\n-                                dummy_input = dummy_input[:batch_size]\n-                                if dummy_input.shape[0] != batch_size:\n-                                    if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                        extension = torch.rand(\n-                                            batch_size - dummy_input.shape[0],\n-                                            *dummy_input.shape[1:],\n-                                            dtype=torch_dtype,\n-                                            device=torch_device,\n-                                        )\n-                                        dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-                                    else:\n-                                        extension = torch.randint(\n-                                            high=5,\n-                                            size=(batch_size - dummy_input.shape[0], *dummy_input.shape[1:]),\n-                                            dtype=dummy_input.dtype,\n-                                            device=torch_device,\n-                                        )\n-                                        dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-\n-                                if not use_mask:\n-                                    dummy_attention_mask = None\n-                                else:\n-                                    dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-                                    if dummy_attention_mask is None:\n-                                        if is_encoder_decoder:\n-                                            seqlen = inputs_dict.get(\"decoder_input_ids\", dummy_input).shape[-1]\n-                                        else:\n-                                            seqlen = dummy_input.shape[-1]\n-                                        dummy_attention_mask = (\n-                                            torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n-                                        )\n-\n-                                    dummy_attention_mask = dummy_attention_mask[:batch_size]\n-                                    if dummy_attention_mask.shape[0] != batch_size:\n-                                        extension = torch.ones(\n-                                            batch_size - dummy_attention_mask.shape[0],\n-                                            *dummy_attention_mask.shape[1:],\n-                                            dtype=dummy_attention_mask.dtype,\n-                                            device=torch_device,\n-                                        )\n-                                        dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n-                                        dummy_attention_mask = dummy_attention_mask.to(torch_device)\n-\n-                                    dummy_attention_mask[:] = 1\n-                                    if padding_side == \"left\":\n-                                        dummy_attention_mask[-1, :-1] = 1\n-                                        dummy_attention_mask[-1, -4:] = 0\n-                                    elif padding_side == \"right\":\n-                                        dummy_attention_mask[-1, 1:] = 1\n-                                        dummy_attention_mask[-1, :3] = 0\n-\n-                                for enable_kernels in [False, True]:\n-                                    failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n-                                    if is_encoder_decoder:\n-                                        decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[\n-                                            :batch_size\n-                                        ]\n-                                        if decoder_input_ids.shape[0] != batch_size:\n-                                            extension = torch.ones(\n-                                                batch_size - decoder_input_ids.shape[0],\n-                                                *decoder_input_ids.shape[1:],\n-                                                dtype=decoder_input_ids.dtype,\n-                                                device=torch_device,\n-                                            )\n-                                            decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n-                                            decoder_input_ids = decoder_input_ids.to(torch_device)\n-\n-                                        # TODO: never an `attention_mask` arg here?\n-                                        processed_inputs = {\n-                                            model.main_input_name: dummy_input,\n-                                            \"decoder_input_ids\": decoder_input_ids,\n-                                            \"decoder_attention_mask\": dummy_attention_mask,\n-                                            \"output_hidden_states\": True,\n-                                        }\n-                                    else:\n-                                        processed_inputs = {\n-                                            model.main_input_name: dummy_input,\n-                                            \"output_hidden_states\": True,\n-                                        }\n-\n-                                        # Otherwise fails for e.g. WhisperEncoderModel\n-                                        if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n-                                            processed_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                                        if (\n-                                            self.has_attentions\n-                                            and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                                        ):\n-                                            processed_inputs[\"output_attentions\"] = output_attentions\n-                                    if not deactivate_mask and (\n-                                        \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters\n-                                    ):\n-                                        dummy_mask = torch.ones((self.model_tester.num_masks,))\n-\n-                                        # In case of additional token (like class) we define a custom `mask_length`\n-                                        if hasattr(self.model_tester, \"mask_length\"):\n-                                            mask_length = self.model_tester.mask_length - dummy_mask.size(0)\n-                                        else:\n-                                            mask_length = self.model_tester.seq_length - dummy_mask.size(0)\n-                                        dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n-                                        dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n-                                        processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n-\n-                                    if \"noise\" in inspect.signature(model_eager.forward).parameters:\n-                                        np.random.seed(2)\n-                                        num_patches = int(\n-                                            (self.model_tester.image_size // self.model_tester.patch_size) ** 2\n-                                        )\n-                                        noise = np.random.uniform(size=(batch_size, num_patches))\n-                                        processed_inputs[\"noise\"] = torch.from_numpy(noise)\n-\n-                                    # TODO: test gradients as well (& for FA2 as well!)\n-                                    with torch.no_grad():\n-                                        with torch.backends.cuda.sdp_kernel(\n-                                            enable_flash=enable_kernels,\n-                                            enable_math=True,\n-                                            enable_mem_efficient=enable_kernels,\n-                                        ):\n-                                            prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-                                            outputs_eager = model_eager(**prepared_inputs)\n-                                            outputs_sdpa = model_sdpa(**prepared_inputs)\n-\n-                                    logits_eager = (\n-                                        outputs_eager.hidden_states[-1]\n-                                        if not is_encoder_decoder\n-                                        else outputs_eager.decoder_hidden_states[-1]\n-                                    )\n-                                    logits_sdpa = (\n-                                        outputs_sdpa.hidden_states[-1]\n-                                        if not is_encoder_decoder\n-                                        else outputs_sdpa.decoder_hidden_states[-1]\n-                                    )\n-\n-                                    if torch_device in [\"cpu\", \"cuda\"]:\n-                                        atol = atols[torch_device, enable_kernels, torch_dtype]\n-                                        rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                                    else:\n-                                        atol = 1e-7\n-                                        rtol = 1e-4\n-\n-                                    # Masked tokens output slightly deviates - we don't mind that.\n-                                    if use_mask:\n-                                        if padding_side == \"left\":\n-                                            sub_sdpa = logits_sdpa[:-1]\n-                                            sub_eager = logits_eager[:-1]\n-                                            if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                                fail_cases.append(\n-                                                    get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                                )\n-\n-                                            sub_sdpa = logits_sdpa[-1, :-4]\n-                                            sub_eager = logits_eager[-1, :-4]\n-                                            if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                                fail_cases.append(\n-                                                    get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                                )\n-\n-                                            # Testing the padding tokens is not really meaningful but anyway\n-                                            # sub_sdpa = logits_sdpa[-1, -4:]\n-                                            # sub_eager = logits_eager[-1, -4:]\n-                                            # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n-                                        elif padding_side == \"right\":\n-                                            sub_sdpa = logits_sdpa[:-1]\n-                                            sub_eager = logits_eager[:-1]\n-                                            if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                                fail_cases.append(\n-                                                    get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                                )\n-\n-                                            sub_sdpa = logits_sdpa[-1, 3:]\n-                                            sub_eager = logits_eager[-1, 3:]\n-                                            if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                                fail_cases.append(\n-                                                    get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                                )\n-\n-                                            # Testing the padding tokens is not really meaningful but anyway\n-                                            # sub_sdpa = logits_sdpa[-1, :3]\n-                                            # sub_eager = logits_eager[-1, :3]\n-                                            # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n-\n-                                    else:\n-                                        if not torch.allclose(logits_sdpa, logits_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                            )\n-\n-                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "60eb964927278a8d0614a07064985d113ff7a45a",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -25,7 +25,6 @@\n     require_read_token,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -445,15 +444,6 @@ def test_use_flash_attention_2_true(self):\n                 if not has_flash:\n                     raise ValueError(\"The flash model should have flash attention layers\")\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        \"\"\"\n-        skipping the test since mup is very flaky and gets consistently different outputs\n-        \"\"\"\n-        self.skipTest(\"skipping the test since mup is very flaky and gets consistently different outputs\")\n-\n \n @require_torch_gpu\n class GraniteIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "97af65667ed04857ff1d1eadfb9da4a6d8405a63",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -25,7 +25,6 @@\n     require_read_token,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -444,15 +443,6 @@ def test_use_flash_attention_2_true(self):\n                 if not has_flash:\n                     raise ValueError(\"The flash model should have flash attention layers\")\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        \"\"\"\n-        skipping the test since mup is very flaky and gets consistently different outputs\n-        \"\"\"\n-        self.skipTest(\"skipping the test since mup is very flaky and gets consistently different outputs\")\n-\n \n @require_torch_gpu\n class GraniteMoeIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "12004cc3c8ad8932890cdc9e349bfb2ef41f334f",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -134,7 +134,7 @@ def __init__(\n             num_attention_heads=self.vision_num_attention_heads,\n             num_hidden_layers=self.vision_num_hidden_layers,\n             intermediate_size=self.vision_intermediate_size,\n-        )\n+        ).to_dict()\n \n         self.perceiver_qk_layer_norms_perceiver = perceiver_qk_layer_norms_perceiver\n         self.perceiver_resampler_depth = perceiver_resampler_depth\n@@ -316,7 +316,6 @@ def prepare_pixel_values(self):\n         return floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n \n     @require_torch_sdpa\n-    @slow\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         self.skipTest(reason=\"Idefics has a hard requirement on SDPA, skipping this test\")\n@@ -353,6 +352,12 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n \n         return inputs_dict\n \n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @require_torch_sdpa\n+    @unittest.skip(\"Idefics requires both text and image inputs which is currently not done in this test.\")\n+    def test_eager_matches_sdpa_inference(self):\n+        pass\n+\n     def test_model_outputs_equivalence(self):\n         try:\n             orig = self.all_model_classes\n@@ -602,6 +607,12 @@ def setUp(self):\n         )\n         self.config_tester = ConfigTester(self, config_class=IdeficsConfig, hidden_size=37)\n \n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @require_torch_sdpa\n+    @unittest.skip(\"Idefics requires both text and image inputs which is currently not done in this test.\")\n+    def test_eager_matches_sdpa_inference(self, torch_dtype):\n+        pass\n+\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n         \"\"\"Overwrite because IDEFICS needs image attention mask to be also padded\"\"\""
        },
        {
            "sha": "82508f57e0f1f58f64b0489a658dac837572ba23",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -90,7 +90,7 @@ def __init__(\n         },\n         is_training=True,\n         vision_config={\n-            \"image_size\": 16,\n+            \"image_size\": 8,\n             \"patch_size\": 4,\n             \"num_channels\": 3,\n             \"is_training\": True,\n@@ -123,10 +123,10 @@ def __init__(\n         self.batch_size = 3\n         self.num_channels = 3\n         self.image_size = 30\n-        self.encoder_seq_length = 95\n-        self.image_grid_pinpoints = [[32, 32]]\n-        self.num_image_tokens = 88\n+        self.image_grid_pinpoints = [[16, 16]]\n+        self.num_image_tokens = 24\n         self.seq_length = seq_length + self.num_image_tokens\n+        self.encoder_seq_length = self.seq_length\n \n     def get_config(self):\n         return LlavaNextConfig("
        },
        {
            "sha": "83caabe16bb68ba475a68f10558fe3247e2d378c",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -91,7 +91,7 @@ def __init__(\n         },\n         is_training=True,\n         vision_config={\n-            \"image_size\": 16,\n+            \"image_size\": 8,\n             \"patch_size\": 4,\n             \"num_channels\": 3,\n             \"is_training\": True,\n@@ -125,10 +125,10 @@ def __init__(\n         self.batch_size = 3\n         self.num_channels = 3\n         self.image_size = 30\n-        self.encoder_seq_length = 127\n-        self.image_grid_pinpoints = [[32, 32]]\n-        self.num_image_tokens = 88\n-        self.num_video_tokens = 32\n+\n+        self.image_grid_pinpoints = [[16, 16]]\n+        self.num_image_tokens = 24\n+        self.num_video_tokens = 8\n         self.seq_length = seq_length + self.num_image_tokens + self.num_video_tokens\n \n     def get_config(self):"
        },
        {
            "sha": "7ddc6b747447a3f9a860df059d7686d8cf19f95e",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 34,
            "deletions": 50,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -409,10 +409,14 @@ def test_identity_shortcut(self):\n         config.use_conv_shortcut = False\n         self.model_tester.create_and_check_model_forward(config, inputs_dict)\n \n+    # Overwrite to use `audio_values` as the tensors to compare.\n+    # TODO: Try to do this in the parent class.\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n-    @slow\n     def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n+        if torch_dtype == \"float16\" and torch_device == \"cpu\":\n+            self.skipTest(\"`replication_pad1d` not implemented for 'Half\")\n+\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n@@ -513,7 +517,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                             can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n                             if not (self.has_attentions and can_output_attn) and output_attentions:\n                                 continue\n-                            for batch_size in [1, 5]:\n+                            for batch_size in [7]:\n                                 dummy_input = inputs_dict[model.main_input_name]\n \n                                 if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n@@ -564,11 +568,11 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                     dummy_attention_mask[:] = 1\n                                     if padding_side == \"left\":\n-                                        dummy_attention_mask[-1, :-1] = 1\n-                                        dummy_attention_mask[-1, -4:] = 0\n+                                        dummy_attention_mask[-1, :2] = 0\n+                                        dummy_attention_mask[-1, 2:] = 1\n                                     elif padding_side == \"right\":\n-                                        dummy_attention_mask[-1, 1:] = 1\n-                                        dummy_attention_mask[-1, :3] = 0\n+                                        dummy_attention_mask[-1, -2:] = 0\n+                                        dummy_attention_mask[-1, :-2] = 1\n \n                                 for enable_kernels in [False, True]:\n                                     failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n@@ -655,52 +659,32 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                     # Masked tokens output slightly deviates - we don't mind that.\n                                     if use_mask:\n+                                        _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                                        _logits_eager = torch.zeros_like(input=logits_eager)\n+\n+                                        _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                                        _logits_eager[:-1] = logits_eager[:-1]\n+\n                                         if padding_side == \"left\":\n-                                            sub_sdpa = logits_sdpa[:-1]\n-                                            sub_eager = logits_eager[:-1]\n-                                            if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                                fail_cases.append(\n-                                                    get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                                )\n-\n-                                            sub_sdpa = logits_sdpa[-1, :-4]\n-                                            sub_eager = logits_eager[-1, :-4]\n-                                            if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                                fail_cases.append(\n-                                                    get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                                )\n-\n-                                            # Testing the padding tokens is not really meaningful but anyway\n-                                            # sub_sdpa = logits_sdpa[-1, -4:]\n-                                            # sub_eager = logits_eager[-1, -4:]\n-                                            # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n-                                        elif padding_side == \"right\":\n-                                            sub_sdpa = logits_sdpa[:-1]\n-                                            sub_eager = logits_eager[:-1]\n-                                            if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                                fail_cases.append(\n-                                                    get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                                )\n-\n-                                            sub_sdpa = logits_sdpa[-1, 3:]\n-                                            sub_eager = logits_eager[-1, 3:]\n-                                            if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                                fail_cases.append(\n-                                                    get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                                )\n-\n-                                            # Testing the padding tokens is not really meaningful but anyway\n-                                            # sub_sdpa = logits_sdpa[-1, :3]\n-                                            # sub_eager = logits_eager[-1, :3]\n-                                            # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n+                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                                            _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n \n-                                    else:\n-                                        if not torch.allclose(logits_sdpa, logits_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                            )\n+                                        elif padding_side == \"right\":\n+                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                                            _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n+\n+                                        logits_sdpa = _logits_sdpa\n+                                        logits_eager = _logits_eager\n+\n+                                    results = [\n+                                        torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                                        for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+                                    ]\n+                                    # If 80% batch elements have matched results, it's fine\n+                                    if np.mean(results) < 0.8:\n+                                        fail_cases.append(\n+                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n+                                        )\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n "
        },
        {
            "sha": "8da927f815db81857ca0a009c577d4be35805fe0",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -30,12 +30,10 @@\n from transformers.models.mllama.configuration_mllama import MllamaTextConfig\n from transformers.testing_utils import (\n     cleanup,\n-    is_flaky,\n     require_bitsandbytes,\n     require_read_token,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -359,13 +357,6 @@ def _check_attentions_for_generate(\n \n             self.assertListEqual([layer_attention.shape for layer_attention in iter_attentions], expected_shapes)\n \n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_inference_1_bfloat16(self):\n-        # A workaround to override parametrized test with flaky decorator\n-        super().test_eager_matches_sdpa_inference_1_bfloat16()\n-\n     @unittest.skip(\"For some unknown reasons the tests fails in CrossAttention layer when doing torch.sdpa(). \")\n     def test_sdpa_can_compile_dynamic(self):\n         pass"
        },
        {
            "sha": "37b5af3ae7e312736ee92d71d517386c6e97b295",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 89,
            "deletions": 99,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -452,7 +452,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n-    @slow\n     # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_inference\n     def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         if not self.has_attentions:\n@@ -479,8 +478,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n \n         atols = {\n             (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-6,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -491,8 +492,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         }\n         rtols = {\n             (\"cpu\", False, torch.float32): 1e-4,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-4,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-4,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -528,7 +531,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 fail_cases = []\n                 for padding_side in [\"left\", \"right\"]:\n                     for use_mask in [False, True]:\n-                        for batch_size in [1, 5]:\n+                        for batch_size in [7]:\n                             # Ignore copy\n                             batch_size_input_ids = self.model_tester.num_codebooks * batch_size\n                             dummy_input = inputs_dict[model.main_input_name]\n@@ -585,11 +588,11 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 dummy_attention_mask[:] = 1\n                                 if padding_side == \"left\":\n-                                    dummy_attention_mask[-1, :-1] = 1\n-                                    dummy_attention_mask[-1, -4:] = 0\n+                                    dummy_attention_mask[-1, :2] = 0\n+                                    dummy_attention_mask[-1, 2:] = 1\n                                 elif padding_side == \"right\":\n-                                    dummy_attention_mask[-1, 1:] = 1\n-                                    dummy_attention_mask[-1, :3] = 0\n+                                    dummy_attention_mask[-1, -2:] = 0\n+                                    dummy_attention_mask[-1, :-2] = 1\n \n                             for enable_kernels in [False, True]:\n                                 failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n@@ -632,52 +635,32 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 # Masked tokens output slightly deviates - we don't mind that.\n                                 if use_mask:\n+                                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                                    _logits_eager = torch.zeros_like(input=logits_eager)\n+\n+                                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                                    _logits_eager[:-1] = logits_eager[:-1]\n+\n                                     if padding_side == \"left\":\n-                                        sub_sdpa = logits_sdpa[:-1]\n-                                        sub_eager = logits_eager[:-1]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        sub_sdpa = logits_sdpa[-1, :-4]\n-                                        sub_eager = logits_eager[-1, :-4]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        # Testing the padding tokens is not really meaningful but anyway\n-                                        # sub_sdpa = logits_sdpa[-1, -4:]\n-                                        # sub_eager = logits_eager[-1, -4:]\n-                                        # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                        #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n+                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n+\n                                     elif padding_side == \"right\":\n-                                        sub_sdpa = logits_sdpa[:-1]\n-                                        sub_eager = logits_eager[:-1]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        sub_sdpa = logits_sdpa[-1, 3:]\n-                                        sub_eager = logits_eager[-1, 3:]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        # Testing the padding tokens is not really meaningful but anyway\n-                                        # sub_sdpa = logits_sdpa[-1, :3]\n-                                        # sub_eager = logits_eager[-1, :3]\n-                                        # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                        #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n+                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n \n-                                else:\n-                                    if not torch.allclose(logits_sdpa, logits_eager, atol=atol, rtol=rtol):\n-                                        fail_cases.append(\n-                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                        )\n+                                    logits_sdpa = _logits_sdpa\n+                                    logits_eager = _logits_eager\n+\n+                                results = [\n+                                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+                                ]\n+                                # If 80% batch elements have matched results, it's fine\n+                                if np.mean(results) < 0.8:\n+                                    fail_cases.append(\n+                                        get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n+                                    )\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n \n@@ -1496,8 +1479,6 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_inference\n     def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n@@ -1523,8 +1504,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n \n         atols = {\n             (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-6,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -1535,8 +1518,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         }\n         rtols = {\n             (\"cpu\", False, torch.float32): 1e-4,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-4,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-4,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -1549,8 +1534,26 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         def get_mean_reldiff(failcase, x, ref, atol, rtol):\n             return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n \n+        if hasattr(self.model_tester, \"num_hidden_layers\"):\n+            self.model_tester.num_hidden_layers = 1\n+\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            config.rms_norm_eps = 1.0\n+            config.layer_norm_eps = 1.0\n+            config.norm_eps = 1.0\n+            config.norm_epsilon = 1.0\n+            config.layer_norm_epsilon = 1.0\n+\n+            for attr in [\"text_config\", \"vision_config\", \"text_encoder\", \"audio_encoder\", \"decoder\"]:\n+                if hasattr(config, attr):\n+                    getattr(config, attr).rms_norm_eps = 1.0\n+                    getattr(config, attr).layer_norm_eps = 1.0\n+                    getattr(config, attr).norm_eps = 1.0\n+                    getattr(config, attr).norm_epsilon = 1.0\n+                    getattr(config, attr).layer_norm_epsilon = 1.0\n+\n             model = model_class(config)\n \n             is_encoder_decoder = model.config.is_encoder_decoder\n@@ -1567,12 +1570,19 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n+                for x in model_eager.modules():\n+                    if isinstance(x, (torch.nn.LayerNorm, torch.nn.GroupNorm)):\n+                        x.eps = 1.0\n+                for x in model_sdpa.modules():\n+                    if isinstance(x, (torch.nn.LayerNorm, torch.nn.GroupNorm)):\n+                        x.eps = 1.0\n+\n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand\n                 fail_cases = []\n                 for padding_side in [\"left\", \"right\"]:\n                     for use_mask in [False, True]:\n-                        for batch_size in [1, 5]:\n+                        for batch_size in [7]:\n                             dummy_input = inputs_dict[model.main_input_name]\n \n                             if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n@@ -1622,11 +1632,11 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 dummy_attention_mask[:] = 1\n                                 if padding_side == \"left\":\n-                                    dummy_attention_mask[-1, :-1] = 1\n-                                    dummy_attention_mask[-1, -4:] = 0\n+                                    dummy_attention_mask[-1, :2] = 0\n+                                    dummy_attention_mask[-1, 2:] = 1\n                                 elif padding_side == \"right\":\n-                                    dummy_attention_mask[-1, 1:] = 1\n-                                    dummy_attention_mask[-1, :3] = 0\n+                                    dummy_attention_mask[-1, -2:] = 0\n+                                    dummy_attention_mask[-1, :-2] = 1\n \n                             for enable_kernels in [False, True]:\n                                 failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n@@ -1687,52 +1697,32 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 # Masked tokens output slightly deviates - we don't mind that.\n                                 if use_mask:\n+                                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                                    _logits_eager = torch.zeros_like(input=logits_eager)\n+\n+                                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                                    _logits_eager[:-1] = logits_eager[:-1]\n+\n                                     if padding_side == \"left\":\n-                                        sub_sdpa = logits_sdpa[:-1]\n-                                        sub_eager = logits_eager[:-1]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        sub_sdpa = logits_sdpa[-1, :-4]\n-                                        sub_eager = logits_eager[-1, :-4]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        # Testing the padding tokens is not really meaningful but anyway\n-                                        # sub_sdpa = logits_sdpa[-1, -4:]\n-                                        # sub_eager = logits_eager[-1, -4:]\n-                                        # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                        #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n+                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n+\n                                     elif padding_side == \"right\":\n-                                        sub_sdpa = logits_sdpa[:-1]\n-                                        sub_eager = logits_eager[:-1]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        sub_sdpa = logits_sdpa[-1, 3:]\n-                                        sub_eager = logits_eager[-1, 3:]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        # Testing the padding tokens is not really meaningful but anyway\n-                                        # sub_sdpa = logits_sdpa[-1, :3]\n-                                        # sub_eager = logits_eager[-1, :3]\n-                                        # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                        #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n+                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n \n-                                else:\n-                                    if not torch.allclose(logits_sdpa, logits_eager, atol=atol, rtol=rtol):\n-                                        fail_cases.append(\n-                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                        )\n+                                    logits_sdpa = _logits_sdpa\n+                                    logits_eager = _logits_eager\n+\n+                                results = [\n+                                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+                                ]\n+                                # If 80% batch elements have matched results, it's fine\n+                                if np.mean(results) < 0.8:\n+                                    fail_cases.append(\n+                                        get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n+                                    )\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n "
        },
        {
            "sha": "de7a2745ca073f7c5abf5f702539769d7cbc7202",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 64,
            "deletions": 98,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -460,7 +460,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n-    @slow\n     # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_inference\n     def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         if not self.has_attentions:\n@@ -487,8 +486,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n \n         atols = {\n             (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-6,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -499,8 +500,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         }\n         rtols = {\n             (\"cpu\", False, torch.float32): 1e-4,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-4,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-4,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -536,7 +539,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 fail_cases = []\n                 for padding_side in [\"left\", \"right\"]:\n                     for use_mask in [False, True]:\n-                        for batch_size in [1, 5]:\n+                        for batch_size in [7]:\n                             # Ignore copy\n                             batch_size_input_ids = self.model_tester.num_codebooks * batch_size\n                             dummy_input = inputs_dict[model.main_input_name]\n@@ -593,11 +596,11 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 dummy_attention_mask[:] = 1\n                                 if padding_side == \"left\":\n-                                    dummy_attention_mask[-1, :-1] = 1\n-                                    dummy_attention_mask[-1, -4:] = 0\n+                                    dummy_attention_mask[-1, :2] = 0\n+                                    dummy_attention_mask[-1, 2:] = 1\n                                 elif padding_side == \"right\":\n-                                    dummy_attention_mask[-1, 1:] = 1\n-                                    dummy_attention_mask[-1, :3] = 0\n+                                    dummy_attention_mask[-1, -2:] = 0\n+                                    dummy_attention_mask[-1, :-2] = 1\n \n                             for enable_kernels in [False, True]:\n                                 failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n@@ -640,52 +643,32 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 # Masked tokens output slightly deviates - we don't mind that.\n                                 if use_mask:\n+                                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                                    _logits_eager = torch.zeros_like(input=logits_eager)\n+\n+                                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                                    _logits_eager[:-1] = logits_eager[:-1]\n+\n                                     if padding_side == \"left\":\n-                                        sub_sdpa = logits_sdpa[:-1]\n-                                        sub_eager = logits_eager[:-1]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        sub_sdpa = logits_sdpa[-1, :-4]\n-                                        sub_eager = logits_eager[-1, :-4]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        # Testing the padding tokens is not really meaningful but anyway\n-                                        # sub_sdpa = logits_sdpa[-1, -4:]\n-                                        # sub_eager = logits_eager[-1, -4:]\n-                                        # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                        #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n+                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n+\n                                     elif padding_side == \"right\":\n-                                        sub_sdpa = logits_sdpa[:-1]\n-                                        sub_eager = logits_eager[:-1]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        sub_sdpa = logits_sdpa[-1, 3:]\n-                                        sub_eager = logits_eager[-1, 3:]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        # Testing the padding tokens is not really meaningful but anyway\n-                                        # sub_sdpa = logits_sdpa[-1, :3]\n-                                        # sub_eager = logits_eager[-1, :3]\n-                                        # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                        #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n+                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n \n-                                else:\n-                                    if not torch.allclose(logits_sdpa, logits_eager, atol=atol, rtol=rtol):\n-                                        fail_cases.append(\n-                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                        )\n+                                    logits_sdpa = _logits_sdpa\n+                                    logits_eager = _logits_eager\n+\n+                                results = [\n+                                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+                                ]\n+                                # If 80% batch elements have matched results, it's fine\n+                                if np.mean(results) < 0.8:\n+                                    fail_cases.append(\n+                                        get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n+                                    )\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n \n@@ -1486,7 +1469,6 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n-    @slow\n     # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_inference\n     def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         if not self.all_model_classes[0]._supports_sdpa:\n@@ -1510,8 +1492,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n \n         atols = {\n             (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-6,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -1522,8 +1506,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         }\n         rtols = {\n             (\"cpu\", False, torch.float32): 1e-4,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-4,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-4,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -1559,7 +1545,7 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 fail_cases = []\n                 for padding_side in [\"left\", \"right\"]:\n                     for use_mask in [False, True]:\n-                        for batch_size in [1, 5]:\n+                        for batch_size in [7]:\n                             dummy_input = inputs_dict[model.main_input_name]\n \n                             if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n@@ -1609,11 +1595,11 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 dummy_attention_mask[:] = 1\n                                 if padding_side == \"left\":\n-                                    dummy_attention_mask[-1, :-1] = 1\n-                                    dummy_attention_mask[-1, -4:] = 0\n+                                    dummy_attention_mask[-1, :2] = 0\n+                                    dummy_attention_mask[-1, 2:] = 1\n                                 elif padding_side == \"right\":\n-                                    dummy_attention_mask[-1, 1:] = 1\n-                                    dummy_attention_mask[-1, :3] = 0\n+                                    dummy_attention_mask[-1, -2:] = 0\n+                                    dummy_attention_mask[-1, :-2] = 1\n \n                             for enable_kernels in [False, True]:\n                                 failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n@@ -1674,52 +1660,32 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                                 # Masked tokens output slightly deviates - we don't mind that.\n                                 if use_mask:\n+                                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                                    _logits_eager = torch.zeros_like(input=logits_eager)\n+\n+                                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                                    _logits_eager[:-1] = logits_eager[:-1]\n+\n                                     if padding_side == \"left\":\n-                                        sub_sdpa = logits_sdpa[:-1]\n-                                        sub_eager = logits_eager[:-1]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        sub_sdpa = logits_sdpa[-1, :-4]\n-                                        sub_eager = logits_eager[-1, :-4]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        # Testing the padding tokens is not really meaningful but anyway\n-                                        # sub_sdpa = logits_sdpa[-1, -4:]\n-                                        # sub_eager = logits_eager[-1, -4:]\n-                                        # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                        #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n+                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n+\n                                     elif padding_side == \"right\":\n-                                        sub_sdpa = logits_sdpa[:-1]\n-                                        sub_eager = logits_eager[:-1]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        sub_sdpa = logits_sdpa[-1, 3:]\n-                                        sub_eager = logits_eager[-1, 3:]\n-                                        if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                            fail_cases.append(\n-                                                get_mean_reldiff(failcase, sub_sdpa, sub_eager, atol, rtol)\n-                                            )\n-\n-                                        # Testing the padding tokens is not really meaningful but anyway\n-                                        # sub_sdpa = logits_sdpa[-1, :3]\n-                                        # sub_eager = logits_eager[-1, :3]\n-                                        # if not torch.allclose(sub_sdpa, sub_eager, atol=atol, rtol=rtol):\n-                                        #     fail_cases.append(get_mean_reldiff(failcase, sub_sdpa, sub_eager, 4e-2, 4e-2))\n+                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n \n-                                else:\n-                                    if not torch.allclose(logits_sdpa, logits_eager, atol=atol, rtol=rtol):\n-                                        fail_cases.append(\n-                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                        )\n+                                    logits_sdpa = _logits_sdpa\n+                                    logits_eager = _logits_eager\n+\n+                                results = [\n+                                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+                                ]\n+                                # If 80% batch elements have matched results, it's fine\n+                                if np.mean(results) < 0.8:\n+                                    fail_cases.append(\n+                                        get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n+                                    )\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n "
        },
        {
            "sha": "fd62c74d3d6e11a1a522d321f16b062a08a9e217",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -19,7 +19,6 @@\n import unittest\n \n import pytest\n-from parameterized import parameterized\n \n from transformers import NemotronConfig, is_torch_available\n from transformers.testing_utils import (\n@@ -99,15 +98,6 @@ def setUp(self):\n         self.model_tester = NemotronModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=NemotronConfig, hidden_size=37)\n \n-    @require_torch_sdpa\n-    @slow\n-    @unittest.skip(\n-        reason=\"Due to custom causal mask, there is a slightly too big difference between eager and sdpa in bfloat16.\"\n-    )\n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        pass\n-\n     @unittest.skip(\"Eager and SDPA do not produce the same outputs, thus this test fails\")\n     def test_model_outputs_equivalence(self, **kwargs):\n         pass"
        },
        {
            "sha": "ce44436a20ad2c590eb25b0e52df54ca5e8be588",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -17,7 +17,6 @@\n import unittest\n \n import requests\n-from parameterized import parameterized\n \n from transformers import (\n     PaliGemmaConfig,\n@@ -30,7 +29,6 @@\n     cleanup,\n     require_read_token,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -301,14 +299,6 @@ def test_disk_offload_safetensors(self):\n     def test_model_parallelism(self):\n         pass\n \n-    @require_torch_sdpa\n-    @slow\n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        self.skipTest(\n-            \"Due to custom causal mask, there is a slightly too big difference between eager and sdpa in bfloat16.\"\n-        )\n-\n     @unittest.skip(\n         reason=\"PaliGemmma's SigLip encoder uses the same initialization scheme as the Flax original implementation\"\n     )"
        },
        {
            "sha": "c3902c9e75bc6614e77591eb67983174cb77c7e8",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -66,12 +66,12 @@ def __init__(\n         bos_token_id=0,\n         eos_token_id=1,\n         pad_token_id=2,\n-        vision_start_token_id=151652,\n-        image_token_id=151655,\n-        video_token_id=151656,\n+        vision_start_token_id=3,\n+        image_token_id=4,\n+        video_token_id=5,\n         hidden_act=\"silu\",\n         hidden_size=32,\n-        vocab_size=152064,\n+        vocab_size=99,\n         intermediate_size=37,\n         max_position_embeddings=512,\n         max_window_layers=3,\n@@ -166,6 +166,8 @@ def prepare_config_and_inputs_for_common(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n \n+        input_ids[:, -1] = self.pad_token_id\n+        input_ids[input_ids == self.video_token_id] = self.pad_token_id\n         input_ids[input_ids == self.image_token_id] = self.pad_token_id\n         input_ids[:, self.num_image_tokens] = self.image_token_id\n         labels = torch.zeros("
        },
        {
            "sha": "090907b164e80d79887e9f3526e252aec7893ef4",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -57,8 +57,8 @@ def __init__(\n         image_token_index=0,\n         video_token_index=1,\n         projector_hidden_act=\"gelu\",\n-        seq_length=13,\n-        num_frames=8,\n+        seq_length=3,\n+        num_frames=2,\n         vision_feature_select_strategy=\"default\",\n         vision_feature_layer=-1,\n         text_config={\n@@ -88,7 +88,7 @@ def __init__(\n         vision_config={\n             \"model_type\": \"clip_vision_model\",\n             \"batch_size\": 12,\n-            \"image_size\": 30,\n+            \"image_size\": 8,\n             \"patch_size\": 6,\n             \"num_channels\": 3,\n             \"is_training\": True,\n@@ -123,10 +123,11 @@ def __init__(\n         self.batch_size = 5\n         self.num_channels = 3\n         self.image_size = 224\n-        self.encoder_seq_length = 246\n-        self.num_image_tokens = 25\n-        self.num_video_tokens = 26 * self.num_frames\n+\n+        self.num_image_tokens = (vision_config[\"image_size\"] // vision_config[\"patch_size\"]) ** 2\n+        self.num_video_tokens = (self.num_image_tokens + 1) * self.num_frames\n         self.seq_length = seq_length + self.num_image_tokens + self.num_video_tokens\n+        self.encoder_seq_length = self.seq_length\n \n     def get_config(self):\n         return VideoLlavaConfig("
        },
        {
            "sha": "212eae1471222f4d4e043f476116e362f3237e4f",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -22,7 +22,7 @@\n \n from transformers import VideoMAEConfig\n from transformers.models.auto import get_values\n-from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torch_sdpa, require_vision, slow, torch_device\n from transformers.utils import cached_property, is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -213,6 +213,11 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n \n         return inputs_dict\n \n+    @unittest.skip(\"`mse_cpu` not implemented for 'BFloat16'\")\n+    @require_torch_sdpa\n+    def test_eager_matches_sdpa_inference_1_bfloat16(self):\n+        pass\n+\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "94b5e175bf88a29f8cb20f8a0b77f3b6bbc58599",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 33,
            "deletions": 3,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=f2d5dfbab2af25d6fbfb4a315c78bcdfad9f62d7",
            "patch": "@@ -3928,7 +3928,6 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n-    @slow\n     def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n@@ -3954,8 +3953,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n \n         atols = {\n             (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-6,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -3966,8 +3967,10 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         }\n         rtols = {\n             (\"cpu\", False, torch.float32): 1e-4,\n+            (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n             (\"cpu\", True, torch.float32): 1e-4,\n+            (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float32): 1e-4,\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n@@ -3983,12 +3986,31 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n         if hasattr(self.model_tester, \"num_hidden_layers\"):\n             self.model_tester.num_hidden_layers = 1\n         if hasattr(self.model_tester, \"vision_config\") and \"num_hidden_layers\" in self.model_tester.vision_config:\n+            self.model_tester.vision_config = copy.deepcopy(self.model_tester.vision_config)\n             self.model_tester.vision_config[\"num_hidden_layers\"] = 1\n         if hasattr(self.model_tester, \"text_config\") and \"num_hidden_layers\" in self.model_tester.text_config:\n+            self.model_tester.text_config = copy.deepcopy(self.model_tester.text_config)\n             self.model_tester.text_config[\"num_hidden_layers\"] = 1\n \n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            config.rms_norm_eps = 1.0\n+            config.layer_norm_eps = 1.0\n+            config.norm_eps = 1.0\n+            config.norm_epsilon = 1.0\n+            config.layer_norm_epsilon = 1.0\n+\n+            # norm layers (layer/group norm, etc.) could cause flaky tests when the tensors have very small variance.\n+            # (We don't need the original epsilon values to check eager/sdpa matches)\n+            for attr in [\"text_config\", \"vision_config\", \"text_encoder\", \"audio_encoder\", \"decoder\"]:\n+                if hasattr(config, attr):\n+                    getattr(config, attr).rms_norm_eps = 1.0\n+                    getattr(config, attr).layer_norm_eps = 1.0\n+                    getattr(config, attr).norm_eps = 1.0\n+                    getattr(config, attr).norm_epsilon = 1.0\n+                    getattr(config, attr).layer_norm_epsilon = 1.0\n+\n             model = model_class(config)\n             # FIXME: we deactivate boolean mask for models using \"use_mask_token\" in their constructors.\n             # These models support masking only in the case `use_mask_token=True`. Otherwise they cannot consume an input mask.\n@@ -4000,14 +4022,22 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n+                model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n \n                 model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch_dtype,\n                     attn_implementation=\"eager\",\n                 )\n-                model_eager = model_eager.eval().to(torch_device)\n+                model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n+\n+                # Another way to make sure norm layers have desired epsilon. (Some models don't set it from its config.)\n+                for x in model_eager.modules():\n+                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n+                        x.eps = 1.0\n+                for x in model_sdpa.modules():\n+                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n+                        x.eps = 1.0\n \n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand"
        }
    ],
    "stats": {
        "total": 919,
        "additions": 282,
        "deletions": 637
    }
}