{
    "author": "itazap",
    "message": "fix tiktoken convert to pass AddedToken to Tokenizer (#36566)\n\n* pass AddedToken to Tokenizer\n\n* ruff\n\n* handle dict for special tokens\n\n* option: test tokenizer from tiktoken same as fast\n\n* ruff\n\n* ruff",
    "sha": "3f03c379d2b48fa5f84b907599211098d95d670a",
    "files": [
        {
            "sha": "c8b154a8d16ed459191bb8f1271a5841281bc6e6",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f03c379d2b48fa5f84b907599211098d95d670a/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f03c379d2b48fa5f84b907599211098d95d670a/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=3f03c379d2b48fa5f84b907599211098d95d670a",
            "patch": "@@ -1580,7 +1580,9 @@ def __init__(\n         self.vocab_file = vocab_file\n         self.pattern = pattern\n         self.add_prefix_space = add_prefix_space\n-        self.additional_special_tokens = additional_special_tokens\n+        self.additional_special_tokens = (\n+            additional_special_tokens.keys() if type(additional_special_tokens) is dict else additional_special_tokens\n+        )\n \n     def extract_vocab_merges_from_model(self, tiktoken_url: str):\n         try:\n@@ -1629,7 +1631,10 @@ def converted(self) -> Tokenizer:\n             ]\n         )\n         tokenizer.decoder = decoders.ByteLevel()\n-        tokenizer.add_special_tokens(self.additional_special_tokens)\n+\n+        tokenizer.add_special_tokens(\n+            [AddedToken(token, normalized=False, special=True) for token in self.additional_special_tokens]\n+        )\n \n         tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n "
        },
        {
            "sha": "5d63061dafb2cee6d0302fadf1bc4ae39ee5ecdd",
            "filename": "tests/models/gpt2/test_tokenization_gpt2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f03c379d2b48fa5f84b907599211098d95d670a/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f03c379d2b48fa5f84b907599211098d95d670a/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py?ref=3f03c379d2b48fa5f84b907599211098d95d670a",
            "patch": "@@ -20,7 +20,7 @@\n \n from transformers import AutoTokenizer, GPT2Tokenizer, GPT2TokenizerFast\n from transformers.models.gpt2.tokenization_gpt2 import VOCAB_FILES_NAMES\n-from transformers.testing_utils import require_jinja, require_tokenizers\n+from transformers.testing_utils import require_jinja, require_tiktoken, require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -299,6 +299,23 @@ def test_tokenization_for_chat(self):\n         for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n             self.assertListEqual(tokenized_chat, expected_tokens)\n \n+    @require_tiktoken\n+    def test_tokenization_tiktoken(self):\n+        from tiktoken import encoding_name_for_model\n+\n+        from transformers.integrations.tiktoken import convert_tiktoken_to_fast\n+\n+        encoding = encoding_name_for_model(\"gpt2\")\n+        convert_tiktoken_to_fast(encoding, self.tmpdirname)\n+\n+        tiktoken_fast_tokenizer = GPT2TokenizerFast.from_pretrained(self.tmpdirname)\n+        rust_tokenizer = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\")\n+        sequence = \"lower newer\"\n+        self.assertEqual(\n+            rust_tokenizer.decode(rust_tokenizer.encode(sequence)),\n+            tiktoken_fast_tokenizer.decode(rust_tokenizer.encode(sequence)),\n+        )\n+\n \n @require_tokenizers\n class OPTTokenizationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 28,
        "additions": 25,
        "deletions": 3
    }
}