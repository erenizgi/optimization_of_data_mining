{
    "author": "LysandreJik",
    "message": "Cleanup: continue the init refactor (#35167)\n\nRound 2",
    "sha": "8e806a336f53af4603fcc6868e6abd9058fbb8fd",
    "files": [
        {
            "sha": "3fe10d60c03a927265b8bd9f2f0f44f8fe358026",
            "filename": "src/transformers/models/audio_spectrogram_transformer/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 39,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,47 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_audio_spectrogram_transformer\": [\"ASTConfig\"],\n-    \"feature_extraction_audio_spectrogram_transformer\": [\"ASTFeatureExtractor\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_audio_spectrogram_transformer\"] = [\n-        \"ASTForAudioClassification\",\n-        \"ASTModel\",\n-        \"ASTPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_audio_spectrogram_transformer import (\n-        ASTConfig,\n-    )\n-    from .feature_extraction_audio_spectrogram_transformer import ASTFeatureExtractor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_audio_spectrogram_transformer import (\n-            ASTForAudioClassification,\n-            ASTModel,\n-            ASTPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_audio_spectrogram_transformer import *\n+    from .convert_audio_spectrogram_transformer_original_to_pytorch import *\n+    from .feature_extraction_audio_spectrogram_transformer import *\n+    from .modeling_audio_spectrogram_transformer import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "77bec930236f600ae7ee5117d1d2f86a5a12fefd",
            "filename": "src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -126,3 +126,6 @@ def __init__(\n     # generative parameters deprecation cycle, overwriting this function prevents this from happening.\n     def _get_non_default_generation_parameters(self) -> Dict[str, Any]:\n         return {}\n+\n+\n+__all__ = [\"ASTConfig\"]"
        },
        {
            "sha": "b181afe19e9ef8ae13778ddd99fd1f369030c7bd",
            "filename": "src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -234,3 +234,6 @@ def __call__(\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n         return padded_inputs\n+\n+\n+__all__ = [\"ASTFeatureExtractor\"]"
        },
        {
            "sha": "a9fe0d75f5c3809452aee47f9b114f6606255815",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -670,3 +670,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"ASTForAudioClassification\", \"ASTModel\", \"ASTPreTrainedModel\"]"
        },
        {
            "sha": "6c21cf99976a150f1e4e3f6e6ec1574f2b1a17e1",
            "filename": "src/transformers/models/bark/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 55,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbark%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbark%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,63 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_bark\": [\n-        \"BarkCoarseConfig\",\n-        \"BarkConfig\",\n-        \"BarkFineConfig\",\n-        \"BarkSemanticConfig\",\n-    ],\n-    \"processing_bark\": [\"BarkProcessor\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_bark\"] = [\n-        \"BarkFineModel\",\n-        \"BarkSemanticModel\",\n-        \"BarkCoarseModel\",\n-        \"BarkModel\",\n-        \"BarkPreTrainedModel\",\n-        \"BarkCausalModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_bark import (\n-        BarkCoarseConfig,\n-        BarkConfig,\n-        BarkFineConfig,\n-        BarkSemanticConfig,\n-    )\n-    from .processing_bark import BarkProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_bark import (\n-            BarkCausalModel,\n-            BarkCoarseModel,\n-            BarkFineModel,\n-            BarkModel,\n-            BarkPreTrainedModel,\n-            BarkSemanticModel,\n-        )\n-\n+    from .configuration_bark import *\n+    from .convert_suno_to_hf import *\n+    from .generation_configuration_bark import *\n+    from .modeling_bark import *\n+    from .processing_bark import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "932bad618aa187ce0e55dc772fc803ef9d1d08dd",
            "filename": "src/transformers/models/bark/configuration_bark.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -298,3 +298,6 @@ def from_sub_model_configs(\n             codec_config=codec_config.to_dict(),\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"BarkCoarseConfig\", \"BarkConfig\", \"BarkFineConfig\", \"BarkSemanticConfig\"]"
        },
        {
            "sha": "9e225ac9ae15c0e0bfa3eff4607fae646bad6d29",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1819,3 +1819,13 @@ def _check_and_enable_flash_attn_2(\n         config.coarse_acoustics_config._attn_implementation = config._attn_implementation\n         config.fine_acoustics_config._attn_implementation = config._attn_implementation\n         return config\n+\n+\n+__all__ = [\n+    \"BarkFineModel\",\n+    \"BarkSemanticModel\",\n+    \"BarkCoarseModel\",\n+    \"BarkModel\",\n+    \"BarkPreTrainedModel\",\n+    \"BarkCausalModel\",\n+]"
        },
        {
            "sha": "0bed6ca79f410bb9e0ba637192f36d60d726050f",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -285,3 +285,6 @@ def __call__(\n             encoded_text[\"history_prompt\"] = voice_preset\n \n         return encoded_text\n+\n+\n+__all__ = [\"BarkProcessor\"]"
        },
        {
            "sha": "11c3f4863f46a1dd90a918a8ad4ab89e5595d141",
            "filename": "src/transformers/models/bart/__init__.py",
            "status": "modified",
            "additions": 12,
            "deletions": 126,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,134 +13,20 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_bart\": [\"BartConfig\", \"BartOnnxConfig\"],\n-    \"tokenization_bart\": [\"BartTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_bart_fast\"] = [\"BartTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_bart\"] = [\n-        \"BartForCausalLM\",\n-        \"BartForConditionalGeneration\",\n-        \"BartForQuestionAnswering\",\n-        \"BartForSequenceClassification\",\n-        \"BartModel\",\n-        \"BartPreTrainedModel\",\n-        \"BartPretrainedModel\",\n-        \"PretrainedBartModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_bart\"] = [\n-        \"TFBartForConditionalGeneration\",\n-        \"TFBartForSequenceClassification\",\n-        \"TFBartModel\",\n-        \"TFBartPretrainedModel\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_bart\"] = [\n-        \"FlaxBartDecoderPreTrainedModel\",\n-        \"FlaxBartForCausalLM\",\n-        \"FlaxBartForConditionalGeneration\",\n-        \"FlaxBartForQuestionAnswering\",\n-        \"FlaxBartForSequenceClassification\",\n-        \"FlaxBartModel\",\n-        \"FlaxBartPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_bart import BartConfig, BartOnnxConfig\n-    from .tokenization_bart import BartTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_bart_fast import BartTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_bart import (\n-            BartForCausalLM,\n-            BartForConditionalGeneration,\n-            BartForQuestionAnswering,\n-            BartForSequenceClassification,\n-            BartModel,\n-            BartPreTrainedModel,\n-            BartPretrainedModel,\n-            PretrainedBartModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_bart import (\n-            TFBartForConditionalGeneration,\n-            TFBartForSequenceClassification,\n-            TFBartModel,\n-            TFBartPretrainedModel,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_bart import (\n-            FlaxBartDecoderPreTrainedModel,\n-            FlaxBartForCausalLM,\n-            FlaxBartForConditionalGeneration,\n-            FlaxBartForQuestionAnswering,\n-            FlaxBartForSequenceClassification,\n-            FlaxBartModel,\n-            FlaxBartPreTrainedModel,\n-        )\n-\n+    from .configuration_bart import *\n+    from .convert_bart_original_pytorch_checkpoint_to_pytorch import *\n+    from .modeling_bart import *\n+    from .modeling_flax_bart import *\n+    from .modeling_tf_bart import *\n+    from .tokenization_bart import *\n+    from .tokenization_bart_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "4ce4316e3c03150d1845b7a3f96409ae4b637e55",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -400,3 +400,6 @@ def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n             flattened_output = super(OnnxSeq2SeqConfigWithPast, self)._flatten_past_key_values_(\n                 flattened_output, name, idx, t\n             )\n+\n+\n+__all__ = [\"BartConfig\", \"BartOnnxConfig\"]"
        },
        {
            "sha": "dd1b69c8127fb8a3538bc25be296e3bc875d2a5a",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -2158,3 +2158,15 @@ def _reorder_cache(past_key_values, beam_idx):\n                 tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n         return reordered_past\n+\n+\n+__all__ = [\n+    \"BartForCausalLM\",\n+    \"BartForConditionalGeneration\",\n+    \"BartForQuestionAnswering\",\n+    \"BartForSequenceClassification\",\n+    \"BartModel\",\n+    \"BartPreTrainedModel\",\n+    \"BartPretrainedModel\",\n+    \"PretrainedBartModel\",\n+]"
        },
        {
            "sha": "b346eaa39fc199b0267fc59f329c841d05440084",
            "filename": "src/transformers/models/bart/modeling_flax_bart.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1993,3 +1993,14 @@ def update_inputs_for_generation(self, model_outputs, model_kwargs):\n     FlaxCausalLMOutputWithCrossAttentions,\n     _CONFIG_FOR_DOC,\n )\n+\n+\n+__all__ = [\n+    \"FlaxBartDecoderPreTrainedModel\",\n+    \"FlaxBartForCausalLM\",\n+    \"FlaxBartForConditionalGeneration\",\n+    \"FlaxBartForQuestionAnswering\",\n+    \"FlaxBartForSequenceClassification\",\n+    \"FlaxBartModel\",\n+    \"FlaxBartPreTrainedModel\",\n+]"
        },
        {
            "sha": "7ab9817986e6adff40a2dde760d0aea89e7f91ad",
            "filename": "src/transformers/models/bart/modeling_tf_bart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1709,3 +1709,6 @@ def build(self, input_shape=None):\n         if getattr(self, \"classification_head\", None) is not None:\n             with tf.name_scope(self.classification_head.name):\n                 self.classification_head.build(None)\n+\n+\n+__all__ = [\"TFBartForConditionalGeneration\", \"TFBartForSequenceClassification\", \"TFBartModel\", \"TFBartPretrainedModel\"]"
        },
        {
            "sha": "4c516cb81be0d260e562d09103691635eba77118",
            "filename": "src/transformers/models/bart/tokenization_bart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -388,3 +388,6 @@ def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n         if (is_split_into_words or add_prefix_space) and (len(text) > 0 and not text[0].isspace()):\n             text = \" \" + text\n         return (text, kwargs)\n+\n+\n+__all__ = [\"BartTokenizer\"]"
        },
        {
            "sha": "4586ab4797e5ec91ab932e331de5399cdee79b7a",
            "filename": "src/transformers/models/bart/tokenization_bart_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -274,3 +274,6 @@ def create_token_type_ids_from_sequences(\n         if token_ids_1 is None:\n             return len(cls + token_ids_0 + sep) * [0]\n         return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n+\n+\n+__all__ = [\"BartTokenizerFast\"]"
        },
        {
            "sha": "323fe2fe8af9823d4478957b2f94b078ec39b7f3",
            "filename": "src/transformers/models/barthez/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 39,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbarthez%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbarthez%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,49 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_sentencepiece_available, is_tokenizers_available\n-\n-\n-_import_structure = {}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_barthez\"] = [\"BarthezTokenizer\"]\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_barthez_fast\"] = [\"BarthezTokenizerFast\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_barthez import BarthezTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_barthez_fast import BarthezTokenizerFast\n-\n+    from .tokenization_barthez import *\n+    from .tokenization_barthez_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "604f9c7c21519a655b02248e7fc2b41e786d5bd1",
            "filename": "src/transformers/models/barthez/tokenization_barthez.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -284,3 +284,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                 fi.write(content_spiece_model)\n \n         return (out_vocab_file,)\n+\n+\n+__all__ = [\"BarthezTokenizer\"]"
        },
        {
            "sha": "a1d95ef03e48820f39dd91adf117fb6cdc94fe0a",
            "filename": "src/transformers/models/barthez/tokenization_barthez_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -192,3 +192,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n             copyfile(self.vocab_file, out_vocab_file)\n \n         return (out_vocab_file,)\n+\n+\n+__all__ = [\"BarthezTokenizerFast\"]"
        },
        {
            "sha": "597be95d8175cac9d48edf3eb4d42a2a91b95833",
            "filename": "src/transformers/models/bartpho/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 22,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbartpho%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbartpho%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbartpho%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,32 +11,16 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_sentencepiece_available\n-\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-_import_structure = {}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_bartpho\"] = [\"BartphoTokenizer\"]\n \n if TYPE_CHECKING:\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_bartpho import BartphoTokenizer\n-\n+    from .tokenization_bartpho import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e6e4f889842e8f49d64017e1f56324a0fa7fe5b9",
            "filename": "src/transformers/models/bartpho/tokenization_bartpho.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -311,3 +311,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                         fp.write(f\"{str(token)} \\n\")\n \n         return out_vocab_file, out_monolingual_vocab_file\n+\n+\n+__all__ = [\"BartphoTokenizer\"]"
        },
        {
            "sha": "0fc8919c7ea19af6f20199eac15801109e623faa",
            "filename": "src/transformers/models/beit/__init__.py",
            "status": "modified",
            "additions": 11,
            "deletions": 90,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,100 +11,21 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_torch_available,\n-    is_vision_available,\n-)\n-\n-\n-_import_structure = {\"configuration_beit\": [\"BeitConfig\", \"BeitOnnxConfig\"]}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"feature_extraction_beit\"] = [\"BeitFeatureExtractor\"]\n-    _import_structure[\"image_processing_beit\"] = [\"BeitImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_beit\"] = [\n-        \"BeitForImageClassification\",\n-        \"BeitForMaskedImageModeling\",\n-        \"BeitForSemanticSegmentation\",\n-        \"BeitModel\",\n-        \"BeitPreTrainedModel\",\n-        \"BeitBackbone\",\n-    ]\n-\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_beit\"] = [\n-        \"FlaxBeitForImageClassification\",\n-        \"FlaxBeitForMaskedImageModeling\",\n-        \"FlaxBeitModel\",\n-        \"FlaxBeitPreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_beit import BeitConfig, BeitOnnxConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .feature_extraction_beit import BeitFeatureExtractor\n-        from .image_processing_beit import BeitImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_beit import (\n-            BeitBackbone,\n-            BeitForImageClassification,\n-            BeitForMaskedImageModeling,\n-            BeitForSemanticSegmentation,\n-            BeitModel,\n-            BeitPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_beit import (\n-            FlaxBeitForImageClassification,\n-            FlaxBeitForMaskedImageModeling,\n-            FlaxBeitModel,\n-            FlaxBeitPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_beit import *\n+    from .convert_beit_unilm_to_pytorch import *\n+    from .feature_extraction_beit import *\n+    from .image_processing_beit import *\n+    from .modeling_beit import *\n+    from .modeling_flax_beit import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "834988258c6b755fcca9f275db139ed76b9a8b54",
            "filename": "src/transformers/models/beit/configuration_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -224,3 +224,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n     @property\n     def atol_for_validation(self) -> float:\n         return 1e-4\n+\n+\n+__all__ = [\"BeitConfig\", \"BeitOnnxConfig\"]"
        },
        {
            "sha": "141d8bc36d2bbb80c2abcb0084c7369b38dc4f4d",
            "filename": "src/transformers/models/beit/feature_extraction_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Ffeature_extraction_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Ffeature_extraction_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Ffeature_extraction_beit.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -31,3 +31,6 @@ def __init__(self, *args, **kwargs) -> None:\n             FutureWarning,\n         )\n         super().__init__(*args, **kwargs)\n+\n+\n+__all__ = [\"BeitFeatureExtractor\"]"
        },
        {
            "sha": "af76dd2e9656cb46151970b4c470819d57d501e1",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -510,3 +510,6 @@ def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple]\n             semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n \n         return semantic_segmentation\n+\n+\n+__all__ = [\"BeitImageProcessor\"]"
        },
        {
            "sha": "01c16ca2cf000b6cea7edd605576a1f88763593c",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1576,3 +1576,13 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"BeitForImageClassification\",\n+    \"BeitForMaskedImageModeling\",\n+    \"BeitForSemanticSegmentation\",\n+    \"BeitModel\",\n+    \"BeitPreTrainedModel\",\n+    \"BeitBackbone\",\n+]"
        },
        {
            "sha": "2d79c1820088a1c0034ebdb467f9e5c6b3370950",
            "filename": "src/transformers/models/beit/modeling_flax_beit.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -946,3 +946,11 @@ class FlaxBeitForImageClassification(FlaxBeitPreTrainedModel):\n append_replace_return_docstrings(\n     FlaxBeitForImageClassification, output_type=FlaxSequenceClassifierOutput, config_class=BeitConfig\n )\n+\n+\n+__all__ = [\n+    \"FlaxBeitForImageClassification\",\n+    \"FlaxBeitForMaskedImageModeling\",\n+    \"FlaxBeitModel\",\n+    \"FlaxBeitPreTrainedModel\",\n+]"
        },
        {
            "sha": "3ed12a889321e608074929631cc617f47d8777b5",
            "filename": "src/transformers/models/bert/__init__.py",
            "status": "modified",
            "additions": 16,
            "deletions": 173,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,183 +11,26 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_tensorflow_text_available,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_bert\": [\"BertConfig\", \"BertOnnxConfig\"],\n-    \"tokenization_bert\": [\"BasicTokenizer\", \"BertTokenizer\", \"WordpieceTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_bert_fast\"] = [\"BertTokenizerFast\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_bert\"] = [\n-        \"BertForMaskedLM\",\n-        \"BertForMultipleChoice\",\n-        \"BertForNextSentencePrediction\",\n-        \"BertForPreTraining\",\n-        \"BertForQuestionAnswering\",\n-        \"BertForSequenceClassification\",\n-        \"BertForTokenClassification\",\n-        \"BertLayer\",\n-        \"BertLMHeadModel\",\n-        \"BertModel\",\n-        \"BertPreTrainedModel\",\n-        \"load_tf_weights_in_bert\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_bert\"] = [\n-        \"TFBertEmbeddings\",\n-        \"TFBertForMaskedLM\",\n-        \"TFBertForMultipleChoice\",\n-        \"TFBertForNextSentencePrediction\",\n-        \"TFBertForPreTraining\",\n-        \"TFBertForQuestionAnswering\",\n-        \"TFBertForSequenceClassification\",\n-        \"TFBertForTokenClassification\",\n-        \"TFBertLMHeadModel\",\n-        \"TFBertMainLayer\",\n-        \"TFBertModel\",\n-        \"TFBertPreTrainedModel\",\n-    ]\n-try:\n-    if not is_tensorflow_text_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_bert_tf\"] = [\"TFBertTokenizer\"]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_bert\"] = [\n-        \"FlaxBertForCausalLM\",\n-        \"FlaxBertForMaskedLM\",\n-        \"FlaxBertForMultipleChoice\",\n-        \"FlaxBertForNextSentencePrediction\",\n-        \"FlaxBertForPreTraining\",\n-        \"FlaxBertForQuestionAnswering\",\n-        \"FlaxBertForSequenceClassification\",\n-        \"FlaxBertForTokenClassification\",\n-        \"FlaxBertModel\",\n-        \"FlaxBertPreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_bert import BertConfig, BertOnnxConfig\n-    from .tokenization_bert import BasicTokenizer, BertTokenizer, WordpieceTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_bert_fast import BertTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_bert import (\n-            BertForMaskedLM,\n-            BertForMultipleChoice,\n-            BertForNextSentencePrediction,\n-            BertForPreTraining,\n-            BertForQuestionAnswering,\n-            BertForSequenceClassification,\n-            BertForTokenClassification,\n-            BertLayer,\n-            BertLMHeadModel,\n-            BertModel,\n-            BertPreTrainedModel,\n-            load_tf_weights_in_bert,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_bert import (\n-            TFBertEmbeddings,\n-            TFBertForMaskedLM,\n-            TFBertForMultipleChoice,\n-            TFBertForNextSentencePrediction,\n-            TFBertForPreTraining,\n-            TFBertForQuestionAnswering,\n-            TFBertForSequenceClassification,\n-            TFBertForTokenClassification,\n-            TFBertLMHeadModel,\n-            TFBertMainLayer,\n-            TFBertModel,\n-            TFBertPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tensorflow_text_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_bert_tf import TFBertTokenizer\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_bert import (\n-            FlaxBertForCausalLM,\n-            FlaxBertForMaskedLM,\n-            FlaxBertForMultipleChoice,\n-            FlaxBertForNextSentencePrediction,\n-            FlaxBertForPreTraining,\n-            FlaxBertForQuestionAnswering,\n-            FlaxBertForSequenceClassification,\n-            FlaxBertForTokenClassification,\n-            FlaxBertModel,\n-            FlaxBertPreTrainedModel,\n-        )\n-\n+    from .configuration_bert import *\n+    from .convert_bert_original_tf2_checkpoint_to_pytorch import *\n+    from .convert_bert_original_tf_checkpoint_to_pytorch import *\n+    from .convert_bert_pytorch_checkpoint_to_original_tf import *\n+    from .convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch import *\n+    from .modeling_bert import *\n+    from .modeling_flax_bert import *\n+    from .modeling_tf_bert import *\n+    from .tokenization_bert import *\n+    from .tokenization_bert_fast import *\n+    from .tokenization_bert_tf import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "ea29fb81c435aa710dfb8912bf83c7068ed30ed6",
            "filename": "src/transformers/models/bert/configuration_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -149,3 +149,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"token_type_ids\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"BertConfig\", \"BertOnnxConfig\"]"
        },
        {
            "sha": "0c53963cee79220231825eb6c7a8d6dec74e7a6c",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1991,3 +1991,19 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"BertForMaskedLM\",\n+    \"BertForMultipleChoice\",\n+    \"BertForNextSentencePrediction\",\n+    \"BertForPreTraining\",\n+    \"BertForQuestionAnswering\",\n+    \"BertForSequenceClassification\",\n+    \"BertForTokenClassification\",\n+    \"BertLayer\",\n+    \"BertLMHeadModel\",\n+    \"BertModel\",\n+    \"BertPreTrainedModel\",\n+    \"load_tf_weights_in_bert\",\n+]"
        },
        {
            "sha": "83358c86bd280dc171f3d40d9b9b9e1dec93aa43",
            "filename": "src/transformers/models/bert/modeling_flax_bert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1711,3 +1711,17 @@ def update_inputs_for_generation(self, model_outputs, model_kwargs):\n     FlaxCausalLMOutputWithCrossAttentions,\n     _CONFIG_FOR_DOC,\n )\n+\n+\n+__all__ = [\n+    \"FlaxBertForCausalLM\",\n+    \"FlaxBertForMaskedLM\",\n+    \"FlaxBertForMultipleChoice\",\n+    \"FlaxBertForNextSentencePrediction\",\n+    \"FlaxBertForPreTraining\",\n+    \"FlaxBertForQuestionAnswering\",\n+    \"FlaxBertForSequenceClassification\",\n+    \"FlaxBertForTokenClassification\",\n+    \"FlaxBertModel\",\n+    \"FlaxBertPreTrainedModel\",\n+]"
        },
        {
            "sha": "ce862194dc7787b323284c312a0baac40e699e99",
            "filename": "src/transformers/models/bert/modeling_tf_bert.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -2108,3 +2108,19 @@ def build(self, input_shape=None):\n         if getattr(self, \"qa_outputs\", None) is not None:\n             with tf.name_scope(self.qa_outputs.name):\n                 self.qa_outputs.build([None, None, self.config.hidden_size])\n+\n+\n+__all__ = [\n+    \"TFBertEmbeddings\",\n+    \"TFBertForMaskedLM\",\n+    \"TFBertForMultipleChoice\",\n+    \"TFBertForNextSentencePrediction\",\n+    \"TFBertForPreTraining\",\n+    \"TFBertForQuestionAnswering\",\n+    \"TFBertForSequenceClassification\",\n+    \"TFBertForTokenClassification\",\n+    \"TFBertLMHeadModel\",\n+    \"TFBertMainLayer\",\n+    \"TFBertModel\",\n+    \"TFBertPreTrainedModel\",\n+]"
        },
        {
            "sha": "42d4dd94554d411fd37b6328b51887a47401f114",
            "filename": "src/transformers/models/bert/tokenization_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -502,3 +502,6 @@ def tokenize(self, text):\n             else:\n                 output_tokens.extend(sub_tokens)\n         return output_tokens\n+\n+\n+__all__ = [\"BasicTokenizer\", \"BertTokenizer\", \"WordpieceTokenizer\"]"
        },
        {
            "sha": "4a89e6053b988f5b9d8131304c7b3e7e74dba5fd",
            "filename": "src/transformers/models/bert/tokenization_bert_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -170,3 +170,6 @@ def create_token_type_ids_from_sequences(\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"BertTokenizerFast\"]"
        },
        {
            "sha": "b1f49722fbdffab74436eea076bd5ef7cbecee1f",
            "filename": "src/transformers/models/bert/tokenization_bert_tf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -252,3 +252,6 @@ def get_config(self):\n             \"sep_token_id\": self.sep_token_id,\n             \"pad_token_id\": self.pad_token_id,\n         }\n+\n+\n+__all__ = [\"TFBertTokenizer\"]"
        },
        {
            "sha": "3f83b1f6e5bba323d5636820ef89026f072be816",
            "filename": "src/transformers/models/bert_generation/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 51,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_generation%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_generation%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,61 +11,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_sentencepiece_available, is_torch_available\n-\n-\n-_import_structure = {\"configuration_bert_generation\": [\"BertGenerationConfig\"]}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_bert_generation\"] = [\"BertGenerationTokenizer\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_bert_generation\"] = [\n-        \"BertGenerationDecoder\",\n-        \"BertGenerationEncoder\",\n-        \"BertGenerationPreTrainedModel\",\n-        \"load_tf_weights_in_bert_generation\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_bert_generation import BertGenerationConfig\n-\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_bert_generation import BertGenerationTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_bert_generation import (\n-            BertGenerationDecoder,\n-            BertGenerationEncoder,\n-            BertGenerationPreTrainedModel,\n-            load_tf_weights_in_bert_generation,\n-        )\n-\n+    from .configuration_bert_generation import *\n+    from .modeling_bert_generation import *\n+    from .tokenization_bert_generation import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "1abe7c1a1c44ab206f4e3ac459ef599ec007b9bb",
            "filename": "src/transformers/models/bert_generation/configuration_bert_generation.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -122,3 +122,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n+\n+\n+__all__ = [\"BertGenerationConfig\"]"
        },
        {
            "sha": "aaf326aa2de8eb55ca47f4e1d40ed0422f0fc05f",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -996,3 +996,11 @@ def _reorder_cache(self, past_key_values, beam_idx):\n                 tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n         return reordered_past\n+\n+\n+__all__ = [\n+    \"BertGenerationDecoder\",\n+    \"BertGenerationEncoder\",\n+    \"BertGenerationPreTrainedModel\",\n+    \"load_tf_weights_in_bert_generation\",\n+]"
        },
        {
            "sha": "31f046863c289c3bddd71aa408df0223da05c1d9",
            "filename": "src/transformers/models/bert_generation/tokenization_bert_generation.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -170,3 +170,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                 fi.write(content_spiece_model)\n \n         return (out_vocab_file,)\n+\n+\n+__all__ = [\"BertGenerationTokenizer\"]"
        },
        {
            "sha": "f5296087db1d007eab946f795d0c9c8fa4bdaafe",
            "filename": "src/transformers/models/bert_japanese/__init__.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_japanese%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_japanese%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_japanese%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,19 +11,16 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n from ...utils import _LazyModule\n-\n-\n-_import_structure = {\"tokenization_bert_japanese\": [\"BertJapaneseTokenizer\", \"CharacterTokenizer\", \"MecabTokenizer\"]}\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .tokenization_bert_japanese import BertJapaneseTokenizer, CharacterTokenizer, MecabTokenizer\n-\n+    from .tokenization_bert_japanese import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "8a841a3091623d2e50fd62e91a152238789f0159",
            "filename": "src/transformers/models/bert_japanese/tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -977,3 +977,6 @@ def tokenize(self, text):\n                 new_pieces.append(piece)\n \n         return new_pieces\n+\n+\n+__all__ = [\"BertJapaneseTokenizer\", \"CharacterTokenizer\", \"MecabTokenizer\"]"
        },
        {
            "sha": "432622f1595d1a0d8bb1b3c9b9774b7d1e387d3e",
            "filename": "src/transformers/models/bertweet/__init__.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbertweet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbertweet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbertweet%2F__init__.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,19 +11,16 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n from ...utils import _LazyModule\n-\n-\n-_import_structure = {\"tokenization_bertweet\": [\"BertweetTokenizer\"]}\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .tokenization_bertweet import BertweetTokenizer\n-\n+    from .tokenization_bertweet import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "499238e5955fe03fff0fbc51ebec5629dd3a45ba",
            "filename": "src/transformers/models/bertweet/tokenization_bertweet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e806a336f53af4603fcc6868e6abd9058fbb8fd/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py?ref=8e806a336f53af4603fcc6868e6abd9058fbb8fd",
            "patch": "@@ -764,3 +764,6 @@ def casual_tokenize(text, preserve_case=True, reduce_len=False, strip_handles=Fa\n \n \n ###############################################################################\n+\n+\n+__all__ = [\"BertweetTokenizer\"]"
        }
    ],
    "stats": {
        "total": 874,
        "additions": 263,
        "deletions": 611
    }
}