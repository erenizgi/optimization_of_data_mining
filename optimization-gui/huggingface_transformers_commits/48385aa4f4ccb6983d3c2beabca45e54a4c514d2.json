{
    "author": "capemox",
    "message": "Added support for seed in `DataCollatorForWholeWordMask` (#36903)\n\n* Added support for seed in `DataCollatorForWholeWordMask`, and also wrote tests.\n\nAlso fixed bugs where the code hardcoded values for mask replacement probability and random replacement probability, instead of using the values passed by the user.\n\n* formatting issues\n\n* Used better way to generate seed in TF. Made tests more consistent.",
    "sha": "48385aa4f4ccb6983d3c2beabca45e54a4c514d2",
    "files": [
        {
            "sha": "07490a25f9e58682058732f7a7b13601afff7e73",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 120,
            "deletions": 22,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/48385aa4f4ccb6983d3c2beabca45e54a4c514d2/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48385aa4f4ccb6983d3c2beabca45e54a4c514d2/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=48385aa4f4ccb6983d3c2beabca45e54a4c514d2",
            "patch": "@@ -1193,6 +1193,11 @@ class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):\n     </Tip>\"\"\"\n \n     def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+        if self.seed and self.generator is None:\n+            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n+            # If no seed supplied, we will use the global RNG\n+            self.create_rng()\n+\n         if isinstance(examples[0], Mapping):\n             input_ids = [e[\"input_ids\"] for e in examples]\n         else:\n@@ -1223,6 +1228,11 @@ def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> D\n     def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n         import tensorflow as tf\n \n+        if self.seed and self.generator is None:\n+            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n+            # If no seed supplied, we will use the global RNG\n+            self.create_rng()\n+\n         if isinstance(examples[0], Mapping):\n             input_ids = [e[\"input_ids\"] for e in examples]\n         else:\n@@ -1251,6 +1261,11 @@ def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict\n         return {\"input_ids\": inputs, \"labels\": labels}\n \n     def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+        if self.seed and self.generator is None:\n+            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n+            # If no seed supplied, we will use the global RNG\n+            self.create_rng()\n+\n         if isinstance(examples[0], Mapping):\n             input_ids = [e[\"input_ids\"] for e in examples]\n         else:\n@@ -1278,6 +1293,30 @@ def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> D\n         inputs, labels = self.numpy_mask_tokens(batch_input, batch_mask)\n         return {\"input_ids\": inputs, \"labels\": labels}\n \n+    def _shuffle(self, cand_indexes):\n+        # if no seed, just use random's shuffle\n+        if self.seed is None:\n+            random.shuffle(cand_indexes)\n+            return cand_indexes\n+\n+        # if seed is provided, use the generator to shuffle\n+        if self.return_tensors == \"pt\":\n+            import torch\n+\n+            indices = torch.randperm(len(cand_indexes), generator=self.generator)\n+            return [cand_indexes[i] for i in indices]\n+\n+        elif self.return_tensors == \"tf\":\n+            import tensorflow as tf\n+\n+            seed = self.generator.make_seeds(2)[0]\n+            indices = tf.random.experimental.stateless_shuffle(tf.range(len(cand_indexes)), seed=seed).numpy().tolist()\n+            return [cand_indexes[i] for i in indices]\n+\n+        elif self.return_tensors == \"np\":\n+            self.generator.shuffle(cand_indexes)\n+            return cand_indexes\n+\n     def _whole_word_mask(self, input_tokens: List[str], max_predictions=512):\n         \"\"\"\n         Get 0/1 labels for masked tokens with whole word mask proxy\n@@ -1298,7 +1337,7 @@ def _whole_word_mask(self, input_tokens: List[str], max_predictions=512):\n             else:\n                 cand_indexes.append([i])\n \n-        random.shuffle(cand_indexes)\n+        cand_indexes = self._shuffle(cand_indexes)\n         num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * self.mlm_probability))))\n         masked_lms = []\n         covered_indexes = set()\n@@ -1346,16 +1385,32 @@ def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n         masked_indices = probability_matrix.bool()\n         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n \n-        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n+        # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n+        indices_replaced = (\n+            torch.bernoulli(torch.full(labels.shape, self.mask_replace_prob), generator=self.generator).bool()\n+            & masked_indices\n+        )\n         inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n \n-        # 10% of the time, we replace masked input tokens with random word\n-        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n-        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n+        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n+            return inputs, labels\n+\n+        remaining_prob = 1 - self.mask_replace_prob\n+        # scaling the random_replace_prob to the remaining probability for example if\n+        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n+        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n+        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n+\n+        # random_replacement_prob% of the time, we replace masked input tokens with random word\n+        indices_random = (\n+            torch.bernoulli(torch.full(labels.shape, random_replace_prob_scaled), generator=self.generator).bool()\n+            & masked_indices\n+            & ~indices_replaced\n+        )\n+        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long, generator=self.generator)\n         inputs[indices_random] = random_words[indices_random]\n \n-        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n+        # The rest of the time ((1-random_replacement_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n     def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n@@ -1387,17 +1442,35 @@ def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n         # Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens\n         labels = tf.where(masked_indices, inputs, -100)\n \n-        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = self.tf_bernoulli(input_shape, 0.8) & masked_indices\n+        # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n+        indices_replaced = self.tf_bernoulli(input_shape, self.mask_replace_prob, self.generator) & masked_indices\n \n         inputs = tf.where(indices_replaced, self.tokenizer.mask_token_id, inputs)\n \n-        # 10% of the time, we replace masked input tokens with random word\n-        indices_random = self.tf_bernoulli(input_shape, 0.5) & masked_indices & ~indices_replaced\n-        random_words = tf.random.uniform(input_shape, maxval=len(self.tokenizer), dtype=tf.int64)\n+        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n+            return inputs, labels\n+\n+        remaining_prob = 1 - self.mask_replace_prob\n+        # scaling the random_replace_prob to the remaining probability for example if\n+        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n+        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n+        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n+\n+        # random_replace_prob% of the time, we replace masked input tokens with random word\n+        indices_random = (\n+            self.tf_bernoulli(input_shape, random_replace_prob_scaled, self.generator)\n+            & masked_indices\n+            & ~indices_replaced\n+        )\n+\n+        if self.generator:\n+            random_words = self.generator.uniform(input_shape, maxval=len(self.tokenizer), dtype=tf.int64)\n+        else:\n+            random_words = tf.random.uniform(input_shape, maxval=len(self.tokenizer), dtype=tf.int64)\n+\n         inputs = tf.where(indices_random, random_words, inputs)\n \n-        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n+        # The rest of the time ((1-mask_replace_prob-random_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n     def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n@@ -1425,19 +1498,44 @@ def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n \n         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n \n-        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = np.random.binomial(1, 0.8, size=labels.shape).astype(bool) & masked_indices\n+        # mask_replacement_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n+        if self.generator:\n+            indices_replaced = (\n+                self.generator.binomial(1, self.mask_replace_prob, size=labels.shape).astype(bool) & masked_indices\n+            )\n+        else:\n+            indices_replaced = (\n+                np.random.binomial(1, self.mask_replace_prob, size=labels.shape).astype(bool) & masked_indices\n+            )\n         inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n \n-        # 10% of the time, we replace masked input tokens with random word\n-        # indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n-        indices_random = (\n-            np.random.binomial(1, 0.5, size=labels.shape).astype(bool) & masked_indices & ~indices_replaced\n-        )\n-        random_words = np.random.randint(low=0, high=len(self.tokenizer), size=labels.shape, dtype=np.int64)\n+        if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n+            return inputs, labels\n+\n+        remaining_prob = 1 - self.mask_replace_prob\n+        # scaling the random_replace_prob to the remaining probability for example if\n+        # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n+        # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n+        random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n+\n+        if self.generator:\n+            indices_random = (\n+                self.generator.binomial(1, random_replace_prob_scaled, size=labels.shape).astype(bool)\n+                & masked_indices\n+                & ~indices_replaced\n+            )\n+            random_words = self.generator.integers(low=0, high=len(self.tokenizer), size=labels.shape, dtype=np.int64)\n+        else:\n+            indices_random = (\n+                np.random.binomial(1, random_replace_prob_scaled, size=labels.shape).astype(bool)\n+                & masked_indices\n+                & ~indices_replaced\n+            )\n+            random_words = np.random.randint(low=0, high=len(self.tokenizer), size=labels.shape, dtype=np.int64)\n+\n         inputs[indices_random] = random_words[indices_random]\n \n-        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n+        # The rest of the time ((1-mask_replace_prob-random_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n "
        },
        {
            "sha": "a88641ca16944457354eefedb15ec5e0cb5d69c9",
            "filename": "tests/trainer/test_data_collator.py",
            "status": "modified",
            "additions": 133,
            "deletions": 0,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/48385aa4f4ccb6983d3c2beabca45e54a4c514d2/tests%2Ftrainer%2Ftest_data_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48385aa4f4ccb6983d3c2beabca45e54a4c514d2/tests%2Ftrainer%2Ftest_data_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_data_collator.py?ref=48385aa4f4ccb6983d3c2beabca45e54a4c514d2",
            "patch": "@@ -445,6 +445,86 @@ def test_data_collator_for_whole_word_mask(self):\n         self.assertEqual(batch[\"input_ids\"].shape, torch.Size((2, 10)))\n         self.assertEqual(batch[\"labels\"].shape, torch.Size((2, 10)))\n \n+    def test_data_collator_for_whole_word_mask_with_seed(self):\n+        tokenizer = BertTokenizer(self.vocab_file)\n+        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n+\n+        # check if seed is respected between two different DataCollatorForWholeWordMask instances\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42)\n+        batch_1 = data_collator(features)\n+        self.assertEqual(batch_1[\"input_ids\"].shape, torch.Size((2, 1000)))\n+        self.assertEqual(batch_1[\"labels\"].shape, torch.Size((2, 1000)))\n+\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42)\n+        batch_2 = data_collator(features)\n+        self.assertEqual(batch_2[\"input_ids\"].shape, torch.Size((2, 1000)))\n+        self.assertEqual(batch_2[\"labels\"].shape, torch.Size((2, 1000)))\n+\n+        self.assertTrue(torch.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n+        self.assertTrue(torch.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n+\n+        # check if seed is respected in multiple workers situation\n+        features = [{\"input_ids\": list(range(1000))} for _ in range(10)]\n+        dataloader = torch.utils.data.DataLoader(\n+            features,\n+            batch_size=2,\n+            num_workers=2,\n+            generator=torch.Generator().manual_seed(42),\n+            collate_fn=DataCollatorForWholeWordMask(tokenizer, seed=42),\n+        )\n+\n+        batch_3_input_ids = []\n+        batch_3_labels = []\n+        for batch in dataloader:\n+            batch_3_input_ids.append(batch[\"input_ids\"])\n+            batch_3_labels.append(batch[\"labels\"])\n+\n+        batch_3_input_ids = torch.stack(batch_3_input_ids)\n+        batch_3_labels = torch.stack(batch_3_labels)\n+        self.assertEqual(batch_3_input_ids.shape, torch.Size((5, 2, 1000)))\n+        self.assertEqual(batch_3_labels.shape, torch.Size((5, 2, 1000)))\n+\n+        dataloader = torch.utils.data.DataLoader(\n+            features,\n+            batch_size=2,\n+            num_workers=2,\n+            collate_fn=DataCollatorForWholeWordMask(tokenizer, seed=42),\n+        )\n+\n+        batch_4_input_ids = []\n+        batch_4_labels = []\n+        for batch in dataloader:\n+            batch_4_input_ids.append(batch[\"input_ids\"])\n+            batch_4_labels.append(batch[\"labels\"])\n+        batch_4_input_ids = torch.stack(batch_4_input_ids)\n+        batch_4_labels = torch.stack(batch_4_labels)\n+        self.assertEqual(batch_4_input_ids.shape, torch.Size((5, 2, 1000)))\n+        self.assertEqual(batch_4_labels.shape, torch.Size((5, 2, 1000)))\n+\n+        self.assertTrue(torch.all(batch_3_input_ids == batch_4_input_ids))\n+        self.assertTrue(torch.all(batch_3_labels == batch_4_labels))\n+\n+        # try with different seed\n+        dataloader = torch.utils.data.DataLoader(\n+            features,\n+            batch_size=2,\n+            num_workers=2,\n+            collate_fn=DataCollatorForWholeWordMask(tokenizer, seed=43),\n+        )\n+\n+        batch_5_input_ids = []\n+        batch_5_labels = []\n+        for batch in dataloader:\n+            batch_5_input_ids.append(batch[\"input_ids\"])\n+            batch_5_labels.append(batch[\"labels\"])\n+        batch_5_input_ids = torch.stack(batch_5_input_ids)\n+        batch_5_labels = torch.stack(batch_5_labels)\n+        self.assertEqual(batch_5_input_ids.shape, torch.Size((5, 2, 1000)))\n+        self.assertEqual(batch_5_labels.shape, torch.Size((5, 2, 1000)))\n+\n+        self.assertFalse(torch.all(batch_3_input_ids == batch_5_input_ids))\n+        self.assertFalse(torch.all(batch_3_labels == batch_5_labels))\n+\n     def test_plm(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n         no_pad_features = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n@@ -1199,6 +1279,33 @@ def test_data_collator_for_whole_word_mask(self):\n         self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n         self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n \n+    def test_data_collator_for_whole_word_mask_with_seed(self):\n+        tokenizer = BertTokenizer(self.vocab_file)\n+        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n+\n+        # check if seed is respected between two different DataCollatorForWholeWordMask instances\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42, return_tensors=\"tf\")\n+        batch_1 = data_collator(features)\n+        self.assertEqual(batch_1[\"input_ids\"].shape.as_list(), [2, 1000])\n+        self.assertEqual(batch_1[\"labels\"].shape.as_list(), [2, 1000])\n+\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42, return_tensors=\"tf\")\n+        batch_2 = data_collator(features)\n+        self.assertEqual(batch_2[\"input_ids\"].shape.as_list(), [2, 1000])\n+        self.assertEqual(batch_2[\"labels\"].shape.as_list(), [2, 1000])\n+\n+        self.assertTrue(np.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n+        self.assertTrue(np.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n+\n+        # try with different seed\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=43, return_tensors=\"tf\")\n+        batch_3 = data_collator(features)\n+        self.assertEqual(batch_3[\"input_ids\"].shape.as_list(), [2, 1000])\n+        self.assertEqual(batch_3[\"labels\"].shape.as_list(), [2, 1000])\n+\n+        self.assertFalse(np.all(batch_1[\"input_ids\"] == batch_3[\"input_ids\"]))\n+        self.assertFalse(np.all(batch_1[\"labels\"] == batch_3[\"labels\"]))\n+\n     def test_plm(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n         no_pad_features = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n@@ -1920,6 +2027,32 @@ def test_data_collator_for_whole_word_mask(self):\n         self.assertEqual(batch[\"input_ids\"].shape, (2, 10))\n         self.assertEqual(batch[\"labels\"].shape, (2, 10))\n \n+    def test_data_collator_for_whole_word_mask_with_seed(self):\n+        tokenizer = BertTokenizer(self.vocab_file)\n+        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n+\n+        # check if seed is respected between two different DataCollatorForWholeWordMask instances\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42, return_tensors=\"np\")\n+        batch_1 = data_collator(features)\n+        self.assertEqual(batch_1[\"input_ids\"].shape, (2, 1000))\n+        self.assertEqual(batch_1[\"labels\"].shape, (2, 1000))\n+\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42, return_tensors=\"np\")\n+        batch_2 = data_collator(features)\n+        self.assertEqual(batch_2[\"input_ids\"].shape, (2, 1000))\n+        self.assertEqual(batch_2[\"labels\"].shape, (2, 1000))\n+\n+        self.assertTrue(np.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n+        self.assertTrue(np.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n+\n+        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=43, return_tensors=\"np\")\n+        batch_3 = data_collator(features)\n+        self.assertEqual(batch_3[\"input_ids\"].shape, (2, 1000))\n+        self.assertEqual(batch_3[\"labels\"].shape, (2, 1000))\n+\n+        self.assertFalse(np.all(batch_1[\"input_ids\"] == batch_3[\"input_ids\"]))\n+        self.assertFalse(np.all(batch_1[\"labels\"] == batch_3[\"labels\"]))\n+\n     def test_plm(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n         no_pad_features = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]"
        }
    ],
    "stats": {
        "total": 275,
        "additions": 253,
        "deletions": 22
    }
}