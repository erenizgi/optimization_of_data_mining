{
    "author": "yao-matrix",
    "message": "switch to device agnostic device calling for test cases (#38247)\n\n* use device agnostic APIs in test cases\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* add one more\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* xpu now supports integer device id, aligning to CUDA behaviors\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* update to use device_properties\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* update comment\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix comments\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
    "files": [
        {
            "sha": "8beabfa7346dbc31257558b70b22a023e3100343",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -273,7 +273,7 @@ def update_device_map(self, device_map):\n             elif is_torch_hpu_available():\n                 device_map = {\"\": f\"hpu:{torch.hpu.current_device()}\"}\n             elif is_torch_xpu_available():\n-                device_map = {\"\": f\"xpu:{torch.xpu.current_device()}\"}\n+                device_map = {\"\": torch.xpu.current_device()}\n             else:\n                 device_map = {\"\": \"cpu\"}\n             logger.info("
        },
        {
            "sha": "e0b5811fc7fef43c10f527122fca27fd4764cd3e",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -136,7 +136,7 @@ def update_device_map(self, device_map):\n             if torch.cuda.is_available():\n                 device_map = {\"\": torch.cuda.current_device()}\n             elif is_torch_xpu_available():\n-                device_map = {\"\": f\"xpu:{torch.xpu.current_device()}\"}\n+                device_map = {\"\": torch.xpu.current_device()}\n             else:\n                 device_map = {\"\": \"cpu\"}\n             logger.info("
        },
        {
            "sha": "8213a85c1752c498745cc0166160174789ef6ab6",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -28,6 +28,7 @@\n )\n from transformers.testing_utils import (\n     Expectations,\n+    get_device_properties,\n     require_deterministic_for_xpu,\n     require_flash_attn,\n     require_torch,\n@@ -572,10 +573,10 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_seq_id\n                     return_tensors=\"pt\", return_seq_idx=True, return_flash_attn_kwargs=True\n                 )\n                 batch = data_collator(features)\n-                batch_cuda = {k: t.cuda() if torch.is_tensor(t) else t for k, t in batch.items()}\n+                batch_accelerator = {k: t.to(torch_device) if torch.is_tensor(t) else t for k, t in batch.items()}\n \n                 res_padded = model(**inputs_dict)\n-                res_padfree = model(**batch_cuda)\n+                res_padfree = model(**batch_accelerator)\n \n                 logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n                 logits_padfree = res_padfree.logits[0]\n@@ -594,7 +595,7 @@ class BambaModelIntegrationTest(unittest.TestCase):\n     tokenizer = None\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n+    device_properties = None\n \n     @classmethod\n     def setUpClass(cls):\n@@ -606,9 +607,7 @@ def setUpClass(cls):\n         cls.tokenizer.pad_token_id = cls.model.config.pad_token_id\n         cls.tokenizer.padding_side = \"left\"\n \n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n+        cls.device_properties = get_device_properties()\n \n     def test_simple_generate(self):\n         expectations = Expectations(\n@@ -639,7 +638,7 @@ def test_simple_generate(self):\n         self.assertEqual(output_sentence, expected)\n \n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.cuda_compute_capability_major_version == 8:\n+        if self.device_properties == (\"cuda\", 8):\n             with torch.no_grad():\n                 logits = self.model(input_ids=input_ids, logits_to_keep=40).logits\n \n@@ -692,7 +691,7 @@ def test_simple_batched_generate_with_padding(self):\n         self.assertEqual(output_sentences[1], EXPECTED_TEXT[1])\n \n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.cuda_compute_capability_major_version == 8:\n+        if self.device_properties == (\"cuda\", 8):\n             with torch.no_grad():\n                 logits = self.model(input_ids=inputs[\"input_ids\"]).logits\n "
        },
        {
            "sha": "787a99c93290885874d11cc33e3e5cfec99feebe",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -390,7 +390,7 @@ def test_model_from_pretrained(self):\n     def test_simple_generation(self):\n         # This test is a bit flaky. For some GPU architectures, pytorch sets by default allow_fp16_reduced_precision_reduction = True and some operations\n         # do not give the same results under this configuration, especially torch.baddmm and torch.bmm. https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200\n-        # As we leave the default value (True) for allow_fp16_reduced_precision_reduction , the tests failed when running in half-precision with smaller models (560m)\n+        # As we leave the default value (True) for allow_fp16_reduced_precision_reduction, the tests failed when running in half-precision with smaller models (560m)\n         # Please see: https://pytorch.org/docs/stable/notes/cuda.html#reduced-precision-reduction-in-fp16-gemms\n         # This discrepancy is observed only when using small models and seems to be stable for larger models.\n         # Our conclusion is that these operations are flaky for small inputs but seems to be stable for larger inputs (for the functions `baddmm` and `bmm`), and therefore for larger models.\n@@ -763,7 +763,6 @@ def test_embeddings(self):\n \n     @require_torch\n     def test_hidden_states_transformers(self):\n-        cuda_available = torch.cuda.is_available()\n         model = BloomModel.from_pretrained(self.path_bigscience_model, use_cache=False, torch_dtype=\"auto\").to(\n             torch_device\n         )\n@@ -782,7 +781,7 @@ def test_hidden_states_transformers(self):\n             \"max\": logits.last_hidden_state.max(dim=-1).values[0][0].item(),\n         }\n \n-        if cuda_available:\n+        if torch_device == \"cuda\":\n             self.assertAlmostEqual(MEAN_VALUE_LAST_LM, logits.last_hidden_state.mean().item(), places=4)\n         else:\n             self.assertAlmostEqual(MEAN_VALUE_LAST_LM, logits.last_hidden_state.mean().item(), places=3)\n@@ -791,7 +790,6 @@ def test_hidden_states_transformers(self):\n \n     @require_torch\n     def test_logits(self):\n-        cuda_available = torch.cuda.is_available()\n         model = BloomForCausalLM.from_pretrained(self.path_bigscience_model, use_cache=False, torch_dtype=\"auto\").to(\n             torch_device\n         )  # load in bf16\n@@ -807,9 +805,5 @@ def test_logits(self):\n             output = model(tensor_ids).logits\n \n         output_gpu_1, output_gpu_2 = output.split(125440, dim=-1)\n-        if cuda_available:\n-            self.assertAlmostEqual(output_gpu_1.mean().item(), MEAN_LOGITS_GPU_1, places=6)\n-            self.assertAlmostEqual(output_gpu_2.mean().item(), MEAN_LOGITS_GPU_2, places=6)\n-        else:\n-            self.assertAlmostEqual(output_gpu_1.mean().item(), MEAN_LOGITS_GPU_1, places=6)  # 1e-06 precision!!\n-            self.assertAlmostEqual(output_gpu_2.mean().item(), MEAN_LOGITS_GPU_2, places=6)\n+        self.assertAlmostEqual(output_gpu_1.mean().item(), MEAN_LOGITS_GPU_1, places=6)\n+        self.assertAlmostEqual(output_gpu_2.mean().item(), MEAN_LOGITS_GPU_2, places=6)"
        },
        {
            "sha": "9282c22d4134cfa5bdd05e14fc7323fed7ae9c79",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -133,15 +133,6 @@ def test_generate_continue_from_inputs_embeds(self):\n @require_torch_large_gpu\n class Cohere2IntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n     def test_model_bf16(self):\n         model_id = \"CohereForAI/c4ai-command-r7b-12-2024\""
        },
        {
            "sha": "e0a21002ef0f9a12df607816c254d281808dc3b5",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -495,16 +495,6 @@ def test_eager_matches_sdpa_generate(self):\n \n @require_torch_accelerator\n class DeepseekV3IntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n-\n     def tearDown(self):\n         # See LlamaIntegrationTest.tearDown(). Can be removed once LlamaIntegrationTest.tearDown() is removed.\n         cleanup(torch_device, gc_collect=False)"
        },
        {
            "sha": "50525a3ec4ece94b25fdcca890061307ec9622bf",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -565,16 +565,6 @@ def test_eager_matches_sdpa_generate(self):\n \n @require_torch_accelerator\n class DiffLlamaIntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n-\n     def tearDown(self):\n         # See LlamaIntegrationTest.tearDown(). Can be removed once LlamaIntegrationTest.tearDown() is removed.\n         cleanup(torch_device, gc_collect=False)"
        },
        {
            "sha": "940aa7fedc091289d8a6b75d32bc0167f284325e",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 19,
            "deletions": 23,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -21,7 +21,9 @@\n from transformers import AutoModelForCausalLM, AutoTokenizer, GemmaConfig, is_torch_available\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    get_device_properties,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n@@ -105,15 +107,13 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n @require_torch_accelerator\n class GemmaIntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n+    # This variable is used to determine which accelerator are we using for our runners (e.g. A10 or T4)\n     # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n+    device_properties = None\n \n     @classmethod\n     def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n+        cls.device_properties = get_device_properties()\n \n     def tearDown(self):\n         # See LlamaIntegrationTest.tearDown(). Can be removed once LlamaIntegrationTest.tearDown() is removed.\n@@ -270,7 +270,7 @@ def test_model_7b_fp32(self):\n \n     @require_read_token\n     def test_model_7b_fp16(self):\n-        if self.cuda_compute_capability_major_version == 7:\n+        if self.device_properties == (\"cuda\", 7):\n             self.skipTest(\"This test is failing (`torch.compile` fails) on Nvidia T4 GPU (OOM).\")\n \n         model_id = \"google/gemma-7b\"\n@@ -293,7 +293,7 @@ def test_model_7b_fp16(self):\n \n     @require_read_token\n     def test_model_7b_bf16(self):\n-        if self.cuda_compute_capability_major_version == 7:\n+        if self.device_properties == (\"cuda\", 7):\n             self.skipTest(\"This test is failing (`torch.compile` fails) on Nvidia T4 GPU (OOM).\")\n \n         model_id = \"google/gemma-7b\"\n@@ -302,20 +302,16 @@ def test_model_7b_bf16(self):\n         #\n         # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,\n         # considering differences in hardware processing and potential deviations in generated text.\n-        EXPECTED_TEXTS = {\n-            7: [\n-                \"\"\"Hello I am doing a project on a 1991 240sx and I am trying to find\"\"\",\n-                \"Hi today I am going to show you how to make a very simple and easy to make a very simple and\",\n-            ],\n-            8: [\n-                \"Hello I am doing a project for my school and I am trying to make a program that will read a .txt file\",\n-                \"Hi today I am going to show you how to make a very simple and easy to make a very simple and\",\n-            ],\n-            9: [\n-                \"Hello I am doing a project for my school and I am trying to get a servo to move a certain amount of degrees\",\n-                \"Hi today I am going to show you how to make a very simple and easy to make DIY light up sign\",\n-            ],\n-        }\n+        # fmt: off\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [\"\"\"Hello I am doing a project on a 1991 240sx and I am trying to find\"\"\", \"Hi today I am going to show you how to make a very simple and easy to make a very simple and\",],\n+                (\"cuda\", 8): [\"Hello I am doing a project for my school and I am trying to make a program that will read a .txt file\", \"Hi today I am going to show you how to make a very simple and easy to make a very simple and\",],\n+                (\"rocm\", 9): [\"Hello I am doing a project for my school and I am trying to get a servo to move a certain amount of degrees\", \"Hi today I am going to show you how to make a very simple and easy to make DIY light up sign\",],\n+            }\n+        )\n+        # fmt: on\n+        expected_text = EXPECTED_TEXTS.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16).to(\n             torch_device\n@@ -326,11 +322,11 @@ def test_model_7b_bf16(self):\n \n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n-        self.assertEqual(output_text, EXPECTED_TEXTS[self.cuda_compute_capability_major_version])\n+        self.assertEqual(output_text, expected_text)\n \n     @require_read_token\n     def test_model_7b_fp16_static_cache(self):\n-        if self.cuda_compute_capability_major_version == 7:\n+        if self.device_properties == (\"cuda\", 7):\n             self.skipTest(\"This test is failing (`torch.compile` fails) on Nvidia T4 GPU (OOM).\")\n \n         model_id = \"google/gemma-7b\""
        },
        {
            "sha": "5e78efe540ccbcbd5b3f4a3e7aceb9c0674e31a3",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -176,15 +176,6 @@ def test_flash_attn_2_equivalence(self):\n @require_torch_accelerator\n class Gemma2IntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n     @tooslow\n     @require_read_token"
        },
        {
            "sha": "5438b4d158cb60ec0a5c098a5194c38ee03083a2",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -80,15 +80,6 @@ class GlmIntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n     model_id = \"THUDM/glm-4-9b\"\n     revision = \"refs/pr/15\"\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n     def test_model_9b_fp16(self):\n         EXPECTED_TEXTS = ["
        },
        {
            "sha": "d7a8074a5c9a81f290750f0a54b53e7d48fa2966",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -82,15 +82,6 @@ class Glm4IntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n     model_id = \"THUDM/glm-4-0414-9b-chat\"\n     revision = \"refs/pr/15\"\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n     def test_model_9b_fp16(self):\n         EXPECTED_TEXTS = ["
        },
        {
            "sha": "c540655577f4a56386253f92c52ac52ff3964958",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 19,
            "deletions": 28,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -305,16 +305,6 @@ def test_model_rope_scaling(self):\n \n @require_torch_accelerator\n class GraniteIntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n-\n     @slow\n     @require_read_token\n     def test_model_3b_logits_bf16(self):\n@@ -330,24 +320,24 @@ def test_model_3b_logits_bf16(self):\n \n         # fmt: off\n         EXPECTED_MEANS = Expectations(\n-                {\n-                    (\"xpu\", 3): torch.tensor([[-3.1406, -2.5469, -2.6250, -2.1250, -2.6250, -2.6562, -2.6875, -2.9688]]),\n-                    (\"cuda\", 7): torch.tensor([[-1.9798, -3.1626, -2.8062, -2.3777, -2.7091, -2.2338, -2.5924, -2.3974]]),\n-                    (\"cuda\", 8): torch.tensor([[-3.1406, -2.5469, -2.6250, -2.1250, -2.6250, -2.6562, -2.6875, -2.9688]]),\n-                }\n-            )\n+            {\n+                (\"xpu\", 3): torch.tensor([[-3.1406, -2.5469, -2.6250, -2.1250, -2.6250, -2.6562, -2.6875, -2.9688]]),\n+                (\"cuda\", 7): torch.tensor([[-1.9798, -3.1626, -2.8062, -2.3777, -2.7091, -2.2338, -2.5924, -2.3974]]),\n+                (\"cuda\", 8): torch.tensor([[-3.1406, -2.5469, -2.6250, -2.1250, -2.6250, -2.6562, -2.6875, -2.9688]]),\n+            }\n+        )\n         EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n \n         torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.mean(-1).float(), rtol=1e-2, atol=1e-2)\n \n         # slicing logits[0, 0, 0:15]\n         EXPECTED_SLICES = Expectations(\n-                {\n-                    (\"xpu\", 3): torch.tensor([[2.2031, -5.0625, -5.0625, -5.0625, -5.0625, -0.9180, -5.0625, -5.0625, -5.0625, -5.0625, -5.5312, -2.1719, -1.7891, -0.4922, -2.5469]]),\n-                    (\"cuda\", 7): torch.tensor([[4.8750, -2.1875, -2.1875, -2.1875, -2.1875, -2.8438, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875]]),\n-                    (\"cuda\", 8): torch.tensor([[2.0938, -5.0312, -5.0312, -5.0312, -5.0312, -1.0469, -5.0312, -5.0312, -5.0312, -5.0312, -5.5625, -2.1875, -1.7891, -0.5820, -2.6250]]),\n-                }\n-            )\n+            {\n+                (\"xpu\", 3): torch.tensor([[2.2031, -5.0625, -5.0625, -5.0625, -5.0625, -0.9180, -5.0625, -5.0625, -5.0625, -5.0625, -5.5312, -2.1719, -1.7891, -0.4922, -2.5469]]),\n+                (\"cuda\", 7): torch.tensor([[4.8750, -2.1875, -2.1875, -2.1875, -2.1875, -2.8438, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875]]),\n+                (\"cuda\", 8): torch.tensor([[2.0938, -5.0312, -5.0312, -5.0312, -5.0312, -1.0469, -5.0312, -5.0312, -5.0312, -5.0312, -5.5625, -2.1875, -1.7891, -0.5820, -2.6250]]),\n+            }\n+        )\n         EXPECTED_SLICE = EXPECTED_SLICES.get_expectation()\n         # fmt: on\n         self.assertTrue(\n@@ -372,12 +362,13 @@ def test_model_3b_logits(self):\n         # fmt: off\n         # Expected mean on dim = -1\n         EXPECTED_MEANS = Expectations(\n-                {\n-                    (\"xpu\", 3): torch.tensor([[-3.2693, -2.5957, -2.6234, -2.1675, -2.6386, -2.6850, -2.7039, -2.9656]]),\n-                    (\"cuda\", 7): torch.tensor([[-2.0984, -3.1294, -2.8153, -2.3568, -2.7337, -2.2624, -2.6016, -2.4022]]),\n-                    (\"cuda\", 8): torch.tensor([[-3.2934, -2.6019, -2.6258, -2.1691, -2.6394, -2.6876, -2.7032, -2.9688]]),\n-                }\n-            )\n+            {\n+                (\"xpu\", 3): torch.tensor([[-3.2693, -2.5957, -2.6234, -2.1675, -2.6386, -2.6850, -2.7039, -2.9656]]),\n+                (\"cuda\", 7): torch.tensor([[-2.0984, -3.1294, -2.8153, -2.3568, -2.7337, -2.2624, -2.6016, -2.4022]]),\n+                (\"cuda\", 8): torch.tensor([[-3.2934, -2.6019, -2.6258, -2.1691, -2.6394, -2.6876, -2.7032, -2.9688]]),\n+            }\n+        )\n+        # fmt: on\n         EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n \n         torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), rtol=1e-2, atol=1e-2)"
        },
        {
            "sha": "ccc0dfd6a51238a783cb8d874eaeef5e4f486685",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 22,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -304,16 +304,6 @@ def test_model_rope_scaling(self):\n \n @require_torch_accelerator\n class GraniteMoeIntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n-\n     @slow\n     @require_read_token\n     def test_model_3b_logits(self):\n@@ -327,24 +317,24 @@ def test_model_3b_logits(self):\n         # fmt: off\n         # Expected mean on dim = -1\n         EXPECTED_MEANS = Expectations(\n-                {\n-                    (\"xpu\", 3): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n-                    (\"cuda\", 7): torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]]),\n-                    (\"cuda\", 8): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n-                }\n-            )\n+            {\n+                (\"xpu\", 3): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n+                (\"cuda\", 7): torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]]),\n+                (\"cuda\", 8): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n+            }\n+        )\n         EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n \n         torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), rtol=1e-2, atol=1e-2)\n \n         # slicing logits[0, 0, 0:15]\n         EXPECTED_SLICES = Expectations(\n-                {\n-                    (\"xpu\", 3): torch.tensor([[2.5479, -9.2123, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2161, -9.2122, -6.3100, -3.6223, -3.6377, -5.2542, -5.2523]]),\n-                    (\"cuda\", 7): torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892, -2.2895, -2.2891, -2.2887, -2.2882, -2.2889, -2.2898, -2.2892]]),\n-                    (\"cuda\", 8): torch.tensor([[2.5479, -9.2124, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2162, -9.2122, -6.3101, -3.6224, -3.6377, -5.2542, -5.2524]]),\n-                }\n-            )\n+            {\n+                (\"xpu\", 3): torch.tensor([[2.5479, -9.2123, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2161, -9.2122, -6.3100, -3.6223, -3.6377, -5.2542, -5.2523]]),\n+                (\"cuda\", 7): torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892, -2.2895, -2.2891, -2.2887, -2.2882, -2.2889, -2.2898, -2.2892]]),\n+                (\"cuda\", 8): torch.tensor([[2.5479, -9.2124, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2162, -9.2122, -6.3101, -3.6224, -3.6377, -5.2542, -5.2524]]),\n+            }\n+        )\n         EXPECTED_SLICE = EXPECTED_SLICES.get_expectation()\n         # fmt: on\n \n@@ -360,6 +350,7 @@ def test_model_3b_logits(self):\n     @slow\n     def test_model_3b_generation(self):\n         # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n+        # fmt: off\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n                 (\"xpu\", 3): (\n@@ -378,6 +369,7 @@ def test_model_3b_generation(self):\n                 ),\n             }\n         )\n+        # fmt: on\n         EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n \n         prompt = \"Simply put, the theory of relativity states that \""
        },
        {
            "sha": "fc3d93a6640246c6b9a5e0f410b11c55a98817a8",
            "filename": "tests/models/granitemoehybrid/test_modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -105,16 +105,6 @@ def test_config_requires_mamba_or_attention_layers(self):\n @unittest.skip(reason=\"GraniteMoeHybrid models are not yet released\")\n @require_torch_gpu\n class GraniteMoeHybridIntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n-\n     @slow\n     def test_model_logits(self):\n         input_ids = [31390, 631, 4162, 30, 322, 25342, 432, 1875, 43826, 10066, 688, 225]"
        },
        {
            "sha": "bfd9464c75a00aa281089319c12310a099f5b8d9",
            "filename": "tests/models/granitemoeshared/test_modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 14,
            "deletions": 22,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -307,16 +307,6 @@ def test_model_rope_scaling(self):\n \n @require_torch_accelerator\n class GraniteMoeSharedIntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n-\n     @slow\n     @require_read_token\n     def test_model_3b_logits(self):\n@@ -330,24 +320,24 @@ def test_model_3b_logits(self):\n         # fmt: off\n         # Expected mean on dim = -1\n         EXPECTED_MEANS = Expectations(\n-                {\n-                    (\"xpu\", 3): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n-                    (\"cuda\", 7): torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]]),\n-                    (\"cuda\", 8): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n-                }\n-            )\n+            {\n+                (\"xpu\", 3): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n+                (\"cuda\", 7): torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]]),\n+                (\"cuda\", 8): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n+            }\n+        )\n \n         EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n         torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), rtol=1e-2, atol=1e-2)\n \n         # slicing logits[0, 0, 0:15]\n         EXPECTED_SLICES = Expectations(\n-                {\n-                    (\"xpu\", 3): torch.tensor([[2.5479, -9.2123, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2161, -9.2122, -6.3100, -3.6223, -3.6377, -5.2542, -5.2523]]),\n-                    (\"cuda\", 7): torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892, -2.2895, -2.2891, -2.2887, -2.2882, -2.2889, -2.2898, -2.2892]]),\n-                    (\"cuda\", 8): torch.tensor([[2.5479, -9.2123, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2161, -9.2122, -6.3100, -3.6223, -3.6377, -5.2542, -5.2523]]),\n-                }\n-            )\n+            {\n+                (\"xpu\", 3): torch.tensor([[2.5479, -9.2123, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2161, -9.2122, -6.3100, -3.6223, -3.6377, -5.2542, -5.2523]]),\n+                (\"cuda\", 7): torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892, -2.2895, -2.2891, -2.2887, -2.2882, -2.2889, -2.2898, -2.2892]]),\n+                (\"cuda\", 8): torch.tensor([[2.5479, -9.2123, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2161, -9.2122, -6.3100, -3.6223, -3.6377, -5.2542, -5.2523]]),\n+            }\n+        )\n         EXPECTED_SLICE = EXPECTED_SLICES.get_expectation()\n         # fmt: on\n \n@@ -363,6 +353,7 @@ def test_model_3b_logits(self):\n     @slow\n     def test_model_3b_generation(self):\n         # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n+        # fmt: off\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n                 (\"xpu\", 3): (\n@@ -381,6 +372,7 @@ def test_model_3b_generation(self):\n                 ),\n             }\n         )\n+        # fmt: on\n         EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n \n         prompt = \"Simply put, the theory of relativity states that \""
        },
        {
            "sha": "cb46167bae4149a34cbb8118b33ecd34aafb4bae",
            "filename": "tests/models/helium/test_modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -79,15 +79,6 @@ def setUp(self):\n # @require_torch_gpu\n class HeliumIntegrationTest(unittest.TestCase):\n     input_text = [\"Hello, today is a great day to\"]\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n     @require_read_token\n     def test_model_2b(self):"
        },
        {
            "sha": "cd27180a5cfec98a381880204f863c1d6a406f83",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 32,
            "deletions": 33,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -21,6 +21,8 @@\n \n from transformers import AutoTokenizer, JambaConfig, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n+    get_device_properties,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n@@ -554,30 +556,32 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n class JambaModelIntegrationTest(unittest.TestCase):\n     model = None\n     tokenizer = None\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n+    # This variable is used to determine which acclerator are we using for our runners (e.g. A10 or T4)\n     # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n+    device_properties = None\n \n     @classmethod\n     def setUpClass(cls):\n         model_id = \"ai21labs/Jamba-tiny-dev\"\n         cls.model = JambaForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n         cls.tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n+        cls.device_properties = get_device_properties()\n \n     @slow\n     def test_simple_generate(self):\n-        # Key 9 for MI300, Key 8 for A100/A10, and Key 7 for T4.\n+        # (\"cuda\", 8) for A100/A10, and (\"cuda\", 7) for T4.\n         #\n-        # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,\n         # considering differences in hardware processing and potential deviations in generated text.\n-        EXPECTED_TEXTS = {\n-            7: \"<|startoftext|>Hey how are you doing on this lovely evening? Canyon rins hugaughter glamour Rutgers Singh<|reserved_797|>cw algunas\",\n-            8: \"<|startoftext|>Hey how are you doing on this lovely evening? I'm so glad you're here.\",\n-            9: \"<|startoftext|>Hey how are you doing on this lovely evening? Canyon rins hugaughter glamour Rutgers Singh Hebrew llam bb\",\n-        }\n+        # fmt: off\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): \"<|startoftext|>Hey how are you doing on this lovely evening? Canyon rins hugaughter glamour Rutgers Singh<|reserved_797|>cw algunas\",\n+                (\"cuda\", 8): \"<|startoftext|>Hey how are you doing on this lovely evening? I'm so glad you're here.\",\n+                (\"rocm\", 9): \"<|startoftext|>Hey how are you doing on this lovely evening? Canyon rins hugaughter glamour Rutgers Singh Hebrew llam bb\",\n+            }\n+        )\n+        # fmt: on\n+        expected_sentence = EXPECTED_TEXTS.get_expectation()\n \n         self.model.to(torch_device)\n \n@@ -586,10 +590,10 @@ def test_simple_generate(self):\n         ].to(torch_device)\n         out = self.model.generate(input_ids, do_sample=False, max_new_tokens=10)\n         output_sentence = self.tokenizer.decode(out[0, :])\n-        self.assertEqual(output_sentence, EXPECTED_TEXTS[self.cuda_compute_capability_major_version])\n+        self.assertEqual(output_sentence, expected_sentence)\n \n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.cuda_compute_capability_major_version == 8:\n+        if self.device_properties == (\"cuda\", 8):\n             with torch.no_grad():\n                 logits = self.model(input_ids=input_ids).logits\n \n@@ -607,24 +611,19 @@ def test_simple_generate(self):\n \n     @slow\n     def test_simple_batched_generate_with_padding(self):\n-        # Key 9 for MI300, Key 8 for A100/A10, and Key 7 for T4.\n+        # (\"cuda\", 8) for A100/A10, and (\"cuda\", 7) for T4.\n         #\n-        # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,\n         # considering differences in hardware processing and potential deviations in generated text.\n-        EXPECTED_TEXTS = {\n-            7: [\n-                \"<|startoftext|>Hey how are you doing on this lovely evening? Canyon rins hugaughter glamour Rutgers Singh Hebrew cases Cats\",\n-                \"<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|startoftext|>Tell me a storyptus Nets Madison El chamadamodern updximVaparsed\",\n-            ],\n-            8: [\n-                \"<|startoftext|>Hey how are you doing on this lovely evening? I'm so glad you're here.\",\n-                \"<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|startoftext|>Tell me a story about a woman who was born in the United States\",\n-            ],\n-            9: [\n-                \"<|startoftext|>Hey how are you doing on this lovely evening? Canyon rins hugaughter glamour Rutgers Singh<|reserved_797|>cw algunas\",\n-                \"<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|startoftext|>Tell me a storyptus Nets Madison El chamadamodern updximVaparsed\",\n-            ],\n-        }\n+        # fmt: off\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [\"<|startoftext|>Hey how are you doing on this lovely evening? Canyon rins hugaughter glamour Rutgers Singh Hebrew cases Cats\", \"<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|startoftext|>Tell me a storyptus Nets Madison El chamadamodern updximVaparsed\",],\n+                (\"cuda\", 8): [\"<|startoftext|>Hey how are you doing on this lovely evening? I'm so glad you're here.\", \"<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|startoftext|>Tell me a story about a woman who was born in the United States\",],\n+                (\"rocm\", 9): [\"<|startoftext|>Hey how are you doing on this lovely evening? Canyon rins hugaughter glamour Rutgers Singh<|reserved_797|>cw algunas\", \"<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|startoftext|>Tell me a storyptus Nets Madison El chamadamodern updximVaparsed\",],\n+            }\n+        )\n+        # fmt: on\n+        expected_sentences = EXPECTED_TEXTS.get_expectation()\n \n         self.model.to(torch_device)\n \n@@ -633,11 +632,11 @@ def test_simple_batched_generate_with_padding(self):\n         ).to(torch_device)\n         out = self.model.generate(**inputs, do_sample=False, max_new_tokens=10)\n         output_sentences = self.tokenizer.batch_decode(out)\n-        self.assertEqual(output_sentences[0], EXPECTED_TEXTS[self.cuda_compute_capability_major_version][0])\n-        self.assertEqual(output_sentences[1], EXPECTED_TEXTS[self.cuda_compute_capability_major_version][1])\n+        self.assertEqual(output_sentences[0], expected_sentences[0])\n+        self.assertEqual(output_sentences[1], expected_sentences[1])\n \n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.cuda_compute_capability_major_version == 8:\n+        if self.device_properties == (\"cuda\", 8):\n             with torch.no_grad():\n                 logits = self.model(input_ids=inputs[\"input_ids\"]).logits\n "
        },
        {
            "sha": "d9362d397e02b20dfc64548e41307e1406a5ed63",
            "filename": "tests/models/llama4/test_modeling_llama4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -38,15 +38,9 @@\n @require_read_token\n class Llama4IntegrationTest(unittest.TestCase):\n     model_id = \"meta-llama/Llama-4-Scout-17B-16E\"\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n \n     @classmethod\n     def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n         cls.model = Llama4ForConditionalGeneration.from_pretrained(\n             \"meta-llama/Llama-4-Scout-17B-16E\",\n             device_map=\"auto\","
        },
        {
            "sha": "8410bfcfb6e855b8f0f65dcf29d414604e9dea03",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -21,8 +21,10 @@\n \n from transformers import AutoTokenizer, MistralConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    Expectations,\n     backend_empty_cache,\n     cleanup,\n+    get_device_properties,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n@@ -110,15 +112,13 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n @require_torch_accelerator\n class MistralIntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n+    # This variable is used to determine which accelerator are we using for our runners (e.g. A10 or T4)\n     # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n+    device_properties = None\n \n     @classmethod\n     def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n+        cls.device_properties = get_device_properties()\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n@@ -136,19 +136,20 @@ def test_model_7b_logits(self):\n         EXPECTED_MEAN = torch.tensor([[-2.5548, -2.5737, -3.0600, -2.5906, -2.8478, -2.8118, -2.9325, -2.7694]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n \n-        # Key 9 for MI300, Key 8 for A100/A10, and Key 7 for T4.\n-        #\n-        # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,\n+        # (\"cuda\", 8) for A100/A10, and (\"cuda\", 7) 7 for T4.\n         # considering differences in hardware processing and potential deviations in output.\n-        EXPECTED_SLICE = {\n-            7: torch.tensor([-5.8828, -5.8633, -0.1042, -4.7266, -5.8828, -5.8789, -5.8789, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -1.0801,  1.7598, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828]),\n-            8: torch.tensor([-5.8711, -5.8555, -0.1050, -4.7148, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -1.0781, 1.7568, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711]),\n-            9: torch.tensor([-5.8750, -5.8594, -0.1047, -4.7188, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -1.0781,  1.7578, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750]),\n-        }  # fmt: skip\n-\n-        torch.testing.assert_close(\n-            out[0, 0, :30], EXPECTED_SLICE[self.cuda_compute_capability_major_version], atol=1e-4, rtol=1e-4\n+        # fmt: off\n+        EXPECTED_SLICES = Expectations(\n+            {\n+                (\"cuda\", 7): torch.tensor([-5.8828, -5.8633, -0.1042, -4.7266, -5.8828, -5.8789, -5.8789, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -1.0801,  1.7598, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828, -5.8828]),\n+                (\"cuda\", 8): torch.tensor([-5.8711, -5.8555, -0.1050, -4.7148, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -1.0781, 1.7568, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711, -5.8711]),\n+                (\"rocm\", 9): torch.tensor([-5.8750, -5.8594, -0.1047, -4.7188, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -1.0781,  1.7578, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750, -5.8750]),\n+            }\n         )\n+        # fmt: on\n+        expected_slice = EXPECTED_SLICES.get_expectation()\n+\n+        torch.testing.assert_close(out[0, 0, :30], expected_slice, atol=1e-4, rtol=1e-4)\n \n     @slow\n     @require_bitsandbytes\n@@ -278,7 +279,7 @@ def test_compile_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.3.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n \n-        if self.cuda_compute_capability_major_version == 7:\n+        if self.device_properties == (\"cuda\", 7):\n             self.skipTest(reason=\"This test is failing (`torch.compile` fails) on Nvidia T4 GPU.\")\n \n         NUM_TOKENS_TO_GENERATE = 40"
        },
        {
            "sha": "efe076e70abf3bcfc276632e7c7e765b05b8f712",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 40,
            "deletions": 51,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -19,6 +19,8 @@\n \n from transformers import MixtralConfig, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n+    get_device_properties,\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n@@ -142,13 +144,11 @@ def test_load_balancing_loss(self):\n class MixtralIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n+    device_properties = None\n \n     @classmethod\n     def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n+        cls.device_properties = get_device_properties()\n \n     @slow\n     @require_torch_accelerator\n@@ -161,32 +161,26 @@ def test_small_model_logits(self):\n         )\n         # TODO: might need to tweak it in case the logits do not match on our daily runners\n         # these logits have been obtained with the original megablocks implementation.\n-        # Key 9 for MI300, Key 8 for A100/A10, and Key 7 for T4.\n-        #\n-        # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,\n+        # (\"cuda\", 8) for A100/A10, and (\"cuda\", 7) for T4\n         # considering differences in hardware processing and potential deviations in output.\n-        EXPECTED_LOGITS = {\n-            7: torch.Tensor([[0.1640, 0.1621, 0.6093], [-0.8906, -0.1640, -0.6093], [0.1562, 0.1250, 0.7226]]).to(\n-                torch_device\n-            ),\n-            8: torch.Tensor([[0.1631, 0.1621, 0.6094], [-0.8906, -0.1621, -0.6094], [0.1572, 0.1270, 0.7227]]).to(\n-                torch_device\n-            ),\n-            9: torch.Tensor([[0.1641, 0.1621, 0.6094], [-0.8906, -0.1631, -0.6094], [0.1572, 0.1260, 0.7227]]).to(\n-                torch_device\n-            ),\n-        }\n+        # fmt: off\n+        EXPECTED_LOGITS = Expectations(\n+            {\n+                (\"cuda\", 7): torch.Tensor([[0.1640, 0.1621, 0.6093], [-0.8906, -0.1640, -0.6093], [0.1562, 0.1250, 0.7226]]).to(torch_device),\n+                (\"cuda\", 8): torch.Tensor([[0.1631, 0.1621, 0.6094], [-0.8906, -0.1621, -0.6094], [0.1572, 0.1270, 0.7227]]).to(torch_device),\n+                (\"rocm\", 9): torch.Tensor([[0.1641, 0.1621, 0.6094], [-0.8906, -0.1631, -0.6094], [0.1572, 0.1260, 0.7227]]).to(torch_device),\n+            }\n+        )\n+        # fmt: on\n+        expected_logit = EXPECTED_LOGITS.get_expectation()\n+\n         with torch.no_grad():\n             logits = model(dummy_input).logits\n \n         logits = logits.float()\n \n-        torch.testing.assert_close(\n-            logits[0, :3, :3], EXPECTED_LOGITS[self.cuda_compute_capability_major_version], atol=1e-3, rtol=1e-3\n-        )\n-        torch.testing.assert_close(\n-            logits[1, :3, :3], EXPECTED_LOGITS[self.cuda_compute_capability_major_version], atol=1e-3, rtol=1e-3\n-        )\n+        torch.testing.assert_close(logits[0, :3, :3], expected_logit, atol=1e-3, rtol=1e-3)\n+        torch.testing.assert_close(logits[1, :3, :3], expected_logit, atol=1e-3, rtol=1e-3)\n \n     @slow\n     @require_torch_accelerator\n@@ -201,47 +195,42 @@ def test_small_model_logits_batched(self):\n \n         # TODO: might need to tweak it in case the logits do not match on our daily runners\n         #\n-        # Key 9 for MI300, Key 8 for A100/A10, and Key 7 for T4.\n+        # (\"cuda\", 8) for A100/A10, and (\"cuda\", 7) for T4.\n         #\n-        # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,\n         # considering differences in hardware processing and potential deviations in generated text.\n-        EXPECTED_LOGITS_LEFT_UNPADDED = {\n-            7: torch.Tensor(\n-                [[0.2236, 0.5195, -0.3828], [0.8203, -0.2275, 0.6054], [0.2656, -0.7070, 0.2460]],\n-            ).to(torch_device),\n-            8: torch.Tensor([[0.2207, 0.5234, -0.3828], [0.8203, -0.2285, 0.6055], [0.2656, -0.7109, 0.2451]]).to(\n-                torch_device,\n-            ),\n-            9: torch.Tensor([[0.2236, 0.5195, -0.3828], [0.8203, -0.2285, 0.6055], [0.2637, -0.7109, 0.2451]]).to(\n-                torch_device\n-            ),\n-        }\n-\n-        EXPECTED_LOGITS_RIGHT_UNPADDED = {\n-            7: torch.Tensor([[0.2167, 0.1269, -0.1640], [-0.3496, 0.2988, -1.0312], [0.0688, 0.7929, 0.8007]]).to(\n-                torch_device\n-            ),\n-            8: torch.Tensor([[0.2178, 0.1270, -0.1621], [-0.3496, 0.3008, -1.0312], [0.0693, 0.7930, 0.7969]]).to(\n-                torch_device,\n-            ),\n-            9: torch.Tensor([[0.2197, 0.1250, -0.1611], [-0.3516, 0.3008, -1.0312], [0.0684, 0.7930, 0.8008]]).to(\n-                torch_device\n-            ),\n-        }\n+        # fmt: off\n+        EXPECTED_LOGITS_LEFT_UNPADDED = Expectations(\n+            {\n+                (\"cuda\", 7): torch.Tensor([[0.2236, 0.5195, -0.3828], [0.8203, -0.2275, 0.6054], [0.2656, -0.7070, 0.2460]]).to(torch_device),\n+                (\"cuda\", 8): torch.Tensor([[0.2207, 0.5234, -0.3828], [0.8203, -0.2285, 0.6055], [0.2656, -0.7109, 0.2451]]).to(torch_device),\n+                (\"rocm\", 9): torch.Tensor([[0.2236, 0.5195, -0.3828], [0.8203, -0.2285, 0.6055], [0.2637, -0.7109, 0.2451]]).to(torch_device),\n+            }\n+        )\n+        expected_left_unpadded = EXPECTED_LOGITS_LEFT_UNPADDED.get_expectation()\n+\n+        EXPECTED_LOGITS_RIGHT_UNPADDED = Expectations(\n+            {\n+                (\"cuda\", 7): torch.Tensor([[0.2167, 0.1269, -0.1640], [-0.3496, 0.2988, -1.0312], [0.0688, 0.7929, 0.8007]]).to(torch_device),\n+                (\"cuda\", 8): torch.Tensor([[0.2178, 0.1270, -0.1621], [-0.3496, 0.3008, -1.0312], [0.0693, 0.7930, 0.7969]]).to(torch_device),\n+                (\"rocm\", 9): torch.Tensor([[0.2197, 0.1250, -0.1611], [-0.3516, 0.3008, -1.0312], [0.0684, 0.7930, 0.8008]]).to(torch_device),\n+            }\n+        )\n+        expected_right_unpadded = EXPECTED_LOGITS_RIGHT_UNPADDED.get_expectation()\n+        # fmt: on\n \n         with torch.no_grad():\n             logits = model(dummy_input, attention_mask=attention_mask).logits\n         logits = logits.float()\n \n         torch.testing.assert_close(\n             logits[0, -3:, -3:],\n-            EXPECTED_LOGITS_LEFT_UNPADDED[self.cuda_compute_capability_major_version],\n+            expected_left_unpadded,\n             atol=1e-3,\n             rtol=1e-3,\n         )\n         torch.testing.assert_close(\n             logits[1, -3:, -3:],\n-            EXPECTED_LOGITS_RIGHT_UNPADDED[self.cuda_compute_capability_major_version],\n+            expected_right_unpadded,\n             atol=1e-3,\n             rtol=1e-3,\n         )"
        },
        {
            "sha": "d24ab44736ed41823ed0978907ee5d8b3556c7e1",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -99,16 +99,6 @@ def test_model_outputs_equivalence(self, **kwargs):\n \n @require_torch_accelerator\n class NemotronIntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    cuda_compute_capability_major_version = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        if is_torch_available() and torch.cuda.is_available():\n-            # 8 is for A100 / A10 and 7 for T4\n-            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n-\n     @slow\n     @require_read_token\n     def test_nemotron_8b_generation_sdpa(self):"
        },
        {
            "sha": "b339343627b366ed692ba34c346ec6b1e0bb5210",
            "filename": "tests/quantization/aqlm_integration/test_aqlm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import AqlmConfig, AutoConfig, AutoModelForCausalLM, AutoTokenizer, OPTForCausalLM, StaticCache\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_aqlm,\n     require_torch_gpu,\n@@ -81,8 +82,6 @@ class AqlmTest(unittest.TestCase):\n \n     EXPECTED_OUTPUT = \"Hello my name is Katie. I am a 20 year old college student. I am a very outgoing person. I love to have fun and be active. I\"\n \n-    device_map = \"cuda\"\n-\n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n@@ -92,12 +91,12 @@ def setUpClass(cls):\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n             cls.model_name,\n-            device_map=cls.device_map,\n+            device_map=torch_device,\n         )\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_quantized_model_conversion(self):\n@@ -170,7 +169,7 @@ def test_save_pretrained(self):\n         \"\"\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             self.quantized_model.save_pretrained(tmpdirname)\n-            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.device_map)\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)\n \n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n "
        },
        {
            "sha": "542344996730191577eb21a1d3460e56f34f13af",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -19,6 +19,7 @@\n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AwqConfig, OPTForCausalLM\n from transformers.testing_utils import (\n     backend_empty_cache,\n+    get_device_properties,\n     require_accelerate,\n     require_auto_awq,\n     require_flash_attn,\n@@ -61,12 +62,10 @@ def test_wrong_backend(self):\n \n         # Only cuda and xpu devices can run this function\n         support_llm_awq = False\n-        if torch.cuda.is_available():\n-            compute_capability = torch.cuda.get_device_capability()\n-            major, minor = compute_capability\n-            if major >= 8:\n-                support_llm_awq = True\n-        elif torch.xpu.is_available():\n+        device_type, major = get_device_properties()\n+        if device_type == \"cuda\" and major >= 8:\n+            support_llm_awq = True\n+        elif device_type == \"xpu\":\n             support_llm_awq = True\n \n         if support_llm_awq:\n@@ -357,7 +356,7 @@ def test_fused_modules_to_not_convert(self):\n         self.assertTrue(isinstance(model.model.layers[0].block_sparse_moe.gate, torch.nn.Linear))\n \n     @unittest.skipIf(\n-        torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n+        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8,\n         \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n     )\n     @require_flash_attn\n@@ -388,7 +387,7 @@ def test_generation_fused(self):\n     @require_flash_attn\n     @require_torch_gpu\n     @unittest.skipIf(\n-        torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n+        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8,\n         \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n     )\n     def test_generation_fused_batched(self):\n@@ -441,7 +440,7 @@ def test_generation_llava_fused(self):\n     @require_flash_attn\n     @require_torch_multi_gpu\n     @unittest.skipIf(\n-        torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,\n+        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8,\n         \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n     )\n     def test_generation_custom_model(self):"
        },
        {
            "sha": "fabe980ca29bf63116f6e40a7f107c888c932f1a",
            "filename": "tests/quantization/bitnet_integration/test_bitnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -23,6 +23,7 @@\n     OPTForCausalLM,\n )\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_torch_gpu,\n     slow,\n@@ -56,7 +57,6 @@ def test_to_dict(self):\n @require_accelerate\n class BitNetTest(unittest.TestCase):\n     model_name = \"HF1BitLLM/Llama3-8B-1.58-100B-tokens\"\n-    device = \"cuda\"\n \n     # called only once for all test in this class\n     @classmethod\n@@ -65,11 +65,11 @@ def setUpClass(cls):\n         Load the model\n         \"\"\"\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n-        cls.quantized_model = AutoModelForCausalLM.from_pretrained(cls.model_name, device_map=cls.device)\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(cls.model_name, device_map=torch_device)\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_replace_with_bitlinear(self):\n@@ -100,7 +100,7 @@ def test_quantized_model(self):\n         \"\"\"\n         input_text = \"What are we having for dinner?\"\n         expected_output = \"What are we having for dinner? What are we going to do for fun this weekend?\"\n-        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").to(torch_device)\n \n         output = self.quantized_model.generate(**input_ids, max_new_tokens=11, do_sample=False)\n         self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), expected_output)\n@@ -127,7 +127,7 @@ def test_activation_quant(self):\n         from transformers.integrations import BitLinear\n \n         layer = BitLinear(in_features=4, out_features=2, bias=False, dtype=torch.float32)\n-        layer.to(self.device)\n+        layer.to(torch_device)\n \n         input_tensor = torch.tensor([1.0, -1.0, -1.0, 1.0], dtype=torch.float32).to(torch_device)\n \n@@ -202,9 +202,8 @@ def forward(self, x):\n class BitNetSerializationTest(unittest.TestCase):\n     def test_model_serialization(self):\n         model_name = \"HF1BitLLM/Llama3-8B-1.58-100B-tokens\"\n-        device = \"cuda\"\n-        quantized_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)\n-        input_tensor = torch.zeros((1, 8), dtype=torch.int32, device=device)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=torch_device)\n+        input_tensor = torch.zeros((1, 8), dtype=torch.int32, device=torch_device)\n \n         with torch.no_grad():\n             logits_ref = quantized_model.forward(input_tensor).logits\n@@ -215,10 +214,10 @@ def test_model_serialization(self):\n \n         # Remove old model\n         del quantized_model\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n         # Load and check if the logits match\n-        model_loaded = AutoModelForCausalLM.from_pretrained(\"quant_model\", device_map=device)\n+        model_loaded = AutoModelForCausalLM.from_pretrained(\"quant_model\", device_map=torch_device)\n \n         with torch.no_grad():\n             logits_loaded = model_loaded.forward(input_tensor).logits"
        },
        {
            "sha": "5887445bbc026165cb962c88a33dd0503f3093f7",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -32,6 +32,7 @@\n from transformers.testing_utils import (\n     apply_skip_if_not_implemented,\n     backend_empty_cache,\n+    backend_torch_accelerator_module,\n     is_bitsandbytes_available,\n     is_torch_available,\n     require_accelerate,\n@@ -376,7 +377,7 @@ def tearDown(self):\n         avoid unexpected behaviors. Please see: https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530/27\n         \"\"\"\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_inference_without_keep_in_fp32(self):\n         r\"\"\"\n@@ -460,7 +461,7 @@ def tearDown(self):\n         del self.seq_to_seq_model\n \n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_correct_head_class(self):\n         r\"\"\"\n@@ -491,7 +492,7 @@ def tearDown(self):\n             del self.pipe\n \n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_pipeline(self):\n         r\"\"\"\n@@ -589,10 +590,10 @@ def test_training(self):\n         # Step 1: freeze all parameters\n         model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)\n \n-        if torch.cuda.is_available():\n-            self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})\n-        elif torch.xpu.is_available():\n-            self.assertEqual(set(model.hf_device_map.values()), {f\"xpu:{torch.xpu.current_device()}\"})\n+        if torch_device in [\"cuda\", \"xpu\"]:\n+            self.assertEqual(\n+                set(model.hf_device_map.values()), {backend_torch_accelerator_module(torch_device).current_device()}\n+            )\n         else:\n             self.assertTrue(all(param.device.type == \"cpu\" for param in model.parameters()))\n "
        },
        {
            "sha": "5790497a405f2f7246ca6af772ae4214bb8766a0",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -31,6 +31,8 @@\n from transformers.models.opt.modeling_opt import OPTAttention\n from transformers.testing_utils import (\n     apply_skip_if_not_implemented,\n+    backend_empty_cache,\n+    backend_torch_accelerator_module,\n     is_accelerate_available,\n     is_bitsandbytes_available,\n     is_torch_available,\n@@ -137,7 +139,7 @@ def tearDown(self):\n         del self.model_8bit\n \n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_get_keys_to_not_convert(self):\n         r\"\"\"\n@@ -484,7 +486,7 @@ def tearDown(self):\n         avoid unexpected behaviors. Please see: https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530/27\n         \"\"\"\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_inference_without_keep_in_fp32(self):\n         r\"\"\"\n@@ -599,7 +601,7 @@ def tearDown(self):\n         del self.seq_to_seq_model\n \n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_correct_head_class(self):\n         r\"\"\"\n@@ -631,7 +633,7 @@ def tearDown(self):\n             del self.pipe\n \n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_pipeline(self):\n         r\"\"\"\n@@ -872,10 +874,10 @@ def test_training(self):\n         model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)\n         model.train()\n \n-        if torch.cuda.is_available():\n-            self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})\n-        elif torch.xpu.is_available():\n-            self.assertEqual(set(model.hf_device_map.values()), {f\"xpu:{torch.xpu.current_device()}\"})\n+        if torch_device in [\"cuda\", \"xpu\"]:\n+            self.assertEqual(\n+                set(model.hf_device_map.values()), {backend_torch_accelerator_module(torch_device).current_device()}\n+            )\n         else:\n             self.assertTrue(all(param.device.type == \"cpu\" for param in model.parameters()))\n "
        },
        {
            "sha": "f956f0c08c1e3de74cc80d43092b360a9259f1ee",
            "filename": "tests/quantization/compressed_tensors_integration/test_compressed_models.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_models.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -3,7 +3,7 @@\n import warnings\n \n from transformers import AutoModelForCausalLM, AutoTokenizer\n-from transformers.testing_utils import require_compressed_tensors, require_torch\n+from transformers.testing_utils import backend_empty_cache, require_compressed_tensors, require_torch, torch_device\n from transformers.utils import is_torch_available\n from transformers.utils.quantization_config import CompressedTensorsConfig\n \n@@ -41,7 +41,7 @@ class StackCompressedModelTest(unittest.TestCase):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_compressed_uncompressed_model_shapes(self):\n@@ -160,7 +160,7 @@ class RunCompressedTest(unittest.TestCase):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_default_run_compressed__True(self):"
        },
        {
            "sha": "d44e560fff0dccf48defa2cec4108f97199b0949",
            "filename": "tests/quantization/compressed_tensors_integration/test_compressed_tensors.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensors_integration%2Ftest_compressed_tensors.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -2,7 +2,7 @@\n import unittest\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, CompressedTensorsConfig\n-from transformers.testing_utils import require_compressed_tensors, require_torch\n+from transformers.testing_utils import backend_empty_cache, require_compressed_tensors, require_torch, torch_device\n from transformers.utils import is_torch_available\n \n \n@@ -22,7 +22,7 @@ class CompressedTensorsTest(unittest.TestCase):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_config_args(self):"
        },
        {
            "sha": "1bd1fbe45c1e7f6c8bc985e71061f58a1632c0d6",
            "filename": "tests/quantization/eetq_integration/test_eetq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -18,6 +18,7 @@\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, EetqConfig, OPTForCausalLM\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_eetq,\n     require_torch_gpu,\n@@ -87,7 +88,7 @@ def setUpClass(cls):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_quantized_model_conversion(self):"
        },
        {
            "sha": "e31bd9adf51a456a853f1940d4ab933022fe4e63",
            "filename": "tests/quantization/fbgemm_fp8/test_fbgemm_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -18,6 +18,7 @@\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, FbgemmFp8Config, OPTForCausalLM\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_fbgemm_gpu,\n     require_read_token,\n@@ -126,7 +127,7 @@ def setUpClass(cls):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_quantized_model_conversion(self):"
        },
        {
            "sha": "5622ab252fed8ec7c52ddda0a28e41d7fa35f635",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -19,6 +19,7 @@\n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, FineGrainedFP8Config, OPTForCausalLM\n from transformers.testing_utils import (\n     backend_empty_cache,\n+    get_device_properties,\n     require_accelerate,\n     require_read_token,\n     require_torch_accelerator,\n@@ -254,7 +255,7 @@ class FP8LinearTest(unittest.TestCase):\n     device = torch_device\n \n     @unittest.skipIf(\n-        torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 9,\n+        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 9,\n         \"Skipping FP8LinearTest because it is not supported on GPU with capability < 9.0\",\n     )\n     def test_linear_preserves_shape(self):\n@@ -270,7 +271,7 @@ def test_linear_preserves_shape(self):\n         self.assertEqual(x_.shape, x.shape)\n \n     @unittest.skipIf(\n-        torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 9,\n+        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 9,\n         \"Skipping FP8LinearTest because it is not supported on GPU with capability < 9.0\",\n     )\n     def test_linear_with_diff_feature_size_preserves_shape(self):"
        },
        {
            "sha": "20727620269813cdcd23bc1d4c88e699adc97dc8",
            "filename": "tests/quantization/higgs/test_higgs.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -18,6 +18,7 @@\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, HiggsConfig, OPTForCausalLM\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_flute_hadamard,\n     require_torch_gpu,\n@@ -87,7 +88,7 @@ def setUpClass(cls):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_quantized_model_conversion(self):"
        },
        {
            "sha": "5effe1c8616bec402ad4b6288b7a39b192daf72f",
            "filename": "tests/quantization/hqq/test_hqq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhqq%2Ftest_hqq.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -17,6 +17,7 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_hqq,\n     require_torch_gpu,\n@@ -50,7 +51,7 @@ def __init__(self, model_id, quant_config, compute_dtype, device, cache_dir=None\n \n \n def cleanup():\n-    torch.cuda.empty_cache()\n+    backend_empty_cache(torch_device)\n     gc.collect()\n \n \n@@ -187,7 +188,7 @@ def test_save_and_load_quantized_model(self):\n             hqq_runner.model.save_pretrained(tmpdirname)\n \n             del hqq_runner.model\n-            torch.cuda.empty_cache()\n+            backend_empty_cache(torch_device)\n \n             model_loaded = AutoModelForCausalLM.from_pretrained(\n                 tmpdirname, torch_dtype=torch.float16, device_map=torch_device\n@@ -228,7 +229,7 @@ def test_model_serialization(self):\n \n         # Remove old model\n         del hqq_runner.model\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n         # Load and check if the logits match\n         model_loaded = AutoModelForCausalLM.from_pretrained("
        },
        {
            "sha": "9f7ab7f4b9b16a881c0bdb0fb773978dddd4cc87",
            "filename": "tests/quantization/spqr_integration/test_spqr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -18,6 +18,7 @@\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, SpQRConfig, StaticCache\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_spqr,\n     require_torch_gpu,\n@@ -82,8 +83,6 @@ class SpQRTest(unittest.TestCase):\n     )\n     EXPECTED_OUTPUT_COMPILE = \"Hello my name is Jake and I am a 20 year old student at the University of North Texas. (Go Mean Green!) I am a huge fan of the Dallas\"\n \n-    device_map = \"cuda\"\n-\n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n@@ -93,12 +92,12 @@ def setUpClass(cls):\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n             cls.model_name,\n-            device_map=cls.device_map,\n+            device_map=torch_device,\n         )\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_quantized_model_conversion(self):\n@@ -158,7 +157,7 @@ def test_save_pretrained(self):\n         \"\"\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             self.quantized_model.save_pretrained(tmpdirname)\n-            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.device_map)\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)\n \n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n "
        },
        {
            "sha": "37fc538fdbd82a045501d509f1543f66bc5bfee4",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 13,
            "deletions": 10,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -21,10 +21,13 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n from transformers.testing_utils import (\n+    backend_empty_cache,\n+    get_device_properties,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     require_torchao,\n     require_torchao_version_greater_or_equal,\n+    torch_device,\n )\n from transformers.utils import is_torch_available, is_torchao_available\n \n@@ -131,7 +134,7 @@ class TorchAoTest(unittest.TestCase):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_int4wo_quant(self):\n@@ -260,7 +263,7 @@ def test_per_module_config_skip(self):\n \n @require_torch_gpu\n class TorchAoGPUTest(TorchAoTest):\n-    device = \"cuda\"\n+    device = torch_device\n     quant_scheme_kwargs = {\"group_size\": 32}\n \n     def test_int4wo_offload(self):\n@@ -397,7 +400,7 @@ def setUp(self):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_original_model_expected_output(self):\n@@ -452,33 +455,33 @@ def test_serialization_expected_output_on_cuda(self):\n @require_torch_gpu\n class TorchAoSerializationGPTTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int4_weight_only\", {\"group_size\": 32}\n-    device = \"cuda:0\"\n+    device = f\"{torch_device}:0\"\n \n \n @require_torch_gpu\n class TorchAoSerializationW8A8GPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n     EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    device = \"cuda:0\"\n+    device = f\"{torch_device}:0\"\n \n \n @require_torch_gpu\n class TorchAoSerializationW8GPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n     EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    device = \"cuda:0\"\n+    device = f\"{torch_device}:0\"\n \n \n @require_torch_gpu\n @require_torchao_version_greater_or_equal(\"0.10.0\")\n class TorchAoSerializationFP8GPUTest(TorchAoSerializationTest):\n     EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    device = \"cuda:0\"\n+    device = f\"{torch_device}:0\"\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n-        if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 9:\n+        if not (get_device_properties()[0] == \"cuda\" and get_device_properties()[1] >= 9):\n             raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n \n         from torchao.quantization import Float8WeightOnlyConfig\n@@ -493,12 +496,12 @@ def setUpClass(cls):\n @require_torchao_version_greater_or_equal(\"0.10.0\")\n class TorchAoSerializationA8W4Test(TorchAoSerializationTest):\n     EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    device = \"cuda:0\"\n+    device = f\"{torch_device}:0\"\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n-        if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 9:\n+        if not (get_device_properties()[0] == \"cuda\" and get_device_properties()[1] >= 9):\n             raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n \n         from torchao.quantization import Int8DynamicActivationInt4WeightConfig"
        },
        {
            "sha": "0f9e03d4b748f5decbb629de64e9e3ab376b8a20",
            "filename": "tests/quantization/vptq_integration/test_vptq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fvptq_integration%2Ftest_vptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Fquantization%2Fvptq_integration%2Ftest_vptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fvptq_integration%2Ftest_vptq.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -18,6 +18,7 @@\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, VptqConfig\n from transformers.testing_utils import (\n+    backend_empty_cache,\n     require_accelerate,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n@@ -74,7 +75,7 @@ def setUpClass(cls):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_quantized_model(self):"
        },
        {
            "sha": "512f6346e7f772f1839434dc081dfcbaeb8e4def",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -73,7 +73,10 @@\n )\n from transformers.testing_utils import (\n     CaptureLogger,\n+    backend_device_count,\n     backend_empty_cache,\n+    backend_memory_allocated,\n+    backend_torch_accelerator_module,\n     get_device_properties,\n     hub_retry,\n     is_flaky,\n@@ -2613,7 +2616,7 @@ def test_multi_gpu_data_parallel_forward(self):\n         for k in blacklist_non_batched_params:\n             inputs_dict.pop(k, None)\n \n-        # move input tensors to cuda:O\n+        # move input tensors to accelerator O\n         for k, v in inputs_dict.items():\n             if torch.is_tensor(v):\n                 inputs_dict[k] = v.to(0)\n@@ -2636,12 +2639,12 @@ def test_model_parallelization(self):\n \n         # a candidate for testing_utils\n         def get_current_gpu_memory_use():\n-            \"\"\"returns a list of cuda memory allocations per GPU in MBs\"\"\"\n+            \"\"\"returns a list of VRAM allocations per GPU in MBs\"\"\"\n \n             per_device_memory = []\n-            for id in range(torch.cuda.device_count()):\n-                with torch.cuda.device(id):\n-                    per_device_memory.append(torch.cuda.memory_allocated() >> 20)\n+            for id in range(backend_device_count(torch_device)):\n+                with backend_torch_accelerator_module(torch_device).device(id):\n+                    per_device_memory.append(backend_memory_allocated(torch_device) >> 20)\n \n             return per_device_memory\n \n@@ -2657,7 +2660,7 @@ def get_current_gpu_memory_use():\n \n             # Put model on device 0 and take a memory snapshot\n             model = model_class(config)\n-            model.to(\"cuda:0\")\n+            model.to(f\"{torch_device}:0\")\n             memory_after_model_load = get_current_gpu_memory_use()\n \n             # The memory use on device 0 should be higher than it was initially.\n@@ -2717,7 +2720,7 @@ def cast_to_device(dictionary, device):\n \n             model.parallelize()\n \n-            parallel_output = model(**cast_to_device(inputs_dict, \"cuda:0\"))\n+            parallel_output = model(**cast_to_device(inputs_dict, f\"{torch_device}:0\"))\n \n             for value, parallel_value in zip(output, parallel_output):\n                 if isinstance(value, torch.Tensor):\n@@ -4240,10 +4243,10 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa\n                 # add position_ids + fa_kwargs\n                 data_collator = DataCollatorWithFlattening(return_tensors=\"pt\", return_flash_attn_kwargs=True)\n                 batch = data_collator(features)\n-                batch_cuda = {k: t.cuda() if torch.is_tensor(t) else t for k, t in batch.items()}\n+                batch_accelerator = {k: t.to(torch_device) if torch.is_tensor(t) else t for k, t in batch.items()}\n \n                 res_padded = model(**inputs_dict)\n-                res_padfree = model(**batch_cuda)\n+                res_padfree = model(**batch_accelerator)\n \n                 logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n                 logits_padfree = res_padfree.logits[0]"
        },
        {
            "sha": "41bc61e8a7f3c6c5dba0d945b89b20f1434fb0ad",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -3224,7 +3224,7 @@ def test_resume_training_with_randomness(self):\n         # For more than 1 GPUs, since the randomness is introduced in the model and with DataParallel (which is used\n         # in this test for more than 2 GPUs), the calls to the torch RNG will happen in a random order (sometimes\n         # GPU 0 will call first and sometimes GPU 1).\n-        random_torch = not torch.cuda.is_available() or torch.cuda.device_count() <= 1\n+        random_torch = not torch.cuda.is_available() or backend_device_count(torch_device) <= 1\n \n         if torch.cuda.is_available():\n             torch.backends.cudnn.deterministic = True"
        },
        {
            "sha": "9d435cb7ed110cf18915a3968043ed0ba8432132",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=a5a0c7b88828a7273bdedbdcaa2c7a252084c0d8",
            "patch": "@@ -22,6 +22,7 @@\n from transformers.testing_utils import (\n     CaptureStderr,\n     backend_device_count,\n+    backend_torch_accelerator_module,\n     cleanup,\n     get_gpu_count,\n     is_torch_available,\n@@ -430,11 +431,7 @@ def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n         original = GenerationConfig(**common)\n         offloaded = GenerationConfig(cache_implementation=\"offloaded\", **common)\n \n-        torch_accelerator_module = None\n-        if device.type == \"cuda\":\n-            torch_accelerator_module = torch.cuda\n-        elif device.type == \"xpu\":\n-            torch_accelerator_module = torch.xpu\n+        torch_accelerator_module = backend_torch_accelerator_module(device.type)\n \n         torch_accelerator_module.reset_peak_memory_stats(device)\n         model.generate(generation_config=original, **inputs)"
        }
    ],
    "stats": {
        "total": 650,
        "additions": 260,
        "deletions": 390
    }
}