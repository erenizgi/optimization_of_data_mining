{
    "author": "qubvel",
    "message": "Fix `model_accepts_loss_kwargs` for timm model (#35257)\n\n* Fix for timm model\r\n\r\n* Add comment",
    "sha": "5c75087aeee7081025370e10d1f571a11600f1ae",
    "files": [
        {
            "sha": "47e8944583b4ca85b51c215cccfa797feb6a384d",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c75087aeee7081025370e10d1f571a11600f1ae/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c75087aeee7081025370e10d1f571a11600f1ae/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=5c75087aeee7081025370e10d1f571a11600f1ae",
            "patch": "@@ -82,6 +82,9 @@ class TimmWrapperPreTrainedModel(PreTrainedModel):\n     config_class = TimmWrapperConfig\n     _no_split_modules = []\n \n+    # used in Trainer to avoid passing `loss_kwargs` to model forward\n+    accepts_loss_kwargs = False\n+\n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\", \"timm\"])\n         super().__init__(*args, **kwargs)"
        },
        {
            "sha": "655d5b260c1f36f19a30c6b493972060768adf87",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c75087aeee7081025370e10d1f571a11600f1ae/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c75087aeee7081025370e10d1f571a11600f1ae/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=5c75087aeee7081025370e10d1f571a11600f1ae",
            "patch": "@@ -622,7 +622,15 @@ def __init__(\n             else unwrapped_model.get_base_model().forward\n         )\n         forward_params = inspect.signature(model_forward).parameters\n-        self.model_accepts_loss_kwargs = any(k.kind == inspect.Parameter.VAR_KEYWORD for k in forward_params.values())\n+\n+        # Check if the model has explicit setup for loss kwargs,\n+        # if not, check if `**kwargs` are in model.forward\n+        if hasattr(model, \"accepts_loss_kwargs\"):\n+            self.model_accepts_loss_kwargs = model.accepts_loss_kwargs\n+        else:\n+            self.model_accepts_loss_kwargs = any(\n+                k.kind == inspect.Parameter.VAR_KEYWORD for k in forward_params.values()\n+            )\n \n         self.neftune_noise_alpha = args.neftune_noise_alpha\n "
        }
    ],
    "stats": {
        "total": 13,
        "additions": 12,
        "deletions": 1
    }
}