{
    "author": "cyyever",
    "message": "Fix more typos (#40212)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
    "files": [
        {
            "sha": "fe4e939c2dc8610b31c6085af6de9b48cb3ed624",
            "filename": "docs/source/en/model_doc/blip-2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -46,7 +46,7 @@ The original code can be found [here](https://github.com/salesforce/LAVIS/tree/5\n - One can use [`Blip2Processor`] to prepare images for the model, and decode the predicted tokens ID's back to text.\n \n > [!NOTE]\n-> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there wil be failure when merging the embeddings.\n+> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n The attributes can be obtained from model config, as `model.config.num_query_tokens` and model embeddings expansion can be done by following [this link](https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042).\n \n ## Resources"
        },
        {
            "sha": "e1f7f1d430e94623488c6c4bdc6bb9e09131bbe6",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -126,7 +126,7 @@ GOT-OCR2 can also generate formatted text, such as markdown or LaTeX. Here is an\n \n ### Inference on multiple pages\n \n-Although it might be reasonable in most cases to use a “for loop” for multi-page processing, some text data with formatting across several pages make it necessary to process all pages at once. GOT introduces a multi-page OCR (without “for loop”) feature, where multiple pages can be processed by the model at once, whith the output being one continuous text.\n+Although it might be reasonable in most cases to use a “for loop” for multi-page processing, some text data with formatting across several pages make it necessary to process all pages at once. GOT introduces a multi-page OCR (without “for loop”) feature, where multiple pages can be processed by the model at once, with the output being one continuous text.\n Here is an example of how to process multiple pages at once:\n \n "
        },
        {
            "sha": "5de42ff993f807f6d1642810c7c1dd952ebae083",
            "filename": "docs/source/en/model_doc/granite_speech.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -33,7 +33,7 @@ The [Granite Speech](https://huggingface.co/papers/2505.08699) model ([blog post\n 4. LoRA adapter(s): The Granite Speech model contains a modality specific LoRA, which will be enabled when audio features are provided, and disabled otherwise.\n \n \n-Note that most of the aforementioned components are implemented generically to enable compatability and potential integration with other model architectures in transformers.\n+Note that most of the aforementioned components are implemented generically to enable compatibility and potential integration with other model architectures in transformers.\n \n \n This model was contributed by [Alexander Brooks](https://huggingface.co/abrooks9944), [Avihu Dekel](https://huggingface.co/Avihu), and [George Saon](https://huggingface.co/gsaon)."
        },
        {
            "sha": "e34b454a12371d9903e05657a7d3623edd1b9e90",
            "filename": "docs/source/en/model_doc/instructblipvideo.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -39,7 +39,7 @@ The original code can be found [here](https://github.com/salesforce/LAVIS/tree/m\n - The model was trained by sampling 4 frames per video, so it's recommended to sample 4 frames\n \n > [!NOTE]\n-> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there wil be failure when merging the embeddings.\n+> BLIP models after release v4.46 will raise warnings about adding `processor.num_query_tokens = {{num_query_tokens}}` and expand model embeddings layer to add special `<image>` token. It is strongly recommended to add the attributes to the processor if you own the model checkpoint, or open a PR if it is not owned by you. Adding these attributes means that BLIP will add the number of query tokens required per image and expand the text with as many `<image>` placeholders as there will be query tokens. Usually it is around 500 tokens per image, so make sure that the text is not truncated as otherwise there will be failure when merging the embeddings.\n The attributes can be obtained from model config, as `model.config.num_query_tokens` and model embeddings expansion can be done by following [this link](https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042).\n \n ## InstructBlipVideoConfig\n@@ -88,4 +88,4 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n \n [[autodoc]] InstructBlipVideoForConditionalGeneration\n     - forward\n-    - generate\n\\ No newline at end of file\n+    - generate"
        },
        {
            "sha": "92af0c3aa385185140c1bc342efdbfef7b4f5849",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -41,7 +41,7 @@ The [Llama 3.2-Vision](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-ed\n \n Mllama has an extra token used as a placeholder for image positions in the text. It means that input ids and an input embedding layer will have an extra token. But since the weights for input and output embeddings are not tied, the `lm_head` layer has one less token and will fail if you want to calculate loss on image tokens or apply some logit processors. In case you are training, make sure to mask out special `\"<|image|>\"` tokens in the `labels` as the model should not be trained on predicting them.\n \n-Otherwise if you see CUDA-side index erros when generating, use the below code to expand the `lm_head` by one more token. \n+Otherwise if you see CUDA-side index errors when generating, use the below code to expand the `lm_head` by one more token.\n \n \n ```python"
        },
        {
            "sha": "99ffde6288ffa6e9e109d143138bb72ebd7640e7",
            "filename": "docs/source/en/model_doc/shieldgemma2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -33,7 +33,7 @@ This model was contributed by [Ryan Mullins](https://huggingface.co/RyanMullins)\n ## Usage Example\n \n - ShieldGemma 2 provides a Processor that accepts a list of `images` and an optional list of `policies` as input, and constructs a batch of prompts as the product of these two lists using the provided chat template.\n-- You can extend ShieldGemma's built-in in policies with the `custom_policies` argument to the Processor. Using the same key as one of the built-in policies will overwrite that policy with your custom defintion.\n+- You can extend ShieldGemma's built-in in policies with the `custom_policies` argument to the Processor. Using the same key as one of the built-in policies will overwrite that policy with your custom definition.\n - ShieldGemma 2 does not support the image cropping capabilities used by Gemma 3.\n \n ### Classification against Built-in Policies"
        },
        {
            "sha": "b0502664146f2213065b9bf839f921f714ae330b",
            "filename": "docs/source/en/model_doc/superpoint.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -121,7 +121,7 @@ processed_outputs = processor.post_process_keypoint_detection(outputs, [image_si\n \n ## Resources\n \n-- Refer to this [noteboook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SuperPoint/Inference_with_SuperPoint_to_detect_interest_points_in_an_image.ipynb) for an inference and visualization example.\n+- Refer to this [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SuperPoint/Inference_with_SuperPoint_to_detect_interest_points_in_an_image.ipynb) for an inference and visualization example.\n \n ## SuperPointConfig\n "
        },
        {
            "sha": "66d5c2a5d00556c9dde64da752bcb6e461cd2c29",
            "filename": "docs/source/en/model_doc/t5gemma.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -27,9 +27,9 @@ rendered properly in your Markdown viewer.\n \n T5Gemma (aka encoder-decoder Gemma) was proposed in a [research paper](https://huggingface.co/papers/2504.06225) by Google. It is a family of encoder-decoder large language models, developed by adapting pretrained decoder-only models into encoder-decoder. T5Gemma includes pretrained and instruction-tuned variants. The architecture is based on transformer encoder-decoder design following T5, with improvements from Gemma 2: GQA, RoPE, GeGLU activation, RMSNorm, and interleaved local/global attention.\n \n-T5Gemma has two groups of model sizes: 1) [Gemma 2](https://ai.google.dev/gemma/docs/core/model_card_2) sizes (2B-2B, 9B-2B, and 9B-9B), which are based on the offical Gemma 2 models (2B and 9B); and 2) [T5](https://huggingface.co/papers/1910.10683) sizes (Small, Base, Large, and XL), where are pretrained under the Gemma 2 framework following T5 configuration. In addition, we also provide a model at ML size (medium large, ~2B in total), which is in-between T5 Large and T5 XL.\n+T5Gemma has two groups of model sizes: 1) [Gemma 2](https://ai.google.dev/gemma/docs/core/model_card_2) sizes (2B-2B, 9B-2B, and 9B-9B), which are based on the official Gemma 2 models (2B and 9B); and 2) [T5](https://huggingface.co/papers/1910.10683) sizes (Small, Base, Large, and XL), where are pretrained under the Gemma 2 framework following T5 configuration. In addition, we also provide a model at ML size (medium large, ~2B in total), which is in-between T5 Large and T5 XL.\n \n-The pretrained varaints are trained with two objectives: prefix language modeling with knowledge distillation (PrefixLM) and UL2, separately. We release both variants for each model size. The instruction-turned varaints was post-trained with supervised fine-tuning and reinforcement learning.\n+The pretrained variants are trained with two objectives: prefix language modeling with knowledge distillation (PrefixLM) and UL2, separately. We release both variants for each model size. The instruction-turned variants was post-trained with supervised fine-tuning and reinforcement learning.\n \n > [!TIP]\n > Click on the T5Gemma models in the right sidebar for more examples of how to apply T5Gemma to different language tasks."
        },
        {
            "sha": "77616be4cbb8526ef9841cc5582671de0b55caa1",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -161,7 +161,7 @@ from torchao.dtypes import MarlinSparseLayout\n quant_config = Int4WeightOnlyConfig(layout=MarlinSparseLayout())\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n-# Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuraccy loss\n+# Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuracy loss\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"RedHatAI/Sparse-Llama-3.1-8B-2of4\",\n     torch_dtype=torch.float16,\n@@ -260,7 +260,7 @@ from torchao.dtypes import MarlinSparseLayout\n quant_config = Int4WeightOnlyConfig(layout=MarlinSparseLayout())\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n-# Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuraccy loss\n+# Load and quantize the model with sparsity. A sparse checkpoint is needed to accelerate without accuracy loss\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"RedHatAI/Sparse-Llama-3.1-8B-2of4\",\n     torch_dtype=torch.float16,"
        },
        {
            "sha": "afa2cc59b194dba98fc9407b126a02e0c4f6a5ca",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -112,7 +112,7 @@ def _init_weights(self, module):\n \n @auto_docstring(\n     custom_intro=\"\"\"\n-    The Base NewTaskModel model which consists of a vision backbone and a language model withou language modeling head.,\n+    The Base NewTaskModel model which consists of a vision backbone and a language model without language modeling head.,\n     \"\"\"\n )\n class NewTaskModelModel(NewTaskModelPreTrainedModel):\n@@ -310,7 +310,7 @@ def forward(\n \n         is_training = token_type_ids is not None and labels is not None\n \n-        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()"
        },
        {
            "sha": "27510f0b923a385c8d85644ed6902d4ecac04986",
            "filename": "examples/pytorch/text-classification/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/examples%2Fpytorch%2Ftext-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/examples%2Fpytorch%2Ftext-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2FREADME.md?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -85,7 +85,7 @@ python run_glue.py \\\n As an alternative, we can use the script [`run_classification.py`](./run_classification.py) to fine-tune models on a single/multi-label classification task. \n \n The following example fine-tunes BERT on the `en` subset of  [`amazon_reviews_multi`](https://huggingface.co/datasets/amazon_reviews_multi) dataset.\n-We can specify the metric, the label column and aso choose which text columns to use jointly for classification. \n+We can specify the metric, the label column and also choose which text columns to use jointly for classification.\n ```bash\n dataset=\"amazon_reviews_multi\"\n subset=\"en\""
        },
        {
            "sha": "cb799f202049330fed4b74e08fa5bca34e51bf4e",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -1123,7 +1123,7 @@ def get_image_features(\n             vision_feature_layer=vision_feature_layer,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "2e489c6f3a032b808c6a3b473c4aa536266b35eb",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -377,7 +377,7 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "9cb2e50f267ef7a207a7930e6c3e025181b042ad",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -760,7 +760,7 @@ def forward(self, input_feature: torch.Tensor, input_dimensions: tuple[int, int]\n         batch_size, dim, num_channels = input_feature.shape\n \n         input_feature = input_feature.view(batch_size, height, width, num_channels)\n-        # pad input to be disible by width and height, if needed\n+        # pad input to be divisible by width and height, if needed\n         input_feature = self.maybe_pad(input_feature, height, width)\n         # [batch_size, height/2, width/2, num_channels]\n         input_feature_0 = input_feature[:, 0::2, 0::2, :]"
        },
        {
            "sha": "e8d490b14656da88c6a4c929f9835dabcbe811a8",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -295,7 +295,7 @@ def get_decoder(self):\n     def get_image_features(self, pixel_values: torch.FloatTensor):\n         return self.model.get_image_features(pixel_values=pixel_values)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "7eaca133273d44ec9b879639e809f8064e0c6b14",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -962,7 +962,7 @@ def forward(\n             the input_values_cutoffs would be: [[l1, 2 * l1], [l2, -1]].\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[config.audio_token_id, -100, -101]`.\n-            Requires targeted `input_values` to be provided as audio tokens will be infered from it using the `codec_model`.\n+            Requires targeted `input_values` to be provided as audio tokens will be inferred from it using the `codec_model`.\n             - `config.audio_token_id` indicates an audio frames (considering sequence length elements as frames)\n             - `-100` will be ignored in the loss computation\n             - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)"
        },
        {
            "sha": "1838e53a06ad49bed3d4ad5e54dbc945e6049dab",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -634,7 +634,7 @@ def forward(\n             the input_values_cutoffs would be: [[l1, 2 * l1], [l2, -1]].\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[config.audio_token_id, -100, -101]`.\n-            Requires targeted `input_values` to be provided as audio tokens will be infered from it using the `codec_model`.\n+            Requires targeted `input_values` to be provided as audio tokens will be inferred from it using the `codec_model`.\n             - `config.audio_token_id` indicates an audio frames (considering sequence length elements as frames)\n             - `-100` will be ignored in the loss computation\n             - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)"
        },
        {
            "sha": "3a0f7cead0eef3510586c56925cf63d439482509",
            "filename": "src/transformers/models/deprecated/mega/convert_mega_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconvert_mega_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconvert_mega_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconvert_mega_original_pytorch_checkpoint_to_pytorch.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -179,7 +179,7 @@ def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, i\n \n     hf_mlm = MegaForMaskedLM(hf_config).eval()\n \n-    # the originl checkpoint just uses nn.Embedding for the word embeddings\n+    # the original checkpoint just uses nn.Embedding for the word embeddings\n     # we use a wrapper module for embeddings to add support for positional embeddings\n     hf_mlm.mega.embedding_layer.word_embeddings.weight = original_mlm.mega.embedding_layer.weight\n "
        },
        {
            "sha": "3126e88f251a7cba81da633dddd6974a2e01bd4a",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -310,7 +310,7 @@ def forward(self, input_feature: torch.Tensor, input_dimensions: tuple[int, int]\n         batch_size, dim, num_channels = input_feature.shape\n \n         input_feature = input_feature.view(batch_size, height, width, num_channels)\n-        # pad input to be disible by width and height, if needed\n+        # pad input to be divisible by width and height, if needed\n         input_feature = self.maybe_pad(input_feature, height, width)\n         # [batch_size, height/2, width/2, num_channels]\n         input_feature_0 = input_feature[:, 0::2, 0::2, :]"
        },
        {
            "sha": "f7590b6f9310c421ffd70e70c62be365ee998dcd",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -1494,7 +1494,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def text_model(self):\n         return self.model.text_model"
        },
        {
            "sha": "f0a913a9277a02a64ef8a21e36277c20bfe7f69f",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -1073,7 +1073,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def text_model(self):\n         return self.model.text_model"
        },
        {
            "sha": "a421287676112b73cdfa4adc67a9b1dd13ecc861",
            "filename": "src/transformers/models/evolla/processing_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -226,7 +226,7 @@ def save_pretrained(self, save_directory, **kwargs):\n \n         return outputs\n \n-    # overwirte to load the protein tokenizer from a separate folder\n+    # overwrite to load the protein tokenizer from a separate folder\n     # Adapted from instructblip.processing_instructblip.py (https://github.com/huggingface/transformers/blob/9b479a245b793cac2a8b2e87c6d8e81bb24e20c4/src/transformers/models/instructblip/processing_instructblip.py#L191-L221)\n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):"
        },
        {
            "sha": "a07e7bb7e40496a8b915bbec446aabdc67b2b521",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -882,7 +882,7 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()\n@@ -997,7 +997,7 @@ def get_decoder(self):\n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "75115a953c95c5de713138bc934257291d244843",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -788,7 +788,7 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()"
        },
        {
            "sha": "5fd02a33c3a3e874250b9d09ba9eefe3783fadd4",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -2201,7 +2201,7 @@ def get_decoder(self):\n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "9e4a0327c26ac1cdcbfdbdbdc68c23030aa709e1",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -1357,7 +1357,7 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "8abd45f150b1919ac7b87430080e7d2efa3275f2",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -1473,7 +1473,7 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "394d501cf8907fe1183675562e7b6550c860a4e2",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -705,7 +705,7 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "0d214c57b1b52291cf6b88060a938ebd627a6759",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -96,7 +96,7 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n             with torch.no_grad():\n                 expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts)\n                 expert_mask = expert_mask.permute(2, 1, 0)\n-                # we sum on the top_k and on the sequence lenght to get which experts\n+                # we sum on the top_k and on the sequence length to get which experts\n                 # are hit this time around\n                 expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n             for expert_idx in expert_hit[:]:"
        },
        {
            "sha": "226e83a71cf7f50667a0d1ad6ad0fe55fa3b38c4",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -94,7 +94,7 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n             with torch.no_grad():\n                 expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts)\n                 expert_mask = expert_mask.permute(2, 1, 0)\n-                # we sum on the top_k and on the sequence lenght to get which experts\n+                # we sum on the top_k and on the sequence length to get which experts\n                 # are hit this time around\n                 expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n             for expert_idx in expert_hit[:]:"
        },
        {
            "sha": "bfc05d896a6b4709f6ace55c7de9d1fb861f18ec",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -843,7 +843,7 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "f81555279410099734a5b2117a7ba5774355439b",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -366,7 +366,7 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "888fd9b9e64e4e427ff10c346b47ea4b1929a316",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -588,7 +588,7 @@ def get_image_features(\n             vision_feature_select_strategy=vision_feature_select_strategy,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "16e5fac66820c94bcb18153560c1a389535cbd5c",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -729,7 +729,7 @@ def get_image_features(\n             vision_feature_select_strategy=vision_feature_select_strategy,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "d9086226a76aaad2999de6d8fe036ed7d6ec2940",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -759,7 +759,7 @@ def get_image_features(\n             vision_feature_select_strategy=vision_feature_select_strategy,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "417345437145868fa664d620828fa141e6cba373",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -280,7 +280,7 @@ def forward(self, input_feature: torch.Tensor, input_dimensions: tuple[int, int]\n         batch_size, dim, num_channels = input_feature.shape\n \n         input_feature = input_feature.view(batch_size, height, width, num_channels)\n-        # pad input to be disible by width and height, if needed\n+        # pad input to be divisible by width and height, if needed\n         input_feature = self.maybe_pad(input_feature, height, width)\n         # [batch_size, height/2, width/2, num_channels]\n         input_feature_0 = input_feature[:, 0::2, 0::2, :]"
        },
        {
            "sha": "afa6bf44734c7b2ba2040c206f7ab2020534c3c1",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -403,7 +403,7 @@ def get_image_features(\n             **kwargs,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "a8aa1e6cc1a8b186dd90a82cd5a47b735376d83f",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -1571,7 +1571,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "bbbdc4d9d90a9b9311c47547e54d4a992312fe24",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -332,7 +332,7 @@ def forward(\n \n         is_training = token_type_ids is not None and labels is not None\n \n-        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()\n@@ -421,7 +421,7 @@ def get_decoder(self):\n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "9e154d90e7e69a5cb7ce24c0a49b6dbbbb8705ca",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -1415,7 +1415,7 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "a3e821fc84d9111f4a7e8825b0c208b9bfd89f6d",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -1304,7 +1304,7 @@ def get_video_features(\n     def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n         return self.model.get_image_features(pixel_values, image_grid_thw)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "ac81288fa182b027752e16f0ff2525803f58a8b7",
            "filename": "src/transformers/models/sam/modeling_tf_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -465,7 +465,7 @@ def call(\n             if output_attentions:\n                 all_attentions = all_attentions + (attention_outputs,)\n \n-        # Apply the final attenion layer from the points to the image\n+        # Apply the final attention layer from the points to the image\n         query = queries + point_embeddings\n         key = keys + image_positional_embeddings\n "
        },
        {
            "sha": "28855aee3c00c506b96f6026bdebb627f34a1c33",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -777,8 +777,8 @@ def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -\n         # torch.where and expanding the labels tensor is required by the ONNX export\n         point_embedding = torch.where(labels[..., None] == -1, self.not_a_point_embed.weight, point_embedding)\n \n-        # This is required for the ONNX export. The dtype, device need to be explicitely\n-        # specificed as otherwise torch.onnx.export interprets as double\n+        # This is required for the ONNX export. The dtype, device need to be explicitly\n+        # specified as otherwise torch.onnx.export interprets as double\n         point_embedding = torch.where(\n             labels[..., None] != -10,\n             point_embedding,\n@@ -1435,7 +1435,7 @@ def forward(\n             Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n             much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n             that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n-            size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+            size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.\n             In the order (`x1`, `y1`, `x2`, `y2`):\n \n             - `x1`: the x coordinate of the top left point of the input box"
        },
        {
            "sha": "5392e375298708594182c9fb56d950e5979d1397",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -869,8 +869,8 @@ def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -\n         # torch.where and expanding the labels tensor is required by the ONNX export\n         point_embedding = torch.where(labels[..., None] == -1, self.not_a_point_embed.weight, point_embedding)\n \n-        # This is required for the ONNX export. The dtype, device need to be explicitely\n-        # specificed as otherwise torch.onnx.export interprets as double\n+        # This is required for the ONNX export. The dtype, device need to be explicitly\n+        # specified as otherwise torch.onnx.export interprets as double\n         point_embedding = torch.where(\n             labels[..., None] != -10,\n             point_embedding,\n@@ -1332,7 +1332,7 @@ def forward(\n             Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n             much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n             that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n-            size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+            size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.\n             In the order (`x1`, `y1`, `x2`, `y2`):\n \n             - `x1`: the x coordinate of the top left point of the input box"
        },
        {
            "sha": "2c4f0a9b7df351223abae8807516d1ca03ad0162",
            "filename": "src/transformers/models/sam2/processing_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -117,7 +117,7 @@ def __call__(\n         else:\n             raise ValueError(\"Either images or original_sizes must be provided\")\n \n-        # pop arguments that are not used in the foward but used nevertheless\n+        # pop arguments that are not used in the forward but used nevertheless\n         original_sizes = encoding_image_processor[\"original_sizes\"]\n         # Check original_sizes is of length 1 or len(images)\n         if images is not None and len(original_sizes) != 1 and len(original_sizes) != len(images):"
        },
        {
            "sha": "38a9d16115b12aed04f8ec5a375ced8f06e9c243",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -1213,8 +1213,8 @@ def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -\n         # torch.where and expanding the labels tensor is required by the ONNX export\n         point_embedding = torch.where(labels[..., None] == -1, self.not_a_point_embed.weight, point_embedding)\n \n-        # This is required for the ONNX export. The dtype, device need to be explicitely\n-        # specificed as otherwise torch.onnx.export interprets as double\n+        # This is required for the ONNX export. The dtype, device need to be explicitly\n+        # specified as otherwise torch.onnx.export interprets as double\n         point_embedding = torch.where(\n             labels[..., None] != -10,\n             point_embedding,"
        },
        {
            "sha": "30ab7aff233c8d7a5b9c5a791c87dc6a1435de48",
            "filename": "src/transformers/models/sam2_video/processing_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -121,7 +121,7 @@ def __call__(\n         else:\n             raise ValueError(\"Either images or original_sizes must be provided\")\n \n-        # pop arguments that are not used in the foward but used nevertheless\n+        # pop arguments that are not used in the forward but used nevertheless\n         original_sizes = encoding_image_processor[\"original_sizes\"]\n         # Check original_sizes is of length 1 or len(images)\n         if images is not None and len(original_sizes) != 1 and len(original_sizes) != len(images):"
        },
        {
            "sha": "948957fcf7e2561bc7892b9098bcc851106a442e",
            "filename": "src/transformers/models/speech_encoder_decoder/convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconvert_mbart_wav2vec2_seq2seq_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconvert_mbart_wav2vec2_seq2seq_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconvert_mbart_wav2vec2_seq2seq_original_to_pytorch.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -335,7 +335,7 @@ def convert_wav2vec2_checkpoint(\n         type=str,\n         help=\"Path to hf decoder checkpoint config\",\n     )\n-    parser.add_argument(\"--add_adapter\", default=True, type=bool, help=\"whethere to add model adapter layers\")\n+    parser.add_argument(\"--add_adapter\", default=True, type=bool, help=\"whether to add model adapter layers\")\n     parser.add_argument(\"--adapter_stride\", default=2, type=int, help=\"stride of adapter layers\")\n     parser.add_argument(\"--adapter_kernel_size\", default=3, type=int, help=\"kernel size of adapter layers\")\n     parser.add_argument(\"--encoder_output_dim\", default=1024, type=int, help=\"encoder output dim\")"
        },
        {
            "sha": "3287fe7933c782f86d9ff9782570223f146f4149",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -341,7 +341,7 @@ def forward(self, input_feature: torch.Tensor, input_dimensions: tuple[int, int]\n         batch_size, dim, num_channels = input_feature.shape\n \n         input_feature = input_feature.view(batch_size, height, width, num_channels)\n-        # pad input to be disible by width and height, if needed\n+        # pad input to be divisible by width and height, if needed\n         input_feature = self.maybe_pad(input_feature, height, width)\n         # [batch_size, height/2, width/2, num_channels]\n         input_feature_0 = input_feature[:, 0::2, 0::2, :]"
        },
        {
            "sha": "7fa54e958046cc2c8b9e40c69959775013755d68",
            "filename": "src/transformers/models/swin/modeling_tf_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -439,7 +439,7 @@ def call(self, input_feature: tf.Tensor, input_dimensions: tuple[int, int], trai\n         batch_size, _, num_channels = shape_list(input_feature)\n \n         input_feature = tf.reshape(input_feature, (batch_size, height, width, num_channels))\n-        # pad input to be disible by width and height, if needed\n+        # pad input to be divisible by width and height, if needed\n         input_feature = self.maybe_pad(input_feature, height, width)\n         # [batch_size, height/2, width/2, num_channels]\n         input_feature_0 = input_feature[:, 0::2, 0::2, :]"
        },
        {
            "sha": "c2d12e8d78aea6038b2d76777459602e60512cdd",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -213,7 +213,7 @@ def forward(self, input_feature: torch.Tensor, input_dimensions: tuple[int, int]\n         batch_size, dim, num_channels = input_feature.shape\n \n         input_feature = input_feature.view(batch_size, height, width, num_channels)\n-        # pad input to be disible by width and height, if needed\n+        # pad input to be divisible by width and height, if needed\n         input_feature = self.maybe_pad(input_feature, height, width)\n         # [batch_size, height/2, width/2, num_channels]\n         input_feature_0 = input_feature[:, 0::2, 0::2, :]"
        },
        {
            "sha": "57731192d769e7a5d69e1c561c4f0c77d6b2c536",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -385,7 +385,7 @@ def forward(self, input_feature: torch.Tensor, input_dimensions: tuple[int, int]\n         batch_size, dim, num_channels = input_feature.shape\n \n         input_feature = input_feature.view(batch_size, height, width, num_channels)\n-        # pad input to be disible by width and height, if needed\n+        # pad input to be divisible by width and height, if needed\n         input_feature = self.maybe_pad(input_feature, height, width)\n         # [batch_size, height/2, width/2, num_channels]\n         input_feature_0 = input_feature[:, 0::2, 0::2, :]"
        },
        {
            "sha": "cb67b628b717c1ef6d49a2024a9b86d40668ab55",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -461,7 +461,7 @@ def get_image_features(\n             vision_feature_select_strategy=vision_feature_select_strategy,\n         )\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "02c27d8be5781369b00cc42e59c59cb8b0644afd",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -320,7 +320,7 @@ def get_image_features(\n     ):\n         return self.model.get_image_features(pixel_values=pixel_values, vision_feature_layers=vision_feature_layers)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        },
        {
            "sha": "53f96c70f2d762324ecc1e85fab1d4fda952445f",
            "filename": "src/transformers/pipelines/pt_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fpipelines%2Fpt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fpipelines%2Fpt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fpt_utils.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -185,7 +185,7 @@ def __next__(self):\n             # Try to return next item\n             processed = next(self.subiterator)\n         except StopIteration:\n-            # When a preprocess iterator ends, we can start lookig at the next item\n+            # When a preprocess iterator ends, we can start looking at the next item\n             # ChunkIterator will keep feeding until ALL elements of iterator\n             # all have created their subiterator and have been iterating against.\n             #"
        },
        {
            "sha": "9d78ca437dedfde55fad9067535f229cd09811ea",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6333eb986ac85b3ae4cf4c7c819363ef9cd374e7/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=6333eb986ac85b3ae4cf4c7c819363ef9cd374e7",
            "patch": "@@ -255,7 +255,7 @@ def create_quantized_param(\n                     )\n \n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        # we are not really dequantizing, we are just removing everthing related to quantization here\n+        # we are not really dequantizing, we are just removing everything related to quantization here\n         if self.quantization_config.dequantize:\n             self.remove_quantization_config(model)\n         # clean cache due to triton ops"
        }
    ],
    "stats": {
        "total": 134,
        "additions": 67,
        "deletions": 67
    }
}