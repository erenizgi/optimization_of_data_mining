{
    "author": "jmamou",
    "message": "prune LM Head for USD (#36695)\n\n* initial commit\n\n* fix\n\n* fix style\n\n* set default to prune\n\n* add tests\n\n* comment\n\n* remove prune flag from generate\n\n* address Joao's comments\n\n* deprecate_kwarg\n\n* add doc\n\n* fix target_vocab_size\n\n* Update src/transformers/generation/candidate_generator.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/generation/candidate_generator.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/generation/candidate_generator.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/generation/candidate_generator.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* fix deprecated argument assistant_model_device\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "121f91d36c171b67c62320507dfaa460eab7657c",
    "files": [
        {
            "sha": "3425a0234b42a6faa7d33d843aed612387475f85",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 128,
            "deletions": 14,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/121f91d36c171b67c62320507dfaa460eab7657c/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/121f91d36c171b67c62320507dfaa460eab7657c/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=121f91d36c171b67c62320507dfaa460eab7657c",
            "patch": "@@ -19,7 +19,9 @@\n \n import numpy as np\n import torch\n+import torch.nn as nn\n \n+from ..pytorch_utils import prune_linear_layer\n from ..utils import is_sklearn_available\n \n \n@@ -36,6 +38,8 @@\n     from ..tokenization_utils_base import PreTrainedTokenizerBase\n     from .configuration_utils import GenerationConfig\n \n+from ..utils.deprecation import deprecate_kwarg\n+\n \n class CandidateGenerator:\n     \"\"\"Abstract base class for all candidate generators that can be applied during assisted generation.\"\"\"\n@@ -612,6 +616,63 @@ def _process_assistant_outputs(\n         return new_target_ids\n \n \n+class _PruneReindexingLMHead(nn.Module):\n+    \"\"\"\n+    A class to prune and reindex the language model head.\n+\n+    This class prunes the language model head to only include the specified token IDs and reindexes the logits\n+    to map back to the original vocabulary.\n+\n+    Args:\n+        original_lm_head (nn.Module): The original language model head.\n+        token_ids (list[int]): The list of token IDs to keep.\n+    \"\"\"\n+\n+    def __init__(self, original_lm_head, assistant_overlap_token_ids):\n+        super().__init__()\n+        self.pruned_lm_head = prune_linear_layer(original_lm_head, assistant_overlap_token_ids).to(\n+            original_lm_head.weight.dtype\n+        )\n+\n+    def forward(self, hidden_states):\n+        pruned_logits = self.pruned_lm_head(hidden_states)\n+        return pruned_logits\n+\n+\n+class _MapInputEmbedding(nn.Module):\n+    def __init__(self, original_embedding: nn.Embedding, assistant_overlap_token_ids):\n+        \"\"\"\n+        Wraps an existing embedding layer and remaps token IDs before lookup.\n+\n+        Args:\n+            original_embedding (nn.Embedding): Pre-trained or existing embedding layer.\n+            assistant_overlap_token_ids (dict): Mapping from original token IDs to new token IDs.\n+                          Example: {old_id: new_id}\n+        \"\"\"\n+        super().__init__()\n+        self.original_embedding = original_embedding\n+        self.weight = original_embedding.weight\n+        self.assistant_overlap_token_ids = assistant_overlap_token_ids\n+        self.map = False\n+\n+    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            input_ids (torch.LongTensor): Tensor of token IDs (batch_size, seq_len).\n+\n+        Returns:\n+            torch.FloatTensor: Corresponding input embeddings.\n+        \"\"\"\n+        if self.map:\n+            # Get the last item from input_ids\n+            my_input_ids = self.assistant_overlap_token_ids[input_ids[0, -1]].unsqueeze(0).unsqueeze(0)\n+        else:\n+            self.map = True\n+            my_input_ids = input_ids\n+\n+        return self.original_embedding(my_input_ids)\n+\n+\n class AssistantToTargetTranslator:\n     \"\"\"\n     Translates token ids and logits between assistant and target model vocabularies. This class is used to handle\n@@ -625,36 +686,74 @@ class AssistantToTargetTranslator:\n             The tokenizer used by the target (main) model.\n         assistant_tokenizer (`PreTrainedTokenizerBase`):\n             The tokenizer used by the assistant model.\n-        assistant_model_device (`str`, defaults to \"cpu\"):\n-            The device where the assistant model is located. Used for placing tensors.\n-        target_vocab_size (`int`, *optional*):\n+        target_vocab_size (`int`):\n             The size of the target model's vocabulary. If not provided, will be inferred from the target tokenizer.\n+        assistant_model_device (str, optional): The device on which the assistant model is loaded.\n+                Defaults to \"cpu\".\n+        assistant_model_device (`str`, defaults to \"cpu\"): The device where the assistant model is located. Used for placing tensors.\n+        assistant_model (Optional[PreTrainedModel], optional): The assistant model to be used. Defaults to None for backward compatibility.\n+        assistant_prune_lm_head (bool): Whether to prune the assistant model's language model\n+            head to match the target vocabulary. This is only applicable if `assistant_model` is provided.\n+            Defaults to False for backward compatibility.\n     \"\"\"\n \n     FILTER_VALUE: float = -float(\"Inf\")  # The value used to filter out unmapped tokens in the logits.\n     SUPPRESS_TOKEN_ID: int = -1  # The ID used to mark suppressed tokens in the mapping.\n \n+    @deprecate_kwarg(\"assistant_model_device\", version=\"4.53\")\n     def __init__(\n         self,\n         target_tokenizer: \"PreTrainedTokenizerBase\",\n         assistant_tokenizer: \"PreTrainedTokenizerBase\",\n         target_vocab_size: int,  # required since target_vocab_size can be different from the length of target_tokenizer.get_vocab()\n         assistant_model_device: str = \"cpu\",\n+        assistant_model: Optional[\"PreTrainedModel\"] = None,\n+        assistant_prune_lm_head: bool = False,\n     ):\n         self._target_tokenizer: \"PreTrainedTokenizerBase\" = target_tokenizer\n         self._assistant_tokenizer: \"PreTrainedTokenizerBase\" = assistant_tokenizer\n-        self._assistant_model_device: str = assistant_model_device\n+        self._assistant_model_device: str = (\n+            assistant_model_device if assistant_model is None else assistant_model.device\n+        )\n         self.target_vocab_size: int = target_vocab_size\n         self._assistant_to_target_input_ids, self.target_to_assistant_input_ids = (\n             self._get_assistant_to_target_input_ids()\n         )\n         self._suppress_input_ids: list[int] = self._get_suppress_input_ids()\n         self.logits_processors: Optional[LogitsProcessorList] = None\n+        self.assistant_prune_lm_head = assistant_prune_lm_head and assistant_model is not None\n         if len(self._suppress_input_ids) > 0:\n-            # len(self._suppress_input_ids) = 0 if the assistant vocab is a subset of the target vocab\n-            self.logits_processors = LogitsProcessorList(\n-                [SuppressTokensLogitsProcessor(self._get_suppress_input_ids(), self._assistant_model_device)]\n-            )\n+            # the assistant vocab is not a subset of the target vocab\n+            if self.assistant_prune_lm_head:\n+                self.assistant_overlap_token_ids = torch.tensor(\n+                    list(self.target_to_assistant_input_ids.values()),\n+                    dtype=torch.long,\n+                    device=self._assistant_model_device,\n+                )\n+                original_lm_head = assistant_model.get_output_embeddings()\n+                pruned_lm_head = _PruneReindexingLMHead(original_lm_head, self.assistant_overlap_token_ids)\n+                del original_lm_head\n+                assistant_model.set_output_embeddings(pruned_lm_head)\n+\n+                original_input_embeddings = assistant_model.get_input_embeddings()\n+                map_input_embeddings = _MapInputEmbedding(original_input_embeddings, self.assistant_overlap_token_ids)\n+                del original_input_embeddings\n+                assistant_model.set_input_embeddings(map_input_embeddings)\n+                self.map_input_embeddings = map_input_embeddings\n+            else:\n+                self.logits_processors = LogitsProcessorList(\n+                    [SuppressTokensLogitsProcessor(self._get_suppress_input_ids(), self._assistant_model_device)]\n+                )\n+\n+    def unmap_input_ids(self):\n+        \"\"\"\n+        Disables the mapping of input ids despite the assistant pruning for the language model head being enabled.\n+\n+        This method is required for the first forward pass of `_MapInputEmbedding` where input ids are already in the assistant vocabulary space. By disabling the mapping, it ensures that the input ids are processed correctly without remapping.\n+\n+        \"\"\"\n+        if self.assistant_prune_lm_head:\n+            self.map_input_embeddings.map = False\n \n     def _get_assistant_to_target_input_ids(self):\n         target_vocab = self._target_tokenizer.get_vocab()\n@@ -710,7 +809,12 @@ def get_target_ids(\n         if num_new_tokens == 0:\n             return target_input_ids\n         else:\n-            transformed_slice = self._assistant_to_target_input_ids[assistant_candidate_ids[0, -num_new_tokens:]]\n+            # Get last `num_new_tokens` candidate IDs\n+            last_candidate_ids = assistant_candidate_ids[0, -num_new_tokens:]\n+            if self.assistant_prune_lm_head:\n+                # Map assistant IDs -> target input IDs\n+                last_candidate_ids = self.assistant_overlap_token_ids[last_candidate_ids]\n+            transformed_slice = self._assistant_to_target_input_ids[last_candidate_ids]\n             return torch.cat((target_input_ids, transformed_slice.unsqueeze(0)), dim=1)\n \n     def get_target_logits(self, assistant_logits: torch.FloatTensor) -> torch.FloatTensor:\n@@ -726,10 +830,12 @@ def get_target_logits(self, assistant_logits: torch.FloatTensor) -> torch.FloatT\n         assistant_indices_mask = self._assistant_to_target_input_ids != self.SUPPRESS_TOKEN_ID\n         # Exclude invalid indices\n         target_logits_supported_indices = self._assistant_to_target_input_ids[assistant_indices_mask]\n-        valid_assistant_logits = assistant_logits[..., : self._assistant_to_target_input_ids.shape[0]]\n-\n-        target_logits[..., target_logits_supported_indices] = valid_assistant_logits[..., assistant_indices_mask]\n \n+        if self.assistant_prune_lm_head:\n+            target_logits[..., target_logits_supported_indices] = assistant_logits\n+        else:\n+            valid_assistant_logits = assistant_logits[..., : self._assistant_to_target_input_ids.shape[0]]\n+            target_logits[..., target_logits_supported_indices] = valid_assistant_logits[..., assistant_indices_mask]\n         return target_logits\n \n \n@@ -742,12 +848,15 @@ class AssistantVocabTranslatorCache:\n     _cache = weakref.WeakKeyDictionary()\n \n     @classmethod\n+    @deprecate_kwarg(\"assistant_model_device\", version=\"4.53\")\n     def get_translator(\n         cls,\n         target_tokenizer: \"PreTrainedTokenizerBase\",\n         assistant_tokenizer: \"PreTrainedTokenizerBase\",\n         target_vocab_size: int,\n         assistant_model_device: str = \"cpu\",\n+        assistant_model: Optional[\"PreTrainedModel\"] = None,\n+        assistant_prune_lm_head: bool = False,\n     ) -> AssistantToTargetTranslator:\n         assistant_dict = cls._cache.get(target_tokenizer)\n         if assistant_dict is None:\n@@ -757,7 +866,12 @@ def get_translator(\n         mapping = assistant_dict.get(assistant_tokenizer)\n         if mapping is None:\n             mapping = AssistantToTargetTranslator(\n-                target_tokenizer, assistant_tokenizer, target_vocab_size, assistant_model_device\n+                target_tokenizer,\n+                assistant_tokenizer,\n+                target_vocab_size,\n+                assistant_model_device,\n+                assistant_model,\n+                assistant_prune_lm_head,\n             )\n             assistant_dict[assistant_tokenizer] = mapping\n \n@@ -894,7 +1008,7 @@ def _prepare_assistant_input_ids(self, target_input_ids: torch.LongTensor) -> to\n                 self._prev_assistant_ids = self._prev_assistant_ids[:, :-tokens_to_remove]\n             assistant_input_ids = torch.cat([self._prev_assistant_ids, assistant_new_ids], dim=-1)\n         assistant_input_ids = assistant_input_ids.to(dtype=torch.long)\n-\n+        self._atm_translator.unmap_input_ids()\n         return assistant_input_ids, len(assistant_new_ids[0])\n \n "
        },
        {
            "sha": "4ea1f88136d3956d77b706bdd1e52245aec18e50",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/121f91d36c171b67c62320507dfaa460eab7657c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/121f91d36c171b67c62320507dfaa460eab7657c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=121f91d36c171b67c62320507dfaa460eab7657c",
            "patch": "@@ -962,8 +962,14 @@ def _get_candidate_generator(\n         elif different_tokenizers:\n             if generation_config.do_sample is True:\n                 atm_translator = AssistantVocabTranslatorCache.get_translator(\n-                    target_tokenizer, assistant_tokenizer, self.config.vocab_size, assistant_model.device\n+                    target_tokenizer,\n+                    assistant_tokenizer,\n+                    self.config.vocab_size,\n+                    assistant_model=assistant_model,\n+                    assistant_prune_lm_head=True,  # prune LM head of assistant model\n                 )\n+                # Since we prune the LM head, we cannot use the repetition penalty on the assistant model due to mismaches between token ids and logits index\n+                assistant_model.generation_config.repetition_penalty = None\n                 candidate_generator = UniversalSpeculativeDecodingGenerator(\n                     input_ids=input_ids,\n                     assistant_model=assistant_model,"
        },
        {
            "sha": "3a50a963a9a2fc82925ff0ee2e59c7ae8073c0db",
            "filename": "tests/generation/test_candidate_generator.py",
            "status": "modified",
            "additions": 38,
            "deletions": 24,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/121f91d36c171b67c62320507dfaa460eab7657c/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/121f91d36c171b67c62320507dfaa460eab7657c/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_candidate_generator.py?ref=121f91d36c171b67c62320507dfaa460eab7657c",
            "patch": "@@ -20,22 +20,23 @@ def setUp(self):\n         # Create mock tokenizers with predefined vocabularies\n         self.target_tokenizer = MagicMock()\n         self.assistant_tokenizer = MagicMock()\n+        self.assistant_model = MagicMock(device=torch_device)\n \n         # Define mock vocabularies for the tokenizers\n         self.target_vocab = {\"hello\": 0, \"world\": 1, \"foo\": 2, \"bar\": 3}\n         self.assistant_vocab = {\"hello\": 0, \"world\": 1, \"foo\": 2, \"baz\": 4}\n \n         self.target_tokenizer.get_vocab.return_value = self.target_vocab\n         self.assistant_tokenizer.get_vocab.return_value = self.assistant_vocab\n-        self.assistant_model_device = torch_device\n         self.target_vocab_size = 6\n \n         # Instantiate the class under test\n         self.translator = AssistantToTargetTranslator(\n             target_tokenizer=self.target_tokenizer,\n             assistant_tokenizer=self.assistant_tokenizer,\n-            assistant_model_device=self.assistant_model_device,\n             target_vocab_size=self.target_vocab_size,\n+            assistant_model=self.assistant_model,\n+            assistant_prune_lm_head=False,\n         )\n \n     def test_get_assistant_to_target_input_ids(self):\n@@ -53,19 +54,19 @@ def test_get_suppress_input_ids(self):\n     def test_get_target_ids(self):\n         \"\"\"Test the translation of assistant candidate IDs to target candidate IDs.\"\"\"\n         assistant_input_ids = torch.LongTensor([[0, 1, 2]]).to(\n-            self.assistant_model_device\n+            self.assistant_model.device\n         )  # 'hello world foo' in assistant tokenizer\n         target_input_ids = torch.LongTensor([[0, 1, 2]]).to(\n-            self.assistant_model_device\n+            self.assistant_model.device\n         )  # 'hello world foo' in target tokenizer\n         assistant_candidate_ids = torch.LongTensor([[0, 1, 2, 4]]).to(\n-            self.assistant_model_device\n+            self.assistant_model.device\n         )  # 'hello world foo baz' in assistant tokenizer\n \n         expected_target_ids = torch.LongTensor(\n             [[0, 1, 2, self.translator.SUPPRESS_TOKEN_ID]]\n         ).to(\n-            self.assistant_model_device\n+            self.assistant_model.device\n         )  # 'hello world foo baz' in target tokenizer (baz is mapped to self.translator.suppress_tokens_id since it does not exist in target vocab)\n \n         actual_target_ids = self.translator.get_target_ids(\n@@ -77,12 +78,12 @@ def test_get_target_logits(self):\n         \"\"\"Test the conversion of assistant logits to target logits.\"\"\"\n         # Assistant logits for IDs 0, 1, 2\n         assistant_logits = torch.FloatTensor([[[0.1, 0.2, 0.3, 0.4, self.translator.FILTER_VALUE]]]).to(\n-            self.assistant_model_device\n+            self.assistant_model.device\n         )  # Shape (1, 1, 5)\n \n         # Expected target logits (target_vocab_size = 4)\n         expected_target_logits = torch.full((1, 1, self.target_vocab_size), self.translator.FILTER_VALUE).to(\n-            self.assistant_model_device\n+            self.assistant_model.device\n         )\n         expected_target_logits[0, 0, 0] = 0.1  # 'hello'\n         expected_target_logits[0, 0, 1] = 0.2  # 'world'\n@@ -119,22 +120,25 @@ def setUp(self):\n         self.assistant_tokenizer = MockTokenizer({\"hello\": 0, \"world\": 1, \"foo\": 2})\n         self.other_target_tokenizer = MockTokenizer({\"foo\": 2, \"bar\": 3})\n         self.other_assistant_tokenizer = MockTokenizer({\"baz\": 4, \"qux\": 5})\n-        self.assistant_model_device = torch_device\n+        self.assistant_model = MagicMock(device=torch_device)\n+\n         self.target_vocab_size = 6\n \n     def test_same_instance_for_same_tokenizers(self):\n         \"\"\"Test that the same translator is returned for the same tokenizers.\"\"\"\n         translator1 = AssistantVocabTranslatorCache.get_translator(\n             self.target_tokenizer,\n             self.assistant_tokenizer,\n-            assistant_model_device=self.assistant_model_device,\n             target_vocab_size=self.target_vocab_size,\n+            assistant_model=self.assistant_model,\n+            assistant_prune_lm_head=False,\n         )\n         translator2 = AssistantVocabTranslatorCache.get_translator(\n             self.target_tokenizer,\n             self.assistant_tokenizer,\n-            assistant_model_device=self.assistant_model_device,\n             target_vocab_size=self.target_vocab_size,\n+            assistant_model=self.assistant_model,\n+            assistant_prune_lm_head=False,\n         )\n         self.assertIs(translator1, translator2, \"Translators should be cached and identical\")\n \n@@ -143,14 +147,16 @@ def test_different_instances_for_different_tokenizers(self):\n         translator1 = AssistantVocabTranslatorCache.get_translator(\n             self.target_tokenizer,\n             self.assistant_tokenizer,\n-            assistant_model_device=self.assistant_model_device,\n             target_vocab_size=self.target_vocab_size,\n+            assistant_model=self.assistant_model,\n+            assistant_prune_lm_head=False,\n         )\n         translator2 = AssistantVocabTranslatorCache.get_translator(\n             self.other_target_tokenizer,\n             self.other_assistant_tokenizer,\n-            assistant_model_device=self.assistant_model_device,\n             target_vocab_size=self.target_vocab_size,\n+            assistant_model=self.assistant_model,\n+            assistant_prune_lm_head=False,\n         )\n         self.assertIsNot(translator1, translator2, \"Translators should differ for different tokenizers\")\n \n@@ -164,8 +170,9 @@ def test_cache_with_weakref_key(self):\n         translator = AssistantVocabTranslatorCache.get_translator(\n             target_tokenizer,\n             assistant_tokenizer,\n-            assistant_model_device=self.assistant_model_device,\n             target_vocab_size=self.target_vocab_size,\n+            assistant_model=self.assistant_model,\n+            assistant_prune_lm_head=False,\n         )\n         self.assertEqual(len(AssistantVocabTranslatorCache._cache), initial_cache_size + 1)\n \n@@ -192,8 +199,9 @@ def create_translator():\n             translator = AssistantVocabTranslatorCache.get_translator(\n                 target_tokenizer,\n                 assistant_tokenizer,\n-                assistant_model_device=self.assistant_model_device,\n                 target_vocab_size=self.target_vocab_size,\n+                assistant_model=self.assistant_model,\n+                assistant_prune_lm_head=False,\n             )\n             # Create weak references before returning\n             refs = (weakref.ref(translator), weakref.ref(target_tokenizer), weakref.ref(assistant_tokenizer))\n@@ -239,16 +247,18 @@ def setUp(self):\n             self.target_tokenizer.bos_token_id = self.target_tokenizer.eos_token_id\n         if self.assistant_tokenizer.pad_token_id is None:\n             self.assistant_tokenizer.pad_token_id = self.assistant_tokenizer.eos_token_id\n-        if self.target_tokenizer.bos_token_id is None:\n+        if self.assistant_tokenizer.bos_token_id is None:\n             self.assistant_tokenizer.bos_token_id = self.assistant_tokenizer.eos_token_id\n \n         self.input_ids = torch.tensor([[1, 2, 3]]).to(torch_device)\n         self.model_kwargs = {\n             \"attention_mask\": torch.ones_like(self.input_ids).to(torch_device),\n         }\n-\n         atm_translator = AssistantVocabTranslatorCache.get_translator(\n-            self.target_tokenizer, self.assistant_tokenizer, self.target_config.vocab_size, torch_device\n+            target_tokenizer=self.target_tokenizer,\n+            assistant_tokenizer=self.assistant_tokenizer,\n+            assistant_model=self.assistant_model,\n+            target_vocab_size=self.target_config.vocab_size,\n         )\n         self.generator = UniversalSpeculativeDecodingGenerator(\n             input_ids=self.input_ids,\n@@ -286,7 +296,7 @@ def test_mismatched_vocabularies(self):\n         )\n         input_ids = torch.tensor([[self.target_tokenizer.convert_tokens_to_ids(missing_token)]])\n         self.generator.input_ids = input_ids\n-        candidates, scores = self.generator.get_candidates(input_ids)\n+        candidates, _ = self.generator.get_candidates(input_ids)\n         self.assertIsNotNone(candidates)\n \n     def test_speculation_depth(self):\n@@ -296,7 +306,7 @@ def test_speculation_depth(self):\n \n         for depth in [1, 8, 17]:\n             self.generator.num_assistant_tokens = depth\n-            candidates, scores = self.generator.get_candidates(input_ids)\n+            candidates, _ = self.generator.get_candidates(input_ids)\n             self.assertLessEqual(candidates.shape[1] - input_ids.shape[1], depth)\n \n     def test_device_consistency(self):\n@@ -310,16 +320,20 @@ def test_usd_vs_vanilla_sampling(cls):\n         \"\"\"Test that USD matches vanilla sampling with temperature set to nearly 0\"\"\"\n         prompt = \"Test text\"\n \n-        pipe_usd = pipeline(\"text-generation\", model=cls.target_name, assistant_model=cls.assistant_name)\n-        pipe_usd_output = pipe_usd(prompt, max_new_tokens=5, do_sample=True, temperature=1e-9)  # Nearly 0 temperature\n-        usd_text = pipe_usd_output[0][\"generated_text\"]\n-\n         pipe_vanilla = pipeline(\n             \"text-generation\",\n             model=cls.target_name,\n         )\n         pipe_vanilla_output = pipe_vanilla(prompt, max_new_tokens=5, do_sample=False)\n         vanilla_text = pipe_vanilla_output[0][\"generated_text\"]\n \n+        pipe_usd = pipeline(\n+            \"text-generation\",\n+            model=cls.target_name,\n+            assistant_model=cls.assistant_name,\n+        )\n+        pipe_usd_output = pipe_usd(prompt, max_new_tokens=5, do_sample=True, temperature=1e-9)  # Nearly 0 temperature\n+        usd_text = pipe_usd_output[0][\"generated_text\"]\n+\n         # Assert that the outputs match\n         cls.assertEqual(usd_text, vanilla_text)"
        }
    ],
    "stats": {
        "total": 212,
        "additions": 173,
        "deletions": 39
    }
}