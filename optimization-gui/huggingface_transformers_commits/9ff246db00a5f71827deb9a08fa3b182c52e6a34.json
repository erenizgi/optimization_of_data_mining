{
    "author": "remi-or",
    "message": "Expectation fixes and added AMD expectations (#38729)",
    "sha": "9ff246db00a5f71827deb9a08fa3b182c52e6a34",
    "files": [
        {
            "sha": "8f7aedb625a0212eacbe27c7c4d330dd992fb902",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 56,
            "deletions": 31,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -3237,7 +3237,9 @@ def cleanup(device: str, gc_collect=False):\n \n \n # Type definition of key used in `Expectations` class.\n-DeviceProperties = tuple[Union[str, None], Union[int, None]]\n+DeviceProperties = tuple[Optional[str], Optional[int], Optional[int]]\n+# Helper type. Makes creating instances of `Expectations` smoother.\n+PackedDeviceProperties = tuple[Optional[str], Union[None, int, tuple[int, int]]]\n \n \n @cache\n@@ -3248,70 +3250,93 @@ def get_device_properties() -> DeviceProperties:\n     if IS_CUDA_SYSTEM or IS_ROCM_SYSTEM:\n         import torch\n \n-        major, _ = torch.cuda.get_device_capability()\n+        major, minor = torch.cuda.get_device_capability()\n         if IS_ROCM_SYSTEM:\n-            return (\"rocm\", major)\n+            return (\"rocm\", major, minor)\n         else:\n-            return (\"cuda\", major)\n+            return (\"cuda\", major, minor)\n     elif IS_XPU_SYSTEM:\n         import torch\n \n         # To get more info of the architecture meaning and bit allocation, refer to https://github.com/intel/llvm/blob/sycl/sycl/include/sycl/ext/oneapi/experimental/device_architecture.def\n         arch = torch.xpu.get_device_capability()[\"architecture\"]\n         gen_mask = 0x000000FF00000000\n         gen = (arch & gen_mask) >> 32\n-        return (\"xpu\", gen)\n+        return (\"xpu\", gen, None)\n     else:\n-        return (torch_device, None)\n+        return (torch_device, None, None)\n \n \n-class Expectations(UserDict[DeviceProperties, Any]):\n+def unpack_device_properties(\n+    properties: Optional[PackedDeviceProperties] = None,\n+) -> DeviceProperties:\n+    \"\"\"\n+    Unpack a `PackedDeviceProperties` tuple into consistently formatted `DeviceProperties` tuple. If properties is None, it is fetched.\n+    \"\"\"\n+    if properties is None:\n+        return get_device_properties()\n+    device_type, major_minor = properties\n+    if major_minor is None:\n+        major, minor = None, None\n+    elif isinstance(major_minor, int):\n+        major, minor = major_minor, None\n+    else:\n+        major, minor = major_minor\n+    return device_type, major, minor\n+\n+\n+class Expectations(UserDict[PackedDeviceProperties, Any]):\n     def get_expectation(self) -> Any:\n         \"\"\"\n         Find best matching expectation based on environment device properties.\n         \"\"\"\n         return self.find_expectation(get_device_properties())\n \n+    def unpacked(self) -> list[tuple[DeviceProperties, Any]]:\n+        return [(unpack_device_properties(k), v) for k, v in self.data.items()]\n+\n     @staticmethod\n-    def is_default(key: DeviceProperties) -> bool:\n-        return all(p is None for p in key)\n+    def is_default(properties: DeviceProperties) -> bool:\n+        return all(p is None for p in properties)\n \n     @staticmethod\n-    def score(key: DeviceProperties, other: DeviceProperties) -> int:\n+    def score(properties: DeviceProperties, other: DeviceProperties) -> float:\n         \"\"\"\n         Returns score indicating how similar two instances of the `Properties` tuple are.\n-        Points are calculated using bits, but documented as int.\n         Rules are as follows:\n-            * Matching `type` gives 8 points.\n-            * Semi-matching `type`, for example cuda and rocm, gives 4 points.\n-            * Matching `major` (compute capability major version) gives 2 points.\n-            * Default expectation (if present) gives 1 points.\n+            * Matching `type` adds one point, semi-matching `type` adds half a point (e.g. cuda and rocm).\n+            * If types match, matching `major` adds another point, and then matching `minor` adds another.\n+            * Default expectation (if present) is worth 0.1 point to distinguish it from a straight-up zero.\n         \"\"\"\n-        (device_type, major) = key\n-        (other_device_type, other_major) = other\n-\n-        score = 0b0\n-        if device_type == other_device_type:\n-            score |= 0b1000\n+        device_type, major, minor = properties\n+        other_device_type, other_major, other_minor = other\n+\n+        score = 0\n+        # Matching device type, maybe major and minor\n+        if device_type is not None and device_type == other_device_type:\n+            score += 1\n+            if major is not None and major == other_major:\n+                score += 1\n+                if minor is not None and minor == other_minor:\n+                    score += 1\n+        # Semi-matching device type\n         elif device_type in [\"cuda\", \"rocm\"] and other_device_type in [\"cuda\", \"rocm\"]:\n-            score |= 0b100\n-\n-        if major == other_major and other_major is not None:\n-            score |= 0b10\n+            score = 0.5\n \n+        # Default expectation\n         if Expectations.is_default(other):\n-            score |= 0b1\n+            score = 0.1\n \n-        return int(score)\n+        return score\n \n-    def find_expectation(self, key: DeviceProperties = (None, None)) -> Any:\n+    def find_expectation(self, properties: DeviceProperties = (None, None, None)) -> Any:\n         \"\"\"\n         Find best matching expectation based on provided device properties.\n         \"\"\"\n-        (result_key, result) = max(self.data.items(), key=lambda x: Expectations.score(key, x[0]))\n+        (result_key, result) = max(self.unpacked(), key=lambda x: Expectations.score(properties, x[0]))\n \n-        if Expectations.score(key, result_key) == 0:\n-            raise ValueError(f\"No matching expectation found for {key}\")\n+        if Expectations.score(properties, result_key) == 0:\n+            raise ValueError(f\"No matching expectation found for {properties}\")\n \n         return result\n "
        },
        {
            "sha": "c5aad0f24a49d14deb3e19fcc4079992e084f3ed",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -347,7 +347,8 @@ def tearDown(self):\n     @classmethod\n     def get_model(cls):\n         # Use 4-bit on T4\n-        load_in_4bit = get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8\n+        device_type, major, _ = get_device_properties()\n+        load_in_4bit = (device_type == \"cuda\") and (major < 8)\n         torch_dtype = None if load_in_4bit else torch.float16\n \n         if cls.model is None:"
        },
        {
            "sha": "a904e85d10b65892ce8920316e1c058682ba04d9",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -27,6 +27,7 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    DeviceProperties,\n     Expectations,\n     get_device_properties,\n     require_deterministic_for_xpu,\n@@ -594,7 +595,7 @@ class BambaModelIntegrationTest(unittest.TestCase):\n     tokenizer = None\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n-    device_properties = None\n+    device_properties: DeviceProperties = (None, None, None)\n \n     @classmethod\n     def setUpClass(cls):\n@@ -637,7 +638,7 @@ def test_simple_generate(self):\n         self.assertEqual(output_sentence, expected)\n \n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.device_properties == (\"cuda\", 8):\n+        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 8:\n             with torch.no_grad():\n                 logits = self.model(input_ids=input_ids, logits_to_keep=40).logits\n \n@@ -690,7 +691,7 @@ def test_simple_batched_generate_with_padding(self):\n         self.assertEqual(output_sentences[1], EXPECTED_TEXT[1])\n \n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.device_properties == (\"cuda\", 8):\n+        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 8:\n             with torch.no_grad():\n                 logits = self.model(input_ids=inputs[\"input_ids\"]).logits\n "
        },
        {
            "sha": "6d863eaf5880b8886566317aaca9634f54d19575",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -21,6 +21,7 @@\n from transformers import AutoModelForCausalLM, AutoTokenizer, GemmaConfig, is_torch_available\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n+    DeviceProperties,\n     Expectations,\n     cleanup,\n     get_device_properties,\n@@ -108,7 +109,7 @@ class GemmaIntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n     # This variable is used to determine which accelerator are we using for our runners (e.g. A10 or T4)\n     # Depending on the hardware we get different logits / generations\n-    device_properties = None\n+    device_properties: DeviceProperties = (None, None, None)\n \n     @classmethod\n     def setUpClass(cls):\n@@ -241,7 +242,7 @@ def test_model_7b_fp32(self):\n \n     @require_read_token\n     def test_model_7b_fp16(self):\n-        if self.device_properties == (\"cuda\", 7):\n+        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 7:\n             self.skipTest(\"This test is failing (`torch.compile` fails) on Nvidia T4 GPU (OOM).\")\n \n         model_id = \"google/gemma-7b\"\n@@ -262,7 +263,7 @@ def test_model_7b_fp16(self):\n \n     @require_read_token\n     def test_model_7b_bf16(self):\n-        if self.device_properties == (\"cuda\", 7):\n+        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 7:\n             self.skipTest(\"This test is failing (`torch.compile` fails) on Nvidia T4 GPU (OOM).\")\n \n         model_id = \"google/gemma-7b\"\n@@ -293,7 +294,7 @@ def test_model_7b_bf16(self):\n \n     @require_read_token\n     def test_model_7b_fp16_static_cache(self):\n-        if self.device_properties == (\"cuda\", 7):\n+        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 7:\n             self.skipTest(\"This test is failing (`torch.compile` fails) on Nvidia T4 GPU (OOM).\")\n \n         model_id = \"google/gemma-7b\""
        },
        {
            "sha": "0202d9b0eb4c5fb6a0e079c6311e68ef12d99ccb",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -19,6 +19,7 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, GlmConfig, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n     require_flash_attn,\n     require_torch,\n     require_torch_large_accelerator,\n@@ -118,10 +119,17 @@ def test_model_9b_bf16(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     def test_model_9b_eager(self):\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n-            \"Hi today I am going to show you how to make a simple and easy to make a DIY paper flower.\",\n-        ]\n+        expected_texts = Expectations({\n+            (\"cuda\", None): [\n+                \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n+                \"Hi today I am going to show you how to make a simple and easy to make a DIY paper flower.\",\n+            ],\n+            (\"rocm\", (9, 5)) : [\n+                \"Hello I am doing a project on the history of the internetSolution:\\n\\nStep 1: Introduction\\nThe history of the\",\n+                \"Hi today I am going to show you how to make a simple and easy to make a paper airplane. First\",\n+            ]\n+        })  # fmt: skip\n+        EXPECTED_TEXTS = expected_texts.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(\n             self.model_id,"
        },
        {
            "sha": "64ebd236a235c10eba763d219bd08548458fbb80",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -821,6 +821,7 @@ def test_gpt2_sample(self):\n         expected_outputs = Expectations(\n             {\n                 (\"rocm\", None): 'Today is a nice day and we can do this again.\"\\n\\nDana said that she will',\n+                (\"rocm\", (9, 5)): \"Today is a nice day and if you don't know anything about the state of play during your holiday\",\n                 (\"cuda\", None): \"Today is a nice day and if you don't know anything about the state of play during your holiday\",\n             }\n         )  # fmt: skip"
        },
        {
            "sha": "d0a2f251509c736f6ffdf0d3486dc3d738cb35a0",
            "filename": "tests/models/helium/test_modeling_helium.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -17,6 +17,7 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, HeliumConfig, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n     require_read_token,\n     require_torch,\n     slow,\n@@ -83,9 +84,13 @@ class HeliumIntegrationTest(unittest.TestCase):\n     @require_read_token\n     def test_model_2b(self):\n         model_id = \"kyutai/helium-1-preview\"\n-        EXPECTED_TEXTS = [\n-            \"Hello, today is a great day to start a new project. I have been working on a new project for a while now and I have\"\n-        ]\n+        expected_texts = Expectations(\n+            {\n+                (\"rocm\", (9, 5)): [\"Hello, today is a great day to start a new project. I have been working on a new project for a while now, and I\"],\n+                (\"cuda\", None): [\"Hello, today is a great day to start a new project. I have been working on a new project for a while now and I have\"],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXTS = expected_texts.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, revision=\"refs/pr/1\").to(\n             torch_device"
        },
        {
            "sha": "f8f2ac414d16879a35a652dbf90bfb351448d4ee",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -30,6 +30,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_bitsandbytes,\n     require_flash_attn,\n@@ -621,8 +622,14 @@ def test_integration_test_4bit(self):\n         generated_ids = model.generate(**inputs, max_new_tokens=10)\n         generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n \n-        expected_generated_text = \"In this image, we see the Statue of Liberty, the Hudson River,\"\n-        self.assertEqual(generated_texts[0], expected_generated_text)\n+        expected_generated_texts = Expectations(\n+            {\n+                (\"cuda\", None): \"In this image, we see the Statue of Liberty, the Hudson River,\",\n+                (\"rocm\", (9, 5)): \"In this image, we see the Statue of Liberty, the New York City\",\n+            }\n+        )\n+        EXPECTED_GENERATED_TEXT = expected_generated_texts.get_expectation()\n+        self.assertEqual(generated_texts[0], EXPECTED_GENERATED_TEXT)\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "963e840e0b7bdda52355d06ceac75a4c6ce9b767",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -537,6 +537,7 @@ def test_qwen2_medium_model_integration_video(self):\n             {\n                 (\"xpu\", 3): \"The man is performing a volley.\",\n                 (\"cuda\", 7): \"The man is performing a forehand shot.\",\n+                (\"rocm\", (9, 5)): \"The man is performing a volley shot.\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()"
        },
        {
            "sha": "98ccf21e59b332b6bfdd0f5c3896eb980616cd9f",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers import AutoTokenizer, JambaConfig, is_torch_available\n from transformers.testing_utils import (\n+    DeviceProperties,\n     Expectations,\n     get_device_properties,\n     require_bitsandbytes,\n@@ -557,7 +558,7 @@ class JambaModelIntegrationTest(unittest.TestCase):\n     tokenizer = None\n     # This variable is used to determine which acclerator are we using for our runners (e.g. A10 or T4)\n     # Depending on the hardware we get different logits / generations\n-    device_properties = None\n+    device_properties: DeviceProperties = (None, None, None)\n \n     @classmethod\n     def setUpClass(cls):\n@@ -595,7 +596,7 @@ def test_simple_generate(self):\n         self.assertEqual(output_sentence, expected_sentence)\n \n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.device_properties == (\"cuda\", 8):\n+        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 8:\n             with torch.no_grad():\n                 logits = self.model(input_ids=input_ids).logits\n \n@@ -638,7 +639,7 @@ def test_simple_batched_generate_with_padding(self):\n         self.assertEqual(output_sentences[1], expected_sentences[1])\n \n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n-        if self.device_properties == (\"cuda\", 8):\n+        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 8:\n             with torch.no_grad():\n                 logits = self.model(input_ids=inputs[\"input_ids\"]).logits\n "
        },
        {
            "sha": "5c142e066f9c174fa9c35c67c77a766918871fba",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 18,
            "deletions": 10,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -541,16 +541,24 @@ def test_model_generate_images(self):\n         # fmt: off\n         expected_tokens =  Expectations(\n             {\n-                (\"rocm\", None): [10367, 1380, 4841, 15155, 1224, 16361, 15834, 13722, 15258, 8321, 10496, 14532, 8770,\n-                                 12353, 5481, 11484, 2585, 8587, 3201, 14292, 3356, 2037, 3077, 6107, 3758, 2572, 9376,\n-                                 13219, 6007, 14292, 12696, 10666, 10046, 13483, 8282, 9101, 5208, 4260, 13886, 13335,\n-                                 6135, 2316, 15423,  311, 5460, 12218, 14172, 8583, 14577, 3648\n-                                 ],\n-                (\"cuda\", None): [4484, 4015, 15750, 506, 3758, 11651, 8597, 5739, 4861, 971, 14985, 14834, 15438, 7548,\n-                                 1820, 1465, 13529, 12761, 10503, 12761, 14303, 6155, 4015, 11766, 705, 15736, 14146,\n-                                 10417, 1951, 7713, 14305, 15617, 6169, 2706, 8006, 14893, 3855, 10188, 15652, 6297,\n-                                 1097, 12108, 15038, 311, 14998, 15165, 897, 4044, 1762, 4676\n-                                 ],\n+                (\"rocm\", None): [\n+                    10367, 1380, 4841, 15155, 1224, 16361, 15834, 13722, 15258, 8321, 10496, 14532, 8770, 12353, 5481,\n+                    11484, 2585, 8587, 3201, 14292, 3356, 2037, 3077, 6107, 3758, 2572, 9376, 13219, 6007, 14292, 12696,\n+                    10666, 10046, 13483, 8282, 9101, 5208, 4260, 13886, 13335, 6135, 2316, 15423, 311, 5460, 12218,\n+                    14172, 8583, 14577, 3648\n+                ],\n+                (\"rocm\", (9, 5)): [\n+                    4484, 4015, 15750, 506, 3758, 11651, 8597, 5739, 4861, 971, 14985, 14834, 15438, 7548, 1820, 1465,\n+                    13529, 12761, 10503, 12761, 14303, 6155, 4015, 11766, 705, 15736, 14146, 10417, 1951, 7713, 14305,\n+                    15617, 6169, 2706, 8006, 14893, 3855, 10188, 15652, 6297, 1097, 12108, 15038, 311, 14998, 15165,\n+                    897, 4044, 1762, 4676\n+                ],\n+                (\"cuda\", None): [\n+                    4484, 4015, 15750, 506, 3758, 11651, 8597, 5739, 4861, 971, 14985, 14834, 15438, 7548, 1820, 1465,\n+                    13529, 12761, 10503, 12761, 14303, 6155, 4015, 11766, 705, 15736, 14146, 10417, 1951, 7713, 14305,\n+                    15617, 6169, 2706, 8006, 14893, 3855, 10188, 15652, 6297, 1097, 12108, 15038, 311, 14998, 15165,\n+                    897, 4044, 1762, 4676\n+                ],\n             }\n         )\n         expected_tokens = torch.tensor(expected_tokens.get_expectation()).to(model.device)"
        },
        {
            "sha": "2e0e9126b1d36818924ec14c5136a8543c3c1f83",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -113,7 +113,7 @@ def test_llama_3_1_hard(self):\n         \"\"\"\n         # diff on `EXPECTED_TEXT`:\n         # 2024-08-26: updating from torch 2.3.1 to 2.4.0 slightly changes the results.\n-        EXPECTED_TEXT = (\n+        expected_base_text = (\n             \"Tell me about the french revolution. The french revolution was a period of radical political and social \"\n             \"upheaval in France that lasted from 1789 until 1799. It was a time of great change and upheaval, marked \"\n             \"by the overthrow of the monarchy, the rise of the middle class, and the eventual establishment of the \"\n@@ -122,6 +122,13 @@ def test_llama_3_1_hard(self):\n             \"demanded greater representation and eventually broke away to form the National Assembly. This marked \"\n             \"the beginning of the end of the absolute monarchy and the rise of the middle class.\\n\"\n         )\n+        expected_texts = Expectations(\n+            {\n+                (\"rocm\", (9, 5)): expected_base_text.replace(\"political and social\", \"social and political\"),\n+                (\"cuda\", None): expected_base_text,\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = expected_texts.get_expectation()\n \n         tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n         model = LlamaForCausalLM.from_pretrained("
        },
        {
            "sha": "edfbbe9f0e1f2f1bc43e44ae92c8774ce45df3bb",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 51,
            "deletions": 11,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -341,7 +341,11 @@ def test_small_model_integration_test(self):\n         inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n-        EXPECTED_DECODED_TEXT = \"\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT: When visiting this place, there are a few things one should be cautious about. Firstly,\"  # fmt: skip\n+        expected_decoded_texts = Expectations({\n+            (\"cuda\", None): \"\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT: When visiting this place, there are a few things one should be cautious about. Firstly,\",\n+            (\"rocm\", (9, 5)): \"\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT: When visiting this place, there are a few things one should be cautious about. First, the\",\n+        })  # fmt: skip\n+        EXPECTED_DECODED_TEXT = expected_decoded_texts.get_expectation()\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n@@ -397,12 +401,28 @@ def test_small_model_integration_test_llama_batched(self):\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n-        EXPECTED_DECODED_TEXT = ['USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, you', 'USER:  \\nWhat is this? ASSISTANT: The image features two cats lying down on a pink couch. One cat is located on']  # fmt: skip\n-\n-        self.assertEqual(\n-            processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n+        expected_decoded_texts = Expectations(\n+            {\n+                (\"cuda\", None): [\n+                    \"USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring \"\n+                    \"with me? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, \"\n+                    \"you\",\n+                    \"USER:  \\nWhat is this? ASSISTANT: The image features two cats lying down on a pink couch. One cat \"\n+                    \"is located on\",\n+                ],\n+                (\"rocm\", (9, 5)): [\n+                    \"USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring \"\n+                    \"with me? ASSISTANT: When visiting this serene location, which features a wooden pier overlooking a \"\n+                    \"lake, you should\",\n+                    \"USER:  \\nWhat is this? ASSISTANT: The image features two cats lying down on a pink couch. One cat \"\n+                    \"is located on\",\n+                ],\n+            }\n         )\n+        EXPECTED_DECODED_TEXT = expected_decoded_texts.get_expectation()\n+\n+        decoded_output = processor.batch_decode(output, skip_special_tokens=True)\n+        self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_bitsandbytes\n@@ -433,6 +453,10 @@ def test_small_model_integration_test_batch(self):\n                     'USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, there are a few things to be cautious about and items to bring along',\n                     'USER:  \\nWhat is this?\\nASSISTANT: Cats',\n                 ],\n+                (\"rocm\", (9, 5)): [\n+                    \"USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this dock on a lake, there are several things to be cautious about and items to\",\n+                    \"USER:  \\nWhat is this?\\nASSISTANT: This is a picture of two cats lying on a couch.\",\n+                ],\n             }\n         )  # fmt: skip\n         EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n@@ -467,12 +491,28 @@ def test_small_model_integration_test_llama_batched_regression(self):\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n-        EXPECTED_DECODED_TEXT = ['USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, which appears to be a dock or pier extending over a body of water', 'USER:  \\nWhat is this?\\nASSISTANT: Two cats lying on a bed!\\nUSER:  \\nAnd this?\\nASSISTANT: A cat sleeping on a bed.']  # fmt: skip\n-\n-        self.assertEqual(\n-            processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n+        expected_decoded_texts = Expectations(\n+            {\n+                (\"cuda\", None): [\n+                    \"USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring \"\n+                    \"with me?\\nASSISTANT: When visiting this place, which appears to be a dock or pier extending over a \"\n+                    \"body of water\",\n+                    \"USER:  \\nWhat is this?\\nASSISTANT: Two cats lying on a bed!\\nUSER:  \\nAnd this?\\nASSISTANT: A cat \"\n+                    \"sleeping on a bed.\",\n+                ],\n+                (\"rocm\", (9, 5)): [\n+                    \"USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring \"\n+                    \"with me?\\nASSISTANT: When visiting this place, which is a pier or dock overlooking a lake, you should \"\n+                    \"be\",\n+                    \"USER:  \\nWhat is this?\\nASSISTANT: Two cats lying on a bed!\\nUSER:  \\nAnd this?\\nASSISTANT: A cat \"\n+                    \"sleeping on a bed.\",\n+                ],\n+            }\n         )\n+        EXPECTED_DECODED_TEXT = expected_decoded_texts.get_expectation()\n+\n+        decoded_output = processor.batch_decode(output, skip_special_tokens=True)\n+        self.assertEqual(decoded_output, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_torch"
        },
        {
            "sha": "b962ef62896d27ed786713698823f33b7a914989",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers import AutoTokenizer, MistralConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    DeviceProperties,\n     Expectations,\n     backend_empty_cache,\n     cleanup,\n@@ -114,7 +115,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n class MistralIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which accelerator are we using for our runners (e.g. A10 or T4)\n     # Depending on the hardware we get different logits / generations\n-    device_properties = None\n+    device_properties: DeviceProperties = (None, None, None)\n \n     @classmethod\n     def setUpClass(cls):\n@@ -279,7 +280,7 @@ def test_compile_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.3.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n \n-        if self.device_properties == (\"cuda\", 7):\n+        if self.device_properties[0] == \"cuda\" and self.device_properties[1] == 7:\n             self.skipTest(reason=\"This test is failing (`torch.compile` fails) on Nvidia T4 GPU.\")\n \n         NUM_TOKENS_TO_GENERATE = 40"
        },
        {
            "sha": "3b53e1cfa53bc77ee9a7003c2d1ed4a99d838f8a",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -20,7 +20,6 @@\n from transformers import MixtralConfig, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n-    get_device_properties,\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n@@ -142,14 +141,6 @@ def test_load_balancing_loss(self):\n \n @require_torch\n class MixtralIntegrationTest(unittest.TestCase):\n-    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n-    # Depending on the hardware we get different logits / generations\n-    device_properties = None\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        cls.device_properties = get_device_properties()\n-\n     @slow\n     @require_torch_accelerator\n     def test_small_model_logits(self):"
        },
        {
            "sha": "449ddfbc2aabec3c1a6396848fa6c3d9b55d640d",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -445,7 +445,11 @@ def test_generation_8k(self):\n         )\n \n         input_text = \"Hello\"\n-        expected_output = \"Hello, I'm a new user of the forum. I have a question about the \\\"Solaris\"\n+        expected_outputs = Expectations({\n+            (\"cuda\", None): \"Hello, I'm a new user of the forum. I have a question about the \\\"Solaris\",\n+            (\"rocm\", (9, 5)): \"Hello, I'm a newbie to the forum. I have a question about the \\\"B\\\" in\",\n+        })  # fmt: off\n+        expected_output = expected_outputs.get_expectation()\n \n         inputs = tokenizer(input_text, return_tensors=\"pt\").to(torch_device)\n         outputs = model.generate(**inputs, max_new_tokens=20)\n@@ -463,19 +467,12 @@ def test_generation(self):\n         )\n \n         input_text = \"Hello\"\n-        expected_outputs = Expectations(\n-            {\n-                (\n-                    \"xpu\",\n-                    3,\n-                ): \"Hello and welcome to the first ever episode of the new and improved, and hopefully improved, podcast.\\n\",\n-                (\"cuda\", 7): \"Hello and welcome to the first episode of the new podcast, The Frugal Feminist.\\n\",\n-                (\n-                    \"cuda\",\n-                    8,\n-                ): \"Hello and welcome to the first day of the new release countdown for the month of May!\\nToday\",\n-            }\n-        )\n+        expected_outputs = Expectations({\n+            (\"rocm\", (9, 5)): \"Hello and welcome to the first day of the new release at The Stamp Man!\\nToday we are\",\n+            (\"xpu\", 3): \"Hello and welcome to the first ever episode of the new and improved, and hopefully improved, podcast.\\n\",\n+            (\"cuda\", 7): \"Hello and welcome to the first episode of the new podcast, The Frugal Feminist.\\n\",\n+            (\"cuda\", 8): \"Hello and welcome to the first day of the new release countdown for the month of May!\\nToday\",\n+        })  # fmt: off\n         expected_output = expected_outputs.get_expectation()\n \n         inputs = tokenizer(input_text, return_tensors=\"pt\").to(torch_device)\n@@ -510,6 +507,10 @@ def test_generation_batched(self):\n                     \"Hello my name is Tiffany and I am a mother of two beautiful children. I have been a nanny for the\",\n                     \"Today I am going at the gym and then I am going to go to the grocery store. I am going to buy some food and some\",\n                 ],\n+                (\"rocm\", (9, 5)): [\n+                    \"Hello my name is Jasmine and I am a very sweet and loving dog. I am a very playful dog and I\",\n+                    \"Today I am going at the gym and then I am going to go to the mall. I am going to buy a new pair of jeans\",\n+                ],\n             }\n         )\n         expected_output = expected_outputs.get_expectation()\n@@ -535,9 +536,10 @@ def test_model_logits(self):\n             {\n                 (\"xpu\", 3): torch.Tensor([-0.2090, -0.2061, -0.1465]),\n                 (\"cuda\", 7): torch.Tensor([-0.2520, -0.2178, -0.1953]),\n+                # TODO: This is quite a bit off, check BnB\n+                (\"rocm\", (9, 5)): torch.Tensor([-0.3008, -0.1309, -0.1562]),\n             }\n         )\n         expected_slice = expected_slices.get_expectation().to(torch_device, torch.bfloat16)\n         predicted_slice = outputs.hidden_states[-1][0, 0, :3]\n-\n         torch.testing.assert_close(expected_slice, predicted_slice, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "0c84b34f3b7b9b6ce11386dc601df06d9547ef08",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -1041,7 +1041,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        (device_type, major) = get_device_properties()\n+        device_type, major, _ = get_device_properties()\n         if device_type == \"cuda\" and major < 8:\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n         elif device_type == \"rocm\" and major < 9:"
        },
        {
            "sha": "c8c505b7545cb4c60c4a9ff938e366118f1ededa",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -1041,7 +1041,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        (device_type, major) = get_device_properties()\n+        device_type, major, _ = get_device_properties()\n         if device_type == \"cuda\" and major < 8:\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n         elif device_type == \"rocm\" and major < 9:"
        },
        {
            "sha": "1be522f3a50fe407bd74d93e63bcbf116e8a37f5",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -27,6 +27,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_read_token,\n     require_torch,\n@@ -590,7 +591,13 @@ def test_integration_detection_bug(self):\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n-        EXPECTED_DECODED_TEXT = \"detect shoe\\n<loc0051><loc0309><loc0708><loc0646> shoe\"  # fmt: skip\n+        expected_decoded_texts = Expectations(\n+            {\n+                (\"rocm\", (9, 5)): \"detect shoe\\n<loc0051><loc0309><loc0708><loc0644> shoe\",\n+                (\"cuda\", None): \"detect shoe\\n<loc0051><loc0309><loc0708><loc0646> shoe\",\n+            }\n+        )  # fmt: skip\n+        EXPECTED_DECODED_TEXT = expected_decoded_texts.get_expectation()\n         self.assertEqual(self.processor.decode(output[0], skip_special_tokens=True), EXPECTED_DECODED_TEXT)\n \n     def test_paligemma_index_error_bug(self):"
        },
        {
            "sha": "b303c325aedb8b9263c88699948abd53d84b2882",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -19,6 +19,7 @@\n from transformers import Phi3Config, StaticCache, is_torch_available\n from transformers.models.auto.configuration_auto import AutoConfig\n from transformers.testing_utils import (\n+    Expectations,\n     require_torch,\n     slow,\n     torch_device,\n@@ -352,9 +353,14 @@ def test_export_static_cache(self):\n         model_id = \"microsoft/Phi-4-mini-instruct\"\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id, pad_token=\"</s>\", padding_side=\"right\")\n-        EXPECTED_TEXT_COMPLETION = [\n-            \"You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user. A 45-year-old patient with a 10-year history of type 2 diabetes mellitus, who is currently on metformin and a SGLT2 inhibitor, presents with a 2-year history\"\n-        ]\n+\n+        expected_text_completions = Expectations(\n+            {\n+                (\"rocm\", (9, 5)): [\"You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user. A 45-year-old patient with a 10-year history of type 2 diabetes mellitus presents with a 2-year history of progressive, non-healing, and painful, 2.5 cm\"],\n+                (\"cuda\", None): [\"You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user. A 45-year-old patient with a 10-year history of type 2 diabetes mellitus, who is currently on metformin and a SGLT2 inhibitor, presents with a 2-year history\"],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT_COMPLETION = expected_text_completions.get_expectation()\n         max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n             \"input_ids\"\n         ].shape[-1]"
        },
        {
            "sha": "59fcf240a8131917c712f0c9a5d512080b5f40bb",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -22,6 +22,7 @@\n from transformers import AutoTokenizer, Qwen2Config, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n+    Expectations,\n     backend_empty_cache,\n     require_bitsandbytes,\n     require_flash_attn,\n@@ -250,9 +251,17 @@ def test_export_static_cache(self):\n         qwen_model = \"Qwen/Qwen2-0.5B\"\n \n         tokenizer = AutoTokenizer.from_pretrained(qwen_model, pad_token=\"</s>\", padding_side=\"right\")\n-        EXPECTED_TEXT_COMPLETION = [\n-            \"My favourite condiment is 100% natural, organic, gluten free, vegan, and free from preservatives. I\"\n-        ]\n+\n+        expected_text_completions = Expectations({\n+            (\"cuda\", None): [\n+                \"My favourite condiment is 100% natural, organic, gluten free, vegan, and free from preservatives. I\"\n+            ],\n+            (\"rocm\", (9, 5)): [\n+                \"My favourite condiment is 100% natural, organic, gluten free, vegan, and vegetarian. I love to use\"\n+            ]\n+        })  # fmt: off\n+        EXPECTED_TEXT_COMPLETION = expected_text_completions.get_expectation()\n+\n         max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n             \"input_ids\"\n         ].shape[-1]"
        },
        {
            "sha": "2664e1b69246f6178653fa3e6c0ad42d619c5f9c",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -22,6 +22,7 @@\n from transformers import AutoTokenizer, Qwen3Config, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n+    Expectations,\n     backend_empty_cache,\n     require_bitsandbytes,\n     require_flash_attn,\n@@ -246,10 +247,18 @@ def test_export_static_cache(self):\n         tokenizer = AutoTokenizer.from_pretrained(qwen_model, pad_token=\"</s>\", padding_side=\"right\")\n         if version.parse(torch.__version__) == version.parse(\"2.7.0\"):\n             strict = False  # Due to https://github.com/pytorch/pytorch/issues/150994\n-            EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% plain, unflavoured, and unadulterated.\"]\n+            cuda_expectation = [\"My favourite condiment is 100% plain, unflavoured, and unadulterated.\"]\n         else:\n             strict = True\n-            EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% plain, unflavoured, and unadulterated. It is\"]\n+            cuda_expectation = [\"My favourite condiment is 100% plain, unflavoured, and unadulterated. It is\"]\n+\n+        expected_text_completions = Expectations(\n+            {\n+                (\"rocm\", (9, 5)): [\"My favourite condiment is 100% plain, unflavoured, and unadulterated.\"],\n+                (\"cuda\", None): cuda_expectation,\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT_COMPLETION = expected_text_completions.get_expectation()\n \n         max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n             \"input_ids\""
        },
        {
            "sha": "3963d7e48e1093868ce9eec5701293f07adb4472",
            "filename": "tests/models/siglip2/test_modeling_siglip2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 11,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -23,6 +23,7 @@\n \n from transformers import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig\n from transformers.testing_utils import (\n+    Expectations,\n     is_flaky,\n     require_flash_attn,\n     require_torch,\n@@ -760,17 +761,19 @@ def test_inference(self):\n \n         # verify the logits values\n         # fmt: off\n-        expected_logits_per_text = torch.tensor(\n-            [\n-                [  1.0195,  -0.0280,  -1.4468],\n-                [ -4.5395,  -6.2269,  -1.5667],\n-                [  4.1757,   5.0358,   3.5159],\n-                [  9.4264,  10.1879,   6.3353],\n-                [  2.4409,   3.1058,   4.5491],\n-                [-12.3230, -13.7355, -13.4632],\n+        expected_logits_per_texts = Expectations({\n+            (\"cuda\", None): [\n+                [  1.0195,  -0.0280,  -1.4468], [ -4.5395,  -6.2269,  -1.5667], [  4.1757,   5.0358,   3.5159],\n+                [  9.4264,  10.1879,   6.3353], [  2.4409,   3.1058,   4.5491], [-12.3230, -13.7355, -13.4632],\n                 [  1.1520,   1.1687,  -1.9647],\n-            ]\n-        ).to(torch_device)\n+            ],\n+            (\"rocm\", (9, 5)): [\n+                [  1.0236,  -0.0376,  -1.4464], [ -4.5358,  -6.2235,  -1.5628], [  4.1708,   5.0334,   3.5187],\n+                [  9.4241,  10.1828,   6.3366], [  2.4371,   3.1062,   4.5530], [-12.3173, -13.7240, -13.4580],\n+                [  1.1502,   1.1716,  -1.9623]\n+            ],\n+        })\n+        EXPECTED_LOGITS_PER_TEXT = torch.tensor(expected_logits_per_texts.get_expectation()).to(torch_device)\n         # fmt: on\n \n-        torch.testing.assert_close(outputs.logits_per_text, expected_logits_per_text, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(outputs.logits_per_text, EXPECTED_LOGITS_PER_TEXT, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "ecd1a73f335f099d10ab1bc9c1316b9893b9031e",
            "filename": "tests/models/vits/test_modeling_vits.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import PretrainedConfig, VitsConfig\n from transformers.testing_utils import (\n+    Expectations,\n     is_flaky,\n     is_torch_available,\n     require_torch,\n@@ -454,13 +455,21 @@ def test_forward_fp16(self):\n \n         self.assertEqual(outputs.waveform.shape, (1, 87040))\n         # fmt: off\n-        EXPECTED_LOGITS = torch.tensor(\n-            [\n+        expected_logits = Expectations({\n+            (\"cuda\", None): [\n                 0.0101,  0.0318,  0.0489,  0.0627,  0.0728,  0.0865,  0.1053,  0.1279,\n                 0.1514,  0.1703,  0.1827,  0.1829,  0.1694,  0.1509,  0.1332,  0.1188,\n                 0.1066,  0.0978,  0.0936,  0.0867,  0.0724,  0.0493,  0.0197, -0.0141,\n                 -0.0501, -0.0817, -0.1065, -0.1223, -0.1311, -0.1339\n+            ],\n+            (\"rocm\", (9, 5)): [\n+                0.0097,  0.0315,  0.0486,  0.0626,  0.0728,  0.0865,  0.1053,  0.1279,\n+                0.1515,  0.1703,  0.1827,  0.1829,  0.1694,  0.1509,  0.1333,  0.1189,\n+                0.1066,  0.0978,  0.0937,  0.0868,  0.0726,  0.0496,  0.0200, -0.0138,\n+                -0.0500, -0.0817, -0.1067, -0.1225, -0.1313, -0.1340\n             ]\n-        ).to(torch.float16)\n+        })\n+        EXPECTED_LOGITS = torch.tensor(expected_logits.get_expectation(), dtype=torch.float16)\n+\n         # fmt: on\n         torch.testing.assert_close(outputs.waveform[0, 10000:10030].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "ace2f0f511d3fca198f66d84738208ceffc8978f",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 17,
            "deletions": 7,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -17,7 +17,9 @@\n \n from transformers import XGLMConfig, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    is_torch_greater_or_equal,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n@@ -422,13 +424,21 @@ def test_xglm_sample(self):\n         output_ids = model.generate(input_ids, do_sample=True, num_beams=1)\n         output_str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n \n-        EXPECTED_OUTPUT_STRS = [\n-            # torch 2.6\n-            \"Today is a nice day and the water is still cold. We just stopped off for some fresh coffee. This place looks like a\",\n-            # torch 2.7\n-            \"Today is a nice day and the sun is shining. A nice day with warm rainy and windy weather today.\",\n-        ]\n-        self.assertIn(output_str, EXPECTED_OUTPUT_STRS)\n+        if is_torch_greater_or_equal(\"2.7.0\"):\n+            cuda_expectation = (\n+                \"Today is a nice day and the sun is shining. A nice day with warm rainy and windy weather today.\"\n+            )\n+        else:\n+            cuda_expectation = \"Today is a nice day and the water is still cold. We just stopped off for some fresh coffee. This place looks like a\"\n+\n+        expected_output_strings = Expectations(\n+            {\n+                (\"rocm\", (9, 5)): \"Today is a nice day and the sun is shining. A nice day with warm rainy and windy weather today.\",\n+                (\"cuda\", None): cuda_expectation,\n+            }\n+        )  # fmt: skip\n+        EXPECTED_OUTPUT_STR = expected_output_strings.get_expectation()\n+        self.assertEqual(output_str, EXPECTED_OUTPUT_STR)\n \n     @require_torch_accelerator\n     @require_torch_fp16"
        },
        {
            "sha": "96ea5ae870b2cf17bfb919da0e81d35ec9fd6816",
            "filename": "tests/pipelines/test_pipelines_mask_generation.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -26,6 +26,7 @@\n )\n from transformers.pipelines import MaskGenerationPipeline\n from transformers.testing_utils import (\n+    Expectations,\n     is_pipeline_test,\n     nested_simplify,\n     require_tf,\n@@ -120,6 +121,11 @@ def test_small_model_pt(self):\n             new_outupt += [{\"mask\": mask_to_test_readable(o), \"scores\": outputs[\"scores\"][i]}]\n \n         # fmt: off\n+        last_output = Expectations({\n+            (\"cuda\", None): {'mask': {'hash': 'b5f47c9191', 'shape': (480, 640)}, 'scores': 0.8871},\n+            (\"rocm\", (9, 5)): {'mask': {'hash': 'b5f47c9191', 'shape': (480, 640)}, 'scores': 0.8872}\n+        }).get_expectation()\n+\n         self.assertEqual(\n             nested_simplify(new_outupt, decimals=4),\n             [\n@@ -152,7 +158,7 @@ def test_small_model_pt(self):\n                 {'mask': {'hash': '7b9e8ddb73', 'shape': (480, 640)}, 'scores': 0.8986},\n                 {'mask': {'hash': 'cd24047c8a', 'shape': (480, 640)}, 'scores': 0.8984},\n                 {'mask': {'hash': '6943e6bcbd', 'shape': (480, 640)}, 'scores': 0.8873},\n-                {'mask': {'hash': 'b5f47c9191', 'shape': (480, 640)}, 'scores': 0.8871}\n+                last_output\n             ],\n         )\n         # fmt: on"
        },
        {
            "sha": "57cffbbb5dc991ab62ba9057aea7f207eeeca8cf",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -62,7 +62,7 @@ def test_wrong_backend(self):\n \n         # Only cuda and xpu devices can run this function\n         support_llm_awq = False\n-        device_type, major = get_device_properties()\n+        device_type, major, _ = get_device_properties()\n         if device_type == \"cuda\" and major >= 8:\n             support_llm_awq = True\n         elif device_type == \"xpu\":"
        },
        {
            "sha": "2bd4cce834f33d7fc5123b67c7edaf304190ce56",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -552,7 +552,8 @@ class TorchAoSerializationFP8AcceleratorTest(TorchAoSerializationTest):\n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n-        if get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 9:\n+        device_type, major, minor = get_device_properties()\n+        if device_type == \"cuda\" and major < 9:\n             raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n \n         from torchao.quantization import Float8WeightOnlyConfig\n@@ -573,7 +574,8 @@ class TorchAoSerializationA8W4Test(TorchAoSerializationTest):\n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n-        if get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 9:\n+        device_type, major, minor = get_device_properties()\n+        if device_type == \"cuda\" and major < 9:\n             raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n \n         from torchao.quantization import Int8DynamicActivationInt4WeightConfig"
        },
        {
            "sha": "8a69b2e0a3a4c7d51b61c559aad07eddac00c895",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -3775,7 +3775,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        (device_type, major) = get_device_properties()\n+        device_type, major, minor = get_device_properties()\n         if device_type == \"cuda\" and major < 8:\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n         elif device_type == \"rocm\" and major < 9:\n@@ -3823,7 +3823,7 @@ def test_sdpa_can_compile_dynamic(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        (device_type, major) = get_device_properties()\n+        device_type, major, minor = get_device_properties()\n         if device_type == \"cuda\" and major < 8:\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n         elif device_type == \"rocm\" and major < 9:"
        },
        {
            "sha": "99e067e424791c1173f9cf0f10145ce44168bd1d",
            "filename": "tests/utils/test_expectations.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Futils%2Ftest_expectations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ff246db00a5f71827deb9a08fa3b182c52e6a34/tests%2Futils%2Ftest_expectations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_expectations.py?ref=9ff246db00a5f71827deb9a08fa3b182c52e6a34",
            "patch": "@@ -5,6 +5,8 @@\n \n class ExpectationsTest(unittest.TestCase):\n     def test_expectations(self):\n+        # We use the expectations below to make sure the right expectations are found for the right devices.\n+        # Each value is just a unique ID.\n         expectations = Expectations(\n             {\n                 (None, None): 1,\n@@ -17,18 +19,20 @@ def test_expectations(self):\n             }\n         )\n \n-        def check(value, key):\n-            assert expectations.find_expectation(key) == value\n+        def check(expected_id, device_prop):\n+            found_id = expectations.find_expectation(device_prop)\n+            assert found_id == expected_id, f\"Expected {expected_id} for {device_prop}, found {found_id}\"\n \n         # npu has no matches so should find default expectation\n-        check(1, (\"npu\", None))\n-        check(7, (\"xpu\", 3))\n-        check(2, (\"cuda\", 8))\n-        check(3, (\"cuda\", 7))\n-        check(4, (\"rocm\", 9))\n-        check(4, (\"rocm\", None))\n-        check(2, (\"cuda\", 2))\n+        check(1, (\"npu\", None, None))\n+        check(7, (\"xpu\", 3, None))\n+        check(2, (\"cuda\", 8, None))\n+        check(3, (\"cuda\", 7, None))\n+        check(4, (\"rocm\", 9, None))\n+        check(4, (\"rocm\", None, None))\n+        check(2, (\"cuda\", 2, None))\n \n+        # We also test that if there is no default excpectation and no match is found, a ValueError is raised.\n         expectations = Expectations({(\"cuda\", 8): 1})\n         with self.assertRaises(ValueError):\n             expectations.find_expectation((\"xpu\", None))"
        }
    ],
    "stats": {
        "total": 457,
        "additions": 311,
        "deletions": 146
    }
}