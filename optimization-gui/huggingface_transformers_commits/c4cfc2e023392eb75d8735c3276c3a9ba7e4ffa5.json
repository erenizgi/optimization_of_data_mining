{
    "author": "Cyrilvallez",
    "message": "[TP] Fix parameter detection issue and some invalid TP-plans (#42129)\n\n* fix\n\n* add test\n\n* fix test\n\n* fix the obvious\n\n* more fix\n\n* fix\n\n* continue to improve\n\n* more fix\n\n* more\n\n* fix\n\n* fix\n\n* finally\n\n* CI",
    "sha": "c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
    "files": [
        {
            "sha": "9ed69d86c731a368f1f3fb911e7ea0e8190fba68",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -140,6 +140,16 @@ def _blocks_to_block_sizes(total_size: int, blocks: int | list[int]) -> list[int\n         return [single_size] * blocks\n \n \n+def replace_layer_number_by_wildcard(name: str) -> str:\n+    \"\"\"\n+    Replace the numbers in the `name` by wildcards, only if they are in-between dots (`.`) or if they are between\n+    a dot (`.`) and the end of the string.\n+    This matches how modules are named/numbered when using a nn.ModuleList or nn.Sequential, but will NOT match\n+    numbers in a parameter name itself, e.g. if the param is named `\"w1\"` or `\"w2\"`.\n+    \"\"\"\n+    return re.sub(r\"\\.\\d+(\\.|$)\", lambda m: \".*\" + m.group(1), name)\n+\n+\n def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str], is_weight=True) -> str | None:\n     \"\"\"\n     Get the TP style for a parameter from the TP plan.\n@@ -150,11 +160,11 @@ def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str], is_weig\n     The `is_weight` is important because for weights, we want to support `.weights` and `.bias` cases seamlessly! but\n     not parent classes for `post_init` calls\n     \"\"\"\n-    generic_param_name = re.sub(r\"\\d+\", \"*\", parameter_name)\n+    generic_param_name = replace_layer_number_by_wildcard(parameter_name)\n     if generic_param_name in tp_plan:\n         return tp_plan[generic_param_name]\n-    elif \".\" in generic_param_name and generic_param_name.rsplit(\".\", 1)[0] in tp_plan and is_weight:\n-        return tp_plan[generic_param_name.rsplit(\".\", 1)[0]]\n+    elif is_weight and \".\" in generic_param_name and (module_name := generic_param_name.rsplit(\".\", 1)[0]) in tp_plan:\n+        return tp_plan[module_name]\n     return None\n \n \n@@ -1086,7 +1096,7 @@ def verify_tp_plan(expected_keys: list[str], tp_plan: dict[str, str] | None):\n     if tp_plan is None:\n         return\n \n-    generic_keys = {re.sub(r\"\\d+\", \"*\", key) for key in expected_keys}\n+    generic_keys = {replace_layer_number_by_wildcard(key) for key in expected_keys}\n     unsharded_layers = set(generic_keys)\n     unused_rules = tp_plan\n "
        },
        {
            "sha": "98fe8e157016e60068e5c6da7952cd7f379721c5",
            "filename": "src/transformers/models/apertus/configuration_apertus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -106,7 +106,6 @@ class ApertusConfig(PreTrainedConfig):\n         \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "b52e8bd82344b8b5a20404f977b3de676ae8c750",
            "filename": "src/transformers/models/apertus/modular_apertus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -123,7 +123,6 @@ class ApertusConfig(LlamaConfig):\n         \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n     }\n \n     def __init__("
        },
        {
            "sha": "7e11d4d99d118d140eeb3719849b06803f6329a6",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -99,15 +99,14 @@ class AriaTextConfig(PreTrainedConfig):\n \n     model_type = \"aria_text\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    # Default tensor parallel plan for base model `AriaTextModel`\n     base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.up_proj\": \"colwise\",\n-        \"layers.*.mlp.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.shared_experts.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.up_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.down_proj\": \"rowwise\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "287437491445e213f62e616943543b8d2dcdbfa5",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -169,6 +169,15 @@ class AriaTextConfig(LlamaConfig):\n \n     model_type = \"aria_text\"\n     base_config_key = \"text_config\"\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.shared_experts.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.up_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "b1058db36a726c65a1a79716fa76e5191ab2d5e3",
            "filename": "src/transformers/models/doge/configuration_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -118,9 +118,9 @@ class DogeConfig(PreTrainedConfig):\n         \"layers.*.self_attn.dt_proj\": \"rowwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n         \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n-        \"layers.*.input_residual.weight\": \"sequence_parallel\",\n+        \"layers.*.input_residual\": \"sequence_parallel\",\n         \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n-        \"layers.*.post_attention_residual.weight\": \"sequence_parallel\",\n+        \"layers.*.post_attention_residual\": \"sequence_parallel\",\n         \"norm.weight\": \"sequence_parallel\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\","
        },
        {
            "sha": "d0337f505358ffb4ea4eeacf4829a5df782b34a3",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -146,9 +146,9 @@ class DogeConfig(PreTrainedConfig):\n         \"layers.*.self_attn.dt_proj\": \"rowwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n         \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n-        \"layers.*.input_residual.weight\": \"sequence_parallel\",\n+        \"layers.*.input_residual\": \"sequence_parallel\",\n         \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n-        \"layers.*.post_attention_residual.weight\": \"sequence_parallel\",\n+        \"layers.*.post_attention_residual\": \"sequence_parallel\",\n         \"norm.weight\": \"sequence_parallel\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\","
        },
        {
            "sha": "febce4cf6a958514cc0951451616562dfc8cd43c",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -114,9 +114,9 @@ class FlexOlmoConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.up_proj\": \"colwise\",\n-        \"layers.*.mlp.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n+        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "a363fe1bb3a43ca319529fec873132589d0b201f",
            "filename": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -125,9 +125,9 @@ class FlexOlmoConfig(OlmoeConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.up_proj\": \"colwise\",\n-        \"layers.*.mlp.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n+        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "acdd27231210240f6e58465fb952c70177354153",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -630,7 +630,7 @@ class Gemma3ForCausalLM(Gemma3PreTrainedModel, GenerationMixin):\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     config: Gemma3TextConfig\n-    base_model_prefix = \"language_model\"\n+    base_model_prefix = \"model\"\n \n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "c79761d7cc5939fecf539a30910137a599f17ba5",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -715,7 +715,7 @@ def forward(\n \n class Gemma3ForCausalLM(Gemma2ForCausalLM):\n     config: Gemma3TextConfig\n-    base_model_prefix = \"language_model\"\n+    base_model_prefix = \"model\"\n \n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "efd7c27af72fd22e618bcfb1c20804fe5512eccc",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -214,8 +214,9 @@ class Glm4vMoeTextConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_up_proj\": \"colwise_rep\",  # we need to replicate here due to the `chunk` operation\n-        \"layers.*.mlp.down_proj\": \"rowwise_rep\",  # we need to replicate here due to the `chunk` operation\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "5ed96e5fef50d5b3b28d863d31295a8f117dcc3a",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -159,8 +159,9 @@ class Glm4vMoeTextConfig(Glm4MoeConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_up_proj\": \"colwise_rep\",  # we need to replicate here due to the `chunk` operation\n-        \"layers.*.mlp.down_proj\": \"rowwise_rep\",  # we need to replicate here due to the `chunk` operation\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "17ce194bff84dbbb8491a65404face0f6d7bdf61",
            "filename": "src/transformers/models/qwen3_next/configuration_qwen3_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -138,9 +138,9 @@ class Qwen3NextConfig(PreTrainedConfig):\n         \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n         \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n         \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n-        \"layers.*.mlp.shared_experts.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.shared_experts.up_proj\": \"colwise\",\n-        \"layers.*.mlp.shared_experts.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.shared_expert.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_expert.up_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_expert.down_proj\": \"rowwise\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\","
        },
        {
            "sha": "ae51ab07269b37de5b9583bcd6fa2b8990078178",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 32,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -228,38 +228,7 @@ class T5GemmaConfig(PreTrainedConfig):\n \n     model_type = \"t5gemma\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    base_model_tp_plan = {\n-        # encoder\n-        \"encoder.layers.*.self_attn.q_proj\": \"colwise\",\n-        \"encoder.layers.*.self_attn.k_proj\": \"colwise\",\n-        \"encoder.layers.*.self_attn.v_proj\": \"colwise\",\n-        \"encoder.layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"encoder.layers.*.mlp.gate_proj\": \"colwise\",\n-        \"encoder.layers.*.mlp.up_proj\": \"colwise\",\n-        \"encoder.layers.*.mlp.down_proj\": \"rowwise\",\n-        # decoder\n-        \"decoder.layers.*.self_attn.q_proj\": \"colwise\",\n-        \"decoder.layers.*.self_attn.k_proj\": \"colwise\",\n-        \"decoder.layers.*.self_attn.v_proj\": \"colwise\",\n-        \"decoder.layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"decoder.layers.*.cross_attn.q_proj\": \"colwise\",\n-        \"decoder.layers.*.cross_attn.k_proj\": \"colwise\",\n-        \"decoder.layers.*.cross_attn.v_proj\": \"colwise\",\n-        \"decoder.layers.*.cross_attn.o_proj\": \"rowwise\",\n-        \"decoder.layers.*.mlp.gate_proj\": \"colwise\",\n-        \"decoder.layers.*.mlp.up_proj\": \"colwise\",\n-        \"decoder.layers.*.mlp.down_proj\": \"rowwise\",\n-    }\n-    base_model_pp_plan = {\n-        # encoder\n-        \"encoder.embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n-        \"encoder.layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n-        \"encoder.norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n-        # decoder\n-        \"decoder.embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n-        \"decoder.layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n-        \"decoder.norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n-    }\n+    sub_configs = {\"encoder\": T5GemmaModuleConfig, \"decoder\": T5GemmaModuleConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "c8d6f26a43733f41c6b35cf951600cbc6d00460e",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 32,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -236,38 +236,7 @@ class T5GemmaConfig(PreTrainedConfig):\n \n     model_type = \"t5gemma\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    base_model_tp_plan = {\n-        # encoder\n-        \"encoder.layers.*.self_attn.q_proj\": \"colwise\",\n-        \"encoder.layers.*.self_attn.k_proj\": \"colwise\",\n-        \"encoder.layers.*.self_attn.v_proj\": \"colwise\",\n-        \"encoder.layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"encoder.layers.*.mlp.gate_proj\": \"colwise\",\n-        \"encoder.layers.*.mlp.up_proj\": \"colwise\",\n-        \"encoder.layers.*.mlp.down_proj\": \"rowwise\",\n-        # decoder\n-        \"decoder.layers.*.self_attn.q_proj\": \"colwise\",\n-        \"decoder.layers.*.self_attn.k_proj\": \"colwise\",\n-        \"decoder.layers.*.self_attn.v_proj\": \"colwise\",\n-        \"decoder.layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"decoder.layers.*.cross_attn.q_proj\": \"colwise\",\n-        \"decoder.layers.*.cross_attn.k_proj\": \"colwise\",\n-        \"decoder.layers.*.cross_attn.v_proj\": \"colwise\",\n-        \"decoder.layers.*.cross_attn.o_proj\": \"rowwise\",\n-        \"decoder.layers.*.mlp.gate_proj\": \"colwise\",\n-        \"decoder.layers.*.mlp.up_proj\": \"colwise\",\n-        \"decoder.layers.*.mlp.down_proj\": \"rowwise\",\n-    }\n-    base_model_pp_plan = {\n-        # encoder\n-        \"encoder.embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n-        \"encoder.layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n-        \"encoder.norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n-        # decoder\n-        \"decoder.embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n-        \"decoder.layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n-        \"decoder.norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n-    }\n+    sub_configs = {\"encoder\": T5GemmaModuleConfig, \"decoder\": T5GemmaModuleConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "d8808da90ac60592f942f30b1c91b82b84fb7bfe",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -140,6 +140,17 @@ def test_model_rope_scaling_frequencies(self):\n     def test_torch_compile_for_training(self):\n         pass\n \n+    def test_tp_plan_matches_params(self):\n+        \"\"\"Need to overwrite as the plan contains keys that are valid but depend on some configs flags and cannot\n+        be valid all at the same time\"\"\"\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        # The key is valid but not always used based on the flag\n+        if config.q_lora_rank is not None:\n+            config.base_model_tp_plan.pop(\"layers.*.self_attn.q_proj\")\n+        super().test_tp_plan_matches_params()\n+        # Put them back in class attribute\n+        config.base_model_tp_plan.update({\"layers.*.self_attn.q_proj\": \"colwise\"})\n+\n \n @slow\n @require_read_token"
        },
        {
            "sha": "2875f91d62710d028b122bbc07e3489c4019e09a",
            "filename": "tests/models/doge/test_modeling_doge.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -337,6 +337,23 @@ def test_doge_sequence_classification_model_for_multi_label(self):\n     def test_save_load_fast_init_from_base(self):\n         pass\n \n+    def test_tp_plan_matches_params(self):\n+        \"\"\"Need to overwrite as the plan contains keys that are valid but depend on some configs flags and cannot\n+        be valid all at the same time\"\"\"\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        # They are valid but not always used, depending on config.is_moe flag (the modules are not the same in both cases)\n+        problematic_keys = {\n+            \"layers.*.mlp.router_gate\": \"colwise_rep\",\n+            \"layers.*.mlp.down_embed\": \"rowwise_rep\",\n+            \"layers.*.mlp.up_embed\": \"rowwise_rep\",\n+        }\n+        if not config.is_moe:\n+            for key in problematic_keys:\n+                config.base_model_tp_plan.pop(key)\n+        super().test_tp_plan_matches_params()\n+        # Put them back in class attribute\n+        config.base_model_tp_plan.update(problematic_keys)\n+\n \n @require_torch_accelerator\n class DogeIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "2c46dd06cbb5c17a60f5e75587e052e9cc0cd069",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -635,6 +635,8 @@ def is_pipeline_test_to_skip(\n         return False\n \n     def test_config(self):\n+        # Skip `create_and_test_config_from_and_save_pretrained_composite` because the config has twice the same subconfig\n+        self.config_tester.create_and_test_config_from_and_save_pretrained_composite = lambda: None\n         self.config_tester.run_common_tests()\n \n     def test_shift_right(self):\n@@ -1485,6 +1487,8 @@ def setUp(self):\n         )\n \n     def test_config(self):\n+        # Skip `create_and_test_config_from_and_save_pretrained_composite` because the config has twice the same subconfig\n+        self.config_tester.create_and_test_config_from_and_save_pretrained_composite = lambda: None\n         self.config_tester.run_common_tests()\n \n     @unittest.skip(\"This was not properly written, submodules need the attribute to be overwritten\")"
        },
        {
            "sha": "bdd7d3efabe85cb19e62817804bf36da77fc7c5d",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=c4cfc2e023392eb75d8735c3276c3a9ba7e4ffa5",
            "patch": "@@ -123,6 +123,7 @@\n     from torch import nn\n \n     from transformers import MODEL_MAPPING\n+    from transformers.integrations.tensor_parallel import _get_parameter_tp_plan\n     from transformers.modeling_utils import load_state_dict\n     from transformers.pytorch_utils import id_tensor_storage\n \n@@ -3946,6 +3947,48 @@ def test_bc_torch_dtype(self):\n                         self.assertEqual(v1.dtype, v2.dtype)\n                         self.assertTrue((v1 == v2).all())\n \n+    def test_tp_plan_matches_params(self):\n+        \"\"\"Make sure that each entry of the tp plan matches at least one param (this avoid typos and/or edge cases\n+        with regexes)\"\"\"\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        # If none of the config and subconfigs have a tp_plan, then skip (otherwise we should make sure to respect the plan)\n+        if config.base_model_tp_plan is None and all(\n+            getattr(getattr(config, key), \"base_model_tp_plan\", None) is None for key in config.sub_configs\n+        ):\n+            self.skipTest(\"Model does not have a TP plan.\")\n+\n+        # Some MoE models alternate between a classic MLP and a MoE layer, in which case we want to have each one\n+        # in order to test the whole tp plan\n+        config_to_set = config.get_text_config()\n+        config_to_set.first_k_dense_replace = 1  # means that the first layer (idx 0) will be MLP, then MoE\n+        config_to_set.moe_layer_start_index = 1  # same as above but for Ernie 4.5...\n+        config_to_set.mlp_only_layers = [0]  # same but for qwens\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(copy.deepcopy(config))\n+            param_names = {name for name, _ in model.named_parameters()} | {name for name, _ in model.named_buffers()}\n+            module_names = {name for name, _ in model.named_modules()}\n+            tp_plan = model.tp_plan\n+            # Make sure the plan is not empty\n+            self.assertTrue(\n+                len(tp_plan) > 0,\n+                f\"No TP-plan found for class {model_class.__name__} even though the associated config has one\",\n+            )\n+            pattern_usage = {}\n+            for pattern in tp_plan:\n+                # Check if this given pattern matches any param or module (the value attributed to the pattern does not matter)\n+                pattern_usage[pattern] = any(\n+                    _get_parameter_tp_plan(param, {pattern: \"\"}, is_weight=True) is not None for param in param_names\n+                ) or any(\n+                    _get_parameter_tp_plan(module, {pattern: \"\"}, is_weight=False) is not None\n+                    for module in module_names\n+                )\n+\n+            unused_entries = {k for k, v in pattern_usage.items() if not v}\n+            self.assertTrue(\n+                len(unused_entries) == 0, f\"The following entries of the TP-plan are not valid: {unused_entries}\"\n+            )\n+\n \n global_rng = random.Random()\n "
        }
    ],
    "stats": {
        "total": 217,
        "additions": 124,
        "deletions": 93
    }
}