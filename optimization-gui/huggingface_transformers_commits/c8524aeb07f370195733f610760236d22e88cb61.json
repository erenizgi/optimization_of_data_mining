{
    "author": "zucchini-nlp",
    "message": "[cache] make all classes cache compatible finally (#38635)\n\n* dump\n\n* push other models\n\n* fix simple greedy generation\n\n* xmod\n\n* add fmst and clean up some mentions of old cache format\n\n* gpt-bigcode now follows standards\n\n* delete tuple cache reference in generation\n\n* fix some models\n\n* fix some models\n\n* fix mambas and support cache in tapas\n\n* fix some more tests\n\n* fix copies\n\n* delete `_reorder_cache`\n\n* another fix copies\n\n* fix typos and delete unnecessary test\n\n* fix rag generate, needs special cache reordering\n\n* fix tapas and superglue\n\n* reformer create special cache\n\n* recurrent gemma `reorder_cache` was a no-op, delete\n\n* fix-copies\n\n* fix blio and musicgen pipeline tests\n\n* fix reformer\n\n* fix reformer, again...\n\n* delete `_supports_cache_class`\n\n* delete `supports_quantized_cache`\n\n* fix failing tests\n\n* fix copies\n\n* some minor clean up\n\n* style\n\n* style\n\n* fix copies\n\n* fix tests\n\n* fix copies\n\n* create causal mask now needs positions?\n\n* fixc copies\n\n* style\n\n* Update tests/test_modeling_common.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* clean-up of non-generative model after merging main\n\n* check `is_decoder` for cache\n\n* delete transpose for scores\n\n* remove tuple cache from docs everywhere\n\n* fix tests\n\n* fix copies\n\n* fix copies once more\n\n* properly deprecate `encoder_attention_mask` in Bert-like models\n\n* import `deprecate_kwarg` where needed\n\n* fix copies again\n\n* fix copies\n\n* delete `nex_decoder_cache`\n\n* fix copies asks to update for PLM\n\n* fix copies\n\n* rebasing had a few new models, fix them and merge asap!\n\n* fix copies once more\n\n* fix slow tests\n\n* fix tests and updare PLM checkpoint\n\n* add read token and revert accidentally removed line\n\n* oh com -on, style\n\n* just skip it, read token has no access to PLM yet\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "c8524aeb07f370195733f610760236d22e88cb61",
    "files": [
        {
            "sha": "13f3106692005bf385abec06cd177bd551d3b1c4",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -141,7 +141,7 @@ The legacy format is essentially the same data structure but organized different\n - The tensors have the same shape `[batch_size, num_heads, seq_len, head_dim]`.\n - The format is less flexible and doesn't support features like quantization or offloading.\n \n-If your project depends on this legacy format, you can convert between [`DynamicCache`] and a tuple of tuples as shown below with the [`~DynamicCache.from_legacy_cache`] and [`DynamicCache.to_legacy_cache`] functions. This is helpful if you have custom logic for manipulating a cache in a specific format.\n+If your project depends on this legacy format, we recommend to convert to [`DynamicCache`] with [`~DynamicCache.from_legacy_cache`]. Note that legacy cache format is deprecated and not used anymore in `Transformers`. You can convert back to tuple format with [`DynamicCache.to_legacy_cache`] functions, which is helpful if you have custom logic for manipulating a cache in a specific format.\n \n ```py\n import torch"
        },
        {
            "sha": "9ee097bba21fcb424678bf50bfbe208335ecd7c4",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 1,
            "deletions": 45,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -28,7 +28,6 @@\n if is_sklearn_available():\n     from sklearn.metrics import roc_curve\n \n-from ..cache_utils import Cache\n from ..pytorch_utils import isin_mps_friendly\n from .logits_process import LogitsProcessorList, MinLengthLogitsProcessor, SuppressTokensLogitsProcessor\n \n@@ -295,9 +294,7 @@ def _update_past_and_masks(\n         has_past_key_values = self.assistant_kwargs.get(\"past_key_values\", None) is not None\n         if has_past_key_values:\n             new_cache_size = input_ids.shape[-1] - 1 - remove_from_pkv\n-            self.assistant_kwargs[\"past_key_values\"] = _crop_past_key_values(\n-                self.assistant_model, self.assistant_kwargs[\"past_key_values\"], new_cache_size - num_added_tokens\n-            )\n+            self.assistant_kwargs[\"past_key_values\"].crop(new_cache_size - num_added_tokens)\n             self.assistant_kwargs = _prepare_attention_mask(\n                 self.assistant_kwargs, input_ids.shape[-1], self.assistant_model.config.is_encoder_decoder\n             )\n@@ -1180,47 +1177,6 @@ def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor,\n         return candidate_ids, candidate_logits\n \n \n-def _crop_past_key_values(model, past_key_values, max_length):\n-    \"\"\"Crops the past key values up to a certain maximum length.\"\"\"\n-    new_past = []\n-    if isinstance(past_key_values, Cache):\n-        past_key_values.crop(max_length)\n-    elif model.config.is_encoder_decoder:\n-        for idx in range(len(past_key_values)):\n-            new_past.append(\n-                (\n-                    past_key_values[idx][0][:, :, :max_length, :],\n-                    past_key_values[idx][1][:, :, :max_length, :],\n-                    past_key_values[idx][2],\n-                    past_key_values[idx][3],\n-                )\n-            )\n-        past_key_values = tuple(new_past)\n-    # gptbigcode is special and stores kv in shape (batch_size, seq_len, dim), if it's a multi_query model\n-    elif \"gptbigcode\" in model.__class__.__name__.lower() or (\n-        model.config.architectures is not None and \"gptbigcode\" in model.config.architectures[0].lower()\n-    ):\n-        if model.config.multi_query:\n-            for idx in range(len(past_key_values)):\n-                past_key_values[idx] = past_key_values[idx][:, :max_length, :]\n-        else:\n-            for idx in range(len(past_key_values)):\n-                past_key_values[idx] = past_key_values[idx][:, :, :max_length, :]\n-    elif past_key_values is not None:\n-        for idx in range(len(past_key_values)):\n-            if past_key_values[idx] != ([], []):\n-                new_past.append(\n-                    (\n-                        past_key_values[idx][0][:, :, :max_length, :],\n-                        past_key_values[idx][1][:, :, :max_length, :],\n-                    )\n-                )\n-            else:\n-                new_past.append((past_key_values[idx][0], past_key_values[idx][1]))\n-        past_key_values = tuple(new_past)\n-    return past_key_values\n-\n-\n def _prepare_attention_mask(model_kwargs: dict[str, Any], new_length: int, is_encoder_decoder: bool) -> dict[str, Any]:\n     \"\"\"Expands or crops the model's mask for decoding purposes, to the defined length\"\"\"\n "
        },
        {
            "sha": "e9b28eb102dd2212b3d5d009368561cfe90682b0",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 35,
            "deletions": 75,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -68,7 +68,6 @@\n     EarlyExitCandidateGenerator,\n     PromptLookupCandidateGenerator,\n     UniversalSpeculativeDecodingGenerator,\n-    _crop_past_key_values,\n     _prepare_attention_mask,\n     _prepare_token_type_ids,\n )\n@@ -567,15 +566,7 @@ def prepare_inputs_for_generation(\n \n         # 1. Handle BC:\n         model_inputs = {}\n-        # - some models don't have `Cache` support (which implies they don't expect `cache_position` in `forward`)\n-        if self._supports_cache_class:\n-            model_inputs[\"cache_position\"] = cache_position\n-        # - `cache_position` was not a mandatory input in `prepare_inputs_for_generation` for those models, and this\n-        #   function may be called outside of `generate`. Handle most use cases by creating `cache_position` on the fly\n-        #   (this alternative is not as robust as calling `generate` and letting it create `cache_position`)\n-        elif cache_position is None:\n-            past_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n-            cache_position = torch.arange(past_length, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n+        model_inputs[\"cache_position\"] = cache_position\n \n         # 2. Generic cache-dependent input preparation\n         if past_key_values is not None:\n@@ -1014,12 +1005,6 @@ def _update_model_kwargs_for_generation(\n             model_kwargs[\"cache_position\"] = torch.cat((past_positions, new_positions))\n         return model_kwargs\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        raise NotImplementedError(\n-            f\"Make sure that a `_reorder_cache` function is correctly implemented in {self.__class__.__module__} to\"\n-            f\" enable beam search for {self.__class__}\"\n-        )\n-\n     def _get_candidate_generator(\n         self,\n         generation_config: GenerationConfig,\n@@ -1559,13 +1544,6 @@ def _validate_assistant(self, assistant_model, tokenizer, assistant_tokenizer):\n \n     def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n         \"\"\"Validates model kwargs for generation. Generate argument typos will also be caught here.\"\"\"\n-        # If a `Cache` instance is passed, checks whether the model is compatible with it\n-        if isinstance(model_kwargs.get(\"past_key_values\", None), Cache) and not self._supports_cache_class:\n-            raise ValueError(\n-                f\"{self.__class__.__name__} does not support an instance of `Cache` as `past_key_values`. Please \"\n-                \"check the model documentation for supported cache formats.\"\n-            )\n-\n         # Excludes arguments that are handled before calling any model function\n         if self.config.is_encoder_decoder:\n             for key in [\"decoder_input_ids\"]:\n@@ -1975,21 +1953,23 @@ def _get_cache(\n             self._cache.reset()\n         return self._cache\n \n-    def _supports_default_dynamic_cache(self) -> bool:\n+    @classmethod\n+    def _supports_default_dynamic_cache(cls) -> bool:\n         \"\"\"\n         Return `True` if current model can use a `DynamicCache` instance when initializing the `past_key_values`.\n-        This is mostly the same as `_supports_cache_class` attribute, but add exception for `Jamba` model which\n-        uses its own `HybridMambaAttentionDynamicCache` and do not need to initialize the Cache in advance in\n-        order to save memory (because no back and forth `to_legacy_cache` and `from_legacy_cache` will be performed\n-        for `HybridMambaAttentionDynamicCache`).\n+        This adds exception for some models like `Jamba` model which uses its own `HybridMambaAttentionDynamicCache`\n+        and do not need to initialize the Cache in advance in order to save memory (because no back and forth\n+        `to_legacy_cache` and `from_legacy_cache` will be performed for `HybridMambaAttentionDynamicCache`).\n         \"\"\"\n-        return (\n-            self._supports_cache_class\n-            and \"jamba\" not in self.__class__.__name__.lower()\n-            and \"zamba\" not in self.__class__.__name__.lower()\n-            and \"bamba\" not in self.__class__.__name__.lower()\n-            and \"minimax\" not in self.__class__.__name__.lower()\n-            and \"lfm2\" not in self.__class__.__name__.lower()\n+        # NOTE: remove xlnet/reformer when the models are deprecated, non-standard model architecture/cache name\n+        return not cls._is_stateful and all(\n+            special_model_name not in cls.__name__.lower()\n+            for special_model_name in [\n+                \"reformer\",\n+                \"minimax\",\n+                \"xlnet\",\n+                \"lfm2\",\n+            ]\n         )\n \n     def _prepare_cache_for_generation(\n@@ -2076,7 +2056,7 @@ def _prepare_cache_for_generation(\n                     model_kwargs=model_kwargs,\n                 )\n             elif generation_config.cache_implementation == \"quantized\":\n-                if not self._supports_quantized_cache:\n+                if self.config.is_encoder_decoder or not self._supports_default_dynamic_cache():\n                     raise ValueError(\n                         \"This model does not support the quantized cache. If you want your model to support quantized \"\n                         \"cache, please open an issue and tag @zucchini-nlp.\"\n@@ -3708,33 +3688,6 @@ def _sample(\n         else:\n             return input_ids\n \n-    # Auxiliary functions for beam search\n-    def _temporary_reorder_cache(self, past_key_values, beam_idx):\n-        \"\"\"\n-        Temporary function to handle the different types of cache reordering processes while we roll out `Cache`.\n-\n-        TODO: standardize cache formats and make all models compatible with `Cache`. It would remove the need\n-        for this function, with `Cache.reorder_cache` being the sole remaining code path\n-        \"\"\"\n-        model_class = self.__class__.__name__.lower()\n-        # Exception 1: code path for models using the legacy cache format\n-        if isinstance(past_key_values, (tuple, list)):\n-            past_key_values = self._reorder_cache(past_key_values, beam_idx)\n-        # Exception 2: models with different cache formats. These are limited to `DynamicCache` until their\n-        # cache format is standardized, to avoid adding complexity to the codebase.\n-        elif \"gptbigcode\" in model_class:\n-            if not isinstance(past_key_values, (DynamicCache, EncoderDecoderCache)):\n-                raise ValueError(\n-                    f\"Using an unsupported cache format with {model_class}. Currently, it only supports the \"\n-                    \"legacy tuple format or `DynamicCache`\"\n-                )\n-            past_key_values = self._reorder_cache(past_key_values, beam_idx)\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-        # Standard code path: use the `Cache.reorder_cache`\n-        else:\n-            past_key_values.reorder_cache(beam_idx)\n-        return past_key_values\n-\n     @staticmethod\n     def _flatten_beam_dim(tensor: torch.Tensor) -> torch.Tensor:\n         \"\"\"[batch_size, num_beams, ...] -> [batch_size * num_beams, ...]\"\"\"\n@@ -4230,11 +4183,13 @@ def _beam_search(\n             # beam search as a whole (as opposed to individual beams, i.e. `stopping_criteria`)\n \n             # pluck the cache from the beam indices that will be used in the next iteration\n+            # NOTE: we need to check if `self._reorder_cache` exists for special models like RAG, RecurrentGemma etc.\n             if model_kwargs.get(\"past_key_values\", None) is not None:\n-                model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n-                    past_key_values=model_kwargs[\"past_key_values\"],\n-                    beam_idx=self._flatten_beam_dim(running_beam_indices[..., cur_len - decoder_prompt_len]),\n-                )\n+                beam_idx = self._flatten_beam_dim(running_beam_indices[..., cur_len - decoder_prompt_len])\n+                if hasattr(self, \"_reorder_cache\"):\n+                    model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n+                else:\n+                    model_kwargs[\"past_key_values\"].reorder_cache(beam_idx)\n \n             cur_len = cur_len + 1\n             is_early_stop_heuristic_unsatisfied = self._check_early_stop_heuristic(\n@@ -4537,10 +4492,14 @@ def _group_beam_search(\n             # (that way the memory peak does not include outputs.logits)\n             del outputs\n \n+            # NOTE: we need to check if `self._reorder_cache` exists for special models like RAG, RecurrentGemma etc.\n             if model_kwargs.get(\"past_key_values\", None) is not None:\n-                model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n-                    model_kwargs[\"past_key_values\"], reordering_indices\n-                )\n+                if hasattr(self, \"_reorder_cache\"):\n+                    model_kwargs[\"past_key_values\"] = self._reorder_cache(\n+                        model_kwargs[\"past_key_values\"], reordering_indices\n+                    )\n+                else:\n+                    model_kwargs[\"past_key_values\"].reorder_cache(reordering_indices)\n \n             # increase cur_len\n             cur_len = cur_len + 1\n@@ -4774,10 +4733,12 @@ def _constrained_beam_search(\n             # (that way the memory peak does not include outputs.logits)\n             del outputs\n \n+            # NOTE: we need to check if `self._reorder_cache` exists for special models like RAG, RecurrentGemma etc.\n             if model_kwargs.get(\"past_key_values\", None) is not None:\n-                model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n-                    model_kwargs[\"past_key_values\"], beam_idx\n-                )\n+                if hasattr(self, \"_reorder_cache\"):\n+                    model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n+                else:\n+                    model_kwargs[\"past_key_values\"].reorder_cache(beam_idx)\n \n             if return_dict_in_generate and output_scores:\n                 beam_indices = tuple(beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices)))\n@@ -5002,8 +4963,7 @@ def _assisted_decoding(\n             new_cur_len = input_ids.shape[1]\n \n             # 4.2. Discard past key values relative to unused assistant tokens\n-            new_cache_size = new_cur_len - 1\n-            outputs.past_key_values = _crop_past_key_values(self, outputs.past_key_values, new_cache_size)\n+            outputs.past_key_values.crop(new_cur_len - 1)\n \n             # 5. Update the candidate generation strategy if needed\n             candidate_generator.update_candidate_strategy(input_ids, new_logits, n_matches)"
        },
        {
            "sha": "11fa589ff8def4c112f348648591d1c7a7077c1c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1971,13 +1971,9 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMi\n     # Flex Attention support\n     _supports_flex_attn = False\n \n-    # Has support for a `Cache` instance as `past_key_values`? Does it support a `StaticCache`?\n-    _supports_cache_class = False\n+    # Has support `torch.compile(fullgraph=True)`\n     _supports_static_cache = False\n \n-    # Has support for a `QuantoQuantizedCache` instance as `past_key_values`\n-    _supports_quantized_cache = False\n-\n     # A tensor parallel plan to be applied to the model when TP is enabled. For\n     # top-level models, this attribute is currently defined in respective model\n     # code. For base models, this attribute comes from"
        },
        {
            "sha": "005665d324b9785df6f8b43d53743497f8048d08",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 26,
            "deletions": 16,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -271,12 +271,6 @@ def __init__(self, config: AlbertConfig):\n             self.max_position_embeddings = config.max_position_embeddings\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-    # Copied from transformers.models.bert.modeling_bert.BertSelfAttention.transpose_for_scores\n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def prune_heads(self, heads: list[int]) -> None:\n         if len(heads) == 0:\n             return\n@@ -302,13 +296,17 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-        mixed_key_layer = self.key(hidden_states)\n-        mixed_value_layer = self.value(hidden_states)\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-        key_layer = self.transpose_for_scores(mixed_key_layer)\n-        value_layer = self.transpose_for_scores(mixed_value_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        key_layer = self.key(hidden_states)\n+        value_layer = self.value(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n+        key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        value_layer = value_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -378,9 +376,21 @@ def forward(\n             return super().forward(hidden_states, attention_mask, output_attentions=output_attentions)\n \n         batch_size, seq_len, _ = hidden_states.size()\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0."
        },
        {
            "sha": "448ef08632e34311b922d8f0c59b2f0e042861fd",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -316,8 +316,7 @@ class ArceePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "99c6030125564594f3ced932df2f54b66bd4f93c",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -631,7 +631,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = False\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": AriaTextDecoderLayer,\n@@ -664,8 +664,6 @@ class AriaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n     _supports_static_cache = False  # MoE models don't work with torch.compile (dynamic slicing)\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "aa9aea69f8ac9cd4c71121715c7fd0abd42cba29",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1286,7 +1286,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = False\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": AriaTextDecoderLayer,"
        },
        {
            "sha": "32a2c8bad1ea63aa1d38299bcda86c970e8d0043",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -149,17 +149,28 @@ def __init__(self, config: ASTConfig) -> None:\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        batch_size, seq_length, _ = hidden_states.shape\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "974d2a5e4d3c7311b4937f009ab91424c26d122d",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 93,
            "deletions": 78,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -422,10 +423,11 @@ def __init__(\n         self,\n         embed_dim: int,\n         num_heads: int,\n-        dropout: float = 0.0,\n-        is_decoder: bool = False,\n-        bias: bool = True,\n-        autocorrelation_factor: int = 3,\n+        dropout: Optional[float] = 0.0,\n+        is_decoder: Optional[bool] = False,\n+        bias: Optional[bool] = True,\n+        autocorrelation_factor: Optional[int] = 3,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -440,6 +442,7 @@ def __init__(\n             )\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n+        self.layer_idx = layer_idx\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n@@ -448,69 +451,63 @@ def __init__(\n \n         self.autocorrelation_factor = autocorrelation_factor\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n+        output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n         query_states = self.q_proj(hidden_states)\n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n+        query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.reshape(*proj_shape)\n+        key_states = key_states.reshape(*proj_shape)\n+        value_states = value_states.reshape(*proj_shape)\n \n         # (1) period-based dependencies discovery\n         # Resize (truncation or zero filling)\n@@ -631,7 +628,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped\n \n \n class AutoformerEncoderLayer(GradientCheckpointingLayer):\n@@ -673,7 +670,7 @@ def forward(\n                 returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -709,7 +706,7 @@ def forward(\n \n \n class AutoformerDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: AutoformerConfig):\n+    def __init__(self, config: AutoformerConfig, layer_idx=None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -719,6 +716,7 @@ def __init__(self, config: AutoformerConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             autocorrelation_factor=config.autocorrelation_factor,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -731,6 +729,7 @@ def __init__(self, config: AutoformerConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             autocorrelation_factor=config.autocorrelation_factor,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -760,9 +759,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -788,15 +788,13 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n@@ -805,30 +803,25 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n             hidden_states, trend2 = self.decomp2(hidden_states)\n             # added layer norm here as an improvement\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.activation_fn(self.fc1(hidden_states))\n@@ -849,9 +842,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -1047,7 +1037,9 @@ def __init__(self, config: AutoformerConfig):\n         self.embed_positions = AutoformerSinusoidalPositionalEmbedding(\n             config.context_length + config.prediction_length, config.d_model\n         )\n-        self.layers = nn.ModuleList([AutoformerDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList(\n+            [AutoformerDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)]\n+        )\n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n \n         # https://github.com/thuml/Autoformer/blob/e6371e24f2ae2dd53e472edefdd5814c5176f864/models/Autoformer.py#L74\n@@ -1071,6 +1063,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, AutoFormerDecoderOutput]:\n         r\"\"\"\n         Args:\n@@ -1149,6 +1142,22 @@ def forward(\n \n         input_shape = inputs_embeds.size()[:-1]\n \n+        if self.gradient_checkpointing and use_cache:\n+            logger.warning(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+            )\n+            use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n@@ -1167,7 +1176,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1187,25 +1195,21 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n             )\n             (hidden_states, residual_trend) = layer_outputs[0]\n             trend = trend + residual_trend\n \n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1219,17 +1223,26 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, trend, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [\n+                    hidden_states,\n+                    trend,\n+                    past_key_values,\n+                    all_hidden_states,\n+                    all_self_attns,\n+                    all_cross_attentions,\n+                ]\n                 if v is not None\n             )\n         return AutoFormerDecoderOutput(\n             last_hidden_state=hidden_states,\n             trend=trend,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1431,6 +1444,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[AutoformerModelOutput, tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1612,6 +1626,7 @@ def forward(\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n                 return_dict=return_dict,\n+                cache_position=cache_position,\n             )\n         else:\n             decoder_outputs = AutoFormerDecoderOutput()"
        },
        {
            "sha": "da420c82114fe1cb1d2747bbdbcc6f1dd8e49df0",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -93,10 +93,9 @@ class AyaVisionPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = False\n     _supports_static_cache = False\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "58c118d73fad66c630e97784b0a46395da1b445d",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -90,7 +90,6 @@ def pixel_shuffle(self, image_features):  # B, S, D\n \n \n class AyaVisionPreTrainedModel(LlavaPreTrainedModel):\n-    _supports_quantized_cache = False\n     _supports_static_cache = False\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "3e63239970f6ab37a916471d31a331cad8a3d30a",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1041,7 +1041,7 @@ class BambaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n+    # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "937b41113bd157ab586a9d124512d1294bb39bac",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -812,7 +812,7 @@ class BambaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n+    # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "6f01ccd0d265dbf7cfeb795dd9c409e2c50e7b7a",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 53,
            "deletions": 82,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...generation.logits_process import (\n     AlternatingCodebooksLogitsProcessor,\n@@ -66,7 +67,7 @@ class BarkSelfAttention(nn.Module):\n     # adapted from GPTNeoSelfAttention and Bark code\n     # BarkSelfAttention can have two attention type, i.e full attention or causal attention\n \n-    def __init__(self, config, is_causal=False):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n \n         # regularization\n@@ -90,6 +91,7 @@ def __init__(self, config, is_causal=False):\n         self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=config.bias)\n \n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n         if is_causal:\n             block_size = config.block_size\n             bias = torch.tril(torch.ones((block_size, block_size), dtype=bool)).view(1, 1, block_size, block_size)\n@@ -155,6 +157,7 @@ def forward(\n         head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n         query, key, value = self.att_proj(hidden_states).split(self.embed_dim, dim=2)\n@@ -164,27 +167,15 @@ def forward(\n         value = self._split_heads(value, self.num_heads, self.head_dim)\n \n         if past_key_values is not None:\n-            past_key = past_key_values[0]\n-            past_value = past_key_values[1]\n-            key = torch.cat((past_key, key), dim=-2)\n-            value = torch.cat((past_value, value), dim=-2)\n-\n-        if use_cache is True:\n-            present = (key, value)\n-        else:\n-            present = None\n+            key, value = past_key_values.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n \n         attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n \n         attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n         attn_output = self.out_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n \n-        outputs = (attn_output, present)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return attn_output, attn_weights\n \n \n class BarkSelfFlashAttention2(BarkSelfAttention):\n@@ -229,6 +220,7 @@ def forward(\n         head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         batch_size, query_len, _ = hidden_states.size()\n \n@@ -240,18 +232,7 @@ def forward(\n         value = self._split_heads(value, self.num_heads, self.head_dim)\n \n         if past_key_values is not None:\n-            # (batch, head, seq_length, head_features) -> (batch, seq_length, head, head_features)\n-            past_key = past_key_values[0].transpose(1, 2)\n-            past_value = past_key_values[1].transpose(1, 2)\n-            # and merge on seq_length\n-            key = torch.cat((past_key, key), dim=1)\n-            value = torch.cat((past_value, value), dim=1)\n-\n-        if use_cache is True:\n-            #  (batch, head, seq_length, head_features)\n-            present = (key.transpose(1, 2), value.transpose(1, 2))\n-        else:\n-            present = None\n+            key, value = past_key_values.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n \n         attn_output = _flash_attention_forward(\n             query,\n@@ -268,12 +249,7 @@ def forward(\n         attn_output = self.out_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n \n-        outputs = (attn_output, present)\n-        if output_attentions:\n-            attn_weights = None\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return attn_output, None\n \n \n BARK_ATTENTION_CLASSES = {\n@@ -299,7 +275,7 @@ def forward(self, hidden_states):\n \n \n class BarkBlock(GradientCheckpointingLayer):\n-    def __init__(self, config, is_causal=False):\n+    def __init__(self, config, is_causal=False, layer_idx=None):\n         super().__init__()\n \n         if is_causal:\n@@ -311,7 +287,9 @@ def __init__(self, config, is_causal=False):\n             self.layernorm_1 = nn.LayerNorm(config.hidden_size)\n             self.layernorm_2 = nn.LayerNorm(config.hidden_size)\n \n-        self.attn = BARK_ATTENTION_CLASSES[config._attn_implementation](config, is_causal=is_causal)\n+        self.attn = BARK_ATTENTION_CLASSES[config._attn_implementation](\n+            config, is_causal=is_causal, layer_idx=layer_idx\n+        )\n \n         self.mlp = BarkMLP(config)\n \n@@ -323,6 +301,7 @@ def forward(\n         head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         intermediary_hidden_states = self.layernorm_1(hidden_states)\n \n@@ -333,6 +312,7 @@ def forward(\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n \n         attn_output = attn_outputs[0]  # output_attn: output, present_key_values, (attn_weights)\n@@ -343,12 +323,7 @@ def forward(\n             self.layernorm_2(intermediary_hidden_states)\n         )\n \n-        if use_cache:\n-            outputs = (intermediary_hidden_states,) + outputs\n-        else:\n-            outputs = (intermediary_hidden_states,) + outputs[1:]\n-\n-        return outputs  # hidden_states, ((present), attentions)\n+        return (intermediary_hidden_states,) + outputs\n \n \n @auto_docstring\n@@ -411,7 +386,7 @@ def __init__(self, config):\n \n         self.drop = nn.Dropout(config.dropout)\n \n-        self.layers = nn.ModuleList([BarkBlock(config, is_causal=True) for _ in range(config.num_layers)])\n+        self.layers = nn.ModuleList([BarkBlock(config, is_causal=True, layer_idx=i) for i in range(config.num_layers)])\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n         self.layernorm_final = nn.LayerNorm(config.hidden_size, bias=config.bias)\n@@ -428,17 +403,17 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.input_embeds_layer = new_embeddings\n \n-    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwargs):\n+    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, cache_position=None, **kwargs):\n         # Overwritten -- bark has a model-specific hack\n         input_embeds = kwargs.get(\"input_embeds\", None)\n \n         attention_mask = kwargs.get(\"attention_mask\", None)\n         position_ids = kwargs.get(\"position_ids\", None)\n \n-        if past_key_values is not None:\n+        if cache_position[0] != 0:\n             # Omit tokens covered by past_key_values\n             seq_len = input_ids.shape[1]\n-            past_length = past_key_values[0][0].shape[2]\n+            past_length = past_key_values.get_seq_length()\n \n             # Some generation methods already pass only the last input ID\n             if input_ids.shape[1] > past_length:\n@@ -481,13 +456,15 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwarg\n                 \"use_cache\": kwargs.get(\"use_cache\"),\n                 \"position_ids\": position_ids,\n                 \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n             }\n         return {\n             \"input_ids\": input_ids,\n             \"past_key_values\": past_key_values,\n             \"use_cache\": kwargs.get(\"use_cache\"),\n             \"position_ids\": position_ids,\n             \"attention_mask\": attention_mask,\n+            \"cache_position\": cache_position,\n         }\n \n     @auto_docstring\n@@ -504,6 +481,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n         input_embeds (`torch.FloatTensor` of shape `(batch_size, input_sequence_length, hidden_size)`, *optional*):\n@@ -546,11 +524,24 @@ def forward(\n \n         device = input_ids.device if input_ids is not None else input_embeds.device\n \n-        if past_key_values is None:\n-            past_length = 0\n-            past_key_values = tuple([None] * len(self.layers))\n-        else:\n-            past_length = past_key_values[0][0].size(-2)\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n+                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+\n+        past_length = past_key_values.get_seq_length() if past_key_values is not None else past_key_values\n \n         if position_ids is None:\n             position_ids = torch.arange(past_length, seq_length + past_length, dtype=torch.long, device=device)\n@@ -579,37 +570,27 @@ def forward(\n         hidden_states = self.drop(input_embeds + position_embeds)\n         output_shape = input_shape + (hidden_states.size(-1),)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        present_key_values = () if use_cache else None\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n \n-        for i, (block, past_layer_key_values) in enumerate(zip(self.layers, past_key_values)):\n+        for i, block in enumerate(self.layers):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             outputs = block(\n                 hidden_states,\n-                past_key_values=past_layer_key_values,\n+                past_key_values=past_key_values,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask[i],\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = outputs[0]\n \n-            if use_cache:\n-                present_key_values = present_key_values + (outputs[1],)\n-\n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n \n         hidden_states = self.layernorm_final(hidden_states)\n \n@@ -621,34 +602,22 @@ def forward(\n \n         logits = self.lm_head(hidden_states)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n-                v for v in [None, logits, present_key_values, all_hidden_states, all_self_attentions] if v is not None\n+                v for v in [None, logits, past_key_values, all_hidden_states, all_self_attentions] if v is not None\n             )\n \n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n-            past_key_values=present_key_values,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        # Necessary for beam_search\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1007,7 +976,9 @@ def __init__(self, config):\n \n         self.drop = nn.Dropout(config.dropout)\n \n-        self.layers = nn.ModuleList([BarkBlock(config, is_causal=False) for _ in range(config.num_layers)])\n+        self.layers = nn.ModuleList(\n+            [BarkBlock(config, is_causal=False, layer_idx=i) for i in range(config.num_layers)]\n+        )\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n         self.layernorm_final = nn.LayerNorm(config.hidden_size)"
        },
        {
            "sha": "77665f5313b1068cc700ec7f7dc40958a9e5957f",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 8,
            "deletions": 37,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -268,7 +268,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class BartEncoderLayer(GradientCheckpointingLayer):\n@@ -310,7 +310,7 @@ def forward(\n                 returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -411,7 +411,7 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -428,7 +428,7 @@ def forward(\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -455,9 +455,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -496,7 +493,7 @@ class BartPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -1109,7 +1106,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1142,10 +1138,6 @@ def forward(\n                 cache_position=cache_position,\n             )\n             hidden_states = layer_outputs[0]\n-\n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1156,19 +1148,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1543,17 +1534,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1981,15 +1961,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"BartForCausalLM\","
        },
        {
            "sha": "9c964467e513d06a8a81e93950df4cd60bf151a8",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 32,
            "deletions": 14,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -260,11 +260,6 @@ def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> N\n         if self.has_relative_position_bias:\n             self.relative_position_bias = BeitRelativePositionBias(config, window_size=window_size)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -274,11 +269,22 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[tuple[int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -345,10 +351,22 @@ def forward(\n                 resolution=resolution,\n             )\n \n-        mixed_query_layer = self.query(hidden_states)\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         attn_bias = None\n         if self.has_relative_position_bias:"
        },
        {
            "sha": "48832a1cf090313ddaf495ad34e2f0f6edaa786a",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 160,
            "deletions": 133,
            "changes": 293,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -28,6 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -45,6 +46,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, get_torch_version, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_bert import BertConfig\n \n \n@@ -189,7 +191,7 @@ def forward(\n \n \n class BertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -214,66 +216,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n \n-        if is_cross_attention and past_key_value is not None:\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -315,29 +326,27 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n class BertSdpaSelfAttention(BertSelfAttention):\n-    def __init__(self, config, position_embedding_type=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type)\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from BertSelfAttention\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n@@ -356,38 +365,59 @@ def forward(\n                 encoder_attention_mask,\n                 past_key_value,\n                 output_attentions,\n+                cache_position,\n             )\n \n         bsz, tgt_len, _ = hidden_states.size()\n \n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        query_layer = (\n+            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n         # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n-        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n-            key_layer, value_layer = past_key_value\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(current_states))\n-            value_layer = self.transpose_for_scores(self.value(current_states))\n-            if past_key_value is not None and not is_cross_attention:\n-                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = (\n+                self.key(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n@@ -417,10 +447,7 @@ def forward(\n         attn_output = attn_output.transpose(1, 2)\n         attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n \n-        outputs = (attn_output,)\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return attn_output, None\n \n \n class BertSelfOutput(nn.Module):\n@@ -444,10 +471,12 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class BertAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = BERT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = BertSelfOutput(config)\n         self.pruned_heads = set()\n@@ -470,24 +499,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -524,17 +556,17 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class BertLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = BertAttention(config)\n+        self.attention = BertAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n         self.intermediate = BertIntermediate(config)\n         self.output = BertOutput(config)\n \n@@ -545,62 +577,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -610,10 +625,10 @@ def feed_forward_chunk(self, attention_output):\n \n \n class BertEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([BertLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -628,6 +643,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -640,27 +656,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -669,12 +692,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -683,7 +709,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -893,6 +919,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -918,8 +945,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):\n@@ -1004,6 +1036,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n@@ -1170,7 +1203,8 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        **kwargs,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **loss_kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1196,14 +1230,15 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = outputs[0]\n         prediction_scores = self.cls(sequence_output)\n \n         lm_loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(prediction_scores, labels, self.config.vocab_size, **kwargs)\n+            lm_loss = self.loss_function(prediction_scores, labels, self.config.vocab_size, **loss_kwargs)\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]\n@@ -1218,14 +1253,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring\n class BertForMaskedLM(BertPreTrainedModel):"
        },
        {
            "sha": "9dd0f39311013a837bfbd3ce6e75ef3d8f804d91",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 110,
            "deletions": 109,
            "changes": 219,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -22,15 +22,14 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    auto_docstring,\n-    logging,\n-)\n+from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_bert_generation import BertGenerationConfig\n \n \n@@ -54,7 +53,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->BertGeneration\n class BertGenerationSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -79,66 +78,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -180,11 +188,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n BERT_GENERATION_SELF_ATTENTION_CLASSES = {\n@@ -194,10 +198,12 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->BertGeneration,BERT->BERT_GENERATION\n class BertGenerationAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = BERT_GENERATION_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = BertGenerationSelfOutput(config)\n         self.pruned_heads = set()\n@@ -220,24 +226,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -277,17 +286,19 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->BertGeneration\n class BertGenerationLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = BertGenerationAttention(config)\n+        self.attention = BertGenerationAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = BertGenerationAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = BertGenerationAttention(\n+                config, position_embedding_type=\"absolute\", layer_idx=layer_idx\n+            )\n         self.intermediate = BertGenerationIntermediate(config)\n         self.output = BertGenerationOutput(config)\n \n@@ -298,62 +309,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -362,12 +356,11 @@ def feed_forward_chunk(self, attention_output):\n         return layer_output\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->BertGeneration\n class BertEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([BertGenerationLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([BertGenerationLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -382,6 +375,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -394,27 +388,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -423,12 +424,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -437,7 +441,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -678,8 +682,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -873,14 +882,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"BertGenerationDecoder\","
        },
        {
            "sha": "3058bdc94f6e836130414caacda7d993f8bd2487",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 108,
            "deletions": 118,
            "changes": 226,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -296,7 +297,7 @@ def forward(\n \n \n class BigBirdSelfAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -314,11 +315,7 @@ def __init__(self, config):\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n         self.is_decoder = config.is_decoder\n-\n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n+        self.layer_idx = layer_idx\n \n     def forward(\n         self,\n@@ -329,43 +326,41 @@ def forward(\n         encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-\n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+        if is_cross_attention and past_key_value is not None and past_key_value.get_seq_length(self.layer_idx) > 0:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = past_key_value.key_cache[self.layer_idx]\n+            value_layer = past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+            key_layer = (\n+                self.key(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n \n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                key_layer, value_layer = past_key_value.update(\n+                    key_layer,\n+                    value_layer,\n+                    self.layer_idx,\n+                )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -392,11 +387,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n class BigBirdBlockSparseAttention(nn.Module):\n@@ -423,11 +414,6 @@ def __init__(self, config, seed=None):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states,\n@@ -450,9 +436,21 @@ def forward(\n         if to_seq_length % to_block_size != 0:\n             raise ValueError(\"Key/Value sided sequence length must be multiple of block size\")\n \n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         context_layer, attention_probs = self.bigbird_block_sparse_attention(\n             query_layer,\n@@ -478,9 +476,7 @@ def forward(\n         )\n \n         context_layer = context_layer.contiguous().view(batch_size, from_seq_length, -1)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-        return outputs\n+        return context_layer, attention_probs\n \n     @staticmethod\n     def torch_bmm_nd(inp_1, inp_2, ndim=None):\n@@ -1310,7 +1306,7 @@ def __init__(self, config, seed=None):\n         self.seed = seed\n \n         if self.config.attention_type == \"original_full\":\n-            self.self = BigBirdSelfAttention(config)\n+            self.self = BigBirdSelfAttention(config, layer_idx=seed)\n         elif self.config.attention_type == \"block_sparse\":\n             self.self = BigBirdBlockSparseAttention(config, seed)\n         else:\n@@ -1320,7 +1316,7 @@ def __init__(self, config, seed=None):\n \n         self.output = BigBirdSelfOutput(config)\n \n-    def set_attention_type(self, value: str):\n+    def set_attention_type(self, value: str, layer_idx=None):\n         if value not in [\"original_full\", \"block_sparse\"]:\n             raise ValueError(\n                 f\"attention_type can only be set to either 'original_full' or 'block_sparse', but is {value}\"\n@@ -1332,7 +1328,7 @@ def set_attention_type(self, value: str):\n         self.attention_type = value\n         if value == \"original_full\":\n             # copy all weights to new full attention class\n-            attn_weights = BigBirdSelfAttention(self.config)\n+            attn_weights = BigBirdSelfAttention(self.config, layer_idx=layer_idx)\n         else:\n             # copy all weights to new sparse attention class\n             attn_weights = BigBirdBlockSparseAttention(self.config, self.seed)\n@@ -1359,6 +1355,7 @@ def forward(\n         to_mask=None,\n         from_blocked_mask=None,\n         to_blocked_mask=None,\n+        cache_position=None,\n     ):\n         # fp16 compatibility\n         if band_mask is not None:\n@@ -1370,12 +1367,13 @@ def forward(\n         if self.attention_type == \"original_full\":\n             self_outputs = self.self(\n                 hidden_states,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                past_key_value,\n-                output_attentions,\n+                attention_mask=attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n         else:\n             if encoder_hidden_states is not None:\n@@ -1433,11 +1431,11 @@ def __init__(self, config, seed=None):\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise TypeError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = BigBirdAttention(config)\n+            self.crossattention = BigBirdAttention(config, seed=seed)\n         self.intermediate = BigBirdIntermediate(config)\n         self.output = BigBirdOutput(config)\n \n-    def set_attention_type(self, value: str):\n+    def set_attention_type(self, value: str, layer_idx=None):\n         if value not in [\"original_full\", \"block_sparse\"]:\n             raise ValueError(\n                 f\"attention_type can only be set to either 'original_full' or 'block_sparse', but is {value}\"\n@@ -1446,10 +1444,10 @@ def set_attention_type(self, value: str):\n         if value == self.attention_type:\n             return\n         self.attention_type = value\n-        self.attention.set_attention_type(value)\n+        self.attention.set_attention_type(value, layer_idx=layer_idx)\n \n         if self.add_cross_attention:\n-            self.crossattention.set_attention_type(value)\n+            self.crossattention.set_attention_type(value, layer_idx=layer_idx)\n \n     def forward(\n         self,\n@@ -1464,69 +1462,51 @@ def forward(\n         blocked_encoder_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             band_mask=band_mask,\n             from_mask=from_mask,\n             to_mask=to_mask,\n             from_blocked_mask=blocked_encoder_mask,\n             to_blocked_mask=blocked_encoder_mask,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with                    \"\n                     \" cross-attention layers by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n \n-        outputs = (layer_output,) + outputs\n-\n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n-        return outputs\n+        return (layer_output,) + outputs\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -1554,8 +1534,8 @@ def set_attention_type(self, value: str):\n         if value == self.attention_type:\n             return\n         self.attention_type = value\n-        for layer in self.layer:\n-            layer.set_attention_type(value)\n+        for i, layer in enumerate(self.layer):\n+            layer.set_attention_type(value, layer_idx=i)\n \n     def forward(\n         self,\n@@ -1573,6 +1553,7 @@ def forward(\n         to_mask=None,\n         blocked_encoder_mask=None,\n         return_dict=True,\n+        cache_position=None,\n     ) -> Union[BaseModelOutputWithPastAndCrossAttentions, tuple]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -1585,14 +1566,21 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n+                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n@@ -1604,13 +1592,12 @@ def forward(\n                 from_mask,\n                 to_mask,\n                 blocked_encoder_mask,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n+                cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -1619,12 +1606,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -1633,7 +1623,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1868,6 +1858,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, tuple[torch.FloatTensor]]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1894,8 +1885,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -2007,6 +2003,7 @@ def forward(\n             to_mask=to_mask,\n             blocked_encoder_mask=blocked_encoder_mask,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n \n@@ -2396,6 +2393,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[CausalLMOutputWithCrossAttentions, tuple[torch.FloatTensor]]:\n         r\"\"\"\n@@ -2420,6 +2418,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n             **kwargs,\n         )\n \n@@ -2448,15 +2447,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n class BigBirdClassificationHead(nn.Module):\n     \"\"\"Head for sentence-level classification tasks.\"\"\""
        },
        {
            "sha": "2466400b82b3ceac6fc5e9d1cddc260ee5d4ebb1",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 55,
            "deletions": 88,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -108,7 +108,7 @@ def forward(self, input_ids: torch.Tensor):\n \n # Copied from transformers.models.big_bird.modeling_big_bird.BigBirdSelfAttention with BigBird->BigBirdPegasus\n class BigBirdPegasusSelfAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -126,11 +126,7 @@ def __init__(self, config):\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n         self.is_decoder = config.is_decoder\n-\n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n+        self.layer_idx = layer_idx\n \n     def forward(\n         self,\n@@ -141,43 +137,41 @@ def forward(\n         encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n-        # If this is instantiated as a cross-attention module, the keys\n-        # and values come from an encoder; the attention mask needs to be\n-        # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-\n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+        if is_cross_attention and past_key_value is not None and past_key_value.get_seq_length(self.layer_idx) > 0:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = past_key_value.key_cache[self.layer_idx]\n+            value_layer = past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+            key_layer = (\n+                self.key(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n \n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                key_layer, value_layer = past_key_value.update(\n+                    key_layer,\n+                    value_layer,\n+                    self.layer_idx,\n+                )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -204,11 +198,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.big_bird.modeling_big_bird.BigBirdBlockSparseAttention with BigBird->BigBirdPegasus\n@@ -236,11 +226,6 @@ def __init__(self, config, seed=None):\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states,\n@@ -263,9 +248,21 @@ def forward(\n         if to_seq_length % to_block_size != 0:\n             raise ValueError(\"Key/Value sided sequence length must be multiple of block size\")\n \n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         context_layer, attention_probs = self.bigbird_block_sparse_attention(\n             query_layer,\n@@ -291,9 +288,7 @@ def forward(\n         )\n \n         context_layer = context_layer.contiguous().view(batch_size, from_seq_length, -1)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-        return outputs\n+        return context_layer, attention_probs\n \n     @staticmethod\n     def torch_bmm_nd(inp_1, inp_2, ndim=None):\n@@ -1331,7 +1326,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class BigBirdPegasusEncoderLayer(GradientCheckpointingLayer):\n@@ -1492,7 +1487,7 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -1509,7 +1504,7 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -1534,9 +1529,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -1573,7 +1565,7 @@ class BigBirdPegasusPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BigBirdPegasusEncoderLayer\", \"BigBirdPegasusDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_param_buffer_assignment = False\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -2265,7 +2257,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -2298,9 +2289,6 @@ def forward(\n             )\n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -2313,19 +2301,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -2656,17 +2643,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -3064,15 +3040,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"BigBirdPegasusForCausalLM\","
        },
        {
            "sha": "543c6cba5c42bf9d3f90f58d5066a279c09f3527",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 23,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -245,7 +245,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class BioGptDecoderLayer(GradientCheckpointingLayer):\n@@ -307,7 +307,7 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -335,9 +335,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -349,7 +346,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -635,7 +632,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = None\n-        next_decoder_cache = () if use_cache else None\n \n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n@@ -660,9 +656,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -672,19 +665,18 @@ def forward(\n \n         hidden_states = self.layer_norm(hidden_states)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -779,15 +771,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring\n class BioGptForTokenClassification(BioGptPreTrainedModel):"
        },
        {
            "sha": "0994ff64693ed40cb05f556976e50fbe5297805d",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 22,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -132,7 +132,7 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -160,9 +160,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -174,7 +171,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -460,7 +457,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = None\n-        next_decoder_cache = () if use_cache else None\n \n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n@@ -485,9 +481,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -497,19 +490,18 @@ def forward(\n \n         hidden_states = self.layer_norm(hidden_states)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -604,15 +596,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring\n class BioGptForTokenClassification(BioGptPreTrainedModel):"
        },
        {
            "sha": "a9fb4a30f0c5844e3ebf2ca7dc626742f9748d05",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -311,8 +311,7 @@ class BitNetPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "a94a31a04b2b8ce6b5579279e23ceb6e52d9523f",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 9,
            "deletions": 42,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -267,7 +267,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Blenderbot, MBART->BLENDERBOT\n@@ -310,7 +310,7 @@ def forward(\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -331,12 +331,7 @@ def forward(\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states, attn_weights\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Blenderbot, MBART->BLENDERBOT\n@@ -410,7 +405,7 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -427,7 +422,7 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -452,9 +447,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -466,7 +458,7 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -1063,7 +1055,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1096,9 +1087,6 @@ def forward(\n             )\n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1112,19 +1100,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1480,17 +1467,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->Blenderbot\n class BlenderbotDecoderWrapper(BlenderbotPreTrainedModel):\n@@ -1631,15 +1607,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"BlenderbotForCausalLM\","
        },
        {
            "sha": "c6abb963009ffe66cb458720476b0de8a23f08a9",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 8,
            "deletions": 36,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -251,7 +251,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL\n@@ -294,7 +294,7 @@ def forward(\n                 returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -396,7 +396,7 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -413,7 +413,7 @@ def forward(\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -440,9 +440,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -454,7 +451,7 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -1046,7 +1043,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1079,9 +1075,6 @@ def forward(\n             )\n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1092,19 +1085,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1434,17 +1426,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->BlenderbotSmall\n class BlenderbotSmallDecoderWrapper(BlenderbotSmallPreTrainedModel):\n@@ -1585,15 +1566,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"BlenderbotSmallForCausalLM\","
        },
        {
            "sha": "821bd783c67a042a3ff492cba84cf5b43a60ce91",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 122,
            "deletions": 102,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -97,7 +98,7 @@ def forward(\n \n # Adapted from https://github.com/salesforce/BLIP/blob/main/models/med.py#L97\n class BlipTextSelfAttention(nn.Module):\n-    def __init__(self, config, is_cross_attention):\n+    def __init__(self, config, is_cross_attention, layer_idx=None):\n         super().__init__()\n         self.config = config\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n@@ -109,6 +110,7 @@ def __init__(self, config, is_cross_attention):\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.layer_idx = layer_idx\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         if is_cross_attention:\n@@ -136,44 +138,67 @@ def save_attention_map(self, attention_map):\n     def get_attention_map(self):\n         return self.attention_map\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        if is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+            key_layer = (\n+                self.key(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n \n-        past_key_value = (key_layer, value_layer)\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -216,10 +241,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert -> BlipText\n@@ -239,9 +261,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Adapted from https://github.com/salesforce/BLIP/blob/main/models/med.py#242\n class BlipTextAttention(nn.Module):\n-    def __init__(self, config, is_cross_attention=False):\n+    def __init__(self, config, is_cross_attention=False, layer_idx=None):\n         super().__init__()\n-        self.self = BlipTextSelfAttention(config, is_cross_attention)\n+        self.self = BlipTextSelfAttention(config, is_cross_attention, layer_idx=layer_idx)\n         self.output = BlipTextSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -269,18 +291,18 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -324,10 +346,12 @@ def __init__(self, config, layer_num):\n         self.config = config\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = BlipTextAttention(config)\n+        self.attention = BlipTextAttention(config, layer_idx=layer_num)\n         self.layer_num = layer_num\n         if self.config.is_decoder:\n-            self.crossattention = BlipTextAttention(config, is_cross_attention=self.config.is_decoder)\n+            self.crossattention = BlipTextAttention(\n+                config, is_cross_attention=self.config.is_decoder, layer_idx=layer_num\n+            )\n         self.intermediate = BlipTextIntermediate(config)\n         self.output = BlipTextOutput(config)\n \n@@ -338,42 +362,37 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n-\n-        outputs = self_attention_outputs[1:-1]\n-        present_key_value = self_attention_outputs[-1]\n+        outputs = self_attention_outputs[1:]\n \n         if encoder_hidden_states is not None:\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        outputs = outputs + (present_key_value,)\n-\n-        return outputs\n+        return (layer_output,) + outputs\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -401,53 +420,70 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning(\n                     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n                 use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache:\n+            if not isinstance(past_key_values, Cache):\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                return_legacy_cache = True\n+                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            # The model acts as encoder decoder but is not an encoder decoder. So we cast all cache objects to\n+            # `EncoderDecoderCache` type assuming that the incoming cache is from `self_attention`\n+            elif isinstance(past_key_values, DynamicCache):\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.is_decoder else None\n-\n-        next_decoder_cache = () if use_cache else None\n+        all_cross_attentions = () if output_attentions and encoder_hidden_states is not None else None\n \n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n+                cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n \n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -456,7 +492,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -669,6 +705,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         is_decoder: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor`, *optional*):\n@@ -717,8 +754,13 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones((batch_size, seq_length + past_key_values_length)).to(device)\n@@ -776,6 +818,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n@@ -833,6 +876,7 @@ def forward(\n         return_logits: Optional[bool] = False,\n         is_decoder: Optional[bool] = True,\n         reduction: Optional[str] = \"mean\",\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states (`torch.FloatTensor`, *optional*): Sequence of\n@@ -874,6 +918,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             is_decoder=is_decoder,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = outputs[0]\n@@ -908,40 +953,15 @@ def forward(\n     def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, **model_kwargs):\n         # Overwrite -- hardcoded key return (`is_decoder=True`)\n \n-        input_shape = input_ids.shape\n-        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n-        if attention_mask is None:\n-            attention_mask = input_ids.new_ones(input_shape)\n-\n-        # cut decoder_input_ids if past_key_values is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            **model_kwargs,\n+        )\n+        model_inputs[\"is_decoder\"] = True\n \n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"past_key_values\": past_key_values,\n-            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n-            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n-            \"is_decoder\": True,\n-        }\n-\n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n+        return model_inputs\n \n \n __all__ = [\"BlipTextModel\", \"BlipTextLMHeadModel\", \"BlipTextPreTrainedModel\"]"
        },
        {
            "sha": "8235767b7e062cf497e37b46d471c8a07e30b464",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1830,10 +1830,8 @@ def forward(\n class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     config_class = Blip2Config\n     main_input_name = \"pixel_values\"\n-    _supports_cache_class = True\n-    _supports_static_cache = True\n-    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n+    _supports_static_cache = True\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n     _supports_flash_attn = False  # because self.qformer does not support FA2\n "
        },
        {
            "sha": "242061eb4e40588c4c8c3de6126bf3ebfce307f6",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 6,
            "deletions": 30,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -433,9 +433,8 @@ class BloomPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BloomBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n-    _supports_quantized_cache = True\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n@@ -506,7 +505,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -846,7 +845,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -915,29 +914,6 @@ def forward(\n             attentions=transformer_outputs.attentions,\n         )\n \n-    def _reorder_cache(\n-        self, past: tuple[tuple[torch.Tensor, torch.Tensor], ...], beam_idx: torch.LongTensor\n-    ) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-\n-        Output shares the same memory storage as `past`.\n-        \"\"\"\n-        # Get a copy of `beam_idx` on all the devices where we need those indices.\n-        device_to_beam_idx = {\n-            past_state.device: beam_idx.to(past_state.device) for layer_past in past for past_state in layer_past\n-        }\n-        reordered_past = tuple(\n-            (\n-                layer_past[0].index_select(0, device_to_beam_idx[layer_past[0].device]),\n-                layer_past[1].index_select(0, device_to_beam_idx[layer_past[0].device]),\n-            )\n-            for layer_past in past\n-        )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -980,7 +956,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1116,7 +1092,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1207,7 +1183,7 @@ def forward(\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as"
        },
        {
            "sha": "47f68fe23c0a7b27cbda1b75ddebf37d0c8c976d",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 119,
            "deletions": 99,
            "changes": 218,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN, QuickGELUActivation\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -36,6 +37,7 @@\n from ...modeling_utils import PreTrainedModel, apply_chunking_to_forward\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging, torch_int\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_bridgetower import BridgeTowerConfig, BridgeTowerTextConfig, BridgeTowerVisionConfig\n \n \n@@ -401,7 +403,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->BridgeTower\n class BridgeTowerSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -426,66 +428,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n-\n-        if is_cross_attention and past_key_value is not None:\n-            # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n+        if is_cross_attention and encoder_attention_mask is not None:\n             attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n-        else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n \n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -527,11 +538,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n BRIDGE_TOWER_SELF_ATTENTION_CLASSES = {\n@@ -541,10 +548,12 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->BridgeTower,BERT->BRIDGE_TOWER\n class BridgeTowerAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = BRIDGE_TOWER_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = BridgeTowerSelfOutput(config)\n         self.pruned_heads = set()\n@@ -567,39 +576,42 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n         return outputs\n \n \n class BridgeTowerBertCrossLayer(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = BridgeTowerAttention(config)\n+        self.attention = BridgeTowerAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n-        self.crossattention = BridgeTowerAttention(config)\n+        self.crossattention = BridgeTowerAttention(config, layer_idx=layer_idx)\n         self.intermediate = BridgeTowerIntermediate(config)\n         self.output = BridgeTowerOutput(config)\n \n@@ -612,6 +624,7 @@ def forward(\n         encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attention_outputs = self.attention(\n@@ -629,16 +642,16 @@ def forward(\n \n         cross_attention_outputs = self.crossattention(\n             attention_output,\n-            attention_mask=attention_mask,\n+            attention_mask=encoder_attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = cross_attention_outputs[0]\n         # add cross attentions if we output attention weights\n-        outputs = outputs + cross_attention_outputs[1:-1]\n+        outputs = outputs + cross_attention_outputs[1:]\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n@@ -654,17 +667,17 @@ def feed_forward_chunk(self, attention_output):\n \n \n class BridgeTowerTextLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = BridgeTowerAttention(config)\n+        self.attention = BridgeTowerAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = BridgeTowerAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = BridgeTowerAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n         self.intermediate = BridgeTowerIntermediate(config)\n         self.output = BridgeTowerOutput(config)\n \n@@ -675,63 +688,50 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n \n         # if decoder, the last output is tuple of self-attn cache\n         if self.is_decoder:\n             outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n         else:\n             outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n             outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n \n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n-        return outputs\n+        return (layer_output,) + outputs\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -741,10 +741,12 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaEncoder with Roberta->BridgeTowerText\n class BridgeTowerTextEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([BridgeTowerTextLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList(\n+            [BridgeTowerTextLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]\n+        )\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -759,6 +761,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -771,27 +774,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -800,12 +810,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -814,7 +827,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1041,6 +1054,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1066,8 +1080,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -1120,6 +1139,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n@@ -1172,10 +1192,10 @@ def __init__(self, config):\n                 ln.bias.data = self.vision_model.visual.ln_post.bias.data\n \n         self.cross_modal_image_layers = nn.ModuleList(\n-            [BridgeTowerBertCrossLayer(text_config) for _ in range(config.num_hidden_layers)]\n+            [BridgeTowerBertCrossLayer(text_config, layer_idx=i) for i in range(config.num_hidden_layers)]\n         )\n         self.cross_modal_text_layers = nn.ModuleList(\n-            [BridgeTowerBertCrossLayer(text_config) for _ in range(config.num_hidden_layers)]\n+            [BridgeTowerBertCrossLayer(text_config, layer_idx=i) for i in range(config.num_hidden_layers)]\n         )\n \n         # Class token => Linear => Tanh"
        },
        {
            "sha": "dcb0d243d031ef8c5608a2484404cb5ccddd3b58",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 156,
            "deletions": 131,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -41,6 +42,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_camembert import CamembertConfig\n \n \n@@ -139,7 +141,7 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->Camembert\n class CamembertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -164,66 +166,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n \n-        if is_cross_attention and past_key_value is not None:\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -265,30 +276,28 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaSdpaSelfAttention with Roberta->Camembert\n class CamembertSdpaSelfAttention(CamembertSelfAttention):\n-    def __init__(self, config, position_embedding_type=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type)\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from CamembertSelfAttention\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n@@ -307,38 +316,59 @@ def forward(\n                 encoder_attention_mask,\n                 past_key_value,\n                 output_attentions,\n+                cache_position,\n             )\n \n         bsz, tgt_len, _ = hidden_states.size()\n \n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        query_layer = (\n+            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n         # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n-        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n-            key_layer, value_layer = past_key_value\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(current_states))\n-            value_layer = self.transpose_for_scores(self.value(current_states))\n-            if past_key_value is not None and not is_cross_attention:\n-                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = (\n+                self.key(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n@@ -368,10 +398,7 @@ def forward(\n         attn_output = attn_output.transpose(1, 2)\n         attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n \n-        outputs = (attn_output,)\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return attn_output, None\n \n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfOutput with Roberta->Camembert\n@@ -397,10 +424,12 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaAttention with Roberta->Camembert,ROBERTA->CAMEMBERT\n class CamembertAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = CAMEMBERT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = CamembertSelfOutput(config)\n         self.pruned_heads = set()\n@@ -423,24 +452,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -480,17 +512,17 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaLayer with Roberta->Camembert\n class CamembertLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = CamembertAttention(config)\n+        self.attention = CamembertAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = CamembertAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = CamembertAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n         self.intermediate = CamembertIntermediate(config)\n         self.output = CamembertOutput(config)\n \n@@ -501,62 +533,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -567,10 +582,10 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaEncoder with Roberta->Camembert\n class CamembertEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([CamembertLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([CamembertLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -585,6 +600,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -597,27 +613,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -626,12 +649,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -640,7 +666,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -815,6 +841,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -840,8 +867,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):\n@@ -926,6 +958,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n@@ -1541,14 +1574,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n # Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\n def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):"
        },
        {
            "sha": "9866aad87a4ec42d17f950e26f7db4dbea42bb46",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -410,11 +410,6 @@ def __init__(self, config):\n             self.max_position_embeddings = config.max_position_embeddings\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         from_tensor: torch.Tensor,\n@@ -423,16 +418,27 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        mixed_query_layer = self.query(from_tensor)\n+        batch_size, seq_length, _ = from_tensor.shape\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n \n-        key_layer = self.transpose_for_scores(self.key(to_tensor))\n-        value_layer = self.transpose_for_scores(self.value(to_tensor))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        key_layer = (\n+            self.key(to_tensor)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(to_tensor)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        query_layer = (\n+            self.query(from_tensor)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))"
        },
        {
            "sha": "fe4899c7e9326ab1f9585a720bee36b3a2445cdb",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 8,
            "deletions": 24,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -377,10 +377,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with Llama->Chameleon, LLAMA->CHAMELEON\n@@ -430,7 +427,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -453,9 +450,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -504,7 +498,7 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -526,9 +520,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -823,8 +814,7 @@ class ChameleonPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n     _supports_flex_attn = True\n@@ -1009,7 +999,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -1028,9 +1017,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1040,16 +1026,14 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n \n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )"
        },
        {
            "sha": "afe7bdb06a3bc3b7cefe9b3fc6b17af07990c2dd",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -926,11 +926,8 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n-\n         if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n+            attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n \n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):"
        },
        {
            "sha": "16a079c3b83ca7a5911b7ea047404743fd99233d",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 72,
            "deletions": 113,
            "changes": 185,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN, get_activation\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationConfig, GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n@@ -268,7 +269,7 @@ class ClvpSelfAttention(nn.Module):\n     Multi-headed attention to combine Absolute and Rotary Positional Embeddings into a single Attention module.\n     \"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -281,6 +282,7 @@ def __init__(self, config):\n             )\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n+        self.layer_idx = layer_idx\n \n         if hasattr(config, \"max_position_embeddings\"):\n             max_positions = config.max_position_embeddings\n@@ -302,10 +304,11 @@ def forward(\n         rotary_pos_emb: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor], Optional[tuple[torch.FloatTensor]]]:\n         # Raise error when position_ids is None but rotary_pos_emb is provided, because we need that when applying\n         # rotary_pos_emb to query and key states.\n@@ -320,14 +323,9 @@ def forward(\n         value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n \n         if past_key_value is not None:\n-            past_key, past_value = past_key_value\n-            key_states = torch.cat((past_key, key_states), dim=-2)\n-            value_states = torch.cat((past_value, value_states), dim=-2)\n-\n-        if use_cache is True:\n-            present = (key_states, value_states)\n-        else:\n-            present = None\n+            key_states, value_states = past_key_value.update(\n+                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+            )\n \n         if rotary_pos_emb is not None:\n             rotary_emb_dim = rotary_pos_emb.shape[-1]\n@@ -385,10 +383,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, present, attn_weights\n+        return attn_output, attn_weights\n \n \n class ClvpGatedLinearUnit(nn.Module):\n@@ -464,29 +459,22 @@ def forward(\n \n         hidden_states = self.input_rmsnorm(hidden_states)\n \n-        attention_outputs = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             rotary_pos_emb=rotary_pos_emb,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n         )\n \n-        hidden_states = attention_outputs[0]\n-\n         hidden_states = residual + hidden_states\n \n         residual = hidden_states\n         hidden_states = self.post_attention_rmsnorm(hidden_states)\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attention_outputs[-1],)\n-\n-        return outputs\n+        return hidden_states, attn_weights\n \n \n # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->Clvp\n@@ -608,26 +596,27 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n \n \n class ClvpDecoderLayer(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n         inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n \n         self.input_layernorm = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n-        self.attn = ClvpSelfAttention(config)\n+        self.attn = ClvpSelfAttention(config, layer_idx=layer_idx)\n         self.post_attention_layernorm = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n \n         self.mlp = ClvpDecoderMLP(inner_dim, config)\n \n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n@@ -639,9 +628,9 @@ def forward(\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attn_output = attn_outputs[0]\n-        outputs = attn_outputs[1:]\n         # residual connection\n         hidden_states = attn_output + residual\n \n@@ -651,12 +640,7 @@ def forward(\n         # residual connection\n         hidden_states = residual + feed_forward_hidden_states\n \n-        if use_cache:\n-            outputs = (hidden_states,) + outputs\n-        else:\n-            outputs = (hidden_states,) + outputs[1:]\n-\n-        return outputs\n+        return (hidden_states,) + attn_outputs[1:]\n \n \n class ClvpConditioningEncoder(nn.Module):\n@@ -1007,7 +991,9 @@ def __init__(self, config):\n         self.position_embeds_layer = nn.Embedding(self.config.max_position_embeddings, self.config.hidden_size)\n \n         self.drop = nn.Dropout(self.config.embd_pdrop)\n-        self.layers = nn.ModuleList([ClvpDecoderLayer(self.config) for _ in range(self.config.num_hidden_layers)])\n+        self.layers = nn.ModuleList(\n+            [ClvpDecoderLayer(self.config, layer_idx=i) for i in range(self.config.num_hidden_layers)]\n+        )\n         self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=self.config.layer_norm_epsilon)\n \n         self.gradient_checkpointing = False\n@@ -1042,6 +1028,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1068,11 +1055,24 @@ def forward(\n         if token_type_ids is not None:\n             token_type_ids = token_type_ids.view(-1, input_shape[-1])\n \n-        if past_key_values is None:\n-            past_key_values_length = 0\n-            past_key_values = tuple([None] * len(self.layers))\n-        else:\n-            past_key_values_length = past_key_values[0][0].size(-2)\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n+                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if position_ids is None:\n             position_ids = torch.arange(\n                 past_key_values_length, input_shape[-1] + past_key_values_length, dtype=torch.long, device=device\n@@ -1104,18 +1104,10 @@ def forward(\n \n         output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        presents = () if use_cache else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n-        for i, (block, past_key_value) in enumerate(zip(self.layers, past_key_values)):\n+        for i, block in enumerate(self.layers):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n@@ -1127,26 +1119,26 @@ def forward(\n                     attention_mask,\n                     position_ids,\n                     head_mask[i],\n+                    cache_position,\n                 )\n             else:\n                 outputs = block(\n                     hidden_states,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     attention_mask=attention_mask,\n                     position_ids=position_ids,\n                     head_mask=head_mask[i],\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n+                    cache_position=cache_position,\n                 )\n \n             hidden_states = outputs[0]\n-            if use_cache is True:\n-                presents = presents + (outputs[1],)\n \n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n                 if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n+                    all_cross_attentions = all_cross_attentions + (outputs[2],)\n \n         hidden_states = self.layer_norm(hidden_states)\n \n@@ -1156,16 +1148,19 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions, all_cross_attentions]\n                 if v is not None\n             )\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=presents,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1205,6 +1200,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1226,6 +1222,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1332,56 +1329,28 @@ def _prepare_model_inputs(\n         return inputs, input_name, model_kwargs\n \n     def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, inputs_embeds=None, conditioning_embeds=None, **kwargs\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        conditioning_embeds=None,\n+        cache_position=None,\n+        **kwargs,\n     ):\n         # Overwritten: has `conditioning_embeds`-related logic\n \n         input_ids_length = input_ids.shape[-1]\n-        token_type_ids = kwargs.get(\"token_type_ids\", None)\n-        # only last token for inputs_ids if past is defined in kwargs\n-        if past_key_values:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-            if token_type_ids is not None:\n-                token_type_ids = token_type_ids[:, -input_ids.shape[1] :]\n-\n-        attention_mask = kwargs.get(\"attention_mask\", None)\n-        position_ids = kwargs.get(\"position_ids\", None)\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -1].unsqueeze(-1)\n-        else:\n-            position_ids = None\n-\n-        if conditioning_embeds is not None and past_key_values is not None:\n-            position_ids = torch.tensor([input_ids_length], dtype=torch.long, device=input_ids.device)\n \n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_key_values is None:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n-\n-        model_inputs.update(\n-            {\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"position_ids\": position_ids,\n-                \"token_type_ids\": token_type_ids,\n-            }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n         )\n+        if conditioning_embeds is not None and cache_position[0] != 0:\n+            model_inputs[\"position_ids\"] = torch.tensor([input_ids_length], dtype=torch.long, device=input_ids.device)\n+\n         return model_inputs\n \n     @auto_docstring\n@@ -1399,6 +1368,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1426,6 +1396,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         hidden_states = outputs[0]\n@@ -1456,20 +1427,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1708,6 +1665,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, ClvpOutput]:\n         r\"\"\"\n         conditioning_encoder_inputs_embeds (`torch.FloatTensor`, *optional*):\n@@ -1762,6 +1720,7 @@ def forward(\n             inputs_embeds=conditioning_embeds,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         speech_ids = decoder_outputs[0]"
        },
        {
            "sha": "acc7877ede740244e718b22f80dd4155acd2f71b",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 16,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -286,8 +286,7 @@ class CodeGenPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CodeGenBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n \n     def __init__(self, *inputs, **kwargs):\n@@ -676,19 +675,5 @@ def forward(\n             attentions=transformer_outputs.attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n-        [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n __all__ = [\"CodeGenForCausalLM\", \"CodeGenModel\", \"CodeGenPreTrainedModel\"]"
        },
        {
            "sha": "9a601591372bdb2f12c746f544b9f04396ad33c9",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -344,8 +344,7 @@ class CoherePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "10e65a180278bc414bea981a57a7c7de572b333d",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -321,8 +321,7 @@ class Cohere2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "7ef0acf080bedc7d070677c75ea47eea11907cdf",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -43,7 +43,6 @@ class ColQwen2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = []\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n \n     def _init_weights(self, module):\n         std = ("
        },
        {
            "sha": "3d2a57f1c350c8829e2f5fa0bd07d0b5c859f950",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -228,7 +228,6 @@ def __call__(\n class ColQwen2PreTrainedModel(ColPaliPreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n \n \n @dataclass"
        },
        {
            "sha": "a43ebfa0259d3c3e672969ee907cc3183c2d6f14",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -325,11 +325,6 @@ def __init__(self, config):\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -338,8 +333,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-        batch_size = hidden_states.size(0)\n+        batch_size, seq_length, _ = hidden_states.shape\n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n@@ -353,9 +347,16 @@ def forward(\n         mixed_key_conv_attn_layer = self.key_conv_attn_layer(hidden_states.transpose(1, 2))\n         mixed_key_conv_attn_layer = mixed_key_conv_attn_layer.transpose(1, 2)\n \n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-        key_layer = self.transpose_for_scores(mixed_key_layer)\n-        value_layer = self.transpose_for_scores(mixed_value_layer)\n+        mixed_query_layer = self.query(hidden_states)\n+        query_layer = mixed_query_layer.view(\n+            batch_size, -1, self.num_attention_heads, self.attention_head_size\n+        ).transpose(1, 2)\n+        key_layer = mixed_key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n+        value_layer = mixed_value_layer.view(\n+            batch_size, -1, self.num_attention_heads, self.attention_head_size\n+        ).transpose(1, 2)\n         conv_attn_layer = torch.multiply(mixed_key_conv_attn_layer, mixed_query_layer)\n \n         conv_kernel_layer = self.conv_kernel_layer(conv_attn_layer)"
        },
        {
            "sha": "d68521c6dd52e1e779db39ee9f04811ec5ed308d",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 68,
            "deletions": 55,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -60,11 +61,12 @@ def forward(self, hidden_states: torch.Tensor):\n \n \n class CpmAntAttention(nn.Module):\n-    def __init__(self, config: CpmAntConfig):\n+    def __init__(self, config: CpmAntConfig, layer_idx=None):\n         super().__init__()\n         self.dim_model = config.hidden_size\n         self.num_heads = config.num_attention_heads\n         self.dim_head = config.dim_head\n+        self.layer_idx = layer_idx\n \n         self.project_q = nn.Linear(self.dim_model, self.num_heads * self.dim_head, bias=False)\n         self.project_k = nn.Linear(self.dim_model, self.num_heads * self.dim_head, bias=False)\n@@ -86,8 +88,9 @@ def forward(\n         attention_mask: torch.BoolTensor,\n         position_bias: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-        past_key_values: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         \"\"\"\n         Args:\n@@ -120,8 +123,7 @@ def forward(\n         value = value.view(batch_size, len_k, self.num_heads, self.dim_head).permute(0, 2, 1, 3)\n \n         if past_key_values is not None:\n-            key = torch.cat([past_key_values[0], key], dim=-2)\n-            value = torch.cat([past_key_values[1], value], dim=-2)\n+            key, value = past_key_values.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n             len_k = key.size(-2)\n \n         # (batch_size, num_heads, len_q, dim_head) @ (batch_size, num_heads, dim_head, len_k) -> (batch_size, num_heads, len_q, len_k)\n@@ -156,18 +158,14 @@ def forward(\n \n         score = self.attention_out(score)\n \n-        past_key_values = None\n-        if use_cache:\n-            past_key_values = (key, value)\n-\n-        return score, attn_weights, past_key_values\n+        return score, attn_weights\n \n \n class CpmAntSelfAttentionBlock(nn.Module):\n-    def __init__(self, config: CpmAntConfig):\n+    def __init__(self, config: CpmAntConfig, layer_idx=None):\n         super().__init__()\n         self.layernorm_before_attention = CpmAntLayerNorm(config)\n-        self.self_attention = CpmAntAttention(config)\n+        self.self_attention = CpmAntAttention(config, layer_idx=layer_idx)\n         if config.dropout_p:\n             self.dropout = torch.nn.Dropout(config.dropout_p)\n         else:\n@@ -179,8 +177,9 @@ def forward(\n         attention_mask: torch.Tensor,\n         position_bias: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-        past_key_values: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         \"\"\"\n         Args:\n@@ -199,17 +198,22 @@ def forward(\n                 (see `past_key_values`).\n         \"\"\"\n         outputs = self.layernorm_before_attention(hidden_states)\n-        outputs = self.self_attention(\n-            outputs, outputs, attention_mask, position_bias, output_attentions, past_key_values, use_cache\n+        outputs, attn_weights = self.self_attention(\n+            outputs,\n+            outputs,\n+            attention_mask,\n+            position_bias,\n+            output_attentions,\n+            past_key_values,\n+            use_cache,\n+            cache_position,\n         )\n \n-        outputs, attn_weights, current_key_value = outputs\n-\n         if self.dropout is not None:\n             outputs = self.dropout(outputs)\n         hidden_states = hidden_states + outputs\n \n-        return hidden_states, attn_weights, current_key_value\n+        return hidden_states, attn_weights\n \n \n class CpmAntDenseGatedACT(nn.Module):\n@@ -286,9 +290,9 @@ def forward(\n \n \n class CpmAntTransformerBlock(nn.Module):\n-    def __init__(self, config: CpmAntConfig):\n+    def __init__(self, config: CpmAntConfig, layer_idx=None):\n         super().__init__()\n-        self.self_att = CpmAntSelfAttentionBlock(config)\n+        self.self_att = CpmAntSelfAttentionBlock(config, layer_idx=layer_idx)\n         self.ffn = CpmAntFFNBlock(config)\n \n     def forward(\n@@ -297,8 +301,9 @@ def forward(\n         attention_mask: torch.Tensor,\n         position_bias: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-        past_key_values: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         \"\"\"\n         Args:\n@@ -316,27 +321,25 @@ def forward(\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n         \"\"\"\n-        hidden_states = self.self_att(\n+        hidden_states, attn_weights = self.self_att(\n             hidden_states,\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             output_attentions=output_attentions,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n+            cache_position=cache_position,\n         )\n \n-        hidden_states, attn_weights, current_key_value = hidden_states\n-\n         hidden_states = self.ffn(hidden_states)\n-\n-        return hidden_states, attn_weights, current_key_value\n+        return hidden_states, attn_weights\n \n \n class CpmAntEncoder(nn.Module):\n     def __init__(self, config: CpmAntConfig):\n         super().__init__()\n         self.num_layers = config.num_hidden_layers\n-        self.layers = nn.ModuleList([CpmAntTransformerBlock(config) for ith in range(self.num_layers)])\n+        self.layers = nn.ModuleList([CpmAntTransformerBlock(config, layer_idx=i) for i in range(self.num_layers)])\n \n         self.output_layernorm = CpmAntLayerNorm(config)\n \n@@ -347,8 +350,9 @@ def forward(\n         position_bias: torch.Tensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        past_key_values: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n+        cache_postion: Optional[torch.Tensor] = None,\n     ):\n         \"\"\"\n         Args:\n@@ -370,7 +374,6 @@ def forward(\n         \"\"\"\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        current_key_values = () if use_cache else None\n \n         for i, layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -380,21 +383,19 @@ def forward(\n                 attention_mask,\n                 position_bias,\n                 output_attentions=output_attentions,\n-                past_key_values=past_key_values[i] if past_key_values else None,\n+                past_key_values=past_key_values,\n                 use_cache=use_cache,\n             )\n-            hidden_states, attn_weights, current_key_value = layer_outputs\n+            hidden_states, attn_weights = layer_outputs\n             if output_attentions:\n                 all_self_attns += (attn_weights,)\n-            if current_key_value is not None:\n-                current_key_values = current_key_values + (current_key_value,)\n \n         hidden_states = self.output_layernorm(hidden_states)\n \n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        return hidden_states, current_key_values, all_hidden_states, all_self_attns\n+        return hidden_states, all_hidden_states, all_self_attns\n \n \n # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->CPMAnt\n@@ -592,6 +593,7 @@ def forward(\n         past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPast]:\n         r\"\"\"\n@@ -634,17 +636,24 @@ def forward(\n         position = torch.arange(seq_length, dtype=dtype, device=device).repeat(batch, 1)\n         span = torch.full((batch, seq_length), 0, dtype=dtype, device=device)\n \n-        if past_key_values is None:\n-            past_length = 0\n-            past_key_values = tuple([None] * self.encoder.num_layers)\n-            input_ids = input_ids.contiguous()\n-            hidden_states = self.input_embedding(input_ids)\n-            segment_states = self.segment_embedding(segment)\n-            hidden_states = hidden_states + segment_states\n-        else:\n-            past_length = past_key_values[0][0].size(-2)\n-            segment_states = self.segment_embedding(segment)\n-            hidden_states = self.input_embedding(input_ids) + segment_states[:, -1:, :]\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n+                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+\n+        past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        input_ids = input_ids.contiguous()\n+        hidden_states = self.input_embedding(input_ids)\n+        segment_states = self.segment_embedding(segment)\n+        if past_length != 0:\n+            segment_states = segment_states[:, -1:, :]\n+\n+        hidden_states = hidden_states + segment_states\n \n         attention_mask = self._prepare_attention_mask(input_ids, span, context, length)\n         position_bias = self.position_bias(position, position, segment, segment)\n@@ -653,14 +662,15 @@ def forward(\n         position_bias = position_bias[:, :, past_length:, :]\n         hidden_states = hidden_states[:, past_length:, :]\n \n-        hidden_states, present_key_values, all_hidden_states, all_attentions = self.encoder(\n+        hidden_states, all_hidden_states, all_attentions = self.encoder(\n             hidden_states,\n             attention_mask,\n             position_bias,\n             output_attentions,\n             output_hidden_states,\n             past_key_values,\n             use_cache,\n+            cache_position,\n         )\n \n         if past_length == 0:\n@@ -677,14 +687,17 @@ def forward(\n                     new_hidden_states += (hidden_state[:, self.prompt_length :, :],)\n                 all_hidden_states = new_hidden_states\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n-                v for v in [hidden_states, present_key_values, all_hidden_states, all_attentions] if v is not None\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_attentions] if v is not None\n             )\n \n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=present_key_values,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n         )\n@@ -719,6 +732,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n         attention_mask: Optional[torch.Tensor] = None,  # dummy parameter for text-generation pipeline\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -751,7 +765,13 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         model_output = self.cpmant(\n-            input_ids, output_attentions, output_hidden_states, past_key_values, use_cache, return_dict\n+            input_ids,\n+            output_attentions,\n+            output_hidden_states,\n+            past_key_values,\n+            use_cache,\n+            return_dict,\n+            cache_position,\n         )\n         hidden_states = model_output.last_hidden_state if return_dict else model_output[0]\n \n@@ -786,12 +806,5 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        past_key_values = [list(each) if each is not None else each for each in past_key_values]\n-        for key_value_layer in past_key_values:\n-            key_value_layer[0] = key_value_layer[0][beam_idx]\n-            key_value_layer[1] = key_value_layer[1][beam_idx]\n-        return past_key_values\n-\n \n __all__ = [\"CpmAntForCausalLM\", \"CpmAntModel\", \"CpmAntPreTrainedModel\"]"
        },
        {
            "sha": "d7807065e55f6cdcd7ba62717a3b4c05f5894d93",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -370,8 +370,7 @@ class CsmPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     # does not because of Mimi codec model\n     # _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "ffde5c82eb5424b04f68fe604304e87fb81aa416",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -133,8 +133,7 @@ class CsmPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     # does not because of Mimi codec model\n     # _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "ba1a737efc5db89dca81f98989b69552f14d3a34",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 50,
            "deletions": 48,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n@@ -83,10 +84,11 @@ def scaled_dot_product_attention(q, k, v, mask, attention_mask=None, head_mask=N\n \n \n class MultiHeadAttention(nn.Module):\n-    def __init__(self, d_model_size, num_heads):\n+    def __init__(self, d_model_size, num_heads, layer_idx=None):\n         super().__init__()\n         self.num_heads = num_heads\n         self.d_model_size = d_model_size\n+        self.layer_idx = layer_idx\n \n         self.depth = int(d_model_size / self.num_heads)\n \n@@ -129,6 +131,7 @@ def forward(\n         head_mask=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         batch_size = q.shape[0]\n \n@@ -139,37 +142,27 @@ def forward(\n         q = self.split_into_heads(q, batch_size)\n         k = self.split_into_heads(k, batch_size)\n         v = self.split_into_heads(v, batch_size)\n-        if layer_past is not None:\n-            past_key, past_value = layer_past[0], layer_past[1]\n-            k = torch.cat((past_key, k), dim=-2)\n-            v = torch.cat((past_value, v), dim=-2)\n \n-        if use_cache is True:\n-            present = torch.stack((k, v))\n-        else:\n-            present = (None,)\n+        if layer_past is not None:\n+            k, v = layer_past.update(k, v, self.layer_idx, {\"cache_position\": cache_position})\n \n         output = scaled_dot_product_attention(q, k, v, mask, attention_mask, head_mask)\n         scaled_attention = output[0].permute([0, 2, 1, 3])\n         attn = output[1]\n         original_size_attention = scaled_attention.reshape(batch_size, -1, self.d_model_size)\n         output = self.dense(original_size_attention)\n-\n-        outputs = (output, present)\n-        if output_attentions:\n-            outputs = outputs + (attn,)\n-        return outputs\n+        return output, attn\n \n \n def point_wise_feed_forward_network(d_model_size, dff):\n     return nn.Sequential(nn.Linear(d_model_size, dff), nn.ReLU(), nn.Linear(dff, d_model_size))\n \n \n class EncoderLayer(nn.Module):\n-    def __init__(self, d_model_size, num_heads, dff, rate=0.1):\n+    def __init__(self, d_model_size, num_heads, dff, rate=0.1, layer_idx=None):\n         super().__init__()\n \n-        self.multi_head_attention = MultiHeadAttention(d_model_size, num_heads)\n+        self.multi_head_attention = MultiHeadAttention(d_model_size, num_heads, layer_idx=layer_idx)\n         self.ffn = point_wise_feed_forward_network(d_model_size, dff)\n \n         self.layernorm1 = nn.LayerNorm(d_model_size, eps=1e-6)\n@@ -179,7 +172,15 @@ def __init__(self, d_model_size, num_heads, dff, rate=0.1):\n         self.dropout2 = nn.Dropout(rate)\n \n     def forward(\n-        self, x, mask, layer_past=None, attention_mask=None, head_mask=None, use_cache=False, output_attentions=False\n+        self,\n+        x,\n+        mask,\n+        layer_past=None,\n+        attention_mask=None,\n+        head_mask=None,\n+        use_cache=False,\n+        output_attentions=False,\n+        cache_position=None,\n     ):\n         normed = self.layernorm1(x)\n         attn_outputs = self.multi_head_attention(\n@@ -192,6 +193,7 @@ def forward(\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attn_output = attn_outputs[0]\n         attn_output = self.dropout1(attn_output)\n@@ -242,7 +244,10 @@ def __init__(self, config):\n \n         self.dropout = nn.Dropout(config.embd_pdrop)\n         self.h = nn.ModuleList(\n-            [EncoderLayer(config.n_embd, config.n_head, config.dff, config.resid_pdrop) for _ in range(config.n_layer)]\n+            [\n+                EncoderLayer(config.n_embd, config.n_head, config.dff, config.resid_pdrop, layer_idx=i)\n+                for i in range(config.n_layer)\n+            ]\n         )\n         self.layernorm = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n \n@@ -276,6 +281,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPast]:\n         r\"\"\"\n@@ -332,11 +338,17 @@ def forward(\n \n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        if past_key_values is None:\n-            past_length = 0\n-            past_key_values = tuple([None] * len(self.h))\n-        else:\n-            past_length = past_key_values[0][0].size(-2)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n+                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+\n+        past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if position_ids is None:\n             position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n             position_ids = position_ids.unsqueeze(0)\n@@ -387,38 +399,40 @@ def forward(\n \n         hidden_states = self.dropout(hidden_states)\n \n-        presents = () if use_cache else None\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n-        for i, (h, layer_past) in enumerate(zip(self.h, past_key_values)):\n+        for i, h in enumerate(self.h):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n             outputs = h(\n                 hidden_states,\n                 mask,\n-                layer_past=layer_past,\n+                layer_past=past_key_values,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask[i],\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n-            hidden_states, present = outputs[:2]\n-            if use_cache is True:\n-                presents = presents + (present,)\n-\n+            hidden_states = outputs[0]\n             if output_attentions:\n-                all_attentions += (outputs[2],)\n+                all_attentions += (outputs[1],)\n \n         hidden_states = self.layernorm(hidden_states)\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_attentions] if v is not None\n+            )\n \n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=presents,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n         )\n@@ -462,6 +476,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -520,6 +535,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         hidden_states = transformer_outputs[0]\n@@ -552,7 +568,7 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, use_cac\n \n         # only last tokens for inputs_ids if past is defined in kwargs\n         if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n+            past_length = past_key_values.get_seq_length()\n \n             # Some generation methods already pass only the last input ID\n             if input_ids.shape[1] > past_length:\n@@ -565,20 +581,6 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, use_cac\n \n         return {\"input_ids\": input_ids, \"past_key_values\": past_key_values, \"use_cache\": use_cache}\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "419bcb7b68b469042b3be6300ccbc305621c41c8",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -246,7 +246,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,"
        },
        {
            "sha": "93d217eed1280ad01ce9146b5f06c7dfd91b67b8",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 113,
            "deletions": 104,
            "changes": 217,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -38,6 +39,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_data2vec_text import Data2VecTextConfig\n \n \n@@ -139,7 +141,7 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->Data2VecText\n class Data2VecTextSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -164,66 +166,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -265,11 +276,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n@@ -294,10 +301,12 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Data2VecText,BERT->DATA2VEC_TEXT\n class Data2VecTextAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = DATA2VEC_TEXT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = Data2VecTextSelfOutput(config)\n         self.pruned_heads = set()\n@@ -320,24 +329,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -377,17 +389,19 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Data2VecText\n class Data2VecTextLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = Data2VecTextAttention(config)\n+        self.attention = Data2VecTextAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = Data2VecTextAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = Data2VecTextAttention(\n+                config, position_embedding_type=\"absolute\", layer_idx=layer_idx\n+            )\n         self.intermediate = Data2VecTextIntermediate(config)\n         self.output = Data2VecTextOutput(config)\n \n@@ -398,62 +412,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -464,10 +461,10 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Data2VecText\n class Data2VecTextEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([Data2VecTextLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([Data2VecTextLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -482,6 +479,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -494,27 +492,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -523,12 +528,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -537,7 +545,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -649,6 +657,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -674,8 +683,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -728,6 +742,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n@@ -788,6 +803,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -830,6 +846,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = outputs[0]\n@@ -857,14 +874,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring\n class Data2VecTextForMaskedLM(Data2VecTextPreTrainedModel):"
        },
        {
            "sha": "2cf64ac21f81d85311ee2daf6220ed8c1cf42071",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 32,
            "deletions": 14,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -261,11 +261,6 @@ def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] =\n         if self.has_relative_position_bias:\n             self.relative_position_bias = Data2VecVisionRelativePositionBias(config, window_size=window_size)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -275,11 +270,22 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[tuple[int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -347,10 +353,22 @@ def forward(\n                 resolution=resolution,\n             )\n \n-        mixed_query_layer = self.query(hidden_states)\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         attn_bias = None\n         if self.has_relative_position_bias:"
        },
        {
            "sha": "4e3d2cb1b6916221c0be42a0fc4b8d2b5e420836",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -809,8 +809,7 @@ class DbrxPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "689fff29e25c1ba8a9a2fec3896aceded711b1ff",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -452,7 +452,7 @@ class DecisionTransformerGPT2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     is_parallelizable = True\n     supports_gradient_checkpointing = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = False\n \n     def __init__(self, *inputs, **kwargs):"
        },
        {
            "sha": "ff36b6d43f43c931a26c10f4512150fb6f355bb5",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -458,8 +458,7 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "708a370171bdf8f23a445d946c7fc329bdee5304",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -497,8 +497,7 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "2966d465f66ddbde35c0d13cbee335a09f0b20fa",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -214,17 +214,28 @@ def __init__(self, config: DeiTConfig) -> None:\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        batch_size, seq_length, _ = hidden_states.shape\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "cf41dc2e29e80a527d5cfd20466b79d459e6a856",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -539,7 +539,7 @@ def forward(\n \n         past_key_values_length = 0\n         if past_key_values is not None:\n-            past_key_values_length = past_key_values[0][0].shape[2]\n+            past_key_values_length = past_key_values.get_seq_length()\n \n         # Adapted from paddlenlp.transformers.ernie_m.ErnieMModel\n         if attention_mask is None:"
        },
        {
            "sha": "25c56354c3fa0465eda1b5976d9125597d65fc04",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -919,7 +919,7 @@ def forward(\n         num_batch = input_ids.shape[0]\n         pasts_or_spout_value = None\n         if past_key_values is not None:\n-            num_pasts_contexts = past_key_values[0][0].shape[2]\n+            num_pasts_contexts = past_key_values.get_seq_length()\n         elif self.config.d_spout and spout is not None:\n             # `spout` is a special input vector specific to GPTSAN\n             # This controls the output by projecting embedded information such as the class of sentences during learning."
        },
        {
            "sha": "2bead71cadd4e360bd4a3b55390df8dcd5a53772",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -925,7 +925,7 @@ def forward(\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n         # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
        },
        {
            "sha": "2d473cd423df6e68e0270886f9a2b1ac4342e721",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -592,7 +592,7 @@ def forward(\n                 use_cache = False\n \n         if past_key_values is not None:\n-            past_key_values_length = past_key_values[0][0].shape[2]\n+            past_key_values_length = past_key_values.get_seq_length()\n             seq_length_with_past = seq_length_with_past + past_key_values_length\n \n         if position_ids is None:\n@@ -794,7 +794,7 @@ def prepare_inputs_for_generation(\n         self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n     ):\n         if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n+            past_length = past_key_values.get_seq_length()\n \n             # Some generation methods already pass only the last input ID\n             if input_ids.shape[1] > past_length:"
        },
        {
            "sha": "914428b96a23757871e9d386ed074f074b042ea9",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -908,7 +908,7 @@ def forward(\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n         # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -1122,7 +1122,7 @@ def prepare_inputs_for_generation(\n \n         # cut decoder_input_ids if past_key_values is used\n         if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n+            past_length = past_key_values.get_seq_length()\n \n             # Some generation methods already pass only the last input ID\n             if input_ids.shape[1] > past_length:"
        },
        {
            "sha": "68787c60e903590180077dced4debdd487f45fbc",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1047,7 +1047,7 @@ def forward(\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n         # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
        },
        {
            "sha": "ed4c96c89bfbba99e5ddd808eb6a61eedf1a1a5c",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -559,7 +559,7 @@ def forward(\n             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n@@ -886,7 +886,7 @@ def prepare_inputs_for_generation(\n             attention_mask = input_ids.new_ones(input_ids.shape)\n \n         if past_key_values:\n-            past_length = past_key_values[0][0].shape[2]\n+            past_length = past_key_values.get_seq_length()\n \n             # Some generation methods already pass only the last input ID\n             if input_ids.shape[1] > past_length:"
        },
        {
            "sha": "19d7988a691e8f1d055593ed5057102bb78b8c98",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -597,7 +597,7 @@ def forward(self, inputs_shape, device, attention_mask=None, past_key_values=Non\n             if past_key_values is not None:\n                 # position_ids is the same for every token when decoding a single step\n                 # Without the int() cast, it doesn't work in some cases when exporting to ONNX\n-                prev_num_input_ids = past_key_values[0][0].shape[2]\n+                prev_num_input_ids = past_key_values.get_seq_length()\n                 num_input_ids = inputs_shape[1] + prev_num_input_ids\n                 position_ids = torch.ones((1, 1), dtype=torch.long, device=device) * (\n                     int(self.padding_idx + num_input_ids)"
        },
        {
            "sha": "da0f616eda7735c322068f9ca29265082898eb22",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -67,7 +67,6 @@ class DiaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n     _supports_static_cache = True\n     main_input_name = \"input_ids\"\n     _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]"
        },
        {
            "sha": "7da15d7c10b9f26d6fb8630a8a90e15f0b2b254e",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -62,7 +62,6 @@ class DiaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n     _supports_static_cache = True\n     main_input_name = \"input_ids\"\n     _no_split_modules = [\"DiaEncoderLayer\", \"DiaDecoderLayer\"]"
        },
        {
            "sha": "e2b093fd8ec0a952b2fe02c5a6a16d93c2cb2040",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -533,8 +533,7 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = False\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = False\n     _can_record_outputs = {"
        },
        {
            "sha": "140d16bd33b9f86da11a193e6a4a2c7c162a2d87",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -270,19 +270,27 @@ def __init__(self, config, dim, num_heads, kernel_size, dilation):\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 3, 1, 2, 4)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Apply the scale factor before computing attention weights. It's usually more efficient because\n         # attention weights are typically a bigger tensor compared to query."
        },
        {
            "sha": "102b15a5fbdd854d8f8f7073b820a74069342ea1",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -202,17 +202,28 @@ def __init__(self, config: Dinov2Config) -> None:\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        batch_size, seq_length, _ = hidden_states.shape\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "0c09e2f75d1807aaa9d2ed3a9ab7c12313eeb709",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -223,17 +223,28 @@ def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        batch_size, seq_length, _ = hidden_states.shape\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "29aa4b1961023583af63f5ecbe2bddec9f056cad",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -494,8 +494,6 @@ class DogePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = False\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n     _supports_static_cache = False\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "665aa1b85db0abc45157ee817a9e32271e3d4ea3",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -417,8 +417,7 @@ class Dots1PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "fd9fd489938bcadc4f41f48940885a7121a95a01",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -326,17 +326,28 @@ def __init__(self, config: DPTConfig) -> None:\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        batch_size, seq_length, _ = hidden_states.shape\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "c22b47c55ad4af20bdfcb53b62f839b0a2e8534b",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 108,
            "deletions": 110,
            "changes": 218,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -39,11 +40,8 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    ModelOutput,\n-    auto_docstring,\n-    logging,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_electra import ElectraConfig\n \n \n@@ -200,7 +198,7 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Electra\n class ElectraSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -225,66 +223,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -326,11 +333,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n@@ -355,10 +358,12 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Electra,BERT->ELECTRA\n class ElectraAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = ELECTRA_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = ElectraSelfOutput(config)\n         self.pruned_heads = set()\n@@ -381,24 +386,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -438,17 +446,17 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Electra\n class ElectraLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = ElectraAttention(config)\n+        self.attention = ElectraAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = ElectraAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = ElectraAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n         self.intermediate = ElectraIntermediate(config)\n         self.output = ElectraOutput(config)\n \n@@ -459,62 +467,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -525,10 +516,10 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Electra\n class ElectraEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([ElectraLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([ElectraLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -543,6 +534,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -555,27 +547,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -584,12 +583,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -598,7 +600,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -750,8 +752,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(input_shape, device=device)\n@@ -1574,15 +1581,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM._reorder_cache\n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"ElectraForCausalLM\","
        },
        {
            "sha": "6633abc494616468dddf9739ba669af21139d077",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1097,8 +1097,7 @@ class Emu3PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n     _supports_flex_attn = True"
        },
        {
            "sha": "38c7e2197beebc13af64056158649d4a3142e1e3",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -607,9 +607,5 @@ def resize_token_embeddings(self, *args, **kwargs):\n             \" model.decoder.resize_token_embeddings(...))\"\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        # apply decoder cache reordering here\n-        return self.decoder._reorder_cache(past_key_values, beam_idx)\n-\n \n __all__ = [\"EncoderDecoderModel\"]"
        },
        {
            "sha": "a5d55bafd75492b073d686921020addaa4ef8337",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 107,
            "deletions": 106,
            "changes": 213,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -41,6 +42,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_ernie import ErnieConfig\n \n \n@@ -125,7 +127,7 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Ernie\n class ErnieSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -150,66 +152,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -251,11 +262,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->Ernie\n@@ -280,10 +287,12 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Ernie,BERT->ERNIE\n class ErnieAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = ERNIE_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = ErnieSelfOutput(config)\n         self.pruned_heads = set()\n@@ -306,24 +315,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -363,17 +375,17 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Ernie\n class ErnieLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = ErnieAttention(config)\n+        self.attention = ErnieAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = ErnieAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = ErnieAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n         self.intermediate = ErnieIntermediate(config)\n         self.output = ErnieOutput(config)\n \n@@ -384,62 +396,44 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n-\n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -450,10 +444,10 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Ernie\n class ErnieEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([ErnieLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([ErnieLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -468,6 +462,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -480,27 +475,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -509,12 +511,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -523,7 +528,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -767,8 +772,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -1061,15 +1071,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel._reorder_cache\n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring\n class ErnieForMaskedLM(ErniePreTrainedModel):"
        },
        {
            "sha": "c9388e588139c042cc26e03a2e48ff19574cb9b8",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -255,7 +255,7 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n \n \n class EsmSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.config = config\n \n@@ -285,6 +285,7 @@ def __init__(self, config, position_embedding_type=None):\n             self.rotary_embeddings = RotaryEmbedding(dim=self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n     @deprecate_kwarg(\"past_key_value\", version=\"4.54.0\")\n     def forward(\n@@ -390,8 +391,8 @@ class EsmFlashAttention2(EsmSelfAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    def __init__(self, config, position_embedding_type=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type)\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n@@ -504,9 +505,9 @@ def forward(\n \n \n class EsmAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n-        self.self = ESM_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.self = ESM_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n         self.output = EsmSelfOutput(config)\n         self.pruned_heads = set()\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -539,6 +540,7 @@ def forward(\n         encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         hidden_states_ln = self.LayerNorm(hidden_states)\n         self_outputs = self.self(\n@@ -604,6 +606,7 @@ def forward(\n         encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         self_attention_outputs = self.attention(\n             hidden_states,\n@@ -676,6 +679,7 @@ def forward(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n+        cache_position=None,\n     ):\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None"
        },
        {
            "sha": "8e03e28c0d11087d3feaf0c971b43760e081e354",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 31,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -642,8 +642,7 @@ class FalconPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"FalconDecoderLayer\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n \n     def __init__(self, *inputs, **kwargs):\n@@ -727,7 +726,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1029,7 +1028,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1086,30 +1085,6 @@ def forward(\n             attentions=transformer_outputs.attentions,\n         )\n \n-    def _reorder_cache(\n-        self, past: tuple[tuple[torch.Tensor, torch.Tensor], ...], beam_idx: torch.LongTensor\n-    ) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-\n-        Output shares the same memory storage as `past`.\n-        \"\"\"\n-\n-        # Get a copy of `beam_idx` on all the devices where we need those indices.\n-        device_to_beam_idx = {\n-            past_state.device: beam_idx.to(past_state.device) for layer_past in past for past_state in layer_past\n-        }\n-        reordered_past = tuple(\n-            (\n-                layer_past[0].index_select(0, device_to_beam_idx[layer_past[0].device]),\n-                layer_past[1].index_select(0, device_to_beam_idx[layer_past[0].device]),\n-            )\n-            for layer_past in past\n-        )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1151,7 +1126,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1277,7 +1252,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1356,7 +1331,7 @@ def forward(\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as"
        },
        {
            "sha": "a4ab2fe8d16db82564d9844272ebcbbfe42e0774",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1152,7 +1152,6 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True  # Note: only supports FalconHybridMambaAttentionDynamicCache\n     _is_stateful = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "bd4f63375be30925c06f3afd0a0664cd9e32bfa4",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -930,7 +930,6 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True  # Note: only supports FalconHybridMambaAttentionDynamicCache\n     _is_stateful = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "7699d3f31fc68255aad4ab5c984285b6b501094c",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 51,
            "deletions": 51,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import gelu, get_activation\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -87,6 +88,7 @@ def __init__(self, n_heads, dim, config):\n         self.layer_id = next(MultiHeadAttention.NEW_ID)\n         self.dim = dim\n         self.n_heads = n_heads\n+        self.head_dim = dim // n_heads\n         self.dropout = config.attention_dropout\n         assert self.dim % self.n_heads == 0\n \n@@ -111,50 +113,57 @@ def prune_heads(self, heads):\n         self.dim = attention_head_size * self.n_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def forward(self, input, mask, kv=None, cache=None, head_mask=None, output_attentions=False):\n+    def forward(\n+        self,\n+        input,\n+        mask,\n+        kv=None,\n+        cache=None,\n+        head_mask=None,\n+        output_attentions=False,\n+        cache_position=None,\n+    ):\n         \"\"\"\n         Self-attention (if kv is None) or attention over source sentence (provided by kv).\n         \"\"\"\n         # Input is (bs, qlen, dim)\n         # Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n         bs, qlen, dim = input.size()\n-        if kv is None:\n-            klen = qlen if cache is None else cache[\"slen\"] + qlen\n-        else:\n-            klen = kv.size(1)\n-        # assert dim == self.dim, f'Dimensions do not match: {dim} input vs {self.dim} configured'\n-        n_heads = self.n_heads\n-        dim_per_head = self.dim // n_heads\n-        mask_reshape = (bs, 1, qlen, klen) if mask.dim() == 3 else (bs, 1, 1, klen)\n-\n-        def shape(x):\n-            \"\"\"projection\"\"\"\n-            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n-\n-        def unshape(x):\n-            \"\"\"compute context\"\"\"\n-            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n-\n-        q = shape(self.q_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n-        if kv is None:\n-            k = shape(self.k_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n-            v = shape(self.v_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n-        elif cache is None or self.layer_id not in cache:\n-            k = v = kv\n-            k = shape(self.k_lin(k))  # (bs, n_heads, qlen, dim_per_head)\n-            v = shape(self.v_lin(v))  # (bs, n_heads, qlen, dim_per_head)\n+        is_cross_attention = kv is not None\n+        mask_reshape = (bs, 1, qlen, -1) if mask.dim() == 3 else (bs, 1, 1, -1)\n \n+        q = self.q_lin(input).view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n         if cache is not None:\n-            if self.layer_id in cache:\n-                if kv is None:\n-                    k_, v_ = cache[self.layer_id]\n-                    k = torch.cat([k_, k], dim=2)  # (bs, n_heads, klen, dim_per_head)\n-                    v = torch.cat([v_, v], dim=2)  # (bs, n_heads, klen, dim_per_head)\n+            if isinstance(cache, EncoderDecoderCache):\n+                is_updated = cache.is_updated.get(self.layer_id)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = cache.cross_attention_cache\n                 else:\n-                    k, v = cache[self.layer_id]\n-            cache[self.layer_id] = (k, v)\n+                    curr_past_key_value = cache.self_attention_cache\n+            else:\n+                curr_past_key_value = cache\n \n-        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, dim_per_head)\n+        current_states = kv if is_cross_attention else input\n+        if is_cross_attention and cache is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            k = curr_past_key_value.key_cache[self.layer_id]\n+            v = curr_past_key_value.value_cache[self.layer_id]\n+        else:\n+            k = self.k_lin(current_states)\n+            v = self.v_lin(current_states)\n+            k = k.view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n+            v = v.view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n+\n+            if cache is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                k, v = curr_past_key_value.update(k, v, self.layer_id, {\"cache_position\": cache_position})\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    cache.is_updated[self.layer_id] = True\n+\n+        q = q / math.sqrt(self.head_dim)  # (bs, n_heads, qlen, head_dim)\n         scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, qlen, klen)\n         mask = (mask == 0).view(mask_reshape).expand_as(scores)  # (bs, n_heads, qlen, klen)\n         scores.masked_fill_(mask, torch.finfo(scores.dtype).min)  # (bs, n_heads, qlen, klen)\n@@ -166,8 +175,8 @@ def unshape(x):\n         if head_mask is not None:\n             weights = weights * head_mask\n \n-        context = torch.matmul(weights, v)  # (bs, n_heads, qlen, dim_per_head)\n-        context = unshape(context)  # (bs, qlen, dim)\n+        context = torch.matmul(weights, v)  # (bs, n_heads, qlen, head_dim)\n+        context = context.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.head_dim)\n \n         outputs = (self.out_lin(context),)\n         if output_attentions:\n@@ -814,6 +823,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -848,6 +858,9 @@ def forward(\n \n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n+        if not isinstance(cache, Cache):\n+            cache = EncoderDecoderCache.from_legacy_cache(cache)\n+\n         if lengths is None:\n             if input_ids is not None:\n                 lengths = (input_ids != self.pad_index).sum(dim=1).long()\n@@ -893,7 +906,7 @@ def forward(\n \n         # do not recompute cached elements\n         if cache is not None and input_ids is not None:\n-            _slen = slen - cache[\"slen\"]\n+            _slen = slen - cache.get_seq_length()\n             input_ids = input_ids[:, -_slen:]\n             position_ids = position_ids[:, -_slen:]\n             if langs is not None:\n@@ -935,6 +948,7 @@ def forward(\n                     cache=cache,\n                     head_mask=head_mask[i],\n                     output_attentions=output_attentions,\n+                    cache_position=cache_position,\n                 )\n                 attn = attn_outputs[0]\n                 if output_attentions:\n@@ -951,13 +965,6 @@ def forward(\n                 attn = nn.functional.dropout(attn, p=self.dropout, training=self.training)\n                 tensor = tensor + attn\n \n-            # encoder attention (for decoder only)\n-            # if self.is_decoder and src_enc is not None:\n-            #     attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)\n-            #     attn = nn.functional.dropout(attn, p=self.dropout, training=self.training)\n-            #     tensor = tensor + attn\n-            #     tensor = self.layer_norm15[i](tensor)\n-\n             # FFN\n             if not self.pre_norm:\n                 tensor = tensor + self.ffns[i](tensor)\n@@ -972,13 +979,6 @@ def forward(\n         if output_hidden_states:\n             hidden_states = hidden_states + (tensor,)\n \n-        # update cache length\n-        if cache is not None:\n-            cache[\"slen\"] += tensor.size(1)\n-\n-        # move back sequence length to dimension 0\n-        # tensor = tensor.transpose(0, 1)\n-\n         if not return_dict:\n             return tuple(v for v in [tensor, hidden_states, attentions] if v is not None)\n "
        },
        {
            "sha": "64a61e66b520b89335b761ac0d34984d5d6db125",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -442,23 +442,29 @@ def __init__(self, config: FlavaPossibleConfigs) -> None:\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))"
        },
        {
            "sha": "52f3d027c80cbdc36aa16b52e68903b6478fd815",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 83,
            "deletions": 108,
            "changes": 191,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -35,6 +35,7 @@\n from torch.nn import CrossEntropyLoss, LayerNorm\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_outputs import (\n@@ -452,14 +453,15 @@ def forward(\n \n \n class DecoderLayer(nn.Module):\n-    def __init__(self, config: FSMTConfig):\n+    def __init__(self, config: FSMTConfig, layer_idx=None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n         self.self_attn = Attention(\n             embed_dim=self.embed_dim,\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -471,6 +473,7 @@ def __init__(self, config: FSMTConfig):\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n             encoder_decoder_attention=True,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -488,12 +491,10 @@ def forward(\n         cross_attn_layer_head_mask=None,\n         decoder_padding_mask=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         residual = x\n \n-        if layer_state is None:\n-            layer_state = {}\n-\n         # Self Attention\n         x, self_attn_weights = self.self_attn(\n             query=x,\n@@ -503,6 +504,7 @@ def forward(\n             attn_mask=causal_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n         x = residual + x\n@@ -518,6 +520,7 @@ def forward(\n             layer_state=layer_state,  # mutates layer state\n             layer_head_mask=cross_attn_layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n         x = residual + x\n@@ -534,9 +537,8 @@ def forward(\n         return (\n             x,\n             self_attn_weights,\n-            layer_state,\n             cross_attn_weights,\n-        )  # layer_state = cache for decoding\n+        )\n \n \n class FSMTDecoder(nn.Module):\n@@ -559,7 +561,7 @@ def __init__(self, config: FSMTConfig, embed_tokens: nn.Embedding):\n         self.embed_positions = SinusoidalPositionalEmbedding(\n             config.max_position_embeddings + self.padding_idx + 1, embed_dim, self.padding_idx\n         )\n-        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])  # type: list[DecoderLayer]\n+        self.layers = nn.ModuleList([DecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])  # type: list[DecoderLayer]\n \n         if is_deepspeed_zero3_enabled():\n             import deepspeed\n@@ -585,10 +587,11 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[list[torch.FloatTensor]] = None,\n-        use_cache: bool = False,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n+        use_cache: Optional[bool] = False,\n+        output_attentions: Optional[bool] = False,\n+        output_hidden_states: Optional[bool] = False,\n+        return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         \"\"\"\n         Includes several features from \"Jointly Learning to Align and Translate with Transformer Models\" (Garg et al.,\n@@ -645,6 +648,17 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         x += positions\n         x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n \n@@ -656,7 +670,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attns = () if output_attentions else None\n-        next_decoder_cache = []\n \n         # check if head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -676,23 +689,19 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            layer_state = past_key_values[idx] if past_key_values is not None else None\n-\n-            x, layer_self_attn, layer_past, layer_cross_attn = decoder_layer(\n+            x, layer_self_attn, layer_cross_attn = decoder_layer(\n                 x,\n                 encoder_hidden_states,\n                 encoder_attn_mask=encoder_padding_mask,\n                 decoder_padding_mask=decoder_padding_mask,\n-                layer_state=layer_state,\n+                layer_state=past_key_values,\n                 causal_mask=decoder_causal_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n-            if use_cache:\n-                next_decoder_cache.append(layer_past.copy())\n-\n             if output_attentions:\n                 all_self_attns += (layer_self_attn,)\n                 all_cross_attns += (layer_cross_attn,)\n@@ -709,15 +718,16 @@ def forward(\n \n         x = self.output_projection(x)\n \n-        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n-                v for v in [x, next_cache, all_hidden_states, all_self_attns, all_cross_attns] if v is not None\n+                v for v in [x, past_key_values, all_hidden_states, all_self_attns, all_cross_attns] if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=x,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attns,\n@@ -741,6 +751,7 @@ def __init__(\n         dropout=0.0,\n         bias=True,\n         encoder_decoder_attention=False,  # otherwise self_attention\n+        layer_idx=None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -749,6 +760,7 @@ def __init__(\n         self.head_dim = embed_dim // num_heads\n         assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n         self.scaling = self.head_dim**-0.5\n+        self.layer_idx = layer_idx\n \n         self.encoder_decoder_attention = encoder_decoder_attention\n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n@@ -757,64 +769,65 @@ def __init__(\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.cache_key = \"encoder_decoder\" if self.encoder_decoder_attention else \"self\"\n \n-    def _shape(self, tensor, seq_len, bsz):\n-        return tensor.contiguous().view(seq_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n-\n     def forward(\n         self,\n         query,\n         key: Optional[Tensor],\n         key_padding_mask: Optional[Tensor] = None,\n-        layer_state: Optional[dict[str, Optional[Tensor]]] = None,\n+        layer_state: Optional[Cache] = None,\n         attn_mask: Optional[Tensor] = None,\n         layer_head_mask: Optional[Tensor] = None,\n-        output_attentions=False,\n+        output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[Tensor, Optional[Tensor]]:\n         \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\"\n-        static_kv: bool = self.encoder_decoder_attention\n         tgt_len, bsz, embed_dim = query.size()\n         assert embed_dim == self.embed_dim\n         assert list(query.size()) == [tgt_len, bsz, embed_dim]\n-        # get here for encoder decoder cause of static_kv\n-        if layer_state is not None:  # reuse k,v and encoder_padding_mask\n-            saved_state = layer_state.get(self.cache_key, {})\n-            if \"prev_key\" in saved_state and static_kv:\n-                # previous time steps are cached - no need to recompute key and value if they are static\n-                key = None\n-        else:\n-            saved_state = None\n-            layer_state = {}\n \n-        q = self.q_proj(query) * self.scaling\n-        if static_kv:\n-            if key is None:\n-                k = v = None\n+        if layer_state is not None:\n+            if isinstance(layer_state, EncoderDecoderCache):\n+                is_updated = layer_state.is_updated.get(self.layer_idx)\n+                if self.encoder_decoder_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = layer_state.cross_attention_cache\n+                else:\n+                    curr_past_key_value = layer_state.self_attention_cache\n             else:\n-                k = self.k_proj(key)\n-                v = self.v_proj(key)\n+                curr_past_key_value = layer_state\n+\n+        # NOTE: FSMT has format (seq_len, BS, model_dim) ofr inputs\n+        current_states = key if self.encoder_decoder_attention else query\n+        if self.encoder_decoder_attention and layer_state is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            k = self.k_proj(query)\n-            v = self.v_proj(query)\n-\n-        q = self._shape(q, tgt_len, bsz)\n-        if k is not None:\n-            k = self._shape(k, -1, bsz)\n-        if v is not None:\n-            v = self._shape(v, -1, bsz)\n-\n-        if saved_state is not None:\n-            k, v, key_padding_mask = self._use_saved_state(k, v, saved_state, key_padding_mask, static_kv, bsz)\n-\n-        # Update cache\n-        layer_state[self.cache_key] = {\n-            \"prev_key\": k.view(bsz, self.num_heads, -1, self.head_dim),\n-            \"prev_value\": v.view(bsz, self.num_heads, -1, self.head_dim),\n-            \"prev_key_padding_mask\": key_padding_mask if not static_kv else None,\n-        }\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(-1, bsz, self.num_heads, self.head_dim).permute(1, 2, 0, 3)\n+            value_states = value_states.view(-1, bsz, self.num_heads, self.head_dim).permute(1, 2, 0, 3)\n+\n+            if layer_state is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not self.encoder_decoder_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if self.encoder_decoder_attention:\n+                    layer_state.is_updated[self.layer_idx] = True\n \n-        assert k is not None\n-        src_len = k.size(1)\n-        attn_weights = torch.bmm(q, k.transpose(1, 2))\n+        query_states = self.q_proj(query) * self.scaling\n+\n+        # Reshape back to 3D tensors for `bmm`\n+        query_states = query_states.view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n+        key_states = key_states.reshape(bsz * self.num_heads, -1, self.head_dim)\n+        value_states = value_states.reshape(bsz * self.num_heads, -1, self.head_dim)\n+\n+        assert key_states is not None\n+        src_len = key_states.size(1)\n+        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n         assert attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len)\n \n         if attn_mask is not None:\n@@ -857,45 +870,14 @@ def forward(\n             training=self.training,\n         )\n \n-        assert v is not None\n-        attn_output = torch.bmm(attn_probs, v)\n+        assert value_states is not None\n+        attn_output = torch.bmm(attn_probs, value_states)\n         assert attn_output.size() == (bsz * self.num_heads, tgt_len, self.head_dim)\n         attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights_reshaped\n \n-    def _use_saved_state(self, k, v, saved_state, key_padding_mask, static_kv, bsz):\n-        # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n-        if \"prev_key\" in saved_state:\n-            _prev_key = saved_state[\"prev_key\"]\n-            assert _prev_key is not None\n-            prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)\n-            if static_kv:\n-                k = prev_key\n-            else:\n-                assert k is not None\n-                k = torch.cat([prev_key, k], dim=1)\n-        if \"prev_value\" in saved_state:\n-            _prev_value = saved_state[\"prev_value\"]\n-            assert _prev_value is not None\n-            prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)\n-            if static_kv:\n-                v = prev_value\n-            else:\n-                assert v is not None\n-                v = torch.cat([prev_value, v], dim=1)\n-        assert k is not None and v is not None\n-        prev_key_padding_mask: Optional[Tensor] = saved_state.get(\"prev_key_padding_mask\", None)\n-        if prev_key_padding_mask is not None:\n-            if static_kv:\n-                new_key_padding_mask = prev_key_padding_mask\n-            else:\n-                new_key_padding_mask = torch.cat([prev_key_padding_mask, key_padding_mask], dim=1)\n-        else:\n-            new_key_padding_mask = key_padding_mask\n-        return k, v, new_key_padding_mask\n-\n \n def fill_with_neg_inf(t):\n     \"\"\"FP16-compatible function that fills a input_ids with -inf.\"\"\"\n@@ -953,6 +935,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1033,6 +1016,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1098,6 +1082,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1161,6 +1146,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = outputs[0]\n \n@@ -1189,17 +1175,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = []\n-        for layer_past in past_key_values:\n-            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n-            layer_past_new = {\n-                attn_key: _reorder_buffer(attn_cache, beam_idx) for attn_key, attn_cache in layer_past.items()\n-            }\n-            reordered_past.append(layer_past_new)\n-        return reordered_past\n-\n     def get_encoder(self):\n         return self.model.encoder\n "
        },
        {
            "sha": "57e256c8fab394c9b8a89f85a26c016cb2ce5932",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -41,7 +41,6 @@ class FuyuPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n     _no_split_modules = []\n     _skip_keys_device_placement = \"past_key_values\"\n \n@@ -390,14 +389,5 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\"FuyuForCausalLM\", \"FuyuPreTrainedModel\", \"FuyuModel\"]"
        },
        {
            "sha": "2a5c08f1b1d1a9cdfceec644c0e3bf5760c31bd6",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -313,8 +313,7 @@ class GemmaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "3a45c3275abf051d2247c207fabe6d9f6958ba33",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -343,8 +343,7 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "5f939c8248a5f1e64ed84dadd502438295ff2404",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -433,8 +433,7 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "c63f9b31c04e12d9e73970ec8bb7115e4456cce9",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1489,8 +1489,7 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "c6b1aa1f88dbabfec2a248849e2519592830cd2e",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 17,
            "deletions": 21,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -151,11 +151,6 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.max_position_embeddings = config.max_position_embeddings\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -165,11 +160,24 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         pixel_values_present: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         cutoff = self.image_patch_tokens if pixel_values_present else 0\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n         if past_key_value is not None:\n             # NOTE: like in other caches, we store the text component. In GIT it means we discard the image component.\n             key_layer_past, value_layer_past = past_key_value.update(\n@@ -178,8 +186,6 @@ def forward(\n             key_layer = torch.cat([key_layer[:, :, :cutoff, :], key_layer_past], dim=2)\n             value_layer = torch.cat([value_layer[:, :, :cutoff, :], value_layer_past], dim=2)\n \n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n@@ -450,8 +456,6 @@ class GitPreTrainedModel(PreTrainedModel):\n     config_class = GitConfig\n     base_model_prefix = \"git\"\n     supports_gradient_checkpointing = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -1095,7 +1099,7 @@ def forward(\n         past_key_values_length = 0\n         if past_key_values is not None:\n             past_key_values_length = (\n-                past_key_values[0][0].shape[2]\n+                past_key_values.get_seq_length()\n                 if not isinstance(past_key_values, Cache)\n                 else past_key_values.get_seq_length()\n             )\n@@ -1452,13 +1456,5 @@ def prepare_inputs_for_generation(\n             \"use_cache\": use_cache,\n         }\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\"GitForCausalLM\", \"GitModel\", \"GitPreTrainedModel\", \"GitVisionModel\"]"
        },
        {
            "sha": "147ccde41adb019e6a04661f06b967211a86a512",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -330,8 +330,7 @@ class GlmPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "b1c6421fe16e8b1181d2e649ca65883b5a5c977f",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -334,8 +334,7 @@ class Glm4PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "2e8a4149d76bc73c8feec99bece70b9b773ce546",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -406,7 +406,7 @@ class Glm4vPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "b21d2f14d765ce2829ba6dc11d853826900f7845",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -126,19 +126,19 @@ def __init__(self, config, hidden_size, num_attention_heads, sequence_reduction_\n             )\n             self.layer_norm = nn.LayerNorm(hidden_size)\n \n-    def transpose_for_scores(self, hidden_states):\n-        new_shape = hidden_states.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        hidden_states = hidden_states.view(new_shape)\n-        return hidden_states.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states,\n         height,\n         width,\n         output_attentions=False,\n     ):\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         if self.sr_ratio > 1:\n             batch_size, seq_len, num_channels = hidden_states.shape\n@@ -150,8 +150,16 @@ def forward(\n             hidden_states = hidden_states.reshape(batch_size, num_channels, -1).permute(0, 2, 1)\n             hidden_states = self.layer_norm(hidden_states)\n \n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))"
        },
        {
            "sha": "f11f12cd5409c39f970853ffcaef8f7d09c7db49",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -280,10 +280,10 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "78cc233809720ed79c9d311b5275bad314cf86fa",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 21,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -562,7 +562,7 @@ class GPT2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_attention_backend = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def __init__(self, *inputs, **kwargs):\n@@ -785,7 +785,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1169,7 +1169,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1323,7 +1323,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1424,20 +1424,6 @@ def forward(\n             attentions=transformer_outputs.attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1486,7 +1472,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1619,7 +1605,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1705,7 +1691,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as"
        },
        {
            "sha": "127a0eed4732c15ef565a306a1a25f86b4e51ce4",
            "filename": "src/transformers/models/gpt_bigcode/configuration_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fconfiguration_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fconfiguration_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fconfiguration_gpt_bigcode.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -134,6 +134,7 @@ def __init__(\n         self.attention_softmax_in_fp32 = attention_softmax_in_fp32\n         self.scale_attention_softmax_in_fp32 = scale_attention_softmax_in_fp32\n         self.multi_query = multi_query\n+        self.num_key_value_heads = 1 if multi_query else n_head\n \n         self.bos_token_id = bos_token_id\n         self.eos_token_id = eos_token_id"
        },
        {
            "sha": "aae9a3e9b027e7cc459d4f6659c08e96fdf2ce32",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 167,
            "deletions": 583,
            "changes": 750,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -14,35 +14,35 @@\n \"\"\"PyTorch GPTBigCode model.\"\"\"\n \n import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n-from ...modeling_layers import GradientCheckpointingLayer\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import is_torch_greater_or_equal_than_2_2\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     auto_docstring,\n+    can_return_tuple,\n     logging,\n )\n from .configuration_gpt_bigcode import GPTBigCodeConfig\n \n \n if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n+    pass\n \n \n logger = logging.get_logger(__name__)\n@@ -78,6 +78,49 @@ def masked_softmax(x: torch.Tensor, mask: torch.Tensor, mask_value: torch.Tensor\n     return x\n \n \n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class GPTBigCodeAttention(nn.Module):\n     def __init__(self, config, is_cross_attention=False, layer_idx=None):\n         super().__init__()\n@@ -90,6 +133,7 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n         self.head_dim = self.embed_dim // self.num_heads\n         self.kv_heads = 1 if self.multi_query else self.num_heads\n         self.kv_dim = self.kv_heads * self.head_dim\n+        self.num_key_value_groups = self.num_heads // self.kv_heads\n         self.split_size = self.embed_dim\n         self.is_causal = True\n \n@@ -100,6 +144,7 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n             )\n \n         self.scale_attn_weights = config.scale_attn_weights\n+        self.scaling = self.head_dim**0.5 if config.scale_attn_weights else 1.0\n         self.is_cross_attention = is_cross_attention\n \n         self.layer_idx = layer_idx\n@@ -120,418 +165,93 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n \n         self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n-        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n+        self.attn_dropout = config.attn_pdrop\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n \n-    def _get_mask_value(self, device, dtype):\n-        # torch.where expects a tensor. We use a cache to avoid recreating it every time.\n-        if self.mask_value is None or self.mask_value.dtype != dtype or self.mask_value.device != device:\n-            self.mask_value = torch.full([], torch.finfo(dtype).min, dtype=dtype, device=device)\n-        return self.mask_value\n-\n-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n-        dtype = query.dtype\n-        softmax_dtype = torch.float32 if self.attention_softmax_in_fp32 else dtype\n-        upcast = dtype != softmax_dtype\n-\n-        unscale = self.layer_idx + 1 if self.scale_attention_softmax_in_fp32 and upcast else 1\n-        scale_factor = unscale**-1\n-        if self.scale_attn_weights:\n-            scale_factor /= self.head_dim**0.5\n-\n-        # MQA models: (batch_size, query_length, num_heads * head_dim)\n-        # MHA models: (batch_size, num_heads, query_length, head_dim)\n-        query_shape = query.shape\n-        batch_size = query_shape[0]\n-        key_length = key.size(-1)\n-        if self.multi_query:\n-            # (batch_size, query_length, num_heads, head_dim) x (batch_size, head_dim, key_length)\n-            # -> (batch_size, query_length, num_heads, key_length)\n-            query_length = query_shape[1]\n-            attn_shape = (batch_size, query_length, self.num_heads, key_length)\n-            attn_view = (batch_size, query_length * self.num_heads, key_length)\n-            # No copy needed for MQA 2, or when layer_past is provided.\n-            query = query.reshape(batch_size, query_length * self.num_heads, self.head_dim)\n-        else:\n-            # (batch_size, num_heads, query_length, head_dim) x (batch_size, num_heads, head_dim, key_length)\n-            # -> (batch_size, num_heads, query_length, key_length)\n-            query_length = query_shape[2]\n-            attn_shape = (batch_size, self.num_heads, query_length, key_length)\n-            attn_view = (batch_size * self.num_heads, query_length, key_length)\n-            # Always copies\n-            query = query.reshape(batch_size * self.num_heads, query_length, self.head_dim)\n-            # No copy when layer_past is provided.\n-            key = key.reshape(batch_size * self.num_heads, self.head_dim, key_length)\n-\n-        attn_weights = torch.empty(attn_view, device=query.device, dtype=query.dtype)\n-        if query.device.type == \"cpu\":\n-            # This is needed because of a bug in pytorch https://github.com/pytorch/pytorch/issues/80588.\n-            # The bug was fixed in https://github.com/pytorch/pytorch/pull/96086,\n-            # but the fix has not been released as of pytorch version 2.0.0.\n-            attn_weights = torch.zeros_like(attn_weights)\n-            beta = 1\n-        else:\n-            beta = 0\n-        attn_weights = torch.baddbmm(attn_weights, query, key, beta=beta, alpha=scale_factor).view(attn_shape)\n-\n-        if upcast:\n-            # Use a fused kernel to prevent a large overhead from casting and scaling.\n-            # Sub-optimal when the key length is not a multiple of 8.\n-            if attention_mask is None:\n-                attn_weights = upcast_softmax(attn_weights, unscale, softmax_dtype)\n-            else:\n-                mask_value = self._get_mask_value(attn_weights.device, softmax_dtype)\n-                attn_weights = upcast_masked_softmax(attn_weights, attention_mask, mask_value, unscale, softmax_dtype)\n-        else:\n-            if attention_mask is not None:\n-                mask_value = self._get_mask_value(attn_weights.device, softmax_dtype)\n-\n-                # The fused kernel is very slow when the key length is not a multiple of 8, so we skip fusion.\n-                attn_weights = torch.where(attention_mask, attn_weights, mask_value)\n-\n-            attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)\n-\n-        attn_weights = self.attn_dropout(attn_weights)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            if self.multi_query:\n-                head_mask = head_mask.transpose(1, 2)\n-            attn_weights = attn_weights * head_mask\n-\n-        if self.multi_query:\n-            attn_output = torch.bmm(attn_weights.view(attn_view), value).view(query_shape)\n-        else:\n-            attn_output = torch.matmul(attn_weights, value)\n-\n-        return attn_output, attn_weights\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        layer_past: Optional[torch.Tensor] = None,\n+        layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[\n         tuple[torch.Tensor, Optional[torch.Tensor]],\n         tuple[torch.Tensor, Optional[torch.Tensor], tuple[torch.Tensor, ...]],\n     ]:\n-        if encoder_hidden_states is not None:\n-            if not hasattr(self, \"q_attn\") or not self.is_cross_attention:\n-                raise ValueError(\n-                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n-                    \"Please make sure to instantiate class with `GPTBigCodeAttention(..., is_cross_attention=True)`.\"\n-                )\n-\n-            query = self.q_attn(hidden_states)\n-            key_value = self.c_attn(encoder_hidden_states)\n-            attention_mask = encoder_attention_mask\n-        elif self.multi_query:\n-            query, key_value = self.c_attn(hidden_states).split((self.embed_dim, 2 * self.kv_dim), dim=2)\n-        else:\n-            # Note: We split as (self.num_heads, 3, self.head_dim) instead of (3, self.num_heads, self.head_dim),\n-            # i.e., the memory layout is not the same as GPT2.\n-            # This makes the concatenation with past_key_value more efficient.\n-            query, key_value = (\n-                self.c_attn(hidden_states)\n-                .view(*hidden_states.shape[:2], self.num_heads, 3 * self.head_dim)\n-                .transpose(1, 2)\n-                .split((self.head_dim, 2 * self.head_dim), dim=3)\n-            )\n+        input_shape = hidden_states.shape[:-1]\n \n         if layer_past is not None:\n-            key_value = torch.cat((layer_past, key_value), dim=-2)\n-        present = key_value if use_cache else None\n-\n-        key, value = key_value.split((self.head_dim, self.head_dim), dim=-1)\n-\n-        attn_output, attn_weights = self._attn(query, key.transpose(-1, -2), value, attention_mask, head_mask)\n-\n-        if not self.multi_query:\n-            attn_output = attn_output.transpose(1, 2).reshape(hidden_states.shape)\n-        attn_output = self.c_proj(attn_output)\n-        attn_output = self.resid_dropout(attn_output)\n-\n-        outputs = (attn_output, present)\n-        if output_attentions:\n-            if self.multi_query:\n-                # Transpose to return weights in the usual format (batch_size, num_heads, query_length, key_length)\n-                attn_weights = attn_weights.transpose(1, 2)\n-            outputs += (attn_weights,)\n-\n-        return outputs  # a, present, (attentions)\n-\n-\n-class GPTBigCodeFlashAttention2(GPTBigCodeAttention):\n-    \"\"\"\n-    GPTBigCode flash attention module. This module inherits from `GPTBigCodeAttention` as the weights of the module\n-    stays untouched. The only required change would be on the forward pass where it needs to correctly call the public\n-    API of flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n+            if isinstance(layer_past, EncoderDecoderCache):\n+                is_updated = layer_past.is_updated.get(self.layer_idx)\n+                if self.is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = layer_past.cross_attention_cache\n+                else:\n+                    curr_past_key_value = layer_past.self_attention_cache\n+            else:\n+                curr_past_key_value = layer_past\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        layer_past: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-    ) -> Union[\n-        tuple[torch.Tensor, Optional[torch.Tensor]],\n-        tuple[torch.Tensor, Optional[torch.Tensor], tuple[torch.Tensor, ...]],\n-    ]:\n-        if encoder_hidden_states is not None:\n+        if self.is_cross_attention:\n             if not hasattr(self, \"q_attn\") or not self.is_cross_attention:\n                 raise ValueError(\n                     \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                     \"Please make sure to instantiate class with `GPTBigCodeAttention(..., is_cross_attention=True)`.\"\n                 )\n-\n-            query = self.q_attn(hidden_states)\n-            key_value = self.c_attn(encoder_hidden_states)\n-            attention_mask = encoder_attention_mask\n-        elif self.multi_query:\n-            query, key_value = self.c_attn(hidden_states).split((self.embed_dim, 2 * self.kv_dim), dim=2)\n-        else:\n-            # Note: We split as (self.num_heads, 3, self.head_dim) instead of (3, self.num_heads, self.head_dim),\n-            # i.e., the memory layout is not the same as GPT2.\n-            # This makes the concatenation with past_key_value more efficient.\n-            query, key_value = (\n-                self.c_attn(hidden_states)\n-                .view(*hidden_states.shape[:2], self.num_heads, 3 * self.head_dim)\n-                .transpose(1, 2)\n-                .split((self.head_dim, 2 * self.head_dim), dim=3)\n-            )\n-\n-        if layer_past is not None:\n-            key_value = torch.cat((layer_past, key_value), dim=-2)\n-        present = key_value if use_cache else None\n-\n-        key, value = key_value.split((self.head_dim, self.head_dim), dim=-1)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        if self.multi_query:\n-            batch_size, query_length, _ = query.shape\n-            query = query.reshape(batch_size, query_length, self.num_heads, self.head_dim)\n-            key = key.unsqueeze(2)\n-            value = value.unsqueeze(2)\n+            if layer_past is not None and is_updated:\n+                # reuse k,v, cross_attentions\n+                key = curr_past_key_value.key_cache[self.layer_idx]\n+                value = curr_past_key_value.value_cache[self.layer_idx]\n+            else:\n+                query = self.q_attn(hidden_states).view(*input_shape, -1, self.head_dim).transpose(1, 2)\n+                key, value = self.c_attn(encoder_hidden_states).split((self.head_dim, self.head_dim), dim=-1)\n         else:\n-            query_length = query.shape[2]\n-            batch_size, _, tgt, _ = key.shape\n-            query = query.transpose(1, 2).reshape(batch_size, query_length, self.num_heads, self.head_dim)\n-            key = key.transpose(1, 2).reshape(batch_size, tgt, self.num_heads, self.head_dim)\n-            value = value.transpose(1, 2).reshape(batch_size, tgt, self.num_heads, self.head_dim)\n-\n-        attn_dropout = self.attn_pdrop if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query.dtype\n-        device_type = query.device.type if query.device.type != \"mps\" else \"cpu\"\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = (\n-                    torch.get_autocast_dtype(device_type)\n-                    if hasattr(torch, \"get_autocast_dtype\")\n-                    else torch.get_autocast_gpu_dtype()\n+            if self.multi_query:\n+                query, key, value = (\n+                    self.c_attn(hidden_states).unsqueeze(1).split((self.embed_dim, self.kv_dim, self.kv_dim), dim=3)\n                 )\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+                query = query.view(*input_shape, -1, self.head_dim).transpose(1, 2)\n             else:\n-                target_dtype = self.c_attn.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-            query = query.to(target_dtype)\n-            key = key.to(target_dtype)\n-            value = value.to(target_dtype)\n+                query, key, value = (\n+                    self.c_attn(hidden_states)\n+                    .view(*hidden_states.shape[:2], self.num_heads, 3 * self.head_dim)\n+                    .transpose(1, 2)\n+                    .split(3 * [self.head_dim], dim=3)\n+                )\n \n-        attn_output = _flash_attention_forward(\n+        if layer_past is not None:\n+            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+            cache_position = cache_position if not self.is_cross_attention else None\n+            key, value = curr_past_key_value.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n+            # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+            if self.is_cross_attention:\n+                layer_past.is_updated[self.layer_idx] = True\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query,\n             key,\n             value,\n             attention_mask,\n-            query_length,\n-            dropout=attn_dropout,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_weights_reshaped = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n-        attn_output = self.c_proj(attn_weights_reshaped)\n-        attn_output = self.resid_dropout(attn_output)\n-\n-        outputs = (attn_output, present)\n-\n-        if output_attentions:\n-            if self.multi_query:\n-                # Transpose to return weights in the usual format (batch_size, num_heads, query_length, key_length)\n-                attn_weights_reshaped = attn_weights_reshaped.transpose(1, 2)\n-        else:\n-            attn_weights_reshaped = None\n-\n-        outputs += (attn_weights_reshaped,)\n-\n-        return outputs  # a, present, (attentions)\n-\n-\n-class GPTBigCodeSdpaAttention(GPTBigCodeAttention):\n-    def _attn(self, query, key, value, attention_mask=None):\n-        scale = None\n-        if not self.scale_attn_weights:\n-            scale = 1\n-\n-        # MQA models: (batch_size, query_length, num_heads * head_dim)\n-        # MHA models: (batch_size, num_heads, query_length, head_dim)\n-        query_shape = query.shape\n-        batch_size = query_shape[0]\n-        key.shape[-2]\n-\n-        if self.multi_query:\n-            query_length = query_shape[1]\n-\n-            # SDPA requires the dimension [..., sequence_length, head_dim].\n-            query = query.view(batch_size, query_length, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-            # Without these unsqueeze, SDPA complains as the query and key/value have a different number of dimensions.\n-            key = key.unsqueeze(1)\n-            value = value.unsqueeze(1)\n-\n-            # Although these expand are not numerically useful, PyTorch can not dispatch to memory-efficient backend\n-            # and flash attention backend (No available kernel.  Aborting execution.) from the shapes\n-            # query = [batch_size, num_heads, query_length, head_dim]\n-            # key = [batch_size, 1, past_length, head_dim]\n-            # value = [batch_size, 1, past_length, head_dim]\n-            #\n-            # torch==2.1.2 is bugged with non-contiguous inputs with custom attn_mask (https://github.com/pytorch/pytorch/issues/112577), hence the check.\n-            if is_torch_greater_or_equal_than_2_2:\n-                key = key.expand(-1, self.num_heads, -1, -1)\n-                value = value.expand(-1, self.num_heads, -1, -1)\n-        else:\n-            query_length = query_shape[-1]\n-\n-            # See the comment above.\n-            if query.device.type == \"cuda\" and attention_mask is not None:\n-                query = query.contiguous()\n-                key = key.contiguous()\n-                value = value.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The query_length > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not\n-        # create a causal mask in case query_length == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and query_length > 1 else False\n-\n-        sdpa_result = torch.nn.functional.scaled_dot_product_attention(\n-            query,\n-            key,\n-            value,\n-            attn_mask=attention_mask,\n-            dropout_p=self.attn_pdrop if self.training else 0.0,\n-            is_causal=is_causal,\n-            scale=scale,\n+            dropout=0.0 if not self.training else self.attn_dropout,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            **kwargs,\n         )\n \n-        if self.multi_query:\n-            # (batch_size, num_heads, seq_len, head_dim) --> (batch_size, seq_len, num_heads, head_dim)\n-            sdpa_result = sdpa_result.transpose(1, 2)\n-\n-            # Reshape is kind of expensive here, as it does a memory copy,\n-            # but I did not manage to make away without it (logits do not match when using view)\n-            # (batch_size, seq_len, num_heads, head_dim) --> (batch_size, seq_len, num_heads * head_dim)\n-            sdpa_result = sdpa_result.reshape(query_shape)\n-\n-        return sdpa_result, None\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        layer_past: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-    ) -> Union[\n-        tuple[torch.Tensor, Optional[torch.Tensor]],\n-        tuple[torch.Tensor, Optional[torch.Tensor], tuple[torch.Tensor, ...]],\n-    ]:\n-        if encoder_hidden_states is not None:\n-            if not hasattr(self, \"q_attn\") or not self.is_cross_attention:\n-                raise ValueError(\n-                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n-                    \"Please make sure to instantiate class with `GPTBigCodeAttention(..., is_cross_attention=True)`.\"\n-                )\n-\n-            query = self.q_attn(hidden_states)\n-            key_value = self.c_attn(encoder_hidden_states)\n-            attention_mask = encoder_attention_mask\n-        elif self.multi_query:\n-            query, key_value = self.c_attn(hidden_states).split((self.embed_dim, 2 * self.kv_dim), dim=2)\n-        else:\n-            # Note: We split as (self.num_heads, 3, self.head_dim) instead of (3, self.num_heads, self.head_dim),\n-            # i.e., the memory layout is not the same as GPT2.\n-            # This makes the concatenation with past_key_value more efficient.\n-            query, key_value = (\n-                self.c_attn(hidden_states)\n-                .view(*hidden_states.shape[:2], self.num_heads, 3 * self.head_dim)\n-                .transpose(1, 2)\n-                .split((self.head_dim, 2 * self.head_dim), dim=3)\n-            )\n-\n-        if layer_past is not None:\n-            key_value = torch.cat((layer_past, key_value), dim=-2)\n-        present = key_value if use_cache else None\n-\n-        key, value = key_value.split((self.head_dim, self.head_dim), dim=-1)\n-\n-        if not output_attentions:\n-            # Difference with the original implementation: there is no need to transpose the key here,\n-            # as SDPA expects seq_length to be at index -2 for the key as well\n-            attn_output, attn_weights = self._attn(query, key, value, attention_mask)\n-        else:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"GPTBigCodeModel is using GPTBigCodeSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`.\"\n-                ' Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            attn_output, attn_weights = super()._attn(query, key.transpose(-1, -2), value, attention_mask)\n-\n-        if not self.multi_query:\n-            attn_output = attn_output.transpose(1, 2).reshape(hidden_states.shape)\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.c_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n-\n-        outputs = (attn_output, present)\n-        if output_attentions:\n-            if self.multi_query:\n-                # Transpose to return weights in the usual format (batch_size, num_heads, query_length, key_length)\n-                attn_weights = attn_weights.transpose(1, 2)\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return attn_output, attn_weights\n \n \n class GPTBigCodeMLP(nn.Module):\n@@ -552,32 +272,23 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n         return hidden_states\n \n \n-GPTBIGCODE_ATTENTION_CLASSES = {\n-    \"eager\": GPTBigCodeAttention,\n-    \"flash_attention_2\": GPTBigCodeFlashAttention2,\n-    \"sdpa\": GPTBigCodeSdpaAttention,\n-}\n-\n-\n-class GPTBigCodeBlock(GradientCheckpointingLayer):\n+class GPTBigCodeBlock(nn.Module):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n         self.inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n \n         self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n \n-        self.attn = GPTBIGCODE_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n+        self.attn = GPTBigCodeAttention(config, layer_idx=layer_idx)\n \n         self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n \n         if config.add_cross_attention:\n             if config.multi_query:\n                 raise NotImplementedError(\"Cross-attention not implemented for MQA\")\n \n-            self.crossattention = GPTBIGCODE_ATTENTION_CLASSES[config._attn_implementation](\n-                config, is_cross_attention=True, layer_idx=layer_idx\n-            )\n+            self.crossattention = GPTBigCodeAttention(config, is_cross_attention=True, layer_idx=layer_idx)\n \n             self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n \n@@ -586,13 +297,14 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.Tensor]],\n-        layer_past: Optional[torch.Tensor] = None,\n+        layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[\n         tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n@@ -606,6 +318,8 @@ def forward(\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n+            **kwargs,\n         )\n         attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n         outputs = attn_outputs[1:]\n@@ -628,24 +342,19 @@ def forward(\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n+                **kwargs,\n             )\n             attn_output = cross_attn_outputs[0]\n             # residual connection\n             hidden_states = residual + attn_output\n-            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n+            outputs = outputs + cross_attn_outputs[1:]  # add cross attentions if we output attention weights\n \n         residual = hidden_states\n         hidden_states = self.ln_2(hidden_states)\n         feed_forward_hidden_states = self.mlp(hidden_states)\n-        # residual connection\n         hidden_states = residual + feed_forward_hidden_states\n-\n-        if use_cache:\n-            outputs = (hidden_states,) + outputs\n-        else:\n-            outputs = (hidden_states,) + outputs[1:]\n-\n-        return outputs  # hidden_states, present, (attentions, cross_attentions)\n+        return (hidden_states,) + outputs\n \n \n @auto_docstring\n@@ -722,6 +431,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.wte = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -738,11 +448,13 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.Tensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -760,16 +472,9 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-            )\n-            use_cache = False\n-\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n         elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n             input_shape = input_ids.size()\n             input_ids = input_ids.view(-1, input_shape[-1])\n             batch_size = input_ids.shape[0]\n@@ -782,81 +487,44 @@ def forward(\n         if batch_size <= 0:\n             raise ValueError(\"batch_size has to be defined and > 0\")\n \n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if token_type_ids is not None:\n-            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n+        if inputs_embeds is None:\n+            inputs_embeds = self.wte(input_ids)\n \n-        if past_key_values is None:\n-            past_length = 0\n-            past_key_values = tuple([None] * len(self.h))\n-        else:\n-            past_length = past_key_values[0].size(-2)\n-\n-        if attention_mask is not None and len(attention_mask.shape) == 2 and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_length > 0:\n-                position_ids = position_ids[:, past_length : input_shape[-1] + past_length :]\n-        elif position_ids is None:\n-            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n-            position_ids = position_ids.unsqueeze(0)\n-\n-        # Self-attention mask.\n-        query_length = input_shape[-1]\n-        key_length = past_length + query_length\n-        self_attention_mask = self.bias[None, key_length - query_length : key_length, :key_length]\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+        )\n \n         if self._use_flash_attention_2:\n-            # 2d mask is passed through the layers\n-            attention_mask = attention_mask.bool() if (attention_mask is not None and 0 in attention_mask) else None\n             encoder_attention_mask = (\n                 encoder_attention_mask.bool()\n                 if (encoder_attention_mask is not None and 0 in encoder_attention_mask)\n                 else None\n             )\n         else:\n-            # 4d mask is passed through the layers\n-            if attention_mask is not None:\n-                self_attention_mask = self_attention_mask * attention_mask.view(batch_size, 1, -1).to(\n-                    dtype=torch.bool, device=self_attention_mask.device\n-                )\n-\n-            # MQA models: (batch_size, query_length, n_heads, key_length)\n-            # MHA models: (batch_size, n_heads, query_length, key_length)\n-            self_attention_mask = self_attention_mask.unsqueeze(2 if self.multi_query else 1)\n-\n-            if self._use_sdpa and head_mask is None and not output_attentions:\n-                # SDPA with a custom mask is much faster in fp16/fp32 dtype rather than bool. Cast here to floating point instead of at every layer.\n-                dtype = self.wte.weight.dtype\n-                min_dtype = torch.finfo(dtype).min\n-                self_attention_mask = torch.where(\n-                    self_attention_mask,\n-                    torch.full([], 0.0, dtype=dtype, device=self_attention_mask.device),\n-                    torch.full([], min_dtype, dtype=dtype, device=self_attention_mask.device),\n-                )\n-\n-                # output_attentions=True can not be supported when using SDPA, and we fall back on\n-                # the manual implementation that requires a 4D causal mask in all cases.\n-                if self.multi_query:\n-                    # gpt_bigcode using MQA has the bad taste to use a causal mask with shape\n-                    # [batch_size, target_length, 1, source_length], not compatible with SDPA, hence this transpose.\n-                    self_attention_mask = self_attention_mask.transpose(1, 2)\n-\n-                if (\n-                    query_length > 1\n-                    and attention_mask is not None\n-                    and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-                ):\n-                    # From PyTorch 2.1 onwards, F.scaled_dot_product_attention with the memory-efficient attention backend\n-                    # produces nans if sequences are completely unattended in the attention mask. Details: https://github.com/pytorch/pytorch/issues/110213\n-                    self_attention_mask = AttentionMaskConverter._unmask_unattended(\n-                        self_attention_mask, min_dtype=min_dtype\n-                    )\n-\n-            attention_mask = self_attention_mask\n-\n             # If a 2D or 3D attention mask is provided for the cross-attention\n             # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n             if (\n@@ -877,46 +545,42 @@ def forward(\n         # head_mask has shape n_layer x batch x n_heads x N x N\n         head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n \n-        if inputs_embeds is None:\n-            inputs_embeds = self.wte(input_ids)\n         position_embeds = self.wpe(position_ids)\n         hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n \n         if token_type_ids is not None:\n+            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n             token_type_embeds = self.wte(token_type_ids)\n             hidden_states = hidden_states + token_type_embeds\n \n         hidden_states = self.drop(hidden_states)\n-\n         output_shape = input_shape + (hidden_states.size(-1),)\n \n-        presents = [] if use_cache else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n-        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n+        for i, block in enumerate(self.h):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             outputs = block(\n                 hidden_states,\n-                layer_past,\n-                attention_mask,\n+                past_key_values,\n+                causal_mask,\n                 head_mask[i],\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n+                **kwargs,\n             )\n \n             hidden_states = outputs[0]\n-            if use_cache:\n-                presents.append(outputs[1])\n-\n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n                 if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n+                    all_cross_attentions = all_cross_attentions + (outputs[2],)\n \n         hidden_states = self.ln_f(hidden_states)\n \n@@ -925,16 +589,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n-                if v is not None\n-            )\n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=presents,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -964,75 +624,6 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n-        # Overwritten -- `past_key_values` with uncommon shape\n-\n-        token_type_ids = kwargs.get(\"token_type_ids\", None)\n-        # Omit tokens covered by past_key_values\n-        if past_key_values:\n-            if self.config.multi_query:\n-                past_length = past_key_values[0].shape[1]\n-            else:\n-                past_length = past_key_values[0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-            if token_type_ids is not None:\n-                token_type_ids = token_type_ids[:, -input_ids.shape[1] :]\n-\n-        attention_mask = kwargs.get(\"attention_mask\", None)\n-        position_ids = kwargs.get(\"position_ids\", None)\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-        else:\n-            position_ids = None\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_key_values is None:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n-\n-        model_inputs.update(\n-            {\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"position_ids\": position_ids,\n-                \"attention_mask\": attention_mask,\n-                \"token_type_ids\": token_type_ids,\n-            }\n-        )\n-        return model_inputs\n-\n-    def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n-        \"\"\"\n-        Calculates `cache_position` for the pre-fill stage based on `input_ids` and optionally past length.\n-        Since gpt bigcode is special, the method is overridden here, other models use it from `generation.utils.py`.\n-        \"\"\"\n-        past_length = 0\n-        if \"past_key_values\" in model_kwargs:\n-            if self.config.multi_query:\n-                past_length = model_kwargs[\"past_key_values\"][0].shape[1]\n-            else:\n-                past_length = model_kwargs[\"past_key_values\"][0].shape[2]\n-        if \"inputs_embeds\" in model_kwargs:\n-            cur_len = model_kwargs[\"inputs_embeds\"].shape[1]\n-        else:\n-            cur_len = seq_length\n-        model_kwargs[\"cache_position\"] = torch.arange(past_length, cur_len, device=device)\n-        return model_kwargs\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1050,12 +641,13 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.Tensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1086,6 +678,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         hidden_states = transformer_outputs[0]\n \n@@ -1113,17 +706,6 @@ def forward(\n             cross_attentions=transformer_outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(layer_past.index_select(0, beam_idx.to(layer_past.device)) for layer_past in past_key_values)\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1164,11 +746,12 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.Tensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1197,6 +780,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            **kwargs,\n         )\n         hidden_states = transformer_outputs[0]\n         logits = self.score(hidden_states)\n@@ -1299,7 +883,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.Tensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as"
        },
        {
            "sha": "e6df1a4225e3a5d0b726fa4e648430deb9f374a9",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 21,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -477,8 +477,6 @@ class GPTNeoPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTNeoBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n     _supports_static_cache = False  # TODO: needs a HybridCache\n \n     def __init__(self, *inputs, **kwargs):\n@@ -542,7 +540,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -817,7 +815,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -883,20 +881,6 @@ def forward(\n             attentions=transformer_outputs.attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n-        [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -941,7 +925,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1064,7 +1048,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -1146,7 +1130,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as"
        },
        {
            "sha": "511ac1a29c8e24d6d13ae32e9a9bc5c2f501c5df",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -363,8 +363,7 @@ class GPTNeoXPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "6f8674b00bc94514668297160d7674b12e8382b0",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -47,8 +47,7 @@ class GPTNeoXJapanesePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"gpt_neox_japanese\"\n     _no_split_modules = [\"GPTNeoXJapaneseLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -750,15 +749,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"GPTNeoXJapaneseForCausalLM\","
        },
        {
            "sha": "7fcc7451ac15b2b25fe51804929d3858f8391dca",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -472,8 +472,6 @@ class GPTJPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"GPTJBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n \n@@ -1017,20 +1015,6 @@ def forward(\n             attentions=transformer_outputs.attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n-        [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "87e2dfd793c228f0c650a92bea05a54f7304443e",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -308,8 +308,7 @@ class GranitePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "2cfaa235828d40d5c86e89bc54d222956971c3d2",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -282,7 +282,7 @@ def forward(self, hidden_states: torch.Tensor):\n @auto_docstring\n class GraniteSpeechPreTrainedModel(PreTrainedModel):\n     config_class = GraniteSpeechConfig\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n "
        },
        {
            "sha": "132c243493f15892ee45ee5d1852375acd3a8eb2",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -591,8 +591,7 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n@@ -1022,14 +1021,5 @@ def forward(\n             router_logits=outputs.router_logits,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\"GraniteMoeForCausalLM\", \"GraniteMoeModel\", \"GraniteMoePreTrainedModel\"]"
        },
        {
            "sha": "761bee178f341d2a7f6aae5d120d1f9d8d3f8c1f",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1164,8 +1164,7 @@ class GraniteMoeHybridPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _is_stateful = True\n \n@@ -1739,15 +1738,6 @@ def forward(\n             router_logits=outputs.router_logits,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n     def prepare_inputs_for_generation(\n         self,\n         input_ids,\n@@ -1805,14 +1795,5 @@ def prepare_inputs_for_generation(\n         )\n         return model_inputs\n \n-    def _supports_default_dynamic_cache(self) -> bool:\n-        \"\"\"\n-        Function overwritten as this class uses its own `HybridMambaAttentionDynamicCache`\n-        and do not need to initialize the Cache in advance in order to save memory\n-        (because no back and forth `to_legacy_cache` and `from_legacy_cache` will be performed\n-        for `HybridMambaAttentionDynamicCache`).\n-        \"\"\"\n-        return False\n-\n \n __all__ = [\"GraniteMoeHybridForCausalLM\", \"GraniteMoeHybridModel\", \"GraniteMoeHybridPreTrainedModel\"]"
        },
        {
            "sha": "eea61219ac11f6cc51b6046aea652b045c28614f",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -383,14 +383,5 @@ def prepare_inputs_for_generation(\n         )\n         return model_inputs\n \n-    def _supports_default_dynamic_cache(self) -> bool:\n-        \"\"\"\n-        Function overwritten as this class uses its own `HybridMambaAttentionDynamicCache`\n-        and do not need to initialize the Cache in advance in order to save memory\n-        (because no back and forth `to_legacy_cache` and `from_legacy_cache` will be performed\n-        for `HybridMambaAttentionDynamicCache`).\n-        \"\"\"\n-        return False\n-\n \n __all__ = [\"GraniteMoeHybridForCausalLM\", \"GraniteMoeHybridModel\", \"GraniteMoeHybridPreTrainedModel\"]"
        },
        {
            "sha": "009ecbf0ddbbd41cdc97b06c95b9406f7c467155",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -509,8 +509,7 @@ class GraniteMoeSharedPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n@@ -1054,14 +1053,5 @@ def forward(\n             router_logits=outputs.router_logits,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\"GraniteMoeSharedForCausalLM\", \"GraniteMoeSharedModel\", \"GraniteMoeSharedPreTrainedModel\"]"
        },
        {
            "sha": "197c99c57be6779c241ecb9ef8d14df425823f75",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1183,11 +1183,6 @@ def __init__(self, config, num_attention_heads=None):\n \n         self.dropout = nn.Dropout(config.attention_dropout)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         queries: torch.Tensor,\n@@ -1196,9 +1191,18 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n-        query_layer = self.transpose_for_scores(self.query(queries))\n-        key_layer = self.transpose_for_scores(self.key(keys))\n-        value_layer = self.transpose_for_scores(self.value(values))\n+        batch_size, seq_length, _ = queries.shape\n+        query_layer = (\n+            self.query(queries)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(keys).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(values).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))"
        },
        {
            "sha": "7140b89b44ea4796ae622c5e23cca509f43c055d",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -315,8 +315,7 @@ class HeliumPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "c4ab2cb2ec354b4a7e17de473ce27f595789873b",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -306,7 +306,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,"
        },
        {
            "sha": "5d9c9b17e496663e516303dc36e1678c2d6cbae4",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -226,11 +226,6 @@ def __init__(self, config):\n \n         self.softmax = IntSoftmax(self.act_bit, quant_mode=self.quant_mode, force_dequant=config.force_dequant)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states,\n@@ -254,9 +249,14 @@ def forward(\n         )\n \n         # Transpose\n-        query_layer = self.transpose_for_scores(query_layer)\n-        key_layer = self.transpose_for_scores(key_layer)\n-        value_layer = self.transpose_for_scores(value_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n+        key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        value_layer = value_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -564,7 +564,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = None  # `config.add_cross_attention` is not supported\n-        next_decoder_cache = None  # `config.use_cache` is not supported\n \n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n@@ -592,7 +591,6 @@ def forward(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -601,7 +599,6 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -765,7 +762,6 @@ def forward(\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n-            past_key_values=encoder_outputs.past_key_values,\n             hidden_states=encoder_outputs.hidden_states,\n             attentions=encoder_outputs.attentions,\n             cross_attentions=encoder_outputs.cross_attentions,"
        },
        {
            "sha": "9812204e91c7bc36f0af788a2bd50fb7fd803c33",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -878,7 +878,7 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"IdeficsDecoderLayer\", \"IdeficsGatedCrossAttentionLayer\"]\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_static_cache = False  # IDEFICS cannot compile due to dynamic control flow when checking inputs\n     _supports_attention_backend = True\n@@ -1554,12 +1554,5 @@ def _update_model_kwargs_for_generation(\n         model_kwargs[\"image_hidden_states\"] = outputs.image_hidden_states\n         return model_kwargs\n \n-    @staticmethod\n-    def _reorder_cache(past, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past:\n-            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n-        return reordered_past\n-\n \n __all__ = [\"IdeficsForVisionText2Text\", \"IdeficsModel\", \"IdeficsPreTrainedModel\"]"
        },
        {
            "sha": "6c93643a3f4fe2ca354b6b26caf43056038ef3e6",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -459,7 +459,7 @@ class Idefics2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "021e0d9e709e3a1d675b3af45873a6fc2922626a",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -476,7 +476,7 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "a0e89f406f31139170bd36b018291d883cc9dac4",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -229,17 +229,28 @@ def __init__(self, config: IJepaConfig) -> None:\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+        self,\n+        hidden_states,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        batch_size, seq_length, _ = hidden_states.shape\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":"
        },
        {
            "sha": "2041c615dfab5e53b1d42cf0625a7a093e8e793b",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 84,
            "deletions": 72,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -333,40 +334,61 @@ def _merge_heads(self, tensor, num_heads, attn_head_size):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        layer_past: Optional[bool] = None,\n+        layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple:\n-        if encoder_hidden_states is not None:\n+        is_cross_attention = encoder_hidden_states is not None\n+        bsz, seq_len, _ = hidden_states.shape\n+\n+        if layer_past is not None:\n+            if isinstance(layer_past, EncoderDecoderCache):\n+                is_updated = layer_past.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = layer_past.cross_attention_cache\n+                else:\n+                    curr_past_key_value = layer_past.self_attention_cache\n+            else:\n+                curr_past_key_value = layer_past\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention:\n             if not hasattr(self, \"q_attn\"):\n                 raise ValueError(\n                     \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                     \"Please make sure to instantiate class with `ImageGPTAttention(..., is_cross_attention=True)`.\"\n                 )\n \n-            query = self.q_attn(hidden_states)\n-            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n-            attention_mask = encoder_attention_mask\n+            if layer_past is not None and is_updated:\n+                # reuse k,v, cross_attentions, and compute only q\n+                query = query = self.q_attn(hidden_states)\n+                key = curr_past_key_value.key_cache[self.layer_idx]\n+                value = curr_past_key_value.value_cache[self.layer_idx]\n+            else:\n+                query = query = self.q_attn(hidden_states)\n+                key, value = self.c_attn(current_states).split(self.split_size, dim=2)\n+                key = key.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+                value = value.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n         else:\n-            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n-\n-        query = self._split_heads(query, self.num_heads, self.head_dim)\n-        key = self._split_heads(key, self.num_heads, self.head_dim)\n-        value = self._split_heads(value, self.num_heads, self.head_dim)\n+            query, key, value = self.c_attn(current_states).split(self.split_size, dim=2)\n+            key = key.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value = value.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n         if layer_past is not None:\n-            past_key, past_value = layer_past\n-            key = torch.cat((past_key, key), dim=-2)\n-            value = torch.cat((past_value, value), dim=-2)\n+            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+            cache_position = cache_position if not is_cross_attention else None\n+            key, value = curr_past_key_value.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n+            # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+            if is_cross_attention:\n+                layer_past.is_updated[self.layer_idx] = True\n \n-        if use_cache is True:\n-            present = (key, value)\n-        else:\n-            present = None\n+        query = query.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n \n         if self.reorder_and_upcast_attn:\n             attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n@@ -377,11 +399,7 @@ def forward(\n         attn_output = self.c_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n \n-        outputs = (attn_output, present)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs  # a, present, (attentions)\n+        return attn_output, attn_weights\n \n \n class ImageGPTMLP(nn.Module):\n@@ -420,13 +438,14 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        layer_past: Optional[bool] = None,\n+        layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple:\n         residual = hidden_states\n         hidden_states = self.ln_1(hidden_states)\n@@ -437,8 +456,9 @@ def forward(\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n-        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n+        attn_output = attn_outputs[0]\n         outputs = attn_outputs[1:]\n         # residual connection\n         hidden_states = attn_output + residual\n@@ -454,26 +474,26 @@ def forward(\n             hidden_states = self.ln_cross_attn(hidden_states)\n             cross_attn_outputs = self.crossattention(\n                 hidden_states,\n+                layer_past=layer_past,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attn_output = cross_attn_outputs[0]\n             # residual connection\n             hidden_states = residual + attn_output\n-            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n+            outputs = outputs + cross_attn_outputs[1:]  # add cross attentions if we output attention weights\n \n         residual = hidden_states\n         hidden_states = self.ln_2(hidden_states)\n         feed_forward_hidden_states = self.mlp(hidden_states)\n         # residual connection\n         hidden_states = residual + feed_forward_hidden_states\n \n-        outputs = (hidden_states,) + (outputs if use_cache else outputs[1:])\n-\n-        return outputs  # hidden_states, present, (attentions, cross_attentions)\n+        return (hidden_states,) + outputs\n \n \n @auto_docstring\n@@ -565,12 +585,13 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Any,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -618,14 +639,28 @@ def forward(\n \n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        past_length = past_key_values.get_seq_length() if past_key_values is not None else past_key_values\n+\n         if token_type_ids is not None:\n             token_type_ids = token_type_ids.view(-1, input_shape[-1])\n \n-        if past_key_values is None:\n-            past_length = 0\n-            past_key_values = tuple([None] * len(self.h))\n-        else:\n-            past_length = past_key_values[0][0].size(-2)\n         if position_ids is None:\n             position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n             position_ids = position_ids.unsqueeze(0)\n@@ -677,27 +712,15 @@ def forward(\n             hidden_states = hidden_states + token_type_embeds\n \n         hidden_states = self.drop(hidden_states)\n-\n         output_shape = input_shape + (hidden_states.size(-1),)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        presents = () if use_cache else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n-        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n+        for i, block in enumerate(self.h):\n             # Model parallel\n             if self.model_parallel:\n                 torch.cuda.set_device(hidden_states.device)\n-                # Ensure layer_past is on same device as hidden_states (might not be correct)\n-                if layer_past is not None:\n-                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n                 # Ensure that attention_mask is always on the same device as hidden_states\n                 if attention_mask is not None:\n                     attention_mask = attention_mask.to(hidden_states.device)\n@@ -708,23 +731,21 @@ def forward(\n \n             outputs = block(\n                 hidden_states,\n-                layer_past,\n+                past_key_values,\n                 attention_mask,\n                 head_mask[i],\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = outputs[0]\n-            if use_cache is True:\n-                presents = presents + (outputs[1],)\n-\n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n                 if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n+                    all_cross_attentions = all_cross_attentions + (outputs[2],)\n \n             # Model Parallel: If it's the last layer for that device, put things on the next device\n             if self.model_parallel:\n@@ -733,22 +754,25 @@ def forward(\n                         hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n \n         hidden_states = self.ln_f(hidden_states)\n-\n         hidden_states = hidden_states.view(*output_shape)\n+\n         # Add last hidden state\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions, all_cross_attentions]\n                 if v is not None\n             )\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=presents,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -798,12 +822,13 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Any,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -867,6 +892,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         hidden_states = transformer_outputs[0]\n \n@@ -894,20 +920,6 @@ def forward(\n             cross_attentions=transformer_outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -945,7 +957,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as"
        },
        {
            "sha": "abd55a22bfd5abf26aecf00bf454ac7c701af749",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -522,7 +522,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class InformerProbSparseAttention(nn.Module):\n@@ -741,7 +741,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped\n \n \n # source: https://github.com/zhouhaoyi/Informer2020/blob/main/models/encoder.py\n@@ -814,7 +814,7 @@ def forward(\n                 returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -924,7 +924,7 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -941,7 +941,7 @@ def forward(\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -968,9 +968,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -1268,7 +1265,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1302,9 +1298,6 @@ def forward(\n             )\n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1315,19 +1308,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,"
        },
        {
            "sha": "79d7c661141f6dfe097163960431dd7d95d0ec98",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -430,7 +430,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped\n \n \n # source: https://github.com/zhouhaoyi/Informer2020/blob/main/models/encoder.py"
        },
        {
            "sha": "c5af37a0d94242022322b35a045b26c95fc17905",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -339,9 +339,8 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n-    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n     _no_split_modules = [\n         \"InstructBlipQFormerEmbeddings\",\n@@ -1354,9 +1353,8 @@ def forward(\n class InstructBlipForConditionalGeneration(InstructBlipPreTrainedModel, GenerationMixin):\n     config_class = InstructBlipConfig\n     main_input_name = \"pixel_values\"\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n-    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n     _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n \n     def __init__(self, config: InstructBlipConfig):"
        },
        {
            "sha": "0f62721d673624dc34d219f78afd4e19e4093081",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -826,9 +826,8 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n-    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n     _no_split_modules = [\n         \"InstructBlipVideoQFormerEmbeddings\",\n@@ -1360,9 +1359,8 @@ def forward(\n class InstructBlipVideoForConditionalGeneration(InstructBlipVideoPreTrainedModel, GenerationMixin):\n     config_class = InstructBlipVideoConfig\n     main_input_name = \"pixel_values\"\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n-    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n     _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n \n     def __init__(self, config: InstructBlipVideoConfig):"
        },
        {
            "sha": "d5768ef4a8c28656d1857ac2abc8ab4dfd427f8b",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -520,10 +520,10 @@ class InternVLPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "c7145cd8256399c55eab5470cb6211ee644ab9a6",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1071,7 +1071,7 @@ class JambaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n+    # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "e85023a208664ed3db19dbcf5c4a21f3030b8b90",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -62,8 +62,7 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n "
        },
        {
            "sha": "588500bae20062c1925560967e9d398d86c091e7",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -389,8 +389,7 @@ class JanusPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n "
        },
        {
            "sha": "c13437c5bb0223d2469e605a4c2081fc1e171799",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -828,7 +828,6 @@ class JetMoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\""
        },
        {
            "sha": "e3cbd12f73327c1f901df124aae518cd031cf461",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 90,
            "deletions": 90,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -677,9 +678,10 @@ def __init__(\n         embed_dim: int,\n         num_heads: int,\n         dropout: float = 0.0,\n-        is_decoder: bool = False,\n-        add_inner_attn_layernorm: bool = False,\n-        bias: bool = True,\n+        is_decoder: Optional[bool] = False,\n+        add_inner_attn_layernorm: Optional[bool] = False,\n+        bias: Optional[bool] = True,\n+        layer_idx: Optional[bool] = None,\n     ):\n         super().__init__()\n         self.config = config\n@@ -695,6 +697,7 @@ def __init__(\n             )\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n+        self.layer_idx = layer_idx\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n@@ -706,56 +709,58 @@ def __init__(\n         if add_inner_attn_layernorm:\n             self.inner_attn_ln = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n-    def _shape(self, projection: torch.Tensor) -> torch.Tensor:\n-        new_projection_shape = projection.size()[:-1] + (self.num_heads, self.head_dim)\n-        # move heads to 2nd position (B, T, H * D) -> (B, T, H, D) -> (B, H, T, D)\n-        new_projection = projection.view(new_projection_shape).permute(0, 2, 1, 3)\n-        return new_projection\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n         # for the decoder\n         is_cross_attention = encoder_hidden_states is not None\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        # use encoder_hidden_states if cross attention\n-        current_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n-        # checking that the `sequence_length` of the `past_key_value` is the same as the he provided\n-        # `encoder_hidden_states` to support prefix tuning\n-        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n+        query_states = self.q_proj(hidden_states)\n+        query_states = query_states.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_states = self._shape(self.k_proj(current_states))\n-            value_states = self._shape(self.v_proj(current_states))\n-            if past_key_value is not None and not is_cross_attention:\n-                # reuse k, v, self_attention\n-                key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-                value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-\n-        query_states = self._shape(self.q_proj(hidden_states))\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n \n@@ -785,7 +790,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class Kosmos2TextFFN(nn.Module):\n@@ -812,7 +817,7 @@ def forward(self, hidden_states):\n \n \n class Kosmos2TextBlock(GradientCheckpointingLayer):\n-    def __init__(self, config: Kosmos2TextConfig):\n+    def __init__(self, config: Kosmos2TextConfig, layer_idx=None):\n         super().__init__()\n         self.embed_dim = config.embed_dim\n \n@@ -823,6 +828,7 @@ def __init__(self, config: Kosmos2TextConfig):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             add_inner_attn_layernorm=True,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n@@ -835,6 +841,7 @@ def __init__(self, config: Kosmos2TextConfig):\n                 dropout=config.attention_dropout,\n                 is_decoder=True,\n                 add_inner_attn_layernorm=False,\n+                layer_idx=layer_idx,\n             )\n             self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n@@ -849,33 +856,28 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n-        # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n             **kwargs,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             if not hasattr(self, \"encoder_attn\"):\n@@ -885,26 +887,21 @@ def forward(\n                 )\n \n             residual = hidden_states\n-\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 encoder_hidden_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n                 **kwargs,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n \n@@ -918,10 +915,6 @@ def forward(\n \n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n-\n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -948,7 +941,7 @@ def __init__(self, config: Kosmos2TextConfig):\n             padding_idx=config.pad_token_id,\n         )\n \n-        self.layers = nn.ModuleList([Kosmos2TextBlock(config) for _ in range(config.layers)])\n+        self.layers = nn.ModuleList([Kosmos2TextBlock(config, layer_idx=i) for i in range(config.layers)])\n         self.layer_norm = nn.LayerNorm(config.embed_dim, config.layer_norm_eps)\n \n         self.gradient_checkpointing = False\n@@ -1027,6 +1020,8 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1045,8 +1040,24 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n \n         # We don't need img info. when `past_key_values_length` > 0\n         if past_key_values_length > 0:\n@@ -1073,18 +1084,10 @@ def forward(\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        present_key_value_states = () if use_cache else None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1104,25 +1107,21 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n                 **kwargs,\n             )\n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                present_key_value_states += (layer_outputs[3 if output_attentions else 1],)\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1132,13 +1131,16 @@ def forward(\n         # add final layer norm\n         hidden_states = self.layer_norm(hidden_states)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         # add hidden states from the last decoder layer\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=present_key_value_states,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1304,6 +1306,8 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n@@ -1336,6 +1340,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n             **kwargs,\n         )\n \n@@ -1391,6 +1397,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -1435,6 +1442,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n+            cache_position=cache_position,\n             **kwargs,\n         )\n         lm_logits = self.lm_head(outputs[0])\n@@ -1466,9 +1474,11 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        if past_key_values is not None:\n+        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+        if cache_position[0] != 0:\n             image_embeds = None\n             image_embeds_position_mask = None\n+\n         # appending `False` to `image_embeds_position_mask` (because `input_ids` grows during generation)\n         elif image_embeds_position_mask is not None:\n             batch_size, seq_len = inputs_embeds.size()[:-1] if inputs_embeds is not None else input_ids.size()\n@@ -1497,16 +1507,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    # Copied from transformers.models.umt5.modeling_umt5.UMT5ForConditionalGeneration._reorder_cache\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n class Kosmos2ImageToTextProjection(nn.Module):\n     \"\"\"The layer that transforms the image model's output to part of the text model's input (namely, image features)\"\"\"\n@@ -1532,7 +1532,7 @@ def forward(self, features):\n         latent_query = self.latent_query.unsqueeze(0).expand(hidden_states.size(0), -1, -1)\n         key_value_states = torch.cat([hidden_states, latent_query], dim=1)\n \n-        hidden_states, attn_weights, _ = self.x_attn(\n+        hidden_states, attn_weights = self.x_attn(\n             hidden_states=latent_query,\n             encoder_hidden_states=key_value_states,\n             past_key_value=None,"
        },
        {
            "sha": "04b38cd10aa24c3d66caac53ce3858acb321fee3",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -119,7 +119,7 @@ class KyutaiSpeechToTextPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"KyutaiSpeechToTextDecoderLayer\", \"MimiTransformerLayer\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     main_input_name = \"input_ids\"\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "7f6a861a674dc3fbf2cbe583ee70f79bc9ea98d7",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -123,11 +123,6 @@ def __init__(self, config):\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def compute_qkv(self, hidden_states):\n         if self.fast_qkv:\n             qkv = self.qkv_linear(hidden_states)\n@@ -154,12 +149,13 @@ def forward(\n         rel_pos=None,\n         rel_2d_pos=None,\n     ):\n-        q, k, v = self.compute_qkv(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query, key, value = self.compute_qkv(hidden_states)\n \n         # (B, L, H*D) -> (B, H, L, D)\n-        query_layer = self.transpose_for_scores(q)\n-        key_layer = self.transpose_for_scores(k)\n-        value_layer = self.transpose_for_scores(v)\n+        query_layer = query.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        key_layer = key.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        value_layer = value.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n \n         query_layer = query_layer / math.sqrt(self.attention_head_size)\n         # [BSZ, NAT, L, L]"
        },
        {
            "sha": "8b5628541092ef60fe55c2f93504b8336e596644",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -245,11 +245,6 @@ def __init__(self, config):\n         self.has_relative_attention_bias = config.has_relative_attention_bias\n         self.has_spatial_attention_bias = config.has_spatial_attention_bias\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def cogview_attention(self, attention_scores, alpha=32):\n         \"\"\"\n         https://huggingface.co/papers/2105.13290 Section 2.4 Stabilization of training: Precision Bottleneck Relaxation\n@@ -271,11 +266,22 @@ def forward(\n         rel_pos=None,\n         rel_2d_pos=None,\n     ):\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         # The attention scores QT K/d could be significantly larger than input elements, and result in overflow."
        },
        {
            "sha": "b665858862abd9416ded30fdf993207b8756fcf9",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 84,
            "deletions": 84,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -765,9 +766,10 @@ def __init__(\n         self,\n         embed_dim: int,\n         num_heads: int,\n-        dropout: float = 0.0,\n-        is_decoder: bool = False,\n-        bias: bool = True,\n+        dropout: Optional[float] = 0.0,\n+        is_decoder: Optional[bool] = False,\n+        bias: Optional[bool] = True,\n+        layer_idx: Optional[bool] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -781,24 +783,23 @@ def __init__(\n             )\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n+        self.layer_idx = layer_idx\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        cache_position: Optional[torch.Tensor] = None,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -808,40 +809,44 @@ def forward(\n \n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        if is_cross_attention and past_key_value is not None:\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n+        query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.reshape(*proj_shape)\n+        key_states = key_states.reshape(*proj_shape)\n+        value_states = value_states.reshape(*proj_shape)\n \n         src_len = key_states.size(1)\n         attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n@@ -964,7 +969,7 @@ def forward(\n \n \n class LEDDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: LEDConfig):\n+    def __init__(self, config: LEDConfig, layer_idx=None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -973,6 +978,7 @@ def __init__(self, config: LEDConfig):\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=True,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -984,6 +990,7 @@ def __init__(self, config: LEDConfig):\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=True,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -998,9 +1005,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         \"\"\"\n         Args:\n@@ -1022,15 +1030,13 @@ def forward(\n         residual = hidden_states\n \n         # Self-Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n@@ -1042,23 +1048,19 @@ def forward(\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.activation_fn(self.fc1(hidden_states))\n@@ -1074,7 +1076,7 @@ def forward(\n             outputs += (self_attn_weights, cross_attn_weights)\n \n         if use_cache:\n-            outputs += (present_key_value,)\n+            outputs += (past_key_value,)\n \n         return outputs\n \n@@ -1629,7 +1631,7 @@ def __init__(self, config: LEDConfig, embed_tokens: Optional[nn.Embedding] = Non\n             self.max_target_positions,\n             config.d_model,\n         )\n-        self.layers = nn.ModuleList([LEDDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([LEDDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -1651,6 +1653,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         r\"\"\"\n         Args:\n@@ -1744,12 +1747,27 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         # create causal mask\n         # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n         combined_attention_mask = None\n@@ -1779,18 +1797,10 @@ def forward(\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if output_attentions else None\n-        next_decoder_cache = () if use_cache else None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1809,25 +1819,20 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 combined_attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n                 all_cross_attentions += (layer_outputs[2],)\n@@ -1836,16 +1841,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1901,6 +1908,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], LEDSeq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1986,6 +1994,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -2071,6 +2080,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], LEDSeq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2190,6 +2200,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n \n@@ -2218,17 +2229,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "dc58e8f2e1eb825f2c0cb9bb18e7e1cc52f84742",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -542,8 +542,6 @@ class Lfm2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n     _supports_static_cache = False\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "deddd105da9e3ebffb1ab170ec20764f9de40695",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -186,7 +186,7 @@ def forward(self, bbox=None, position_ids=None):\n \n \n class LiltSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -221,6 +221,7 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.channel_shrink_ratio = config.channel_shrink_ratio\n+        self.layer_idx = layer_idx\n \n     def transpose_for_scores(self, x, r=1):\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size // r)\n@@ -338,9 +339,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class LiltAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n-        self.self = LiltSelfAttention(config, position_embedding_type=position_embedding_type)\n+        self.self = LiltSelfAttention(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.output = LiltSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -421,11 +422,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class LiltLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = LiltAttention(config)\n+        self.attention = LiltAttention(config, layer_idx=layer_idx)\n         self.intermediate = LiltIntermediate(config)\n         self.output = LiltOutput(config)\n \n@@ -482,10 +483,10 @@ def layout_feed_forward_chunk(self, attention_output):\n \n class LiltEncoder(nn.Module):\n     # Copied from transformers.models.bert.modeling_bert.BertEncoder.__init__ with Bert->Lilt\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([LiltLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([LiltLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward("
        },
        {
            "sha": "c5a57563639f0e0a00658a6088f9e5fcd598b9c6",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -314,8 +314,7 @@ class LlamaPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "24fc14bc96f0d3530e0e4a9d715d4d3f38929a3e",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -436,8 +436,7 @@ class Llama4PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = False\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "7cd79de12d3c451eee51a1886a1f7cdcc7e95b1e",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -117,10 +117,10 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "c019fb275cf3d6c77cbcaf7847891f0f65cc6721",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -228,10 +228,10 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "2d73a51a285613373e0312c139bbdb2c017c724a",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -169,10 +169,10 @@ class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "af16955b413c3372d74582988ac05fce522a8ef8",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -282,10 +282,10 @@ class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "b0613a2ea543a91b1e821321416b38f0961d4453",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1249,7 +1249,7 @@ class LongT5PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LongT5Block\"]\n-    _supports_cache_class = True\n+\n     _supports_static_cache = False  # TODO: @raushan more involved due to local/global attn\n \n     @property\n@@ -2113,30 +2113,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        # if decoder past is not included in output\n-        # speedy decoding is disabled and no need to reorder\n-        if past_key_values is None:\n-            logger.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n-            return past_key_values\n-\n-        reordered_decoder_past = ()\n-        for layer_past_states in past_key_values:\n-            # get the correct batch idx from layer past batch dim\n-            # batch dim of `past` is at 2nd position\n-            reordered_layer_past_states = ()\n-            for layer_past_state in layer_past_states:\n-                # need to set correct `past` for each of the four key / value states\n-                reordered_layer_past_states = reordered_layer_past_states + (\n-                    layer_past_state.index_select(0, beam_idx.to(layer_past_state.device)),\n-                )\n-\n-            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n-            assert len(reordered_layer_past_states) == len(layer_past_states)\n-\n-            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n-        return reordered_decoder_past\n-\n \n @auto_docstring\n class LongT5EncoderModel(LongT5PreTrainedModel):"
        },
        {
            "sha": "4138cb0b82a9ad759e5f9ce06c270f0c0f35885f",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -319,22 +319,21 @@ def __init__(self, config, ctx_dim=None):\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (\n-            self.num_attention_heads,\n-            self.attention_head_size,\n-        )\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(self, hidden_states, context, attention_mask=None, output_attentions=False):\n-        mixed_query_layer = self.query(hidden_states)\n-        mixed_key_layer = self.key(context)\n-        mixed_value_layer = self.value(context)\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-        key_layer = self.transpose_for_scores(mixed_key_layer)\n-        value_layer = self.transpose_for_scores(mixed_value_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(context).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(context)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))"
        },
        {
            "sha": "cc663aa7432d91b56fc2498d14f9f0c5aa1227e3",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 9,
            "deletions": 32,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -332,7 +332,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->M2M100, MBART->M2M100\n@@ -375,7 +375,7 @@ def forward(\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -396,12 +396,7 @@ def forward(\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states, attn_weights\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->M2M100, MBART->M2M100\n@@ -475,7 +470,7 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -492,7 +487,7 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -516,10 +511,6 @@ def forward(\n \n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n-\n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -532,7 +523,7 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     # Doesn't support `compile` (dynamic control flow). Can be fixed but low usage model\n     _supports_static_cache = False\n \n@@ -1111,7 +1102,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if output_attentions else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1154,9 +1144,6 @@ def forward(\n             if skip_the_layer:\n                 continue\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n                 all_cross_attentions += (layer_outputs[2],)\n@@ -1167,19 +1154,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1464,14 +1450,5 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\"M2M100ForConditionalGeneration\", \"M2M100Model\", \"M2M100PreTrainedModel\"]"
        },
        {
            "sha": "94c913ad7a66982113e58a4aa3286d829ca70cdf",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 8,
            "deletions": 37,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -267,7 +267,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->Marian, BART->MARIAN\n@@ -310,7 +310,7 @@ def forward(\n                 returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -412,7 +412,7 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -429,7 +429,7 @@ def forward(\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -455,10 +455,6 @@ def forward(\n \n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n-\n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -470,7 +466,7 @@ class MarianPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalPositionalEmbedding]):\n@@ -1061,7 +1057,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1093,9 +1088,6 @@ def forward(\n             )\n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1106,19 +1098,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1589,17 +1580,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->Marian\n class MarianDecoderWrapper(MarianPreTrainedModel):\n@@ -1740,14 +1720,5 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\"MarianForCausalLM\", \"MarianModel\", \"MarianMTModel\", \"MarianPreTrainedModel\"]"
        },
        {
            "sha": "9fb5a7469bccc97c21f64026a8b2678832a490d7",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -742,15 +742,6 @@ def forward(\n             attentions=encoder_outputs.attentions,\n         )\n \n-    # Copied from transformers.models.bert.modeling_bert.BertModel._reorder_cache\n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring\n class MarkupLMForQuestionAnswering(MarkupLMPreTrainedModel):"
        },
        {
            "sha": "773101861114c2aae6b7dde01ffb781e789bccf1",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 9,
            "deletions": 43,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -277,7 +277,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class MBartEncoderLayer(GradientCheckpointingLayer):\n@@ -319,7 +319,7 @@ def forward(\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -340,12 +340,7 @@ def forward(\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states, attn_weights\n \n \n class MBartDecoderLayer(GradientCheckpointingLayer):\n@@ -418,7 +413,7 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -435,7 +430,7 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -460,9 +455,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -500,7 +492,7 @@ class MBartPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -1103,7 +1095,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1135,10 +1126,6 @@ def forward(\n                 cache_position=cache_position,\n             )\n             hidden_states = layer_outputs[0]\n-\n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1151,19 +1138,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1506,17 +1492,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1943,15 +1918,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"MBartForCausalLM\","
        },
        {
            "sha": "7ed94107b753bef14e83b63a58165b0a5b1334fc",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 107,
            "deletions": 106,
            "changes": 213,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -27,6 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -43,6 +44,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_megatron_bert import MegatronBertConfig\n \n \n@@ -178,7 +180,7 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->MegatronBert\n class MegatronBertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -203,66 +205,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -304,11 +315,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Based transformers.models.bert.modeling_bert.BertSelfOutput. Moved LayerNorm to MegatronBertAttention below.\n@@ -326,10 +333,10 @@ def forward(self, hidden_states: torch.Tensor, residual: torch.Tensor) -> torch.\n \n # Based transformers.models.bert.modeling_bert.BertAttention. Added LayerNorm.\n class MegatronBertAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.self = MegatronBertSelfAttention(config)\n+        self.self = MegatronBertSelfAttention(config, layer_idx=layer_idx)\n         self.output = MegatronBertSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -357,19 +364,19 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         ln_outputs = self.ln(hidden_states)\n         self_outputs = self.self(\n             ln_outputs,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -407,17 +414,17 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Based on transformers.models.bert.modeling_bert.BertLayer. Added LayerNorm.\n class MegatronBertLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = MegatronBertAttention(config)\n+        self.attention = MegatronBertAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise TypeError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = MegatronBertAttention(config)\n+            self.crossattention = MegatronBertAttention(config, layer_idx=layer_idx)\n         self.ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.intermediate = MegatronBertIntermediate(config)\n         self.output = MegatronBertOutput(config)\n@@ -429,63 +436,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise AttributeError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n-        return outputs\n+        return (layer_output,) + outputs\n \n     def feed_forward_chunk(self, attention_output):\n         ln_output = self.ln(attention_output)\n@@ -498,7 +487,7 @@ class MegatronBertEncoder(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([MegatronBertLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([MegatronBertLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n \n         # The final layer norm. We removed the 1st LN, moved LN to each hidden layer and this one\n         # is simply the final LN (Transformer's BERT has it attached to each hidden layer).\n@@ -517,41 +506,49 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning_once(\n                     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n                 use_cache = False\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n \n-        next_decoder_cache = () if use_cache else None\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n+                cache_position,\n             )\n \n             # Because we moved the layer-norm at the end of the hidden layer, we have non-normali-\n             # zed data here. If that's really needed, we must apply LN to match Transformer's BERT.\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -563,12 +560,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -577,7 +577,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -785,6 +785,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -810,8 +811,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -858,6 +864,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n@@ -1027,6 +1034,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -1067,6 +1075,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = outputs[0]\n@@ -1094,14 +1103,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring\n class MegatronBertForMaskedLM(MegatronBertPreTrainedModel):"
        },
        {
            "sha": "6afc4fdb91e36686c6ae627f8378f2e88ba6e561",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1375,7 +1375,7 @@ class MimiPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "057f8d809781a9bcb2bfc46129438c387754f614",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -588,8 +588,7 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True  # Note: only supports MiniMaxCache\n-    _supports_quantized_cache = False\n+    # Note: only supports MiniMaxCache\n     _supports_static_cache = False\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "423ae27717c4bd8fc62d1d38de997d22458d34b7",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -470,9 +470,8 @@ def forward(\n \n \n class MiniMaxPreTrainedModel(MixtralPreTrainedModel):\n-    _supports_cache_class = True  # Note: only supports MiniMaxCache\n+    # Note: only supports MiniMaxCache\n     _supports_static_cache = False\n-    _supports_quantized_cache = False\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(MiniMaxSparseMoeBlock, index=1),\n         \"hidden_states\": MiniMaxDecoderLayer,"
        },
        {
            "sha": "a6fbfbb4898a3f6e0664ba9d770a4215aa68dcee",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -259,8 +259,7 @@ class MistralPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "20e81a8404d1b9a026b73b999b4e814d9393d976",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -182,10 +182,10 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "cc78dfecd5e4fa27f5a9ae631beb21664e75fd09",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -388,8 +388,6 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "fcfa4a3db72e9b223b114938ca3e519b722c0ed1",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -849,11 +849,10 @@ class MllamaPreTrainedModel(PreTrainedModel):\n         \"MllamaCrossAttentionDecoderLayer\",\n         \"MllamaSelfAttentionDecoderLayer\",\n     ]\n-    _supports_cache_class = True\n+\n     _supports_static_cache = False  # static cache cannot have different shapes for each layer\n     _supports_sdpa = True\n     _supports_flash_attn = True\n-    _supports_quantized_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n@@ -1603,7 +1602,6 @@ def forward(\n )\n class MllamaModel(MllamaPreTrainedModel):\n     _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n-    _supports_quantized_cache = False  # quant cache not supported in encoder-decoder setting\n \n     def __init__(self, config: MllamaConfig):\n         super().__init__(config)\n@@ -1763,7 +1761,6 @@ class MllamaForConditionalGeneration(MllamaPreTrainedModel, GenerationMixin):\n         \"^multi_modal_projector\": \"model.multi_modal_projector\",\n         \"^language_model.lm_head\": \"lm_head\",\n     }\n-    _supports_quantized_cache = False  # quant cache not supported in encoder-decoder setting\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: MllamaConfig):"
        },
        {
            "sha": "91508d0997114faca1840d0290764dc1e7d27105",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -231,11 +231,6 @@ def __init__(self, config):\n         )\n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         query_tensor: torch.Tensor,\n@@ -245,13 +240,22 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(query_tensor)\n-        mixed_key_layer = self.key(key_tensor)\n-        mixed_value_layer = self.value(value_tensor)\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-        key_layer = self.transpose_for_scores(mixed_key_layer)\n-        value_layer = self.transpose_for_scores(mixed_value_layer)\n+        batch_size, seq_length, _ = query_tensor.shape\n+        query_layer = (\n+            self.query(query_tensor)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(key_tensor)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(value_tensor)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))"
        },
        {
            "sha": "3f882b9850ff41c4bf7e7ee8067a3b2fad5e384b",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -211,17 +211,23 @@ def __init__(self, config: MobileViTConfig, hidden_size: int) -> None:\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))"
        },
        {
            "sha": "011db51daac44e1098f5f2d400b34f14bc28437d",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 16,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -224,8 +224,6 @@ class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = False\n     _supports_gradient_checkpointing = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n     _supports_static_cache = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n@@ -422,11 +420,10 @@ def forward(\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Returns:\n             [`~modeling_outputs.CausalLMOutputWithPast`]\n@@ -484,15 +481,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "7609c2f1febf8d76902ce50eadc8a73d63a32646",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 16,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -401,8 +401,6 @@ class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = False\n     _supports_gradient_checkpointing = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n     _supports_static_cache = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n@@ -599,11 +597,10 @@ def forward(\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Returns:\n             [`~modeling_outputs.CausalLMOutputWithPast`]\n@@ -661,15 +658,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "9c2641d978a10e04a8bb179153bf020847ff68a3",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -461,7 +461,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n     # TODO arthur, how do we separate when it cross / self coming from different layer?\n "
        },
        {
            "sha": "4e2882fb81dc099c433f226acff6b1faa485a675",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -496,7 +496,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n     # TODO arthur, how do we separate when it cross / self coming from different layer?\n "
        },
        {
            "sha": "4cde9816bf79d9abb4aca2b4162279617a5cedbe",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -807,7 +807,7 @@ class MoshiPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"MoshiDecoderLayer\", \"MimiTransformerLayer\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     main_input_name = \"input_ids\"\n \n     def _init_weights(self, module):\n@@ -2526,19 +2526,5 @@ def _check_and_maybe_initialize_inputs(\n \n         return input_ids, user_audio_codes, moshi_audio_codes, concat_unconditional_inputs\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> tuple[tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n __all__ = [\"MoshiForCausalLM\", \"MoshiForConditionalGeneration\", \"MoshiModel\", \"MoshiPreTrainedModel\"]"
        },
        {
            "sha": "82698f8ecfb2aa6a29bf3cc14b98658ca64731a5",
            "filename": "src/transformers/models/mpnet/modeling_mpnet.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -144,11 +144,6 @@ def __init__(self, config):\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         hidden_states,\n@@ -158,13 +153,22 @@ def forward(\n         output_attentions=False,\n         **kwargs,\n     ):\n-        q = self.q(hidden_states)\n-        k = self.k(hidden_states)\n-        v = self.v(hidden_states)\n-\n-        q = self.transpose_for_scores(q)\n-        k = self.transpose_for_scores(k)\n-        v = self.transpose_for_scores(v)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        q = (\n+            self.q(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        k = (\n+            self.k(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        v = (\n+            self.v(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(q, k.transpose(-1, -2))"
        },
        {
            "sha": "81680bef79501aecc615665f5be4e351d7fb7251",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 55,
            "deletions": 77,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n from torch.nn import functional as F\n \n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -69,7 +70,7 @@ class MptAttention(nn.Module):\n     Using torch or triton attention implementation enables user to also use additive bias.\n     \"\"\"\n \n-    def __init__(self, config: MptConfig):\n+    def __init__(self, config: MptConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.n_heads = config.n_heads\n@@ -83,13 +84,15 @@ def __init__(self, config: MptConfig):\n         self.clip_qkv = config.attn_config.clip_qkv\n         self.Wqkv = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=False)\n         self.out_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n+        self.layer_idx = layer_idx\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_bias: torch.Tensor,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         batch_size, seq_length = hidden_states.shape[:2]\n \n@@ -103,16 +106,11 @@ def forward(\n         value_states = value_states.reshape(batch_size, seq_length, self.n_heads, self.head_dim).transpose(1, 2)\n \n         if past_key_value is not None:\n-            if len(past_key_value) != 0:\n-                key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-                value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-            past_key_value = (key_states, value_states)\n-        else:\n-            past_key_value = (key_states, value_states)\n+            cache_kwargs = {\"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2)) * self.softmax_scale\n-\n-        query_length = seq_length if past_key_value is None else seq_length + past_key_value[0].shape[2]\n+        query_length = seq_length if past_key_value is None else seq_length + past_key_value.get_seq_length()\n \n         if position_bias is not None:\n             if len(position_bias.shape) != 3:\n@@ -137,7 +135,7 @@ def forward(\n         context_states = context_states.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_length, -1)\n         attn_output = self.out_proj(context_states)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class MptMLP(nn.Module):\n@@ -162,7 +160,7 @@ def forward(self, hidden_states: torch.Tensor, residual: torch.Tensor) -> torch.\n \n \n class MptBlock(GradientCheckpointingLayer):\n-    def __init__(self, config: MptConfig):\n+    def __init__(self, config: MptConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         hidden_size = config.hidden_size\n \n@@ -171,7 +169,7 @@ def __init__(self, config: MptConfig):\n         self.norm_1.bias = None\n \n         self.num_heads = config.n_heads\n-        self.attn = MptAttention(config)\n+        self.attn = MptAttention(config, layer_idx)\n \n         self.norm_2 = LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n         # backward compatibility with weights on the Hub\n@@ -187,9 +185,10 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_bias: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        layer_past: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        layer_past: Optional[Cache] = None,\n         use_cache: bool = False,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         # hidden_states: [batch_size, seq_length, hidden_size]\n         # Layer norm at the beginning of the transformer layer.\n@@ -198,11 +197,12 @@ def forward(\n         residual = hidden_states\n \n         # Self attention.\n-        attn_outputs, attn_weights, past_key_value = self.attn(\n+        attn_outputs, attn_weights = self.attn(\n             layernorm_output,\n             position_bias=position_bias,\n             attention_mask=attention_mask,\n             past_key_value=layer_past,\n+            cache_position=cache_position,\n         )\n \n         hidden_states = self.resid_attn_dropout(attn_outputs) + residual\n@@ -214,15 +214,7 @@ def forward(\n \n         # MLP.\n         output = self.ffn(layernorm_output, residual)\n-        outputs = (output,)\n-\n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs  # hidden_states, present, attentions\n+        return output, attn_weights\n \n \n @auto_docstring\n@@ -285,7 +277,7 @@ def __init__(self, config: MptConfig):\n         self.wte = nn.Embedding(config.vocab_size, self.hidden_size)\n \n         # Transformer blocks\n-        self.blocks = nn.ModuleList([MptBlock(config) for _ in range(config.n_layers)])\n+        self.blocks = nn.ModuleList([MptBlock(config, layer_idx=i) for i in range(config.n_layers)])\n \n         # Final Layer Norm\n         self.norm_f = LayerNorm(self.hidden_size, eps=config.layer_norm_epsilon)\n@@ -310,18 +302,19 @@ def set_input_embeddings(self, new_embeddings: torch.Tensor):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor, torch.Tensor], ...]] = None,\n+        past_key_values: Optional[Union[tuple[tuple[torch.Tensor, torch.Tensor], ...], Cache]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -348,31 +341,34 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        if past_key_values is None:\n-            past_key_values = tuple([None] * len(self.blocks))\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n \n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n+                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+\n         hidden_states = inputs_embeds\n \n-        presents = () if use_cache else None\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # Compute alibi tensor: check build_alibi_tensor documentation\n-        seq_length_with_past = seq_length\n-        past_key_values_length = 0\n-        if past_key_values[0] is not None:\n-            past_key_values_length = past_key_values[0][0].shape[2]\n-            seq_length_with_past = seq_length_with_past + past_key_values_length\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        seq_length_with_past = seq_length + past_key_values_length\n         if attention_mask is None:\n             attention_mask = torch.ones((batch_size, seq_length_with_past), device=hidden_states.device)\n         else:\n@@ -385,38 +381,41 @@ def forward(\n         )\n         causal_mask = causal_mask.bool()\n \n-        for block, layer_past in zip(self.blocks, past_key_values):\n+        for block in self.blocks:\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             outputs = block(\n                 hidden_states,\n-                layer_past=layer_past,\n+                layer_past=past_key_values,\n                 attention_mask=causal_mask,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n                 position_bias=alibi,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = outputs[0]\n-            if use_cache is True:\n-                presents = presents + (outputs[1],)\n-\n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n \n         # Add last hidden state\n         hidden_states = self.norm_f(hidden_states)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n         if not return_dict:\n-            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions] if v is not None\n+            )\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=presents,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n         )\n@@ -457,11 +456,12 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -487,6 +487,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         hidden_states = transformer_outputs[0]\n \n@@ -516,29 +517,6 @@ def forward(\n             attentions=transformer_outputs.attentions,\n         )\n \n-    def _reorder_cache(\n-        self, past: tuple[tuple[torch.Tensor, torch.Tensor], ...], beam_idx: torch.LongTensor\n-    ) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-\n-        Output shares the same memory storage as `past`.\n-        \"\"\"\n-        # Get a copy of `beam_idx` on all the devices where we need those indices.\n-        device_to_beam_idx = {\n-            past_state.device: beam_idx.to(past_state.device) for layer_past in past for past_state in layer_past\n-        }\n-        reordered_past = tuple(\n-            (\n-                layer_past[0].index_select(0, device_to_beam_idx[layer_past[0].device]),\n-                layer_past[1].index_select(0, device_to_beam_idx[layer_past[0].device]),\n-            )\n-            for layer_past in past\n-        )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -579,7 +557,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -703,7 +681,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -781,7 +759,7 @@ def forward(\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as"
        },
        {
            "sha": "159299aa3053001ef960e02f676b7538980536a2",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 25,
            "deletions": 18,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -555,32 +555,39 @@ def __init__(self, config, position_embedding_type=None):\n         self.initial_prior_first_n_blocks = config.initial_prior_first_n_blocks\n         self.initial_prior_diagonal_n_blocks = config.initial_prior_diagonal_n_blocks\n \n-    def transpose_for_scores(self, layer):\n-        new_layer_shape = layer.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        layer = layer.view(*new_layer_shape)\n-        return layer.permute(0, 2, 1, 3)\n-\n     def forward(self, hidden_states, attention_mask=None):\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        batch_size, num_heads, seq_len, head_dim = query_layer.size()\n+        batch_size, seq_len, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # revert changes made by get_extended_attention_mask\n         attention_mask = 1.0 + attention_mask / 10000.0\n         attention_mask = (\n-            attention_mask.squeeze().repeat(1, num_heads, 1).reshape(batch_size * num_heads, seq_len).int()\n+            attention_mask.squeeze()\n+            .repeat(1, self.num_attention_heads, 1)\n+            .reshape(batch_size * self.num_attention_heads, seq_len)\n+            .int()\n         )\n \n         # The CUDA kernels are most efficient with inputs whose size is a multiple of a GPU's warp size (32). Inputs\n         # smaller than this are padded with zeros.\n         gpu_warp_size = 32\n \n-        if head_dim < gpu_warp_size:\n-            pad_size = batch_size, num_heads, seq_len, gpu_warp_size - head_dim\n+        if self.attention_head_size < gpu_warp_size:\n+            pad_size = batch_size, self.num_attention_heads, seq_len, gpu_warp_size - self.attention_head_size\n \n             query_layer = torch.cat([query_layer, torch.zeros(pad_size, device=query_layer.device)], dim=-1)\n             key_layer = torch.cat([key_layer, torch.zeros(pad_size, device=key_layer.device)], dim=-1)\n@@ -597,10 +604,10 @@ def forward(self, hidden_states, attention_mask=None):\n             initial_prior_diagonal_n_blocks=self.initial_prior_diagonal_n_blocks,\n         )\n \n-        if head_dim < gpu_warp_size:\n-            context_layer = context_layer[:, :, :, :head_dim]\n+        if self.attention_head_size < gpu_warp_size:\n+            context_layer = context_layer[:, :, :, : self.attention_head_size]\n \n-        context_layer = context_layer.reshape(batch_size, num_heads, seq_len, head_dim)\n+        context_layer = context_layer.reshape(batch_size, self.num_attention_heads, seq_len, self.attention_head_size)\n \n         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)"
        },
        {
            "sha": "87dcd1d41f0e429c182f5f31909400dc75178efa",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 33,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -757,9 +757,8 @@ class MT5PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     is_parallelizable = True\n     supports_gradient_checkpointing = True\n-    _supports_quantized_cache = False  # enc-dec models don't support yet\n     _supports_static_cache = True\n-    _supports_cache_class = True\n+\n     _no_split_modules = [\"MT5Block\"]\n     _keep_in_fp32_modules = [\"wo\"]\n \n@@ -1857,37 +1856,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration._reorder_cache\n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        # if decoder past is not included in output\n-        # speedy decoding is disabled and no need to reorder\n-        if past_key_values is None:\n-            logger.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n-            return past_key_values\n-\n-        reordered_decoder_past = ()\n-        for layer_past_states in past_key_values:\n-            # get the correct batch idx from layer past batch dim\n-            # batch dim of `past` is at 2nd position\n-            reordered_layer_past_states = ()\n-            for layer_past_state in layer_past_states:\n-                # need to set correct `past` for each of the four key / value states\n-                reordered_layer_past_states = reordered_layer_past_states + (\n-                    layer_past_state.index_select(0, beam_idx.to(layer_past_state.device)),\n-                )\n-\n-            if reordered_layer_past_states[0].shape != layer_past_states[0].shape:\n-                raise ValueError(\n-                    f\"reordered_layer_past_states[0] shape {reordered_layer_past_states[0].shape} and layer_past_states[0] shape {layer_past_states[0].shape} mismatched\"\n-                )\n-            if len(reordered_layer_past_states) != len(layer_past_states):\n-                raise ValueError(\n-                    f\"length of reordered_layer_past_states {len(reordered_layer_past_states)} and length of layer_past_states {len(layer_past_states)} mismatched\"\n-                )\n-\n-            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n-        return reordered_decoder_past\n-\n \n @auto_docstring\n class MT5EncoderModel(MT5PreTrainedModel):"
        },
        {
            "sha": "b7f092ea64a80e0ee91665deeff6717a6b32f5b3",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 90,
            "deletions": 91,
            "changes": 181,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import (\n     ClassifierFreeGuidanceLogitsProcessor,\n     GenerationConfig,\n@@ -189,11 +190,12 @@ def __init__(\n         self,\n         embed_dim: int,\n         num_heads: int,\n-        dropout: float = 0.0,\n-        is_decoder: bool = False,\n-        bias: bool = True,\n-        is_causal: bool = False,\n+        dropout: Optional[float] = 0.0,\n+        is_decoder: Optional[bool] = False,\n+        bias: Optional[bool] = True,\n+        is_causal: Optional[bool] = False,\n         config: Optional[MusicgenConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -210,6 +212,7 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n@@ -220,10 +223,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n+        output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -244,42 +248,35 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n-            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -301,11 +298,11 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class MusicgenDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: MusicgenDecoderConfig):\n+    def __init__(self, config: MusicgenDecoderConfig, layer_idx=None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n@@ -317,6 +314,7 @@ def __init__(self, config: MusicgenDecoderConfig):\n             bias=False,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -330,6 +328,7 @@ def __init__(self, config: MusicgenDecoderConfig):\n             is_decoder=True,\n             bias=False,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.ffn_dim, bias=False)\n@@ -346,9 +345,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -372,42 +372,35 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.final_layer_norm(hidden_states)\n@@ -421,10 +414,6 @@ def forward(\n \n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n-\n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -477,7 +466,9 @@ def __init__(self, config: MusicgenDecoderConfig):\n             config.hidden_size,\n         )\n \n-        self.layers = nn.ModuleList([MusicgenDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layers = nn.ModuleList(\n+            [MusicgenDecoderLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]\n+        )\n         self.layer_norm = nn.LayerNorm(config.hidden_size)\n         self.attn_implementation = config._attn_implementation\n \n@@ -506,6 +497,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):\n@@ -565,8 +557,24 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n \n         if inputs_embeds is None:\n             inputs_embeds = sum([self.embed_tokens[codebook](input[:, codebook]) for codebook in range(num_codebooks)])\n@@ -586,23 +594,13 @@ def forward(\n \n         # embed positions\n         positions = self.embed_positions(input, past_key_values_length)\n-\n         hidden_states = inputs_embeds + positions.to(inputs_embeds.device)\n-\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -620,24 +618,19 @@ def forward(\n             if self.training and (dropout_probability < self.layerdrop):\n                 continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n             )\n             hidden_states = layer_outputs[0]\n-\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -650,16 +643,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -772,6 +767,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):\n@@ -831,6 +827,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -898,6 +895,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -960,6 +958,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         hidden_states = outputs[0]\n@@ -1875,6 +1874,7 @@ def prepare_inputs_for_generation(\n         encoder_outputs=None,\n         decoder_delay_pattern_mask=None,\n         guidance_scale=None,\n+        cache_position=None,\n         **kwargs,\n     ):\n         # Overwritten -- MusicGen has custom processing\n@@ -1896,16 +1896,15 @@ def prepare_inputs_for_generation(\n                 decoder_attention_mask = decoder_attention_mask.repeat((2, 1))\n \n         if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n+            if cache_position[-1] >= decoder_input_ids.shape[1]:\n+                decoder_input_ids = decoder_input_ids[:, -cache_position.shape[0] :]\n+            elif (\n+                decoder_input_ids.shape[1] != cache_position.shape[0]\n+            ):  # Default case (the \"else\", a no op, is Exception 2)\n+                decoder_input_ids = decoder_input_ids[:, cache_position]\n             else:\n                 # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n+                decoder_input_ids = decoder_input_ids[:, -1:]\n \n         return {\n             \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed"
        },
        {
            "sha": "e415967b0a9054533827b8668ba9484af96f6dbe",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 81,
            "deletions": 82,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import (\n     ClassifierFreeGuidanceLogitsProcessor,\n     GenerationConfig,\n@@ -197,11 +198,12 @@ def __init__(\n         self,\n         embed_dim: int,\n         num_heads: int,\n-        dropout: float = 0.0,\n-        is_decoder: bool = False,\n-        bias: bool = True,\n-        is_causal: bool = False,\n+        dropout: Optional[float] = 0.0,\n+        is_decoder: Optional[bool] = False,\n+        bias: Optional[bool] = True,\n+        is_causal: Optional[bool] = False,\n         config: Optional[MusicgenMelodyConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -218,6 +220,7 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n@@ -228,10 +231,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n+        output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -252,42 +256,35 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `key_value_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == key_value_states.shape[1]\n-        ):\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self.k_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n-            value_states = self.v_proj(key_value_states).view(*kv_input_shape).transpose(1, 2)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -309,11 +306,11 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class MusicgenMelodyDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: MusicgenMelodyDecoderConfig):\n+    def __init__(self, config: MusicgenMelodyDecoderConfig, layer_idx=None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n@@ -325,6 +322,7 @@ def __init__(self, config: MusicgenMelodyDecoderConfig):\n             bias=False,\n             is_causal=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -341,9 +339,10 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -360,15 +359,13 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n@@ -381,16 +378,7 @@ def forward(\n         hidden_states = self.fc2(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n-        return outputs\n+        return hidden_states, self_attn_weights\n \n \n @auto_docstring\n@@ -444,7 +432,9 @@ def __init__(self, config: MusicgenMelodyDecoderConfig):\n             config.hidden_size,\n         )\n \n-        self.layers = nn.ModuleList([MusicgenMelodyDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layers = nn.ModuleList(\n+            [MusicgenMelodyDecoderLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]\n+        )\n         self.layer_norm = nn.LayerNorm(config.hidden_size)\n         self.attn_implementation = config._attn_implementation\n \n@@ -473,6 +463,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):\n@@ -526,9 +517,24 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n \n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if inputs_embeds is None:\n             inputs_embeds = sum([self.embed_tokens[codebook](input[:, codebook]) for codebook in range(num_codebooks)])\n \n@@ -556,22 +562,12 @@ def forward(\n \n         # embed positions\n         positions = self.embed_positions(inputs_embeds, past_key_values_length)\n-\n         hidden_states = inputs_embeds + positions.to(inputs_embeds.device)\n-\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n-        next_decoder_cache = () if use_cache else None\n \n         # check if head_mask has a correct number of layers specified if desired\n         if head_mask is not None:\n@@ -589,21 +585,16 @@ def forward(\n             if self.training and (dropout_probability < self.layerdrop):\n                 continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask=attention_mask,\n                 layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n             )\n             hidden_states = layer_outputs[0]\n-\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n-\n             if output_attentions:\n                 all_attentions += (layer_outputs[1],)\n \n@@ -613,12 +604,16 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_attentions] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_attentions] if v is not None\n+            )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n         )\n@@ -708,6 +703,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):\n@@ -760,6 +756,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -827,6 +824,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, MusicgenMelodyOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):\n@@ -881,6 +879,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         hidden_states = outputs[0]\n@@ -1793,7 +1792,7 @@ def prepare_inputs_for_generation(\n                 decoder_attention_mask = decoder_attention_mask.repeat((2, 1))\n \n         if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n+            past_length = past_key_values.get_seq_length()\n \n             # Some generation methods already pass only the last input ID\n             if decoder_input_ids.shape[1] > past_length:"
        },
        {
            "sha": "1223d23fba206d66bf2bde0bf03d0ed29a1a583e",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 87,
            "deletions": 105,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n@@ -97,9 +98,10 @@ def __init__(\n         self,\n         embed_dim: int,\n         num_heads: int,\n-        dropout: float = 0.0,\n-        is_decoder: bool = False,\n-        bias: bool = True,\n+        dropout: Optional[float] = 0.0,\n+        is_decoder: Optional[bool] = False,\n+        bias: Optional[bool] = True,\n+        layer_idx: Optional[bool] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -114,24 +116,23 @@ def __init__(\n             )\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n+        self.layer_idx = layer_idx\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         attn_prompt: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -143,35 +144,38 @@ def forward(\n \n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        if is_cross_attention and past_key_value is not None:\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states)\n+            value_states = self.v_proj(current_states)\n+            key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         if attn_prompt is not None:\n             key_states = torch.cat([attn_prompt[0].expand(bsz, -1, -1, -1), key_states], dim=2)\n@@ -181,9 +185,10 @@ def forward(\n                 attention_mask = torch.cat([prompt_mask, attention_mask], dim=(-1))\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n+        query_states = query_states.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.reshape(*proj_shape)\n+        key_states = key_states.reshape(*proj_shape)\n+        value_states = value_states.reshape(*proj_shape)\n \n         src_len = key_states.size(1)\n         attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n@@ -242,7 +247,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped\n \n \n class MvpEncoderLayer(GradientCheckpointingLayer):\n@@ -284,7 +289,7 @@ def forward(\n                 returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -309,16 +314,11 @@ def forward(\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states, attn_weights\n \n \n class MvpDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: MvpConfig):\n+    def __init__(self, config: MvpConfig, layer_idx=None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -327,6 +327,7 @@ def __init__(self, config: MvpConfig):\n             num_heads=config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=True,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -338,6 +339,7 @@ def __init__(self, config: MvpConfig):\n             config.decoder_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=True,\n+            layer_idx=layer_idx,\n         )\n         self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n         self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n@@ -354,9 +356,10 @@ def forward(\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n         self_attn_prompt: Optional[torch.Tensor] = None,\n         cross_attn_prompt: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -383,45 +386,37 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             attn_prompt=self_attn_prompt,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n                 attn_prompt=cross_attn_prompt,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n             hidden_states = residual + hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.activation_fn(self.fc1(hidden_states))\n@@ -436,9 +431,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -735,7 +727,7 @@ def __init__(\n             config.max_position_embeddings,\n             config.d_model,\n         )\n-        self.layers = nn.ModuleList([MvpDecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.layers = nn.ModuleList([MvpDecoderLayer(config, layer_idx=i) for i in range(config.decoder_layers)])\n         self.layernorm_embedding = nn.LayerNorm(config.d_model)\n \n         self.use_prompt = use_prompt\n@@ -776,6 +768,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -862,12 +855,27 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         attention_mask = _prepare_4d_causal_attention_mask(\n             attention_mask, input_shape, inputs_embeds, past_key_values_length\n         )\n@@ -893,18 +901,10 @@ def forward(\n             self_attn_prompt = self.self_attn_prompt(prompt_ids)\n             cross_attn_prompt = self.cross_attn_prompt(prompt_ids)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = () if use_cache else None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -924,8 +924,6 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 attention_mask,\n@@ -935,15 +933,12 @@ def forward(\n                 cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n                 self_attn_prompt=(self_attn_prompt[idx] if self.use_prompt else None),\n                 cross_attn_prompt=(cross_attn_prompt[idx] if self.use_prompt else None),\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n             )\n             hidden_states = layer_outputs[0]\n-\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -954,16 +949,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1028,6 +1025,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1111,6 +1109,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1196,6 +1195,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1285,6 +1285,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n \n@@ -1312,17 +1313,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1739,6 +1729,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -1809,15 +1800,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"MvpForCausalLM\","
        },
        {
            "sha": "745f88b88742716458573342a8a2d39d5ae90917",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -588,8 +588,7 @@ class NemotronPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "ea584137632c5cc6762625f5a2ef731b20a01972",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 85,
            "deletions": 89,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n@@ -511,11 +512,12 @@ def __init__(\n         self,\n         embed_dim: int,\n         num_heads: int,\n-        dropout: float = 0.0,\n-        is_decoder: bool = False,\n-        bias: bool = True,\n-        is_causal: bool = False,\n+        dropout: Optional[float] = 0.0,\n+        is_decoder: Optional[bool] = False,\n+        bias: Optional[bool] = True,\n+        is_causal: Optional[bool] = False,\n         config: Optional[NllbMoeConfig] = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -532,6 +534,7 @@ def __init__(\n         self.scaling = self.head_dim**-0.5\n         self.is_decoder = is_decoder\n         self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n@@ -542,10 +545,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n+        output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n@@ -566,42 +570,35 @@ def forward(\n         # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        # get key, value proj\n-        # `past_key_value[0].shape[2] == encoder_hidden_states.shape[1]`\n-        # is checking that the `sequence_length` of the `past_key_value` is the same as\n-        # the provided `encoder_hidden_states` to support prefix tuning\n-        if (\n-            is_cross_attention\n-            and past_key_value is not None\n-            and past_key_value[0].shape[2] == encoder_hidden_states.shape[1]\n-        ):\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self.k_proj(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            value_states = self.v_proj(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n-        elif past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self.k_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-            value_states = self.v_proj(hidden_states).view(*kv_input_shape).transpose(1, 2)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+            key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n+            value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -623,7 +620,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class NllbMoeEncoderLayer(GradientCheckpointingLayer):\n@@ -669,7 +666,7 @@ def forward(\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -709,7 +706,7 @@ def forward(\n \n \n class NllbMoeDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n+    def __init__(self, config: NllbMoeConfig, is_sparse: bool = False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n         self.is_sparse = is_sparse\n@@ -719,6 +716,7 @@ def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n             dropout=config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.dropout = config.dropout\n         self.activation_fn = ACT2FN[config.activation_function]\n@@ -731,6 +729,7 @@ def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n             config.attention_dropout,\n             is_decoder=True,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.cross_attention_layer_norm = nn.LayerNorm(self.embed_dim)\n         if not self.is_sparse:\n@@ -748,10 +747,11 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = True,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -779,42 +779,35 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        # add present self-attn cache to positions 1,2 of present_key_value tuple\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = self.attn_dropout(hidden_states)\n         hidden_states = residual + hidden_states\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n             hidden_states = self.cross_attention_layer_norm(hidden_states)\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.cross_attention(\n+            hidden_states, cross_attn_weights = self.cross_attention(\n                 hidden_states=hidden_states,\n                 encoder_hidden_states=encoder_hidden_states,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             hidden_states = self.attn_dropout(hidden_states)\n             hidden_states = residual + hidden_states\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value += cross_attn_present_key_value\n-\n         # Fully Connected\n         residual = hidden_states\n \n@@ -833,7 +826,7 @@ def forward(\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (hidden_states, present_key_value)\n+        outputs = (hidden_states,)\n \n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n@@ -1112,7 +1105,7 @@ def __init__(self, config: NllbMoeConfig, embed_tokens: Optional[nn.Embedding] =\n         self.layers = nn.ModuleList()\n         for i in range(config.decoder_layers):\n             is_sparse = (i + 1) % sparse_step == 0 if sparse_step > 0 else False\n-            self.layers.append(NllbMoeDecoderLayer(config, is_sparse))\n+            self.layers.append(NllbMoeDecoderLayer(config, is_sparse, layer_idx=i))\n \n         self.layer_norm = nn.LayerNorm(config.d_model)\n \n@@ -1135,6 +1128,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = True,\n     ):\n         r\"\"\"\n         Args:\n@@ -1222,12 +1216,28 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        # initialize `past_key_values`\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         attention_mask = self._update_causal_mask(\n             attention_mask,\n             input_shape,\n@@ -1249,19 +1259,11 @@ def forward(\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_probs = () if output_router_logits else None\n         all_cross_attentions = () if output_attentions else None\n-        present_key_value_states = () if use_cache else None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1285,8 +1287,6 @@ def forward(\n                 layer_head_mask = head_mask[idx] if head_mask is not None else None\n                 cross_attn_layer_head_mask = cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n \n-                past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n@@ -1295,23 +1295,21 @@ def forward(\n                     encoder_attention_mask=encoder_attention_mask,\n                     layer_head_mask=layer_head_mask,\n                     cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n                     output_router_logits=output_router_logits,\n+                    cache_position=cache_position,\n                 )\n \n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:\n                 continue\n \n-            if use_cache:\n-                present_key_value_states += (layer_outputs[1],)\n-\n             if output_attentions:\n-                all_self_attns += (layer_outputs[2],)\n-                all_cross_attentions += (layer_outputs[3],)\n+                all_self_attns += (layer_outputs[1],)\n+                all_cross_attentions += (layer_outputs[2],)\n \n             if output_router_logits:\n                 all_router_probs += (layer_outputs[-1],)\n@@ -1322,12 +1320,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    present_key_value_states,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attns,\n                     all_cross_attentions,\n@@ -1337,7 +1338,7 @@ def forward(\n             )\n         return MoEModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=present_key_value_states,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1476,6 +1477,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = True,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqMoEModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1555,6 +1557,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1625,6 +1628,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqMoEOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1696,6 +1700,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         lm_logits = self.lm_head(outputs[0])\n \n@@ -1767,15 +1772,6 @@ def _unpack_router_logits(self, router_outputs):\n         total_expert_indexes = torch.stack(total_expert_indexes, dim=1) if len(total_expert_indexes) > 0 else None\n         return total_router_logits, total_expert_indexes\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"NllbMoeForConditionalGeneration\","
        },
        {
            "sha": "babd8acc09f7460d71eb5860b43f396950a1ca3b",
            "filename": "src/transformers/models/nystromformer/modeling_nystromformer.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -167,17 +167,23 @@ def iterative_inv(self, mat, n_iter=6):\n             )\n         return value\n \n-    def transpose_for_scores(self, layer):\n-        new_layer_shape = layer.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        layer = layer.view(*new_layer_shape)\n-        return layer.permute(0, 2, 1, 3)\n-\n     def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         query_layer = query_layer / math.sqrt(math.sqrt(self.attention_head_size))\n         key_layer = key_layer / math.sqrt(math.sqrt(self.attention_head_size))"
        },
        {
            "sha": "77e41e2d62920a1ee4252abf70f6746eab4954ee",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -293,8 +293,7 @@ class OlmoPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "5dd56e2eddfb38b797ad620fe454ebf426223101",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -298,8 +298,7 @@ class Olmo2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "6c491754d448eb3c8412c1f8b69cf572c9cc9906",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -705,8 +705,7 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "ab1ae0b9744bad5fdf7d464a62de4715aae6e5c3",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -494,11 +494,6 @@ def __init__(self, config, hidden_size, num_attention_heads, dropout):\n         self.out_proj = nn.Linear(hidden_size, hidden_size)\n         self.dropout = nn.Dropout(dropout)\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n     def forward(\n         self,\n         queries: torch.Tensor,\n@@ -507,9 +502,18 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n-        query_layer = self.transpose_for_scores(self.query(queries))\n-        key_layer = self.transpose_for_scores(self.key(keys))\n-        value_layer = self.transpose_for_scores(self.value(values))\n+        batch_size, seq_length, _ = queries.shape\n+        query_layer = (\n+            self.query(queries)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(keys).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(values).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))"
        },
        {
            "sha": "b6641f4820f55e0b6dee7e45d0ef527c0728b361",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -312,8 +312,7 @@ class OPTPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -872,15 +871,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "5ab2b93e32a4eb015a2d3bfafff389173d618b01",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -113,8 +113,7 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PaliGemmaMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "bf2dc59d249ddedf136c16c2f93b7db34557c2b3",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -309,7 +309,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,"
        },
        {
            "sha": "c613fd8955afa0ddf7ecd9fe63788f9f85cd1c70",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -106,7 +106,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,"
        },
        {
            "sha": "1eec8b4166dcb68ab1c26981059a8cf01dc1fe17",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 9,
            "deletions": 42,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -266,7 +266,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Pegasus, MBART->PEGASUS\n@@ -309,7 +309,7 @@ def forward(\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -330,12 +330,7 @@ def forward(\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states, attn_weights\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Pegasus, MBART->PEGASUS\n@@ -409,7 +404,7 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -426,7 +421,7 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -451,9 +446,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -465,7 +457,7 @@ class PegasusPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -1108,7 +1100,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1141,9 +1132,6 @@ def forward(\n             )\n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1156,19 +1144,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1537,17 +1524,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->Pegasus\n class PegasusDecoderWrapper(PegasusPreTrainedModel):\n@@ -1710,14 +1686,5 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\"PegasusForCausalLM\", \"PegasusForConditionalGeneration\", \"PegasusModel\", \"PegasusPreTrainedModel\"]"
        },
        {
            "sha": "a371be87de2ee7b9be3726cdd7a71983254dfc28",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 7,
            "deletions": 27,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -287,7 +287,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class PegasusXGlobalLocalAttention(nn.Module):\n@@ -705,7 +705,7 @@ def forward(\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -721,7 +721,7 @@ def forward(\n             residual = hidden_states\n             hidden_states = self.encoder_attn_layer_norm(hidden_states)\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -744,10 +744,6 @@ def forward(\n \n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n-\n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -761,7 +757,7 @@ class PegasusXPreTrainedModel(PreTrainedModel):\n     # Flaky logits\n     _supports_sdpa = False\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -1364,7 +1360,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         for idx, decoder_layer in enumerate(self.layers):\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n@@ -1387,9 +1382,6 @@ def forward(\n             )\n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1402,19 +1394,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1724,17 +1715,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderWrapper with Bart->PegasusX\n class PegasusXDecoderWrapper(PegasusXPreTrainedModel):"
        },
        {
            "sha": "da244141c7d5c7b042e62907cace7e74b8b7bea4",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -91,10 +91,10 @@ class PerceptionLMPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n+\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n@@ -130,10 +130,14 @@ class PerceptionLMModelOutputWithPast(BaseModelOutputWithPast):\n         `past_key_values` input) to speed up sequential decoding.\n     image_hidden_states (`torch.FloatTensor`, *optional*):\n         A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+        Image hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_videos, sequence_length, hidden_size)`.\n+        Video hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None\n+\n     video_hidden_states: Optional[torch.FloatTensor] = None\n \n \n@@ -157,7 +161,10 @@ class PerceptionLMCausalLMOutputWithPast(ModelOutput):\n         `past_key_values` input) to speed up sequential decoding.\n     image_hidden_states (`torch.FloatTensor`, *optional*):\n         A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+        Image hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_videos, sequence_length, hidden_size)`.\n+        Video hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -166,6 +173,7 @@ class PerceptionLMCausalLMOutputWithPast(ModelOutput):\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n+\n     video_hidden_states: Optional[torch.FloatTensor] = None\n \n "
        },
        {
            "sha": "3258fcd79fa9ce4c51d731532a04b2c537ce2806",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -96,10 +96,44 @@ class PerceptionLMPreTrainedModel(LlavaPreTrainedModel):\n \n \n class PerceptionLMModelOutputWithPast(LlavaModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        Image hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_videos, sequence_length, hidden_size)`.\n+        Video hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n     video_hidden_states: Optional[torch.FloatTensor] = None\n \n \n class PerceptionLMCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        Image hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_videos, sequence_length, hidden_size)`.\n+        Video hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n     video_hidden_states: Optional[torch.FloatTensor] = None\n \n "
        },
        {
            "sha": "ef69edc1870d8466cd38695523e6fbcb1c312164",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -388,8 +388,7 @@ class PersimmonPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PersimmonDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "d08d73d87adce22c4b7453e1a7ad7f2628be9807",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -298,8 +298,7 @@ class PhiPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "fb92b54105a5c95844dc99d392f95764344c3b86",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -290,8 +290,7 @@ class Phi3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "58008f692bd862785e804948338d8cc0f59e61ec",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -1592,8 +1592,7 @@ class Phi4MultimodalPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "df2978f123e6b675c3dfab26b5d0550d59aaa4b1",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -889,8 +889,7 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "900660aa5f738df42983635fb736ed7c77109362",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 32,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -350,7 +350,7 @@ def forward(\n @auto_docstring\n class Pix2StructPreTrainedModel(PreTrainedModel):\n     config_class = Pix2StructConfig\n-    _supports_cache_class = True\n+\n     _supports_static_cache = False\n \n     @property\n@@ -1037,37 +1037,6 @@ def __init__(self, config):\n         self.post_init()\n         self.gradient_checkpointing = False\n \n-    # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._reorder_cache\n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        # if decoder past is not included in output\n-        # speedy decoding is disabled and no need to reorder\n-        if past_key_values is None:\n-            logger.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n-            return past_key_values\n-\n-        reordered_decoder_past = ()\n-        for layer_past_states in past_key_values:\n-            # get the correct batch idx from layer past batch dim\n-            # batch dim of `past` is at 2nd position\n-            reordered_layer_past_states = ()\n-            for layer_past_state in layer_past_states:\n-                # need to set correct `past` for each of the four key / value states\n-                reordered_layer_past_states = reordered_layer_past_states + (\n-                    layer_past_state.index_select(0, beam_idx.to(layer_past_state.device)),\n-                )\n-\n-            if reordered_layer_past_states[0].shape != layer_past_states[0].shape:\n-                raise ValueError(\n-                    f\"reordered_layer_past_states[0] shape {reordered_layer_past_states[0].shape} and layer_past_states[0] shape {layer_past_states[0].shape} mismatched\"\n-                )\n-            if len(reordered_layer_past_states) != len(layer_past_states):\n-                raise ValueError(\n-                    f\"length of reordered_layer_past_states {len(reordered_layer_past_states)} and length of layer_past_states {len(layer_past_states)} mismatched\"\n-                )\n-\n-            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n-        return reordered_decoder_past\n-\n     def get_input_embeddings(self):\n         return self.embed_tokens\n "
        },
        {
            "sha": "51c13b87322af5f75fe1d7b79039f6f22f0215ea",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 7,
            "deletions": 36,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -463,7 +463,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class PLBartEncoderLayer(GradientCheckpointingLayer):\n@@ -505,7 +505,7 @@ def forward(\n                 returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -775,7 +775,7 @@ def forward(\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights, past_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             attention_mask=attention_mask,\n@@ -792,7 +792,7 @@ def forward(\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n \n-            hidden_states, cross_attn_weights, past_key_value = self.encoder_attn(\n+            hidden_states, cross_attn_weights = self.encoder_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -819,9 +819,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -1036,7 +1033,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        next_decoder_cache = None\n \n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n@@ -1069,10 +1065,6 @@ def forward(\n                 cache_position=cache_position,\n             )\n             hidden_states = layer_outputs[0]\n-\n-            if use_cache:\n-                next_decoder_cache = layer_outputs[3 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1083,19 +1075,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n+            past_key_values = past_key_values.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_cross_attentions]\n                 if v is not None\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n@@ -1440,17 +1431,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n class PLBartClassificationHead(nn.Module):\n     \"\"\"Head for sentence-level classification tasks.\"\"\"\n@@ -1769,15 +1749,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\n     \"PLBartForCausalLM\","
        },
        {
            "sha": "2aa8568954d1712077d8b1b6721e7ca9764a0cf4",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -596,17 +596,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id)\n \n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n class PLBartClassificationHead(BartClassificationHead):\n     pass"
        },
        {
            "sha": "6b64c1fd8fd8209ddcb891cf68f14dd47ec7b371",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 31,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -576,7 +576,7 @@ class Pop2PianoPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     is_parallelizable = False\n     supports_gradient_checkpointing = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = False\n     _no_split_modules = [\"Pop2PianoBlock\"]\n     _keep_in_fp32_modules = [\"wo\"]\n@@ -1332,35 +1332,5 @@ def generate(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        # if decoder past is not included in output\n-        # speedy decoding is disabled and no need to reorder\n-        if past_key_values is None:\n-            logger.warning(\"You might want to consider setting `use_cache=True` to speed up decoding\")\n-            return past_key_values\n-\n-        reordered_decoder_past = ()\n-        for layer_past_states in past_key_values:\n-            # get the correct batch idx from layer past batch dim\n-            # batch dim of `past` is at 2nd position\n-            reordered_layer_past_states = ()\n-            for layer_past_state in layer_past_states:\n-                # need to set correct `past` for each of the four key / value states\n-                reordered_layer_past_states = reordered_layer_past_states + (\n-                    layer_past_state.index_select(0, beam_idx.to(layer_past_state.device)),\n-                )\n-\n-            if reordered_layer_past_states[0].shape != layer_past_states[0].shape:\n-                raise ValueError(\n-                    f\"reordered_layer_past_states[0] shape {reordered_layer_past_states[0].shape} and layer_past_states[0] shape {layer_past_states[0].shape} mismatched\"\n-                )\n-            if len(reordered_layer_past_states) != len(layer_past_states):\n-                raise ValueError(\n-                    f\"length of reordered_layer_past_states {len(reordered_layer_past_states)} and length of layer_past_states {len(layer_past_states)} mismatched\"\n-                )\n-\n-            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n-        return reordered_decoder_past\n-\n \n __all__ = [\"Pop2PianoForConditionalGeneration\", \"Pop2PianoPreTrainedModel\"]"
        },
        {
            "sha": "d9c7807d528071b5cc73a21d5eff71bd4d5487e3",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 104,
            "deletions": 112,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import LayerNorm\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n@@ -386,10 +387,10 @@ def forward(self, inputs_shape, device, attention_mask=None, past_key_values=Non\n         )\n \n         if position_ids is None:\n-            if past_key_values is not None:\n+            if past_key_values is not None and past_key_values.get_seq_length() != 0:\n                 # position_ids is the same for every token when decoding a single step\n                 # Without the int() cast, it doesn't work in some cases when exporting to ONNX\n-                prev_num_input_ids = past_key_values[0][0].shape[2]\n+                prev_num_input_ids = past_key_values.get_seq_length()\n                 num_input_ids = inputs_shape[1] + prev_num_input_ids\n                 position_ids = torch.ones((1, 1), dtype=torch.long, device=device) * (\n                     int(self.padding_idx + num_input_ids)\n@@ -415,18 +416,15 @@ def _forward(self, position_ids):\n class ProphetNetAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(\n-        self,\n-        config: ProphetNetConfig,\n-        num_attn_heads: int,\n-    ):\n+    def __init__(self, config: ProphetNetConfig, num_attn_heads: int, layer_idx: Optional[int] = None):\n         super().__init__()\n         hidden_size = config.hidden_size\n \n         self.attention_dropout = config.attention_dropout\n         self.dropout = config.dropout\n         self.num_attn_heads = num_attn_heads\n         self.head_dim = hidden_size // num_attn_heads\n+        self.layer_idx = layer_idx\n \n         assert self.head_dim * num_attn_heads == hidden_size, (\n             \"`config.hidden_size` must be divisible by `config.num_encoder_attention_heads` and\"\n@@ -439,17 +437,15 @@ def __init__(\n \n         self.out_proj = nn.Linear(hidden_size, hidden_size)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_attn_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states,\n         key_value_states: Optional[Tensor] = None,\n         attention_mask: Optional[Tensor] = None,\n         layer_head_mask: Optional[Tensor] = None,\n-        past_key_value: Optional[tuple[Tensor]] = None,\n-        output_attentions: bool = False,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[Tensor, Optional[Tensor]]:\n         batch_size, tgt_len, hidden_size = hidden_states.size()\n \n@@ -465,32 +461,41 @@ def forward(\n         # previous time steps are cached - no need to recompute key and value if they are static\n         query_states = self.query_proj(hidden_states) / (self.head_dim**0.5)\n \n-        if is_cross_attention and past_key_value is not None:\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.key_proj(key_value_states), -1, batch_size)\n-            value_states = self._shape(self.value_proj(key_value_states), -1, batch_size)\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            # self_attention\n-            key_states = self._shape(self.key_proj(hidden_states), -1, batch_size)\n-            value_states = self._shape(self.value_proj(hidden_states), -1, batch_size)\n-\n-        if is_cross_attention:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        # project states into the correct shape\n-        proj_shape = (batch_size, self.num_attn_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, batch_size).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n+            key_states = self.key_proj(current_states)\n+            value_states = self.value_proj(current_states)\n+            key_states = key_states.view(batch_size, -1, self.num_attn_heads, self.head_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.num_attn_heads, self.head_dim).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        query_states = query_states.view(batch_size, tgt_len, self.num_attn_heads, self.head_dim).transpose(1, 2)\n         src_len = key_states.size(2)\n+\n         attn_weights = torch.einsum(\"bsij,bsjk->bsik\", query_states, key_states.transpose(2, 3))\n         expected_shape = (batch_size, self.num_attn_heads, tgt_len, src_len)\n         if attn_weights.size() != expected_shape:\n@@ -538,7 +543,7 @@ def forward(\n         attn_output = self.out_proj(attn_output)\n \n         attn_output = nn.functional.dropout(attn_output, p=self.dropout, training=self.training)\n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_weights_reshaped\n \n \n class ProphetNetFeedForward(nn.Module):\n@@ -565,7 +570,7 @@ def forward(self, hidden_states):\n \n \n class ProphetNetNgramSelfAttention(nn.Module):\n-    def __init__(self, config: ProphetNetConfig):\n+    def __init__(self, config: ProphetNetConfig, layer_idx=None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n@@ -576,6 +581,7 @@ def __init__(self, config: ProphetNetConfig):\n         self.attention_dropout = config.attention_dropout\n         self.head_dim = config.hidden_size // self.num_attn_heads\n         self.ngram = config.ngram\n+        self.layer_idx = layer_idx\n \n         assert self.head_dim * self.num_attn_heads == config.hidden_size, (\n             \"config.hidden_size must be divisible by num_attn_heads\"\n@@ -610,6 +616,7 @@ def forward(\n         main_relative_position_buckets=None,\n         predict_relative_position_buckets=None,\n         position_ids=None,\n+        cache_position=None,\n     ):\n         batch_size, ngram_sequence_length, hidden_size = hidden_states.size()\n         assert list(hidden_states.size()) == [batch_size, ngram_sequence_length, hidden_size], (\n@@ -631,9 +638,9 @@ def forward(\n         value_states = self._shape(value_states, -1, batch_size)\n         proj_shape = (batch_size, self.num_attn_heads, -1, self.head_dim)\n \n-        query_states = query_states.view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n+        query_states = query_states.reshape(*proj_shape)\n+        key_states = key_states.reshape(*proj_shape)\n+        value_states = value_states.reshape(*proj_shape)\n \n         # chunk into main stream and predict stream\n         hidden_states_list = hidden_states.chunk(1 + self.ngram, dim=1)\n@@ -646,15 +653,16 @@ def forward(\n         main_key_states, predict_key_states_list = key_states_list[0], key_states_list[1:]\n         main_value_states, predict_value_states_list = value_states_list[0], value_states_list[1:]\n \n-        # saved states are stored with shape (batch_size, num_attn_heads, seq_len, head_dim)\n+        # ProphetNet has two separate attention layers, one for self and one for cross attention\n+        # We need to obtain the self attention only for this module, if `EncoderDecoderCache`\n         if past_key_value is not None:\n-            prev_main_key_states = past_key_value[0]\n-            main_key_states = torch.cat((prev_main_key_states, main_key_states), dim=2)\n-            prev_main_value_states = past_key_value[1]\n-            main_value_states = torch.cat((prev_main_value_states, main_value_states), dim=2)\n-\n-        # Update cache\n-        past_key_value = (main_key_states, main_value_states)\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+            main_key_states, main_value_states = curr_past_key_value.update(\n+                main_key_states, main_value_states, self.layer_idx, {\"cache_position\": cache_position}\n+            )\n \n         # get seq_length of main stream only\n         sequence_length = ngram_sequence_length // (1 + self.ngram)\n@@ -776,7 +784,7 @@ def forward(\n \n         attn_output = nn.functional.dropout(attn_output, p=self.dropout, training=self.training)\n \n-        return attn_output, main_attn_probs, predict_attn_probs, past_key_value\n+        return attn_output, main_attn_probs, predict_attn_probs\n \n     def get_main_relative_pos_embeddings(\n         self, hidden_states, attn_weights, position_ids, main_relative_position_buckets\n@@ -906,7 +914,7 @@ def forward(\n         output_attentions: bool = False,\n     ):\n         # 1st residual block\n-        attention_output, attn_weights, _ = self.self_attn(\n+        attention_output, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -931,15 +939,15 @@ class ProphetNetDecoderLayer(GradientCheckpointingLayer):\n     Decoder block for Prophetnet\n     \"\"\"\n \n-    def __init__(self, config: ProphetNetConfig):\n+    def __init__(self, config: ProphetNetConfig, layer_idx=None):\n         super().__init__()\n         # 1st residual block\n-        self.self_attn = ProphetNetNgramSelfAttention(config)\n+        self.self_attn = ProphetNetNgramSelfAttention(config, layer_idx=layer_idx)\n         self.self_attn_layer_norm = LayerNorm(config.hidden_size)\n \n         # 2nd residual block\n         if config.add_cross_attention:\n-            self.cross_attn = ProphetNetAttention(config, config.num_decoder_attention_heads)\n+            self.cross_attn = ProphetNetAttention(config, config.num_decoder_attention_heads, layer_idx=layer_idx)\n             self.cross_attn_layer_norm = LayerNorm(config.hidden_size)\n \n         # 3rd residual block\n@@ -959,15 +967,14 @@ def forward(\n         predict_relative_position_buckets=None,\n         position_ids=None,\n         past_key_value=None,\n-        use_cache: bool = True,\n-        output_attentions: bool = False,\n+        use_cache: Optional[bool] = True,\n+        output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n         # 1st residual block\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-        ngram_attention_output, self_attn_weights, self_attn_weights_ngram, present_key_value = self.self_attn(\n+        ngram_attention_output, self_attn_weights, self_attn_weights_ngram = self.self_attn(\n             hidden_states=hidden_states,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             extended_predict_attention_mask=extended_predict_attention_mask,\n@@ -977,24 +984,19 @@ def forward(\n         )\n         hidden_states = self.self_attn_layer_norm(hidden_states + ngram_attention_output)\n \n-        # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-        cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n         cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             # 2nd residual block\n-            attention_output, cross_attn_weights, cross_attn_present_key_value = self.cross_attn(\n+            attention_output, cross_attn_weights = self.cross_attn(\n                 hidden_states=hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attn_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n             )\n             hidden_states = self.cross_attn_layer_norm(attention_output + hidden_states)\n \n-            # add cross-attn to positions 3,4 of present_key_value tuple\n-            present_key_value = present_key_value + cross_attn_present_key_value\n-\n         # 3rd residual block\n         feed_forward_output = self.feed_forward(hidden_states)\n         hidden_states = self.feed_forward_layer_norm(feed_forward_output + hidden_states)\n@@ -1004,9 +1006,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights, self_attn_weights_ngram, cross_attn_weights)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -1160,7 +1159,9 @@ def __init__(self, config: ProphetNetConfig, word_embeddings: Optional[nn.Embedd\n         self.position_embeddings = ProphetNetPositionalEmbeddings(config)\n \n         self.ngram_embeddings = nn.Embedding(self.ngram, config.hidden_size, None)\n-        self.layers = nn.ModuleList([ProphetNetDecoderLayer(config) for _ in range(config.num_decoder_layers)])\n+        self.layers = nn.ModuleList(\n+            [ProphetNetDecoderLayer(config, layer_idx=i) for i in range(config.num_decoder_layers)]\n+        )\n         self.embeddings_layer_norm = LayerNorm(config.hidden_size)\n \n         self.gradient_checkpointing = False\n@@ -1188,6 +1189,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, ProphetNetDecoderModelOutput]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n@@ -1225,13 +1227,32 @@ def forward(\n \n         batch_size, sequence_length = inputs_embeds.shape[:2]\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+\n         main_stream_pos_embed, position_ids = self.position_embeddings(\n             (batch_size, sequence_length),\n             device=inputs_embeds.device,\n             past_key_values=past_key_values,\n         )\n \n-        if past_key_values is not None:\n+        if past_key_values_length != 0:\n             main_relative_position_buckets, predict_relative_position_buckets = None, None\n         else:\n             (\n@@ -1246,7 +1267,7 @@ def forward(\n         ngram_embeddings = self.ngram_embeddings.weight\n \n         # prepare attention mask\n-        if past_key_values is not None:\n+        if past_key_values_length != 0:\n             assert hidden_states.size(1) == 1, (\n                 \"At the moment `use_cache` is only supported for `decoder_input_ids` of length 1\"\n             )\n@@ -1288,15 +1309,6 @@ def forward(\n         all_ngram_stream_attns = () if output_attentions else None\n         all_cross_attns = () if output_attentions and self.config.add_cross_attention else None\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        present_key_values = () if use_cache else None\n-\n         # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n             if attn_mask is not None:\n@@ -1311,8 +1323,6 @@ def forward(\n                 if self.config.ngram > 0:\n                     all_ngram_stream_hidden_states += (hidden_states[:, sequence_length:],)\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 extended_attention_mask,\n@@ -1324,16 +1334,13 @@ def forward(\n                 main_relative_position_buckets=main_relative_position_buckets,\n                 predict_relative_position_buckets=predict_relative_position_buckets,\n                 position_ids=position_ids,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-\n-            if use_cache:\n-                present_key_values += (layer_outputs[4 if output_attentions else 1],)\n-\n             if output_attentions:\n                 all_main_stream_attns += (layer_outputs[1],)\n                 all_ngram_stream_attns += (layer_outputs[2],)\n@@ -1346,6 +1353,9 @@ def forward(\n             if self.config.ngram > 0:\n                 all_ngram_stream_hidden_states += (hidden_states[:, sequence_length:],)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         # split last_hidden_state for return\n         last_hidden_state = hidden_states[:, :sequence_length]\n         last_hidden_state_ngram = hidden_states[:, sequence_length:] if self.config.ngram > 0 else None\n@@ -1356,7 +1366,7 @@ def forward(\n                 for v in [\n                     last_hidden_state,\n                     last_hidden_state_ngram,\n-                    present_key_values,\n+                    past_key_values,\n                     all_main_stream_hidden_states,\n                     all_ngram_stream_hidden_states,\n                     all_main_stream_attns,\n@@ -1368,7 +1378,7 @@ def forward(\n         return ProphetNetDecoderModelOutput(\n             last_hidden_state=last_hidden_state,\n             last_hidden_state_ngram=last_hidden_state_ngram,\n-            past_key_values=present_key_values,\n+            past_key_values=past_key_values,\n             hidden_states=all_main_stream_hidden_states,\n             hidden_states_ngram=all_ngram_stream_hidden_states,\n             attentions=all_main_stream_attns,\n@@ -1516,6 +1526,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, ProphetNetSeq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1587,6 +1598,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             use_cache=use_cache,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1657,6 +1669,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, ProphetNetSeq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1722,6 +1735,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         batch_size, sequence_length = (\n             decoder_input_ids.shape if decoder_input_ids is not None else decoder_inputs_embeds.shape[:2]\n@@ -1791,18 +1805,6 @@ def _compute_loss(self, logits, labels, ignore_index=-100):\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n \n-    @staticmethod\n-    # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration._reorder_cache\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            # cached cross_attention states don't have to be reordered -> they are always the same\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n     def get_encoder(self):\n         return self.prophetnet.encoder\n \n@@ -2025,16 +2027,6 @@ def prepare_inputs_for_generation(\n             \"use_cache\": use_cache,\n         }\n \n-    @staticmethod\n-    # Copied from transformers.models.bart.modeling_bart.BartForCausalLM._reorder_cache\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n class ProphetNetDecoderWrapper(ProphetNetPreTrainedModel):\n     \"\"\""
        },
        {
            "sha": "e58c08c223e73e4258db9fd926725ace182373fd",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -262,8 +262,7 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "d576c801a4ced5da275ace7d1e93e13ecfc30542",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -87,7 +87,6 @@ class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n     _supports_static_cache = False\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "cf2d802abb6b691ad1c43e597569f4af54daa624",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -325,7 +325,7 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "eafcbaf01926d1d5af22268c791bfdc2d32160bd",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -188,7 +188,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, None\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.whisper.modeling_whisper.WhisperEncoderLayer with Whisper->Qwen2Audio, WHISPER->QWEN2AUDIO\n@@ -231,7 +231,7 @@ def forward(\n         \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states, attn_weights, _ = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -252,12 +252,7 @@ def forward(\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states, attn_weights\n \n \n @auto_docstring"
        },
        {
            "sha": "9503c92bff6e1011fdf0900fdbbffa22d8d3b9da",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -746,7 +746,6 @@ class Qwen2MoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "d2f9b535c9b0d110964913963da41a48c1a8fce5",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -658,7 +658,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-    _supports_cache_class = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "b6e537ec1825abf2fdceb999ed40b4fab894db66",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -288,8 +288,7 @@ class Qwen3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n+\n     _supports_static_cache = True\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "f7b180558ad6eac546a4a5d112ba618c9533469d",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -411,8 +411,6 @@ class Qwen3MoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n     _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {"
        },
        {
            "sha": "89f9f7d1b93e366a94455ee9f9a2aa26663e2728",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 27,
            "deletions": 8,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -21,7 +21,7 @@\n import torch\n from torch import nn\n \n-from ...cache_utils import EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationConfig, GenerationMixin, LogitsProcessorList, StoppingCriteriaList\n from ...modeling_outputs import ModelOutput\n@@ -50,7 +50,7 @@ class RetrievAugLMMarginOutput(ModelOutput):\n     doc_scores (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`):\n         Score between each retrieved document embeddings (see `retrieved_doc_embeds`) and\n         `question_encoder_last_hidden_state`.\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n         num_heads, sequence_length, embed_size_per_head)`).\n \n@@ -115,7 +115,7 @@ class RetrievAugLMMarginOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     doc_scores: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     retrieved_doc_embeds: Optional[torch.FloatTensor] = None\n     retrieved_doc_ids: Optional[torch.LongTensor] = None\n     context_input_ids: Optional[torch.LongTensor] = None\n@@ -141,7 +141,7 @@ class RetrievAugLMOutput(ModelOutput):\n     doc_scores (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`):\n         Score between each retrieved document embeddings (see `retrieved_doc_embeds`) and\n         `question_encoder_last_hidden_state`.\n-    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n         num_heads, sequence_length, embed_size_per_head)`).\n \n@@ -205,7 +205,7 @@ class RetrievAugLMOutput(ModelOutput):\n \n     logits: Optional[torch.FloatTensor] = None\n     doc_scores: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     retrieved_doc_embeds: Optional[torch.FloatTensor] = None\n     retrieved_doc_ids: Optional[torch.LongTensor] = None\n     context_input_ids: Optional[torch.LongTensor] = None\n@@ -439,7 +439,7 @@ def forward(\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         doc_scores: Optional[torch.FloatTensor] = None,\n         context_input_ids: Optional[torch.LongTensor] = None,\n         context_attention_mask: Optional[torch.LongTensor] = None,\n@@ -713,7 +713,7 @@ def forward(\n         encoder_outputs: Optional[tuple[tuple[torch.Tensor]]] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         context_input_ids: Optional[torch.LongTensor] = None,\n         context_attention_mask: Optional[torch.LongTensor] = None,\n         doc_scores: Optional[torch.FloatTensor] = None,\n@@ -1204,6 +1204,8 @@ def _reorder_stacked(hidden_states, new_order):\n         if isinstance(past_key_values, EncoderDecoderCache):\n             reordered_past = EncoderDecoderCache.from_legacy_cache(reordered_past)\n \n+        if isinstance(past_key_values, EncoderDecoderCache):\n+            reordered_past = EncoderDecoderCache.from_legacy_cache(reordered_past)\n         return reordered_past\n \n     def marginalize(self, seq_logits, doc_scores, n_docs=None):\n@@ -1225,7 +1227,7 @@ def forward(\n         encoder_outputs: Optional[tuple[tuple[torch.Tensor]]] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         context_input_ids: Optional[torch.LongTensor] = None,\n         context_attention_mask: Optional[torch.LongTensor] = None,\n         doc_scores: Optional[torch.FloatTensor] = None,\n@@ -1563,6 +1565,15 @@ def extend_enc_output(tensor, num_beams=None):\n             generation_config=generation_config, stopping_criteria=stopping_criteria\n         )\n \n+        self._prepare_cache_for_generation(\n+            generation_config,\n+            model_kwargs,\n+            assistant_model=None,\n+            batch_size=input_ids.shape[0],\n+            max_cache_length=generation_config.max_length - 1,\n+            device=input_ids.device,\n+        )\n+\n         if generation_config.num_beams == 1:\n             if generation_config.num_return_sequences > 1:\n                 raise ValueError(\n@@ -1581,6 +1592,14 @@ def extend_enc_output(tensor, num_beams=None):\n         elif generation_config.num_beams > 1:\n             if generation_config.num_return_sequences > generation_config.num_beams:\n                 raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n+\n+            # 11. interleave input_ids with `num_beams` additional sequences per batch\n+            input_ids, model_kwargs = self._expand_inputs_for_generation(\n+                input_ids=input_ids,\n+                expand_size=generation_config.num_beams,\n+                is_encoder_decoder=self.config.is_encoder_decoder,\n+                **model_kwargs,\n+            )\n             return self._beam_search(\n                 input_ids,\n                 logits_processor=pre_processor,"
        },
        {
            "sha": "b6c2325a692e0c9e5b152a1300b284aa5082eb93",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -511,8 +511,6 @@ class RecurrentGemmaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"cache\"]\n     _supports_flash_attn = False\n     _supports_sdpa = False  # we can't compare with eager for now\n-    _supports_cache_class = True\n-    _supports_quantized_cache = True\n \n     def _init_weights(self, module):\n         std = math.sqrt(self.config.w_init_variance_scale / self.config.conv1d_width)\n@@ -807,15 +805,5 @@ def forward(\n             hidden_states=outputs.hidden_states,\n         )\n \n-    # Ignore copy\n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        for layer in self.layers:\n-            if hasattr(layer.temporal_block, \"key_states\"):\n-                k_state = layer.temporal_block.key_states\n-                v_state = layer.temporal_block.value_states\n-                k_state = k_state.index_select(0, beam_idx.to(k_state.device))\n-                v_state = v_state.index_select(0, beam_idx.to(v_state.device))\n-        return None\n-\n \n __all__ = [\"RecurrentGemmaForCausalLM\", \"RecurrentGemmaModel\", \"RecurrentGemmaPreTrainedModel\"]"
        },
        {
            "sha": "d0a80755b7d7e507ace8fdaad76d04750b0e76d4",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 185,
            "deletions": 53,
            "changes": 238,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -20,7 +20,7 @@\n from dataclasses import dataclass\n from functools import reduce\n from operator import mul\n-from typing import Optional, Union\n+from typing import Any, Iterable, Optional, Union\n \n import numpy as np\n import torch\n@@ -29,6 +29,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import CausalLMOutput, MaskedLMOutput, QuestionAnsweringModelOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n@@ -60,6 +61,119 @@\n )\n \n \n+class ReformerDynamicCache(DynamicCache):\n+    \"\"\"\n+    A dynamic cache that stores past buckets instead of key/values.\n+    \"\"\"\n+\n+    def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n+        super().__init__()\n+        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n+        self.buckets_cache: list[torch.Tensor] = []\n+        self.states_cache: list[torch.Tensor] = []\n+\n+        if _distributed_cache_data is not None:\n+            for buckets, states in _distributed_cache_data:\n+                self.buckets_cache.append(buckets)\n+                self.states_cache.append(states)\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n+        sequence length.\n+        \"\"\"\n+        if layer_idx < len(self):\n+            return (self.buckets_cache[layer_idx], self.states_cache[layer_idx])\n+        else:\n+            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n+\n+    def __iter__(self):\n+        \"\"\"\n+        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n+        keys and values\n+        \"\"\"\n+        for layer_idx in range(len(self)):\n+            yield (self.buckets_cache[layer_idx], self.states_cache[layer_idx])\n+\n+    def __len__(self):\n+        \"\"\"\n+        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n+        to the number of layers in the model.\n+        \"\"\"\n+        return len(self.states_cache)\n+\n+    def update(\n+        self,\n+        buckets: torch.Tensor,\n+        states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n+\n+        Parameters:\n+            key_states (`torch.Tensor`):\n+                The new key states to cache.\n+            value_states (`torch.Tensor`):\n+                The new value states to cache.\n+            layer_idx (`int`):\n+                The index of the layer to cache the states for.\n+            cache_kwargs (`Dict[str, Any]`, `optional`):\n+                Additional arguments for the cache subclass. No additional arguments are used in `ReformerDynamicCache`.\n+\n+        Return:\n+            A tuple containing the updated key and value states.\n+        \"\"\"\n+        # Update the number of seen tokens\n+        if layer_idx == 0:\n+            self._seen_tokens += states.shape[-2]\n+\n+        # Update the cache\n+        if states is not None:\n+            if len(self.states_cache) <= layer_idx:\n+                self.states_cache.append(states)\n+            else:\n+                self.states_cache[layer_idx] = torch.cat([self.states_cache[layer_idx], states], dim=1)\n+\n+        if buckets is not None:\n+            if len(self.buckets_cache) <= layer_idx:\n+                self.buckets_cache.append(buckets)\n+            else:\n+                self.buckets_cache[layer_idx] = torch.cat([self.buckets_cache[layer_idx], buckets], dim=-1)\n+        else:\n+            # `ReformerLocalAttn` passes `None` to buckets as the module uses no buckets\n+            self.buckets_cache.append(torch.tensor([], device=self.states_cache[layer_idx].device))\n+\n+        return self.buckets_cache[layer_idx], self.states_cache[layer_idx]\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        return None\n+\n+    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n+        \"\"\"Converts the `ReformerDynamicCache` instance into the its equivalent in the legacy cache format. Used for\n+        backward compatibility.\"\"\"\n+        legacy_cache = ()\n+        for layer_idx in range(len(self)):\n+            buckets, states = self.buckets_cache[layer_idx], self.states_cache[layer_idx]\n+            buckets = buckets if buckets.numel() != 0 else None\n+            legacy_cache += ((buckets, states),)\n+        return legacy_cache\n+\n+    @classmethod\n+    def from_legacy_cache(\n+        cls, past_buckets_states: Optional[tuple[tuple[torch.FloatTensor, torch.FloatTensor]]] = None\n+    ) -> \"ReformerDynamicCache\":\n+        \"\"\"Converts a cache in the legacy cache format into an equivalent `ReformerDynamicCache`. Used for\n+        backward compatibility.\"\"\"\n+        cache = cls()\n+        if past_buckets_states is not None:\n+            for layer_idx in range(len(past_buckets_states)):\n+                buckets, states = past_buckets_states[layer_idx]\n+                cache.update(buckets, states, layer_idx)\n+        return cache\n+\n+\n def _stable_argsort(vector, dim):\n     # this function scales the vector so that torch.argsort is stable.\n     # torch.argsort is not stable on its own\n@@ -316,7 +430,7 @@ def _split_seq_length_dim_to(self, vectors, dim_factor_1, dim_factor_2, num_attn\n \n \n class LSHSelfAttention(nn.Module, EfficientAttentionMixin):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n \n@@ -328,6 +442,7 @@ def __init__(self, config):\n         self.hash_seed = config.hash_seed\n         self.is_decoder = config.is_decoder\n         self.max_position_embeddings = config.max_position_embeddings\n+        self.layer_idx = layer_idx\n \n         self.dropout = config.lsh_attention_probs_dropout_prob\n \n@@ -356,6 +471,7 @@ def forward(\n         past_buckets_states=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n         **kwargs,\n     ):\n         sequence_length = hidden_states.shape[1]\n@@ -364,24 +480,24 @@ def forward(\n         # num hashes can optionally be overwritten by user\n         num_hashes = num_hashes if num_hashes is not None else self.num_hashes\n \n-        do_cached_attention = use_cache and past_buckets_states[1] is not None\n-\n         # check if cache shall be used and that hidden states are already cached\n-        if do_cached_attention:\n+        exists_cache = past_buckets_states is not None and len(past_buckets_states) > self.layer_idx\n+        if exists_cache:\n             assert sequence_length == 1, (\n                 \"At the moment, auto-regressive language generation is only possible one word at a time. Make sure\"\n                 f\" that input sequence length {sequence_length} equals 1, when `past_buckets_states` is passed.\"\n             )\n-            past_buckets = past_buckets_states[0]\n-            past_states = past_buckets_states[1]\n \n             # get query vector\n             query_vectors = self.query_key(hidden_states)\n             query_vectors = self._split_hidden_size_dim(\n                 query_vectors, self.num_attention_heads, self.attention_head_size\n             )\n \n-            if past_buckets is not None:\n+            past_buckets = past_buckets_states.buckets_cache[self.layer_idx]\n+            past_states = past_buckets_states.states_cache[self.layer_idx]\n+\n+            if past_buckets.numel() != 0:\n                 key_value_hidden_states, sorted_bucket_idx, buckets = self._get_relevant_hid_states_and_buckets(\n                     query_vectors=query_vectors,\n                     attention_mask=attention_mask,\n@@ -425,7 +541,7 @@ def forward(\n             value_vectors = self.value(hidden_states)\n \n         # if query key is not already split\n-        if not do_cached_attention or past_buckets is None:\n+        if not exists_cache or past_buckets.numel() == 0:\n             query_key_vectors = self._split_hidden_size_dim(\n                 query_key_vectors, self.num_attention_heads, self.attention_head_size\n             )\n@@ -434,7 +550,7 @@ def forward(\n             )\n \n         # cache buckets for next incremental decoding\n-        if do_cached_attention and past_buckets is None and key_value_hidden_states.shape[1] >= self.chunk_length:\n+        if exists_cache and key_value_hidden_states.shape[1] >= self.chunk_length:\n             buckets = self._hash_vectors(query_key_vectors, num_hashes, attention_mask)\n \n         # free memory\n@@ -448,7 +564,7 @@ def forward(\n         )\n \n         do_standard_self_attention = (sequence_length <= self.chunk_length) or (\n-            use_cache and past_buckets_states[1] is not None\n+            exists_cache and past_states is not None\n         )\n         # LSH attention only makes sense if chunked attention should be performed\n         if not do_standard_self_attention:\n@@ -498,7 +614,7 @@ def forward(\n                     \"If `config.chunk_length` is `None`, make sure `config.num_chunks_after` and\"\n                     \" `config.num_chunks_before` are set to 0.\"\n                 )\n-        elif do_cached_attention and past_buckets is not None:\n+        elif exists_cache and past_buckets.numel() != 0:\n             # use max sequence length\n             sorted_bucket_idx_per_hash = sorted_bucket_idx\n         else:\n@@ -526,7 +642,7 @@ def forward(\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             do_standard_self_attention=do_standard_self_attention,\n-            do_cached_attention=do_cached_attention,\n+            use_cache=exists_cache,\n         )\n \n         # free memory\n@@ -537,7 +653,7 @@ def forward(\n             # sort clusters back to correct ordering\n             out_vectors, logits = ReverseSort.apply(out_vectors, logits, sorted_bucket_idx, undo_sorted_bucket_idx)\n \n-        if not do_standard_self_attention or (do_cached_attention and past_buckets is not None):\n+        if not do_standard_self_attention or (exists_cache and past_buckets.numel() != 0):\n             # sum up all hash rounds\n             if num_hashes > 1:\n                 out_vectors = self._split_seq_length_dim_to(\n@@ -721,7 +837,7 @@ def _attend(\n         attention_mask,\n         head_mask,\n         do_standard_self_attention,\n-        do_cached_attention,\n+        use_cache,\n     ):\n         # look at previous and following chunks if chunked attention\n         if not do_standard_self_attention:\n@@ -741,12 +857,12 @@ def _attend(\n                 sorted_bucket_idx_per_hash, -1, self.chunk_length, self.num_attention_heads\n             )\n             key_value_bucket_idx = self._look_adjacent(query_bucket_idx, self.num_chunks_before, self.num_chunks_after)\n-        elif do_cached_attention and query_key_dots.ndim > 4:\n+        elif use_cache and query_key_dots.ndim > 4:\n             key_value_bucket_idx = sorted_bucket_idx_per_hash\n             query_bucket_idx = (\n                 key_value_bucket_idx.new_ones(key_value_bucket_idx.shape[:-1] + (1,)) * key_value_bucket_idx.max()\n             )\n-        elif do_cached_attention and query_key_dots.ndim <= 4:\n+        elif use_cache and query_key_dots.ndim <= 4:\n             query_bucket_idx = (query_key_dots.shape[-1] - 1) * torch.ones_like(query_key_dots)[:, :, :, -1]\n             key_value_bucket_idx = torch.arange(\n                 query_key_dots.shape[-1], dtype=torch.long, device=query_key_dots.device\n@@ -762,7 +878,7 @@ def _attend(\n             self_mask_value = self.self_mask_value_float32\n             mask_value = self.mask_value_float32\n \n-        if not do_cached_attention:\n+        if not use_cache:\n             mask = self._compute_attn_mask(\n                 query_bucket_idx,\n                 key_value_bucket_idx,\n@@ -1016,7 +1132,7 @@ def backward(ctx, grad_out_vectors, grad_logits):\n \n \n class LocalSelfAttention(nn.Module, EfficientAttentionMixin):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n \n         self.num_attention_heads = config.num_attention_heads\n@@ -1029,6 +1145,7 @@ def __init__(self, config):\n         self.attention_head_size = config.attention_head_size\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n         self.hidden_size = config.hidden_size\n+        self.layer_idx = layer_idx\n \n         # projection matrices\n         self.query = nn.Linear(self.hidden_size, self.all_head_size, bias=False)\n@@ -1055,13 +1172,16 @@ def forward(\n         batch_size = hidden_states.shape[0]\n \n         # check if cache shall be used and that hidden states are already cached\n-        if use_cache and past_buckets_states[1] is not None:\n-            assert past_buckets_states[0] is None, (\n+        if past_buckets_states is not None and len(past_buckets_states) > self.layer_idx:\n+            past_buckets = past_buckets_states.buckets_cache[self.layer_idx]\n+            past_states = past_buckets_states.states_cache[self.layer_idx]\n+\n+            assert past_buckets.numel() == 0, (\n                 \"LocalSelfAttention should not make use of `buckets`. There seems to be an error when caching\"\n                 \" hidden_states_and_buckets.\"\n             )\n             key_value_hidden_states = self._retrieve_relevant_hidden_states(\n-                past_buckets_states[1], self.chunk_length, self.num_chunks_before\n+                past_states, self.chunk_length, self.num_chunks_before\n             )\n             key_value_hidden_states = torch.cat([key_value_hidden_states, hidden_states], dim=1)\n \n@@ -1262,15 +1382,15 @@ def __init__(self, config, layer_id=0):\n         self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n         if len(set(self.attn_layers)) == 1 and self.attn_layers[0] == \"lsh\":\n-            self.self_attention = LSHSelfAttention(config)\n+            self.self_attention = LSHSelfAttention(config, layer_idx=layer_id)\n         elif len(set(self.attn_layers)) == 1 and self.attn_layers[0] == \"local\":\n-            self.self_attention = LocalSelfAttention(config)\n+            self.self_attention = LocalSelfAttention(config, layer_idx=layer_id)\n         elif len(set(self.attn_layers)) == 2 and set(self.attn_layers) == {\"lsh\", \"local\"}:\n             # get correct attn layers\n             if self.attn_layers[self.layer_id] == \"lsh\":\n-                self.self_attention = LSHSelfAttention(config)\n+                self.self_attention = LSHSelfAttention(config, layer_idx=layer_id)\n             else:\n-                self.self_attention = LocalSelfAttention(config)\n+                self.self_attention = LocalSelfAttention(config, layer_idx=layer_id)\n         else:\n             raise NotImplementedError(\n                 f\"Only attn layer types 'lsh' and 'local' exist, but got `config.attn_layers`: {self.attn_layers}. \"\n@@ -1289,25 +1409,21 @@ def forward(\n         orig_sequence_length=None,\n         output_attentions=False,\n         buckets=None,\n+        cache_position=None,\n     ):\n         hidden_states = self.layer_norm(hidden_states)\n \n-        # make sure cached hidden states is set to None for backward pass\n-        if past_buckets_states is not None:\n-            past_buckets_states_layer = past_buckets_states[self.layer_id]\n-        else:\n-            past_buckets_states_layer = None\n-\n         # use cached buckets for backprob if buckets not None for LSHSelfAttention\n         self_attention_outputs = self.self_attention(\n             hidden_states=hidden_states,\n             head_mask=head_mask,\n             attention_mask=attention_mask,\n             num_hashes=num_hashes,\n-            past_buckets_states=past_buckets_states_layer,\n+            past_buckets_states=past_buckets_states,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             buckets=buckets,\n+            cache_position=cache_position,\n         )\n \n         # add buckets if necessary\n@@ -1317,24 +1433,26 @@ def forward(\n             buckets = None\n \n         # cache hidden states for future use\n-        if use_cache:\n-            if past_buckets_states[self.layer_id][0] is None:\n-                # padded input should not be cached\n-                past_buckets = (\n-                    buckets[:, :, :, :orig_sequence_length]\n-                    if (buckets is not None and orig_sequence_length > 1)\n-                    else buckets\n+        if use_cache and past_buckets_states is not None:\n+            # padded input should not be cached during prefill\n+            states = (\n+                hidden_states[:, :orig_sequence_length]\n+                if len(past_buckets_states.states_cache) <= self.layer_id\n+                else hidden_states\n+            )\n+            buckets = (\n+                buckets[:, :, :, :orig_sequence_length]\n+                if (\n+                    len(past_buckets_states.buckets_cache) <= self.layer_id\n+                    and buckets is not None\n+                    and orig_sequence_length > 1\n                 )\n-            else:\n-                past_buckets = torch.cat([past_buckets_states[self.layer_id][0], buckets], dim=-1)\n-\n-            if past_buckets_states[self.layer_id][1] is None:\n-                # padded input should not be cached\n-                past_states = hidden_states[:, :orig_sequence_length]\n-            else:\n-                past_states = torch.cat([past_buckets_states[self.layer_id][1], hidden_states], dim=1)\n+                else buckets\n+            )\n+            buckets, hidden_states = past_buckets_states.update(\n+                buckets, states[:, :orig_sequence_length], self.layer_id\n+            )\n \n-            past_buckets_states[self.layer_id] = (past_buckets, past_states)\n         # compute attention feed forward output\n         attention_output = self.output(self_attention_outputs.hidden_states)\n \n@@ -1708,8 +1826,15 @@ def forward(\n         all_attentions = []\n \n         # init cached hidden states if necessary\n-        if past_buckets_states is None:\n-            past_buckets_states = [((None), (None)) for i in range(len(self.layers))]\n+        return_legacy_cache = False\n+        if use_cache or not isinstance(past_buckets_states, ReformerDynamicCache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `ReformerDynamicCache` instead, e.g. \"\n+                \"`past_key_values=ReformerDynamicCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_buckets_states = ReformerDynamicCache.from_legacy_cache(past_buckets_states)\n \n         # concat same tensor for reversible ResNet\n         hidden_states = torch.cat([hidden_states, hidden_states], dim=-1)\n@@ -1734,11 +1859,15 @@ def forward(\n         # Apply dropout\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n+        next_cache = past_buckets_states if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = past_buckets_states.to_legacy_cache()\n+\n         return ReformerEncoderOutput(\n             hidden_states=hidden_states,\n             all_hidden_states=all_hidden_states,\n             all_attentions=all_attentions,\n-            past_buckets_states=past_buckets_states,\n+            past_buckets_states=next_cache,\n         )\n \n \n@@ -2087,7 +2216,7 @@ def _pad_to_mult_of_chunk_length(\n \n         # Extend `inputs_embeds` with padding to match least common multiple chunk_length\n         if inputs_embeds is not None:\n-            padded_inputs_embeds = self.embeddings(padded_input_ids, position_ids)\n+            padded_inputs_embeds = self.get_input_embeddings()(padded_input_ids)\n             inputs_embeds = torch.cat([inputs_embeds, padded_inputs_embeds], dim=-2)\n             input_shape = inputs_embeds.size()\n         return input_ids, inputs_embeds, attention_mask, position_ids, input_shape\n@@ -2240,6 +2369,9 @@ def _reorder_cache(self, past_key_values, beam_idx):\n             # hidden states\n             reord_hidden_states = layer_past[1].index_select(0, beam_idx.to(layer_past[1].device))\n             reord_past_buckets_states.append((reord_buckets, reord_hidden_states))\n+\n+        if isinstance(past_key_values, ReformerDynamicCache):\n+            reord_past_buckets_states = ReformerDynamicCache.from_legacy_cache(reord_past_buckets_states)\n         return reord_past_buckets_states\n \n "
        },
        {
            "sha": "93dd4a31fc96d46d1bea72842e5965bfe2a190c0",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 107,
            "deletions": 101,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -39,6 +40,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_rembert import RemBertConfig\n \n \n@@ -199,7 +201,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class RemBertSelfAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -218,58 +220,71 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n \n-        if is_cross_attention and past_key_value is not None:\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+            key_layer = (\n+                self.key(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n \n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -296,11 +311,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->RemBert\n@@ -319,9 +330,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RemBertAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n-        self.self = RemBertSelfAttention(config)\n+        self.self = RemBertSelfAttention(config, layer_idx=layer_idx)\n         self.output = RemBertSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -344,6 +355,7 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     # Copied from transformers.models.bert.modeling_bert.BertAttention.forward\n     def forward(\n         self,\n@@ -352,17 +364,19 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -401,17 +415,17 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RemBertLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = RemBertAttention(config)\n+        self.attention = RemBertAttention(config, layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = RemBertAttention(config)\n+            self.crossattention = RemBertAttention(config, layer_idx=layer_idx)\n         self.intermediate = RemBertIntermediate(config)\n         self.output = RemBertOutput(config)\n \n@@ -423,62 +437,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     # Copied from transformers.models.bert.modeling_bert.BertLayer.feed_forward_chunk\n@@ -494,7 +491,7 @@ def __init__(self, config):\n         self.config = config\n \n         self.embedding_hidden_mapping_in = nn.Linear(config.input_embedding_size, config.hidden_size)\n-        self.layer = nn.ModuleList([RemBertLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([RemBertLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -509,39 +506,47 @@ def forward(\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning_once(\n                     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n                 use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n \n-        next_decoder_cache = () if use_cache else None\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -550,12 +555,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -564,7 +572,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -699,6 +707,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -724,8 +733,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -772,6 +786,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n@@ -1004,15 +1019,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "003b2fa519bba09d5b79980d49de3b6123031db9",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 156,
            "deletions": 131,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -41,6 +42,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_roberta import RobertaConfig\n \n \n@@ -138,7 +140,7 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\n class RobertaSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -163,66 +165,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n \n-        if is_cross_attention and past_key_value is not None:\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -264,30 +275,28 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSdpaSelfAttention with Bert->Roberta\n class RobertaSdpaSelfAttention(RobertaSelfAttention):\n-    def __init__(self, config, position_embedding_type=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type)\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n         self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from RobertaSelfAttention\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n@@ -306,38 +315,59 @@ def forward(\n                 encoder_attention_mask,\n                 past_key_value,\n                 output_attentions,\n+                cache_position,\n             )\n \n         bsz, tgt_len, _ = hidden_states.size()\n \n-        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        query_layer = (\n+            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n         # mask needs to be such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n-        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n-            key_layer, value_layer = past_key_value\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(current_states))\n-            value_layer = self.transpose_for_scores(self.value(current_states))\n-            if past_key_value is not None and not is_cross_attention:\n-                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = (\n+                self.key(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n         # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n@@ -367,10 +397,7 @@ def forward(\n         attn_output = attn_output.transpose(1, 2)\n         attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n \n-        outputs = (attn_output,)\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return attn_output, None\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n@@ -396,10 +423,12 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta,BERT->ROBERTA\n class RobertaAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = ROBERTA_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = RobertaSelfOutput(config)\n         self.pruned_heads = set()\n@@ -422,24 +451,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -479,17 +511,17 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\n class RobertaLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = RobertaAttention(config)\n+        self.attention = RobertaAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n         self.intermediate = RobertaIntermediate(config)\n         self.output = RobertaOutput(config)\n \n@@ -500,62 +532,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -566,10 +581,10 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Roberta\n class RobertaEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([RobertaLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -584,6 +599,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -596,27 +612,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -625,12 +648,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -639,7 +665,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -755,6 +781,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -780,8 +807,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if token_type_ids is None:\n             if hasattr(self.embeddings, \"token_type_ids\"):\n@@ -866,6 +898,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n@@ -1006,14 +1039,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring\n class RobertaForMaskedLM(RobertaPreTrainedModel):"
        },
        {
            "sha": "e04faa1b6387e8565f9244d1a565c35b211a3bd9",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 105,
            "deletions": 98,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -39,6 +40,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_roberta_prelayernorm import RobertaPreLayerNormConfig\n \n \n@@ -137,7 +139,7 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->RobertaPreLayerNorm\n class RobertaPreLayerNormSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -162,66 +164,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -263,11 +274,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n class RobertaPreLayerNormSelfOutput(nn.Module):\n@@ -284,9 +291,11 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RobertaPreLayerNormAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n-        self.self = RobertaPreLayerNormSelfAttention(config, position_embedding_type=position_embedding_type)\n+        self.self = RobertaPreLayerNormSelfAttention(\n+            config, position_embedding_type=position_embedding_type, layer_idx=layer_idx\n+        )\n         self.output = RobertaPreLayerNormSelfOutput(config)\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.pruned_heads = set()\n@@ -310,15 +319,17 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         hidden_states_pre_layer_norm = self.LayerNorm(hidden_states)\n         self_outputs = self.self(\n@@ -329,6 +340,7 @@ def forward(\n             encoder_attention_mask,\n             past_key_value,\n             output_attentions,\n+            cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -367,17 +379,19 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->RobertaPreLayerNorm\n class RobertaPreLayerNormLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = RobertaPreLayerNormAttention(config)\n+        self.attention = RobertaPreLayerNormAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = RobertaPreLayerNormAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = RobertaPreLayerNormAttention(\n+                config, position_embedding_type=\"absolute\", layer_idx=layer_idx\n+            )\n         self.intermediate = RobertaPreLayerNormIntermediate(config)\n         self.output = RobertaPreLayerNormOutput(config)\n \n@@ -388,62 +402,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -454,10 +451,12 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->RobertaPreLayerNorm\n class RobertaPreLayerNormEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([RobertaPreLayerNormLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList(\n+            [RobertaPreLayerNormLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]\n+        )\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -472,6 +471,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -484,27 +484,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -513,12 +520,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -527,7 +537,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -676,8 +686,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -874,14 +889,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "8d98140aff9a4360115dc38373974d857b164722",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 108,
            "deletions": 106,
            "changes": 214,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -39,6 +40,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_roc_bert import RoCBertConfig\n \n \n@@ -252,7 +254,7 @@ def forward(\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->RoCBert\n class RoCBertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -277,66 +279,75 @@ def __init__(self, config, position_embedding_type=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.layer_idx = layer_idx\n \n-    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n-\n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        mixed_query_layer = self.query(hidden_states)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = self.query(hidden_states)\n+        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+            1, 2\n+        )\n \n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention and encoder_attention_mask is not None:\n+            attention_mask = encoder_attention_mask\n+\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n \n-        if is_cross_attention and past_key_value is not None:\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n-        elif past_key_value is not None:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        use_cache = past_key_value is not None\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+            key_layer = self.key(current_states)\n+            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n+                1, 2\n+            )\n+            value_layer = self.value(current_states)\n+            value_layer = value_layer.view(\n+                batch_size, -1, self.num_attention_heads, self.attention_head_size\n+            ).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n             query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if use_cache:\n+            if past_key_value is not None:\n                 position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n                     -1, 1\n                 )\n@@ -378,11 +389,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->RoCBert\n@@ -407,10 +414,12 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->RoCBert,BERT->ROC_BERT\n class RoCBertAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None):\n+    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()\n         self.self = ROC_BERT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config, position_embedding_type=position_embedding_type\n+            config,\n+            position_embedding_type=position_embedding_type,\n+            layer_idx=layer_idx,\n         )\n         self.output = RoCBertSelfOutput(config)\n         self.pruned_heads = set()\n@@ -433,24 +442,27 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n+    @deprecate_kwarg(\"encoder_attention_mask\", version=\"4.55.0\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            past_key_value,\n-            output_attentions,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -490,17 +502,17 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->RoCBert\n class RoCBertLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = RoCBertAttention(config)\n+        self.attention = RoCBertAttention(config, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = RoCBertAttention(config, position_embedding_type=\"absolute\")\n+            self.crossattention = RoCBertAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n         self.intermediate = RoCBertIntermediate(config)\n         self.output = RoCBertOutput(config)\n \n@@ -511,62 +523,45 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         outputs = (layer_output,) + outputs\n \n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n         return outputs\n \n     def feed_forward_chunk(self, attention_output):\n@@ -577,10 +572,10 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->RoCBert\n class RoCBertEncoder(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([RoCBertLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([RoCBertLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -595,6 +590,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -607,27 +603,34 @@ def forward(\n                 )\n                 use_cache = False\n \n-        next_decoder_cache = () if use_cache else None\n+        return_legacy_cache = False\n+        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -636,12 +639,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -650,7 +656,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -871,8 +877,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -1459,7 +1470,7 @@ def prepare_inputs_for_generation(\n \n         # cut decoder_input_ids if past_key_values is used\n         if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n+            past_length = past_key_values.get_seq_length()\n \n             # Some generation methods already pass only the last input ID\n             if input_ids.shape[1] > past_length:\n@@ -1482,15 +1493,6 @@ def prepare_inputs_for_generation(\n             \"past_key_values\": past_key_values,\n         }\n \n-    # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel._reorder_cache\n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "58c2320ecda6645569844de9486b58295dff593e",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 100,
            "deletions": 106,
            "changes": 206,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -188,7 +189,7 @@ def forward(self, input_ids=None, token_type_ids=None, inputs_embeds=None):\n \n \n class RoFormerSelfAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n@@ -208,11 +209,7 @@ def __init__(self, config):\n \n         self.is_decoder = config.is_decoder\n         self.rotary_value = config.rotary_value\n-\n-    def transpose_for_scores(self, x):\n-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n-        x = x.view(*new_x_shape)\n-        return x.permute(0, 2, 1, 3)\n+        self.layer_idx = layer_idx\n \n     def forward(\n         self,\n@@ -221,50 +218,58 @@ def forward(\n         sinusoidal_pos=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n-        encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n-        mixed_query_layer = self.query(hidden_states)\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        batch_size, seq_length, _ = hidden_states.shape\n+        query_layer = (\n+            self.query(hidden_states)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n \n-        if is_cross_attention and past_key_value is not None:\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value[0]\n-            value_layer = past_key_value[1]\n-            attention_mask = encoder_attention_mask\n-        elif is_cross_attention:\n-            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n-            attention_mask = encoder_attention_mask\n+            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n+            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_layer = self.transpose_for_scores(self.key(hidden_states))\n-            value_layer = self.transpose_for_scores(self.value(hidden_states))\n-            if sinusoidal_pos is not None:\n-                if self.rotary_value:\n-                    query_layer, key_layer, value_layer = self.apply_rotary_position_embeddings(\n-                        sinusoidal_pos, query_layer, key_layer, value_layer\n-                    )\n-                else:\n-                    query_layer, key_layer = self.apply_rotary_position_embeddings(\n-                        sinusoidal_pos, query_layer, key_layer\n-                    )\n+            key_layer = (\n+                self.key(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+            value_layer = (\n+                self.value(current_states)\n+                .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+                .transpose(1, 2)\n+            )\n+\n             if past_key_value is not None:\n-                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n-                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_layer, value_layer)\n+                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_layer, value_layer = curr_past_key_value.update(\n+                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -291,11 +296,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        if self.is_decoder:\n-            outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n     @staticmethod\n     def apply_rotary_position_embeddings(sinusoidal_pos, query_layer, key_layer, value_layer=None):\n@@ -341,9 +342,9 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RoFormerAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n-        self.self = RoFormerSelfAttention(config)\n+        self.self = RoFormerSelfAttention(config, layer_idx=layer_idx)\n         self.output = RoFormerSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -374,19 +375,19 @@ def forward(\n         sinusoidal_pos=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n-        encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n             sinusoidal_pos,\n             head_mask,\n             encoder_hidden_states,\n-            encoder_attention_mask,\n             past_key_value,\n             output_attentions,\n+            cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -425,17 +426,17 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RoFormerLayer(GradientCheckpointingLayer):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = RoFormerAttention(config)\n+        self.attention = RoFormerAttention(config, layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = RoFormerAttention(config)\n+            self.crossattention = RoFormerAttention(config, layer_idx)\n         self.intermediate = RoFormerIntermediate(config)\n         self.output = RoFormerOutput(config)\n \n@@ -449,63 +450,44 @@ def forward(\n         encoder_attention_mask=None,\n         past_key_value=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n             hidden_states,\n-            attention_mask,\n-            sinusoidal_pos,\n-            head_mask,\n+            attention_mask=attention_mask,\n+            sinusoidal_pos=sinusoidal_pos,\n+            head_mask=head_mask,\n             output_attentions=output_attentions,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n \n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-            present_key_value = self_attention_outputs[-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n-\n-        cross_attn_present_key_value = None\n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n                 raise ValueError(\n                     f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention \"\n                     \"layers by setting `config.add_cross_attention=True`\"\n                 )\n \n-            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n             cross_attention_outputs = self.crossattention(\n                 attention_output,\n-                attention_mask,\n-                sinusoidal_pos,\n-                head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                cross_attn_past_key_value,\n-                output_attentions,\n+                attention_mask=encoder_attention_mask,\n+                sinusoidal_pos=sinusoidal_pos,\n+                head_mask=head_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n-\n-            # add cross-attn cache to positions 3,4 of present_key_value tuple\n-            cross_attn_present_key_value = cross_attention_outputs[-1]\n-            present_key_value = present_key_value + cross_attn_present_key_value\n+            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        # if decoder, return the attn key/values as the last output\n-        if self.is_decoder:\n-            outputs = outputs + (present_key_value,)\n-\n-        return outputs\n+        return (layer_output,) + outputs\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -520,7 +502,7 @@ def __init__(self, config):\n         self.embed_positions = RoFormerSinusoidalPositionalEmbedding(\n             config.max_position_embeddings, config.hidden_size // config.num_attention_heads\n         )\n-        self.layer = nn.ModuleList([RoFormerLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.layer = nn.ModuleList([RoFormerLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n     def forward(\n@@ -535,29 +517,39 @@ def forward(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n+        cache_position=None,\n     ):\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning_once(\n                     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n                 use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n \n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n \n         # [sequence_length, embed_size_per_head] -> [batch_size, num_heads, sequence_length, embed_size_per_head]\n         sinusoidal_pos = self.embed_positions(hidden_states.shape[:-1], past_key_values_length)[None, None, :, :]\n \n-        next_decoder_cache = () if use_cache else None\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            past_key_value = past_key_values[i] if past_key_values is not None else None\n \n             layer_outputs = layer_module(\n                 hidden_states,\n@@ -566,13 +558,12 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value,\n+                past_key_values,\n                 output_attentions,\n+                cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache += (layer_outputs[-1],)\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                 if self.config.add_cross_attention:\n@@ -581,12 +572,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        if return_legacy_cache:\n+            past_key_values = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_decoder_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                     all_cross_attentions,\n@@ -595,7 +589,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_decoder_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -837,6 +831,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[BaseModelOutputWithPastAndCrossAttentions, tuple[torch.Tensor]]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -862,8 +857,13 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        # past_key_values_length\n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        past_key_values_length = 0\n+        if past_key_values is not None:\n+            past_key_values_length = (\n+                past_key_values[0][0].shape[-2]\n+                if not isinstance(past_key_values, Cache)\n+                else past_key_values.get_seq_length()\n+            )\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n@@ -909,6 +909,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n \n@@ -1064,6 +1065,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[CausalLMOutputWithCrossAttentions, tuple[torch.Tensor]]:\n         r\"\"\"\n@@ -1103,6 +1105,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = outputs[0]\n@@ -1130,15 +1133,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def _reorder_cache(self, past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n-                + layer_past[2:],\n-            )\n-        return reordered_past\n-\n \n class RoFormerClassificationHead(nn.Module):\n     \"\"\"Head for sentence-level classification tasks.\"\"\""
        },
        {
            "sha": "4614eaff3e85f6e1ed44a3ba7fea34b2655d42b6",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=c8524aeb07f370195733f610760236d22e88cb61",
            "patch": "@@ -526,7 +526,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n@@ -720,7 +720,7 @@ def forward(\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n-            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n+            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n \n             If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as"
        },
        {
            "sha": "d461ee6e3dca4c8e0ae07b85d759c938883ce157",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "3f4595eeee621dac8bc8dc78522cf32432964692",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 91,
            "deletions": 144,
            "changes": 235,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "b998e0546d8f4fafa444c242a2b159550ec30577",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "b21a2a12cccfb5adf1215e44442198ff359f27bb",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "afa85c915bd3651a9b1043a7b5cfd47d106d3022",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "a0da6da35076f83f9b1d7b3df1ec5e19824b1174",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "f5451a0d1e25688542014a949bd6102fb7f03127",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "ae8a1595b86cf58a8a7a73b4533432ce25e330b5",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 84,
            "deletions": 96,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "00655c40608f19b37b10d3f661daf85baeaaf7c5",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 106,
            "deletions": 89,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "d11a1eb60d87dcd4c21be1c0dbbfd15e34b49b1a",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "9e9511202863a619f99815dde4f01a32d7b09bb7",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "9e95bd88b8ee2981de79e032b493d98cb0285a36",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "56506bc7a2354b0816a2671f3023eeabe5f4068d",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 29,
            "deletions": 57,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "de61e5b2d259c57caba6dae6ef940a3cbdc9eea8",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "14ec4791ac8d0bf3186e9008e74463dfb1ce1027",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "7ede856b416b2178cd7fc365f60e8e6a8738b9a1",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 33,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "23a43615b1e0cf214bc32179ee3ffe168d39e75f",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 32,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "2cd8798883fa33d8091168aa99aedda432390502",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "8545bc1021c68818ee33f4aef834d97ef2ec67d0",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 88,
            "deletions": 70,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "3e74c55b8febd84a7df6df64ce8aa608bc18425f",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 15,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "ca10d41d6ce632d4f84d92f0de30f9461e6a3ae8",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 85,
            "deletions": 88,
            "changes": 173,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "a0848a667b4801b19905c64bc9fc36a58bd52c0c",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 32,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "62bbd9d7f7c660b0b2b07943816095c115932648",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "195923642e5718b7f9cfe40a89c9f28eaa971a67",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "62aaff87b7091c1a3f819450c9fe3692f90c02f5",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "b7f55915f1a355edff800a07290951d4cd20e150",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "d7c93bdda0d9085004f491c00be938550dbddc1d",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "2600605fc604aedc304b625ce67c17a89f908c36",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "9a9c986562d0788d8e145d13bfb2a9fe9b1398cc",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "70d3ccedee3efbabfec27f09aec6913183db5e4e",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "255406c6ce2f0e5302e1be7cc83abe25fa971c27",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 23,
            "deletions": 21,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "58738c006345de110c1278a4ca4718d77bd27a84",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "31ee70ad3462201f13fc50f17fba6b84ed286d9b",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "c3640dadef021b4302617eb629f299002d6d508f",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "86d85bb53bbe65ca48d374654d87db879adf2ae2",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "54f07e0be0a6faa25e1bad6066bf45612fb99d02",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "0a7326b916fa2f6436a740a54f811ee9a80cfa45",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 13,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "a5ee3378c119b9c0e0d042b4055620df0b0a9058",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "f72ce7bd40ba59db3a112cc9592a5fad926d9196",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "a03353c68878fd60680525045efddd012a2b4f07",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 6,
            "deletions": 26,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "15fc5c2178a3ca096e5f78febdf4ff3dd105e52b",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 91,
            "deletions": 91,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "b92b3d06d2e3654e05c709fe6e45d516e857dfe1",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 53,
            "deletions": 61,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "43c6680da6a280363fe7b808b82d63c408f76907",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 156,
            "deletions": 131,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "7e3592847bdb41955c24ad1f2a87f6c4b61c9d01",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 152,
            "deletions": 126,
            "changes": 278,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "6266ec88f5450c60604578b5aacc2de4685ed895",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 104,
            "deletions": 101,
            "changes": 205,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "e686f9741b41d24362de0f197e79253b4e3322ac",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "1d999ea4ca5079c7ff1af6631a872eecbb354c72",
            "filename": "src/transformers/models/yoso/modeling_yoso.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "317ae28e4490d754fab8cd42abb3d3f2fe9d0133",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "45e638ba6f8afaded99add4fbc726d03e4bdda78",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "b912a63419a2a1041921c4a63737901fbea16580",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "9004256338478d371d922589acf262a5ae57b12a",
            "filename": "src/transformers/models/zoedepth/modeling_zoedepth.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "3b521a27c5651fe036df0362ccf9b94d14d79f32",
            "filename": "src/transformers/utils/args_doc.py",
            "status": "modified",
            "additions": 5,
            "deletions": 15,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fargs_doc.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "531bf70d5eeb80d2176cc214dd17dda9739c4dba",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 38,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "c75175a811fbd338d945ef657c3f80bd13cec731",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "8ec874d0f7a8306be5c93d6b5d0eb3807099031e",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "f2b6969c41092ac817019f821093289b77078031",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "1dc16992633ce7d3796698e6d4e991b023d0f786",
            "filename": "tests/models/mvp/test_modeling_mvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "28f5a56e4c3afd44b6be6ae178f327449a2cc747",
            "filename": "tests/models/perception_lm/test_processor_perception_lm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fperception_lm%2Ftest_processor_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fperception_lm%2Ftest_processor_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2Ftest_processor_perception_lm.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "2b3fc9aa9e3065c1623e7410cac895067a461f15",
            "filename": "tests/models/prophetnet/test_modeling_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "c3a8025e0fa563e37c2f92ed2aaa38df30804071",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "b33c246e7cfe6d2fd34bf63d8561cdf5f8dd29a2",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 19,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        },
        {
            "sha": "ed8f7d4da15876ee399c3bdba5ed60976bbd4263",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8524aeb07f370195733f610760236d22e88cb61/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8524aeb07f370195733f610760236d22e88cb61/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=c8524aeb07f370195733f610760236d22e88cb61"
        }
    ],
    "stats": {
        "total": 12554,
        "additions": 5715,
        "deletions": 6839
    }
}