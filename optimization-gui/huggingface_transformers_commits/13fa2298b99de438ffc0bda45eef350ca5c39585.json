{
    "author": "zucchini-nlp",
    "message": "GLM4V - delete duplicate config attribute (#42416)\n\n* delete the duplicate attribute\n\n* also delete from docstring",
    "sha": "13fa2298b99de438ffc0bda45eef350ca5c39585",
    "files": [
        {
            "sha": "56ac02ebbfa96c163cfdcccad7587f8c1fdb7afb",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/13fa2298b99de438ffc0bda45eef350ca5c39585/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13fa2298b99de438ffc0bda45eef350ca5c39585/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=13fa2298b99de438ffc0bda45eef350ca5c39585",
            "patch": "@@ -165,10 +165,6 @@ class Glm4vTextConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        image_token_id (`int`, *optional*):\n-            Token index used as placeholder for image embeddings.\n-        video_token_id (`int`, *optional*):\n-            Token index used as placeholder for video embeddings.\n \n     ```python\n     >>> from transformers import Glm4vTextModel, Glm4vConfig\n@@ -217,8 +213,6 @@ def __init__(\n         tie_word_embeddings: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        image_token_id: Optional[int] = None,\n-        video_token_id: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -246,8 +240,6 @@ def __init__(\n         rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n         standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-        self.image_token_id = image_token_id\n-        self.video_token_id = video_token_id\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n "
        },
        {
            "sha": "eecc4f4ab138f90e44bf4585e4e01558284432f9",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/13fa2298b99de438ffc0bda45eef350ca5c39585/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/13fa2298b99de438ffc0bda45eef350ca5c39585/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=13fa2298b99de438ffc0bda45eef350ca5c39585",
            "patch": "@@ -202,10 +202,6 @@ class Glm4vTextConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        image_token_id (`int`, *optional*):\n-            Token index used as placeholder for image embeddings.\n-        video_token_id (`int`, *optional*):\n-            Token index used as placeholder for video embeddings.\n \n     ```python\n     >>> from transformers import Glm4vTextModel, Glm4vConfig\n@@ -254,8 +250,6 @@ def __init__(\n         tie_word_embeddings: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        image_token_id: Optional[int] = None,\n-        video_token_id: Optional[int] = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -283,8 +277,6 @@ def __init__(\n         rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n         standardize_rope_params(self, rope_theta=rope_theta)\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n-        self.image_token_id = image_token_id\n-        self.video_token_id = video_token_id\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n "
        }
    ],
    "stats": {
        "total": 16,
        "additions": 0,
        "deletions": 16
    }
}