{
    "author": "BlackSamorez",
    "message": "FP-Quant NVFP4 and Python 3.9 support (#39876)\n\n* quartet\n\n* quartet qat -> quartet\n\n* format\n\n* bf16 backward\n\n* interfaces\n\n* forward_method\n\n* quartet -> fp_quant\n\n* style\n\n* List -> list\n\n* list typing\n\n* fixed format and annotations\n\n* test_fp_quant\n\n* docstrings and default dtypes\n\n* better docstring and removed noop checks\n\n* docs\n\n* pseudoquantization support to test on non-blackwell\n\n* pseudoquant\n\n* Pseudoquant docs\n\n* Update docs/source/en/quantization/fp_quant.md\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Update docs/source/en/quantization/fp_quant.md\n\n* Update docs/source/en/quantization/fp_quant.md\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update tests/quantization/fp_quant_integration/test_fp_quant.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update tests/quantization/fp_quant_integration/test_fp_quant.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* small test fixes\n\n* dockerfile update\n\n* spec link\n\n* removed `_process_model_after_weight_loading`\n\n* toctree\n\n* nvfp4\n\n* nvfp4 tests\n\n* FP-Quant version bumped\n\n* nvfp4 default and docs update\n\n* trainable\n\n* cpu if pseudoquant\n\n* proper group size selection\n\n* gsr\n\n* qutlass requirement version bumo\n\n* Upstream docker copy\n\n* docs update\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "32567739740da86ddf96c60a23cf2d0494ce4145",
    "files": [
        {
            "sha": "2b25ca091b5c100817116c60716961cf285118de",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/32567739740da86ddf96c60a23cf2d0494ce4145/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/32567739740da86ddf96c60a23cf2d0494ce4145/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=32567739740da86ddf96c60a23cf2d0494ce4145",
            "patch": "@@ -82,6 +82,9 @@ RUN python3 -m pip uninstall -y flash-attn\n # this line must be added in order for python to be aware of transformers.\n RUN cd transformers && python3 setup.py develop\n \n+# Add fp-quant for quantization testing\n+RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.2.0\"\n+\n # Low usage or incompatible lib, will enable later on\n \n # # Add aqlm for quantization testing\n@@ -102,7 +105,3 @@ RUN cd transformers && python3 setup.py develop\n # # TODO: create a new workflow to test them\n # RUN python3 -m pip install --no-cache-dir flute-kernel==0.4.1\n # RUN python3 -m pip install --no-cache-dir git+https://github.com/Dao-AILab/fast-hadamard-transform.git\n-\n-# Add fp-quant for quantization testing\n-# Requires py3.11 but our CI runs on 3.9\n-# RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.1.6\"\n\\ No newline at end of file"
        },
        {
            "sha": "4888795a6d772af96dbc301bcd67da3b0a7c1f3a",
            "filename": "docs/source/en/quantization/fp_quant.md",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/32567739740da86ddf96c60a23cf2d0494ce4145/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/32567739740da86ddf96c60a23cf2d0494ce4145/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md?ref=32567739740da86ddf96c60a23cf2d0494ce4145",
            "patch": "@@ -18,7 +18,9 @@ rendered properly in your Markdown viewer.\n \n [FP-Quant](https://github.com/IST-DASLab/FP-Quant) is a family of quantization algorithms tailored for the Blackwell generation of Nvidia GPUs. The goal is to allow for efficient post-training quantization (PTQ) and quantization-aware training (QAT) of LLMs in the [MXFP4 and NVFP4 data-types](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf).\n \n-Currently, only PTQ with MXFP4 is supported. Models can either be quantized on the fly with `quantization_config=FPQuantConfig()`:\n+This integration accompanies the pre-print of the [**Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization**](https://arxiv.org/abs/2509.23202) pre-print.\n+\n+Currently, only QAT is only supported with `pseudoquantization=True`. Models can either be quantized on the fly with `quantization_config=FPQuantConfig()`:\n \n ```python\n from transformers import AutoModelForCausalLM, AutoTokenizer, FPQuantConfig\n@@ -34,6 +36,8 @@ model = AutoModelForCausalLM.from_pretrained(\n \n or pre-processed with GPTQ for better quality (see [FP Format Quantization Harness](https://github.com/IST-DASLab/FP-Quant)).\n \n+You can choose between MXFP4 and NVFP4 with `FPQuantConfig(forward_dtype=\"mxfp4\")`. NVFP4 provides better quality but uses a little more memory.\n+\n A **Blackwell-generation GPU is required** to run the kernels. Runtime support for FP-Quant is implemented through the [QuTLASS](https://github.com/IST-DASLab/qutlass) library and a lightweight PyTorch interface lib [`fp_quant`](https://github.com/IST-DASLab/FP-Quant/tree/master/inference_lib). We recommend installing the former **from source** and the latter with  `pip install fp_quant`.\n \n Users **without a Blackwell-generation GPU** , can use the method with `quantization_config=FPQuantConfig(pseudoquant=True)` without having to install [QuTLASS](https://github.com/IST-DASLab/qutlass). This would provide no speedups but would fully emulate the effect of quantization."
        },
        {
            "sha": "0ac441e36f934c77e865c5e852c5ea0f27482b5b",
            "filename": "src/transformers/integrations/fp_quant.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/32567739740da86ddf96c60a23cf2d0494ce4145/src%2Ftransformers%2Fintegrations%2Ffp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32567739740da86ddf96c60a23cf2d0494ce4145/src%2Ftransformers%2Fintegrations%2Ffp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffp_quant.py?ref=32567739740da86ddf96c60a23cf2d0494ce4145",
            "patch": "@@ -28,6 +28,8 @@\n def adapt_fp_quant_config(config: FPQuantConfig):\n     if config.forward_dtype == \"mxfp4\":\n         forward_dtype = FPQuantDtype.MXFP4\n+    elif config.forward_dtype == \"nvfp4\":\n+        forward_dtype = FPQuantDtype.NVFP4\n     else:\n         raise ValueError(f\"Unsupported forward dtype: {config.forward_dtype}\")\n \n@@ -43,5 +45,6 @@ def adapt_fp_quant_config(config: FPQuantConfig):\n         store_master_weights=config.store_master_weights,\n         hadamard_group_size=config.hadamard_group_size,\n         pseudoquantization=config.pseudoquantization,\n+        transform_init=config.transform_init,\n         modules_to_not_convert=config.modules_to_not_convert,\n     )"
        },
        {
            "sha": "a7bc077776feb945f98e2854b4b48bb7e4c941f2",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/32567739740da86ddf96c60a23cf2d0494ce4145/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32567739740da86ddf96c60a23cf2d0494ce4145/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=32567739740da86ddf96c60a23cf2d0494ce4145",
            "patch": "@@ -37,7 +37,7 @@ class FPQuantHfQuantizer(HfQuantizer):\n \n     requires_calibration = False\n     requires_parameters_quantization = True\n-    is_qat_trainable = False\n+    is_qat_trainable = True\n     required_packages = [\"fp_quant\"]\n \n     def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n@@ -63,12 +63,16 @@ def validate_environment(self, device_map, **kwargs):\n         if not is_fp_quant_available():\n             raise ImportError(\"Using `fp_quant` quantization requires fp_quant: `pip install fp_quant`\")\n \n-        if device_map is None:\n+        if device_map is None and not self.quantization_config.pseudoquantization:\n             raise ValueError(\n                 \"You are attempting to load a FPQuant model without setting device_map.\"\n                 \" Please set device_map comprised of 'cuda' devices.\"\n             )\n-        elif isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n+        elif (\n+            isinstance(device_map, dict)\n+            and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n+            and not self.quantization_config.pseudoquantization\n+        ):\n             raise ValueError(\n                 \"You are attempting to load a FPQuant model with a device_map that contains a CPU or disk device.\"\n                 \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n@@ -154,7 +158,12 @@ def should_exclude(key: str) -> bool:\n \n     @property\n     def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n-        return False\n+        trainable = self.quantization_config.store_master_weights\n+        if not trainable:\n+            logger.warning(\n+                \"You are attempting to train a model with FPQuant quantization. This is only supported when `store_master_weights=True`. Please set `store_master_weights=True` to train the model.\"\n+            )\n+        return trainable\n \n     def is_serializable(self, safe_serialization=None):\n         return True"
        },
        {
            "sha": "d26c027a7d00413cdcd423bb63d087925cb9b16d",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32567739740da86ddf96c60a23cf2d0494ce4145/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32567739740da86ddf96c60a23cf2d0494ce4145/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=32567739740da86ddf96c60a23cf2d0494ce4145",
            "patch": "@@ -178,7 +178,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _auto_awq_available = importlib.util.find_spec(\"awq\") is not None\n _quark_available = _is_package_available(\"quark\")\n _fp_quant_available, _fp_quant_version = _is_package_available(\"fp_quant\", return_version=True)\n-_qutlass_available = _is_package_available(\"qutlass\")\n+_qutlass_available, _qutlass_version = _is_package_available(\"qutlass\", return_version=True)\n _is_optimum_quanto_available = False\n try:\n     importlib.metadata.version(\"optimum_quanto\")\n@@ -1289,12 +1289,12 @@ def is_quark_available() -> Union[tuple[bool, str], bool]:\n     return _quark_available\n \n \n-def is_fp_quant_available() -> bool:\n-    return _fp_quant_available and version.parse(_fp_quant_version) >= version.parse(\"0.1.6\")\n+def is_fp_quant_available():\n+    return _fp_quant_available and version.parse(_fp_quant_version) >= version.parse(\"0.2.0\")\n \n \n-def is_qutlass_available() -> Union[tuple[bool, str], bool]:\n-    return _qutlass_available\n+def is_qutlass_available():\n+    return _qutlass_available and version.parse(_qutlass_version) >= version.parse(\"0.1.0\")\n \n \n def is_compressed_tensors_available() -> bool:"
        },
        {
            "sha": "6c950bcbd298f48733efc3bd058bbe8a293f2955",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 35,
            "deletions": 11,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/32567739740da86ddf96c60a23cf2d0494ce4145/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32567739740da86ddf96c60a23cf2d0494ce4145/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=32567739740da86ddf96c60a23cf2d0494ce4145",
            "patch": "@@ -1557,31 +1557,33 @@ class FPQuantConfig(QuantizationConfigMixin):\n     FPQuantConfig is a configuration class for quantization using the FPQuant method.\n \n     Args:\n-        forward_dtype (`str`, *optional*, defaults to `\"mxfp4\"`):\n+        forward_dtype (`str`, *optional*, defaults to `\"nvfp4\"`):\n             The dtype to use for the forward pass.\n         forward_method (`str`, *optional*, defaults to `\"abs_max\"`):\n             The scaling to use for the forward pass. Can be `\"abs_max\"` or `\"quest\"`. `\"abs_max\"` is better for PTQ, `\"quest\"` is better for QAT.\n         backward_dtype (`str`, *optional*, defaults to `\"bf16\"`):\n             The dtype to use for the backward pass.\n         store_master_weights (`bool`, *optional*, defaults to `False`):\n             Whether to store the master weights. Needed for QAT over layer weights.\n-        hadamard_group_size (`int`, *optional*, defaults to 32):\n-            The group size for the hadamard transform before quantization for `\"quest\"` it matches the MXFP4 group size (32).\n+        hadamard_group_size (`int`, *optional*):\n+            The group size for the hadamard transform before quantization for `\"quest\"` it matches the MXFP4 group size (32). If `None`, it will be set to 16 for `\"nvfp4\"` and 32 for `\"mxfp4\"`.\n         pseudoquantization (`bool`, *optional*, defaults to `False`):\n             Whether to use Triton-based pseudo-quantization. Is mandatory for non-Blackwell GPUs. Doesn't provide any speedup. For debugging purposes.\n+        transform_init (`str`, *optional*, defaults to `\"hadamard\"`): a method to initialize the pre-processing matrix with. Can be `\"hadamard\"`, `\"identity\"` or `\"gsr\"`.\n         modules_to_not_convert (`list`, *optional*):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have\n             some modules left in their original precision.\n     \"\"\"\n \n     def __init__(\n         self,\n-        forward_dtype: str = \"mxfp4\",\n+        forward_dtype: str = \"nvfp4\",\n         forward_method: str = \"abs_max\",\n         backward_dtype: str = \"bf16\",\n         store_master_weights: bool = False,\n-        hadamard_group_size: int = 32,\n+        hadamard_group_size: Optional[int] = None,\n         pseudoquantization: bool = False,\n+        transform_init: str = \"hadamard\",\n         modules_to_not_convert: Optional[list[str]] = None,\n         **kwargs,\n     ):\n@@ -1591,6 +1593,7 @@ def __init__(\n         self.store_master_weights = store_master_weights\n         self.hadamard_group_size = hadamard_group_size\n         self.pseudoquantization = pseudoquantization\n+        self.transform_init = transform_init\n         self.modules_to_not_convert = modules_to_not_convert\n \n         self.quant_method = QuantizationMethod.FPQUANT\n@@ -1600,14 +1603,35 @@ def post_init(self):\n         r\"\"\"\n         Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n         \"\"\"\n-        if self.forward_dtype not in [\"mxfp4\"]:\n-            raise ValueError(\"Only 'mxfp4' is supported for forward_dtype for now.\")\n-        if self.forward_method not in [\"abs_max\", \"quest\"]:\n-            raise ValueError(\"Only 'abs_max' and 'quest' are supported for forward_method for now.\")\n+\n+        if self.hadamard_group_size is None:\n+            if self.forward_dtype == \"nvfp4\":\n+                self.hadamard_group_size = 16\n+            else:\n+                self.hadamard_group_size = 32\n+\n+        if self.forward_dtype == \"mxfp4\":\n+            if self.forward_method not in [\"abs_max\", \"quest\"]:\n+                raise ValueError(\"Only 'abs_max' and 'quest' are supported for forward_method for 'mxfp4'.\")\n+            if self.hadamard_group_size is None:\n+                self.hadamard_group_size = 32\n+            if self.hadamard_group_size not in [32, 64, 128]:\n+                raise ValueError(\"Only a `hadamard_group_size` of [32, 64, 128] is supported for 'mxfp4'.\")\n+        elif self.forward_dtype == \"nvfp4\":\n+            if self.forward_method not in [\"abs_max\"]:\n+                raise ValueError(\"Only 'abs_max' is supported for forward_method for 'nvfp4'.\")\n+            if self.hadamard_group_size is None:\n+                self.hadamard_group_size = 16\n+            if self.hadamard_group_size not in [16, 32, 64, 128]:\n+                raise ValueError(\"Only a `hadamard_group_size` of [16, 32, 64, 128] is supported for 'nvfp4'.\")\n+        else:\n+            raise ValueError(\"Only 'mxfp4' and 'nvfp4' are supported for forward_dtype for now.\")\n+\n         if self.backward_dtype not in [\"bf16\"]:\n             raise ValueError(\"Only 'bf16' is supported for backward_dtype for now.\")\n-        if self.hadamard_group_size not in [32]:\n-            raise ValueError(\"Only a hadamard_group_size of 32 is supported for now.\")\n+        if self.transform_init not in [\"hadamard\", \"identity\", \"gsr\"]:\n+            raise ValueError(\"Only 'hadamard', 'identity' and 'gsr' are supported for transform_init.\")\n+\n         if self.modules_to_not_convert is None:\n             self.modules_to_not_convert = [\"lm_head\"]\n "
        },
        {
            "sha": "9970381e5397702880b55a043b165780c8e1833a",
            "filename": "tests/quantization/fp_quant_integration/test_fp_quant.py",
            "status": "modified",
            "additions": 29,
            "deletions": 79,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/32567739740da86ddf96c60a23cf2d0494ce4145/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32567739740da86ddf96c60a23cf2d0494ce4145/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py?ref=32567739740da86ddf96c60a23cf2d0494ce4145",
            "patch": "@@ -55,9 +55,8 @@ def test_from_dict(self):\n @slow\n @require_torch_gpu\n @require_fp_quant\n-@require_qutlass\n @require_accelerate\n-class FPQuantTest(unittest.TestCase):\n+class FPQuantBaseTest(unittest.TestCase):\n     model_name = \"unsloth/Llama-3.2-1B\"\n \n     input_text = \"1 2 3 4\"\n@@ -67,13 +66,18 @@ class FPQuantTest(unittest.TestCase):\n \n     device_map = \"cuda\"\n \n+    @classmethod\n+    def getQuantizationConfig(cls):\n+        unittest.skip(\"Subclass must implement this method\")\n+\n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n         \"\"\"\n         Setup quantized model\n         \"\"\"\n-        quantization_config = FPQuantConfig(pseudoquantization=False)\n+\n+        quantization_config = cls.getQuantizationConfig()\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n             cls.model_name, device_map=cls.device_map, quantization_config=quantization_config\n@@ -140,88 +144,34 @@ def test_save_pretrained_multi_gpu(self):\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n \n-@slow\n-@require_torch_gpu\n-@require_fp_quant\n-@require_accelerate\n-class FPQuantPseudoquantTest(unittest.TestCase):\n-    model_name = \"unsloth/Llama-3.2-1B\"\n-\n-    input_text = \"1 2 3 4\"\n-    max_new_tokens = 4\n-\n-    EXPECTED_OUTPUT = \"1 2 3 4 5 6\"\n-\n-    device_map = \"cuda\"\n-\n-    # called only once for all test in this class\n+class FPQuantMXFP4PseudoquantTest(FPQuantBaseTest):\n     @classmethod\n-    def setUpClass(cls):\n-        \"\"\"\n-        Setup quantized model\n-        \"\"\"\n-        quantization_config = FPQuantConfig(pseudoquantization=True)\n-        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n-        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n-            cls.model_name, device_map=cls.device_map, quantization_config=quantization_config\n-        )\n-\n-    def tearDown(self):\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n-    def test_quantized_model(self):\n-        \"\"\"\n-        Simple test that checks if the quantized model is working properly\n-        \"\"\"\n-        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n-\n-        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n-\n-    def test_save_pretrained(self):\n-        \"\"\"\n-        Simple test that checks if the quantized model is working properly after being saved and loaded\n-        \"\"\"\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            self.quantized_model.save_pretrained(tmpdirname)\n+    def getQuantizationConfig(cls):\n+        return FPQuantConfig(forward_dtype=\"mxfp4\", pseudoquantization=True)\n \n-            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.device_map)\n \n-            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n-\n-            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+class FPQuantNVFP4PseudoquantTest(FPQuantBaseTest):\n+    @classmethod\n+    def getQuantizationConfig(cls):\n+        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=True)\n \n-    @require_torch_multi_gpu\n-    def test_quantized_model_multi_gpu(self):\n-        \"\"\"\n-        Simple test that checks if the quantized model is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n-        \"\"\"\n-        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n-        quantization_config = FPQuantConfig(pseudoquantization=True)\n-        quantized_model = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, device_map=\"auto\", quantization_config=quantization_config\n-        )\n-        self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n \n-        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+@require_qutlass\n+class FPQuantMXFP4Test(FPQuantBaseTest):\n+    @classmethod\n+    def getQuantizationConfig(cls):\n+        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=False)\n \n-    @require_torch_multi_gpu\n-    def test_save_pretrained_multi_gpu(self):\n-        \"\"\"\n-        Simple test that checks if the quantized model is working properly after being saved and loaded\n-        \"\"\"\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            self.quantized_model.save_pretrained(tmpdirname)\n \n-            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=\"auto\")\n-            self.assertTrue(set(model.hf_device_map.values()) == {0, 1})\n+@require_qutlass\n+class FPQuantMXFP4GS128Test(FPQuantBaseTest):\n+    @classmethod\n+    def getQuantizationConfig(cls):\n+        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=False, hadamard_group_size=128)\n \n-            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n-            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+@require_qutlass\n+class FPQuantNVFP4GS128Test(FPQuantBaseTest):\n+    @classmethod\n+    def getQuantizationConfig(cls):\n+        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=False, hadamard_group_size=128)"
        }
    ],
    "stats": {
        "total": 197,
        "additions": 93,
        "deletions": 104
    }
}