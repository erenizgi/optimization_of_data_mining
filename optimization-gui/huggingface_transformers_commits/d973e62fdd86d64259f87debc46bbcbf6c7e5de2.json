{
    "author": "Vaibhavs10",
    "message": "fix condition where torch_dtype auto collides with model_kwargs. (#39054)\n\n* fix condition where torch_dtype auto collides with model_kwargs.\n\n* update tests\n\n* update comment\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "d973e62fdd86d64259f87debc46bbcbf6c7e5de2",
    "files": [
        {
            "sha": "2b433d9c7fe240e268576885c4a08d5b38e3ab83",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d973e62fdd86d64259f87debc46bbcbf6c7e5de2/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d973e62fdd86d64259f87debc46bbcbf6c7e5de2/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=d973e62fdd86d64259f87debc46bbcbf6c7e5de2",
            "patch": "@@ -1005,13 +1005,21 @@ def pipeline(\n         model_kwargs[\"device_map\"] = device_map\n     if torch_dtype is not None:\n         if \"torch_dtype\" in model_kwargs:\n-            raise ValueError(\n-                'You cannot use both `pipeline(... torch_dtype=..., model_kwargs={\"torch_dtype\":...})` as those'\n-                \" arguments might conflict, use only one.)\"\n-            )\n-        if isinstance(torch_dtype, str) and hasattr(torch, torch_dtype):\n-            torch_dtype = getattr(torch, torch_dtype)\n-        model_kwargs[\"torch_dtype\"] = torch_dtype\n+            # If the user did not explicitly provide `torch_dtype` (i.e. the function default \"auto\" is still\n+            # present) but a value is supplied inside `model_kwargs`, we silently defer to the latter instead of\n+            # raising. This prevents false positives like providing `torch_dtype` only via `model_kwargs` while the\n+            # top-level argument keeps its default value \"auto\".\n+            if torch_dtype == \"auto\":\n+                torch_dtype = None\n+            else:\n+                raise ValueError(\n+                    'You cannot use both `pipeline(... torch_dtype=..., model_kwargs={\"torch_dtype\":...})` as those'\n+                    \" arguments might conflict, use only one.)\"\n+                )\n+        if torch_dtype is not None:\n+            if isinstance(torch_dtype, str) and hasattr(torch, torch_dtype):\n+                torch_dtype = getattr(torch, torch_dtype)\n+            model_kwargs[\"torch_dtype\"] = torch_dtype\n \n     model_name = model if isinstance(model, str) else None\n "
        },
        {
            "sha": "781fbad8a907ac387d139c159011355146057ae9",
            "filename": "tests/pipelines/test_pipelines_image_text_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d973e62fdd86d64259f87debc46bbcbf6c7e5de2/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d973e62fdd86d64259f87debc46bbcbf6c7e5de2/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py?ref=d973e62fdd86d64259f87debc46bbcbf6c7e5de2",
            "patch": "@@ -161,11 +161,11 @@ def test_small_model_pt_token(self):\n             [\n                 {\n                     \"input_text\": \"<image> What this is? Assistant: This is\",\n-                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are sleeping and appear to be comfortable\",\n+                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are facing the camera, and they\",\n                 },\n                 {\n                     \"input_text\": \"<image> What this is? Assistant: This is\",\n-                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are sleeping and appear to be comfortable\",\n+                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are facing the camera, and they\",\n                 },\n             ],\n         )"
        },
        {
            "sha": "d92a3aefecaca5ea03a9fa3cbe7ecdd29e220c29",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d973e62fdd86d64259f87debc46bbcbf6c7e5de2/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d973e62fdd86d64259f87debc46bbcbf6c7e5de2/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=d973e62fdd86d64259f87debc46bbcbf6c7e5de2",
            "patch": "@@ -441,11 +441,11 @@ def test_small_model_pt_bloom_accelerate(self):\n             [{\"generated_text\": (\"This is a test test test test test test\")}],\n         )\n \n-        # torch_dtype will be automatically set to float32 if not provided - check: https://github.com/huggingface/transformers/pull/20602\n+        # torch_dtype will be automatically set to torch.bfloat16 if not provided - check: https://github.com/huggingface/transformers/pull/38882\n         pipe = pipeline(\n             model=\"hf-internal-testing/tiny-random-bloom\", device_map=\"auto\", max_new_tokens=5, do_sample=False\n         )\n-        self.assertEqual(pipe.model.lm_head.weight.dtype, torch.float32)\n+        self.assertEqual(pipe.model.lm_head.weight.dtype, torch.bfloat16)\n         out = pipe(\"This is a test\")\n         self.assertEqual(\n             out,"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 19,
        "deletions": 11
    }
}