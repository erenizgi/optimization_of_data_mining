{
    "author": "FightingZhen",
    "message": "[bugfix] fix flash_attention_2 unavailable error on Ascend NPU (#39844)",
    "sha": "ac0b4684657cbff1e8eecb0d966d10b71843dca0",
    "files": [
        {
            "sha": "ed1b30d9a6b090d37b7df62b45c41a35ee81c495",
            "filename": "src/transformers/integrations/npu_flash_attention.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac0b4684657cbff1e8eecb0d966d10b71843dca0/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac0b4684657cbff1e8eecb0d966d10b71843dca0/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py?ref=ac0b4684657cbff1e8eecb0d966d10b71843dca0",
            "patch": "@@ -267,3 +267,8 @@ def npu_apply_rotary_emb(x, cos, sin, **kwargs):\n         sin = sin.unsqueeze(0).unsqueeze(2)\n \n     return npu_rotary_mul(x, cos, sin)\n+\n+\n+def get_npu_flash_attn_funcs():\n+    # return flash attention related functions used for Ascend NPU in order\n+    return npu_flash_attn_func, npu_flash_attn_varlen_func, pad_input, unpad_input, False"
        },
        {
            "sha": "7d5d69b720a2edba86f469daa1ece08a9a0384a5",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac0b4684657cbff1e8eecb0d966d10b71843dca0/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac0b4684657cbff1e8eecb0d966d10b71843dca0/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=ac0b4684657cbff1e8eecb0d966d10b71843dca0",
            "patch": "@@ -261,7 +261,7 @@ def fa_peft_integration_check(q, k, v, target_dtype: Optional[torch.dtype] = Non\n \n def _lazy_imports(impl: Optional[str]):\n     # returns funcs and pad/unpad based on impl\n-    is_fa2 = is_flash_attn_2_available() or is_torch_npu_available()\n+    is_fa2 = is_flash_attn_2_available()\n     is_fa3 = is_flash_attn_3_available()\n     if impl == \"flash_attention_2\" or (impl is None and is_fa2 and not is_fa3):\n         try:\n@@ -299,7 +299,12 @@ def _lazy_imports(impl: Optional[str]):\n                 raise ImportError(\n                     \"Failed to import flash attention 2, please install it or use another implementation.\"\n                 ) from e\n-    if impl == \"flash_attention_3\" or (impl is None and is_fa3):\n+    elif is_torch_npu_available():\n+        # get flash attention related functions from `.integrations.npu_flash_attention` module for Ascend NPU\n+        from .integrations.npu_flash_attention import get_npu_flash_attn_funcs\n+\n+        return get_npu_flash_attn_funcs()\n+    elif impl == \"flash_attention_3\" or (impl is None and is_fa3):\n         from flash_attn_interface import flash_attn_func, flash_attn_varlen_func\n \n         pad_input, unpad_input = _fa3_pad_input, _fa3_unpad_input"
        },
        {
            "sha": "4b3be59e339b75fafa861e43c3eab8a8185df824",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac0b4684657cbff1e8eecb0d966d10b71843dca0/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac0b4684657cbff1e8eecb0d966d10b71843dca0/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ac0b4684657cbff1e8eecb0d966d10b71843dca0",
            "patch": "@@ -2483,8 +2483,12 @@ def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n             preface = \"FlashAttention2 has been toggled on, but it cannot be used due to the following error:\"\n             install_message = \"Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\"\n \n-            # package `flash-attn` can not be installed on Ascend NPU, ignore related validation logi\n-            if importlib.util.find_spec(\"flash_attn\") is None and not is_torch_npu_available():\n+            # package `flash-attn` can not be installed on Ascend NPU, following validation logics can be ignored.\n+            if is_torch_npu_available():\n+                logger.info(\"Detect using FlashAttention2 on Ascend NPU.\")\n+                return True\n+\n+            if importlib.util.find_spec(\"flash_attn\") is None:\n                 raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n             else:\n                 # Check FA2 installed version compatibility"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 18,
        "deletions": 4
    }
}