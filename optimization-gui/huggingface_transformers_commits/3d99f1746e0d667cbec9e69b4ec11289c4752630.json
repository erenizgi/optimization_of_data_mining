{
    "author": "Cyrilvallez",
    "message": "Fix glm  (#34388)\n\n* Fix duplicated\r\n\r\n* fix import",
    "sha": "3d99f1746e0d667cbec9e69b4ec11289c4752630",
    "files": [
        {
            "sha": "5f8eaf89ed9353cb5589ad900b7f0b00577d7da4",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3d99f1746e0d667cbec9e69b4ec11289c4752630/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3d99f1746e0d667cbec9e69b4ec11289c4752630/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=3d99f1746e0d667cbec9e69b4ec11289c4752630",
            "patch": "@@ -30,35 +30,26 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_glm import GlmConfig\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n-from ...processing_utils import Unpack\n-\n-\n-_CHECKPOINT_FOR_DOC = \"dummy\"\n-\n-\n _CHECKPOINT_FOR_DOC = \"THUDM/glm-4-9b\"\n \n "
        },
        {
            "sha": "39ee4a2ad5803e5944b99b2ea3b7bc03a56daab9",
            "filename": "src/transformers/models/glm/modular_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3d99f1746e0d667cbec9e69b4ec11289c4752630/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3d99f1746e0d667cbec9e69b4ec11289c4752630/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py?ref=3d99f1746e0d667cbec9e69b4ec11289c4752630",
            "patch": "@@ -44,11 +44,9 @@\n from .configuration_glm import GlmConfig\n \n \n-_CHECKPOINT_FOR_DOC = \"THUDM/glm-4-9b\"\n-\n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"dummy\"\n+_CHECKPOINT_FOR_DOC = \"THUDM/glm-4-9b\"\n \n \n class GlmRMSNorm(Phi3RMSNorm):"
        },
        {
            "sha": "a1a86e3672d5fc3798c60306d08fe66f2fc916f3",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3d99f1746e0d667cbec9e69b4ec11289c4752630/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3d99f1746e0d667cbec9e69b4ec11289c4752630/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=3d99f1746e0d667cbec9e69b4ec11289c4752630",
            "patch": "@@ -28,6 +28,7 @@\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -39,17 +40,13 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_phi3 import Phi3Config\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"microsoft/Phi-3-mini-4k-instruct\""
        }
    ],
    "stats": {
        "total": 22,
        "additions": 4,
        "deletions": 18
    }
}