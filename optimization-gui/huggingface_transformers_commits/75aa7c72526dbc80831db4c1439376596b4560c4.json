{
    "author": "ashmikuz",
    "message": "[ModernBert] Prevent the attention mask from being None in ModernBertForSequenceClassification (#35991)\n\n* [ModernBert] Prevent the attention mask from being None in ModernBertForSequenceClassification\n\n* fix the modular conversion",
    "sha": "75aa7c72526dbc80831db4c1439376596b4560c4",
    "files": [
        {
            "sha": "d36b5a9b9485b9b920f88d6833ac2b5412d52e0d",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/75aa7c72526dbc80831db4c1439376596b4560c4/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75aa7c72526dbc80831db4c1439376596b4560c4/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=75aa7c72526dbc80831db4c1439376596b4560c4",
            "patch": "@@ -1151,6 +1151,19 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         self._maybe_set_compile()\n \n+        if input_ids is not None:\n+            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n+\n+        if batch_size is None and seq_len is None:\n+            if inputs_embeds is not None:\n+                batch_size, seq_len = inputs_embeds.shape[:2]\n+            else:\n+                batch_size, seq_len = input_ids.shape[:2]\n+        device = input_ids.device if input_ids is not None else inputs_embeds.device\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_len), device=device, dtype=torch.bool)\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "394fdce4fe806f7fbc76ae3914d380a541959700",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/75aa7c72526dbc80831db4c1439376596b4560c4/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75aa7c72526dbc80831db4c1439376596b4560c4/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=75aa7c72526dbc80831db4c1439376596b4560c4",
            "patch": "@@ -1277,6 +1277,19 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         self._maybe_set_compile()\n \n+        if input_ids is not None:\n+            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n+\n+        if batch_size is None and seq_len is None:\n+            if inputs_embeds is not None:\n+                batch_size, seq_len = inputs_embeds.shape[:2]\n+            else:\n+                batch_size, seq_len = input_ids.shape[:2]\n+        device = input_ids.device if input_ids is not None else inputs_embeds.device\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_len), device=device, dtype=torch.bool)\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 26,
        "deletions": 0
    }
}