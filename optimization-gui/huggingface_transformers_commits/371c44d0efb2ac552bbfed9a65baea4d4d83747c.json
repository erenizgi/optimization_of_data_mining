{
    "author": "cyyever",
    "message": "Remove old code for  PyTorch,  Accelerator and tokenizers (#37234)\n\n* Remove unneeded library version checks\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Remove PyTorch condition\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Remove PyTorch condition\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Fix ROCm get_device_capability\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Revert \"Fix ROCm get_device_capability\"\n\nThis reverts commit 0e756434bd7e74ffd73de5500476072b096570a6.\n\n* Remove unnecessary check\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Revert changes\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "371c44d0efb2ac552bbfed9a65baea4d4d83747c",
    "files": [
        {
            "sha": "2dab2fb32cd09839ea725cadd9bde1b3a48c8521",
            "filename": "src/transformers/activations.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Factivations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Factivations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Factivations.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -16,7 +16,6 @@\n from collections import OrderedDict\n \n import torch\n-from packaging import version\n from torch import Tensor, nn\n \n from .utils import logging\n@@ -34,14 +33,6 @@ class PytorchGELUTanh(nn.Module):\n     match due to rounding errors.\n     \"\"\"\n \n-    def __init__(self):\n-        super().__init__()\n-        if version.parse(torch.__version__) < version.parse(\"1.12.0\"):\n-            raise ImportError(\n-                f\"You are using torch=={torch.__version__}, but torch>=1.12.0 is required to use \"\n-                \"PytorchGELUTanh. Please upgrade torch.\"\n-            )\n-\n     def forward(self, input: Tensor) -> Tensor:\n         return nn.functional.gelu(input, approximate=\"tanh\")\n \n@@ -145,10 +136,7 @@ class MishActivation(nn.Module):\n \n     def __init__(self):\n         super().__init__()\n-        if version.parse(torch.__version__) < version.parse(\"1.9.0\"):\n-            self.act = self._mish_python\n-        else:\n-            self.act = nn.functional.mish\n+        self.act = nn.functional.mish\n \n     def _mish_python(self, input: Tensor) -> Tensor:\n         return input * torch.tanh(nn.functional.softplus(input))"
        },
        {
            "sha": "226ca6ee6163d7a4b0df75cee0e1c19e028db583",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -1500,7 +1500,6 @@ def create_extended_attention_mask_for_decoder(input_shape, attention_mask, devi\n         seq_ids = torch.arange(seq_length, device=device)\n         causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n         # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n-        # causal and attention masks must have same type with pytorch version < 1.3\n         causal_mask = causal_mask.to(attention_mask.dtype)\n \n         if causal_mask.shape[1] < attention_mask.shape[1]:"
        },
        {
            "sha": "15087677e26e8e845070d5f18013fa5cf2701580",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -633,7 +633,6 @@ def get_extended_attention_mask(\n                 seq_ids = torch.arange(seq_length, device=device)\n                 causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n                 # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n-                # causal and attention masks must have same type with pytorch version < 1.3\n                 causal_mask = causal_mask.to(attention_mask.dtype)\n \n                 if causal_mask.shape[1] < attention_mask.shape[1]:"
        },
        {
            "sha": "f6a17ebc6d19ccad78225bc5f2c919a41e6079a5",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -20,11 +20,8 @@\n \n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n from ...utils import is_sentencepiece_available, logging\n-from ...utils.versions import require_version\n \n \n-require_version(\"tokenizers>=0.13.3\")\n-\n if is_sentencepiece_available():\n     from .tokenization_code_llama import CodeLlamaTokenizer\n else:"
        },
        {
            "sha": "d679f0c4e3a5ad24df76213be0567c9d6fc76b74",
            "filename": "src/transformers/models/cohere/tokenization_cohere_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -23,11 +23,8 @@\n from ...tokenization_utils_base import BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n from ...utils import logging\n-from ...utils.versions import require_version\n \n \n-require_version(\"tokenizers>=0.13.3\")\n-\n logger = logging.get_logger(__name__)\n VOCAB_FILES_NAMES = {\"tokenizer_file\": \"tokenizer.json\"}\n "
        },
        {
            "sha": "24e2c90c307d93f3a67160c94d2d996a8e46adcd",
            "filename": "src/transformers/models/gemma/tokenization_gemma_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -20,11 +20,8 @@\n \n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n from ...utils import is_sentencepiece_available, logging\n-from ...utils.versions import require_version\n \n \n-require_version(\"tokenizers>=0.13.3\")\n-\n if is_sentencepiece_available():\n     from .tokenization_gemma import GemmaTokenizer\n else:"
        },
        {
            "sha": "2f723e4698f7921b59ccfaf6019c8d7a9ec5e9bd",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -42,7 +42,6 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_torch_flex_attn_available,\n-    is_torch_fx_available,\n     logging,\n )\n from .configuration_gpt_neo import GPTNeoConfig\n@@ -60,8 +59,7 @@\n \n # This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n # It means that the function will not be traced through and simply appear as a node in the graph.\n-if is_torch_fx_available():\n-    _prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n+_prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "c348322f2b0bc59e18069a547ab74bdbcf4c6396",
            "filename": "src/transformers/models/llama/tokenization_llama_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -20,11 +20,8 @@\n \n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n from ...utils import is_sentencepiece_available, logging\n-from ...utils.versions import require_version\n \n \n-require_version(\"tokenizers>=0.13.3\")\n-\n if is_sentencepiece_available():\n     from .tokenization_llama import LlamaTokenizer\n else:"
        },
        {
            "sha": "0769b91909f6c5f9d5cb34cfeb0bb4eae042e47b",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -42,7 +42,6 @@\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n-from ...utils.import_utils import is_torch_fx_available\n from .configuration_phimoe import PhimoeConfig\n \n \n@@ -51,8 +50,7 @@\n \n # This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n # It means that the function will not be traced through and simply appear as a node in the graph.\n-if is_torch_fx_available():\n-    _prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n+_prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "1aa529c26dc6cc0eb335a9b2b38906f9af4e9144",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -171,7 +171,7 @@ def visual_embed(self, pixel_values, pixel_mask, max_image_length=200):\n         select = torch.cat(select, dim=0)\n         x = x[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n         x_mask = x_mask[select[:, 0], select[:, 1]].view(batch_size, -1)\n-        # `patch_index` should be on the same device as `select` (for torch>=1.13), which is ensured at definition time.\n+        # `patch_index` should be on the same device as `select`, which is ensured at definition time.\n         patch_index = patch_index[select[:, 0], select[:, 1]].view(batch_size, -1, 2)\n         pos_embed = pos_embed[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n "
        },
        {
            "sha": "98e07f78f715a539510dd1c131fb03be623103aa",
            "filename": "src/transformers/optimization.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Foptimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Foptimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Foptimization.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -25,7 +25,6 @@\n from .trainer_pt_utils import LayerWiseDummyOptimizer, LayerWiseDummyScheduler\n from .trainer_utils import SchedulerType\n from .utils import logging\n-from .utils.versions import require_version\n \n \n logger = logging.get_logger(__name__)\n@@ -701,7 +700,6 @@ def __init__(\n         relative_step=True,\n         warmup_init=False,\n     ):\n-        require_version(\"torch>=1.5.0\")  # add_ with alpha\n         if lr is not None and relative_step:\n             raise ValueError(\"Cannot combine manual `lr` and `relative_step=True` options\")\n         if warmup_init and not relative_step:"
        },
        {
            "sha": "49ea8ef1777fadab1f459c4260549f107ac97238",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -138,7 +138,6 @@\n     is_tokenizers_available,\n     is_torch_available,\n     is_torch_bf16_available_on_device,\n-    is_torch_bf16_cpu_available,\n     is_torch_bf16_gpu_available,\n     is_torch_deterministic,\n     is_torch_fp16_available_on_device,\n@@ -1073,14 +1072,6 @@ def require_torch_bf16_gpu(test_case):\n     )(test_case)\n \n \n-def require_torch_bf16_cpu(test_case):\n-    \"\"\"Decorator marking a test that requires torch>=1.10, using CPU.\"\"\"\n-    return unittest.skipUnless(\n-        is_torch_bf16_cpu_available(),\n-        \"test requires torch>=1.10, using CPU\",\n-    )(test_case)\n-\n-\n def require_deterministic_for_xpu(test_case):\n     if is_torch_xpu_available():\n         return unittest.skipUnless(is_torch_deterministic(), \"test requires torch to use deterministic algorithms\")("
        },
        {
            "sha": "af2cae600b144fc7a758217486abb04c686e79a4",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -164,7 +164,6 @@\n     is_sagemaker_dp_enabled,\n     is_sagemaker_mp_enabled,\n     is_schedulefree_available,\n-    is_torch_compile_available,\n     is_torch_hpu_available,\n     is_torch_mlu_available,\n     is_torch_mps_available,\n@@ -257,7 +256,7 @@\n \n def _is_peft_model(model):\n     if is_peft_available():\n-        classes_to_check = (PeftModel,) if is_peft_available() else ()\n+        classes_to_check = (PeftModel,)\n         # Here we also check if the model is an instance of `PeftMixedModel` introduced in peft>=0.7.0: https://github.com/huggingface/transformers/pull/28321\n         if version.parse(importlib.metadata.version(\"peft\")) >= version.parse(\"0.7.0\"):\n             from peft import PeftMixedModel\n@@ -797,10 +796,6 @@ def __init__(\n         # very last\n         self._memory_tracker.stop_and_update_metrics()\n \n-        # torch.compile\n-        if args.torch_compile and not is_torch_compile_available():\n-            raise RuntimeError(\"Using torch.compile requires PyTorch 2.0 or higher.\")\n-\n         self.is_fsdp_xla_v2_enabled = args.fsdp_config.get(\"xla_fsdp_v2\", False)\n         if self.is_fsdp_xla_v2_enabled:\n             if not IS_XLA_FSDPV2_POST_2_2:\n@@ -1987,7 +1982,7 @@ def _wrap_model(self, model, training=True, dataloader=None):\n         if self.accelerator.unwrap_model(model) is not model:\n             return model\n \n-        # Mixed precision training with apex (torch < 1.6)\n+        # Mixed precision training with apex\n         if self.use_apex and training:\n             model, self.optimizer = amp.initialize(model, self.optimizer, opt_level=self.args.fp16_opt_level)\n \n@@ -3739,7 +3734,7 @@ def training_step(\n                 torch.musa.empty_cache()\n             elif is_torch_npu_available():\n                 torch.npu.empty_cache()\n-            elif is_torch_mps_available(min_version=\"2.0\"):\n+            elif is_torch_mps_available():\n                 torch.mps.empty_cache()\n             elif is_torch_hpu_available():\n                 logger.warning("
        },
        {
            "sha": "65fd93a79ea86842575553d42d5fba9323257b5b",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -44,7 +44,6 @@\n     is_sagemaker_dp_enabled,\n     is_sagemaker_mp_enabled,\n     is_torch_available,\n-    is_torch_bf16_cpu_available,\n     is_torch_bf16_gpu_available,\n     is_torch_hpu_available,\n     is_torch_mlu_available,\n@@ -1161,7 +1160,6 @@ class TrainingArguments:\n             \"help\": (\n                 \"Number of batches loaded in advance by each worker. \"\n                 \"2 means there will be a total of 2 * num_workers batches prefetched across all workers. \"\n-                \"Default is 2 for PyTorch < 2.0.0 and otherwise None.\"\n             )\n         },\n     )\n@@ -1681,7 +1679,7 @@ def __post_init__(self):\n                 self.half_precision_backend = self.fp16_backend\n \n             if self.bf16 or self.bf16_full_eval:\n-                if self.use_cpu and not is_torch_bf16_cpu_available() and not is_torch_xla_available():\n+                if self.use_cpu and not is_torch_available() and not is_torch_xla_available():\n                     # cpu\n                     raise ValueError(\"Your setup doesn't support bf16/(cpu, tpu, neuroncore). You need torch>=1.10\")\n                 elif not self.use_cpu:"
        },
        {
            "sha": "4063d90c6aadaef1b155dc53f4473e60f1524cdf",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -61,10 +61,7 @@\n )\n from .import_utils import (\n     ENV_VARS_TRUE_VALUES,\n-    TORCH_FX_REQUIRED_VERSION,\n-    get_torch_version,\n     is_peft_available,\n-    is_torch_fx_available,\n )\n \n \n@@ -891,12 +888,6 @@ class HFTracer(Tracer):\n     def __init__(self, autowrap_modules=(math,), autowrap_functions=()):\n         super().__init__(autowrap_modules=autowrap_modules, autowrap_functions=autowrap_functions)\n \n-        if not is_torch_fx_available():\n-            raise ImportError(\n-                f\"Found an incompatible version of torch. Found version {get_torch_version()}, but only version \"\n-                f\"{TORCH_FX_REQUIRED_VERSION} is supported.\"\n-            )\n-\n     def _generate_dummy_input(\n         self, model: \"PreTrainedModel\", input_name: str, shape: list[int], input_names: list[str]\n     ) -> dict[str, torch.Tensor]:"
        },
        {
            "sha": "87a43692b2fb2cdfc43da54b8c71587042511281",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 42,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -222,6 +222,10 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _torch_available = False\n if USE_TORCH in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TF not in ENV_VARS_TRUE_VALUES:\n     _torch_available, _torch_version = _is_package_available(\"torch\", return_version=True)\n+    if _torch_available:\n+        _torch_available = version.parse(_torch_version) >= version.parse(\"2.1.0\")\n+        if not _torch_available:\n+            logger.warning(f\"Disabling PyTorch because PyTorch >= 2.1 is required but found {_torch_version}\")\n else:\n     logger.info(\"Disabling PyTorch because USE_TF is set\")\n     _torch_available = False\n@@ -310,15 +314,6 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n             _jax_version = _flax_version = \"N/A\"\n \n \n-_torch_fx_available = False\n-if _torch_available:\n-    torch_version = version.parse(_torch_version)\n-    _torch_fx_available = (torch_version.major, torch_version.minor) >= (\n-        TORCH_FX_REQUIRED_VERSION.major,\n-        TORCH_FX_REQUIRED_VERSION.minor,\n-    )\n-\n-\n _torch_xla_available = False\n if USE_TORCH_XLA in ENV_VARS_TRUE_VALUES:\n     _torch_xla_available, _torch_xla_version = _is_package_available(\"torch_xla\", return_version=True)\n@@ -526,19 +521,8 @@ def is_torch_bf16_gpu_available():\n     return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n \n \n-def is_torch_bf16_cpu_available():\n-    if not is_torch_available():\n-        return False\n-\n-    import torch\n-\n-    try:\n-        # multiple levels of AttributeError depending on the pytorch version so do them all in one check\n-        _ = torch.cpu.amp.autocast\n-    except AttributeError:\n-        return False\n-\n-    return True\n+def is_torch_bf16_cpu_available() -> bool:\n+    return is_torch_available()\n \n \n def is_torch_bf16_available():\n@@ -618,16 +602,11 @@ def is_torch_tf32_available():\n         return False\n     if torch.cuda.get_device_properties(torch.cuda.current_device()).major < 8:\n         return False\n-    if int(torch.version.cuda.split(\".\")[0]) < 11:\n-        return False\n-    if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.7\"):\n-        return False\n-\n     return True\n \n \n def is_torch_fx_available():\n-    return _torch_fx_available\n+    return is_torch_available()\n \n \n def is_peft_available():\n@@ -832,21 +811,11 @@ def is_habana_gaudi1():\n \n \n def is_torchdynamo_available():\n-    if not is_torch_available():\n-        return False\n-\n-    return True\n+    return is_torch_available()\n \n \n def is_torch_compile_available():\n-    if not is_torch_available():\n-        return False\n-\n-    import torch\n-\n-    # We don't do any version check here to support nighlies marked as 1.14. Ultimately needs to check version against\n-    # 2.0 but let's do it later.\n-    return hasattr(torch, \"compile\")\n+    return is_torch_available()\n \n \n def is_torchdynamo_compiling():\n@@ -979,10 +948,10 @@ def is_torch_xpu_available(check_device=False):\n         return False\n \n     torch_version = version.parse(_torch_version)\n-    if torch_version.major < 2 or (torch_version.major == 2 and torch_version.minor < 6):\n+    if torch_version.major == 2 and torch_version.minor < 6:\n         if is_ipex_available():\n             import intel_extension_for_pytorch  # noqa: F401\n-        elif torch_version.major < 2 or (torch_version.major == 2 and torch_version.minor < 4):\n+        elif torch_version.major == 2 and torch_version.minor < 4:\n             return False\n \n     import torch"
        },
        {
            "sha": "96c1860860ff7df3dfc0a99dbbb2963480ef47b7",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -323,7 +323,6 @@ def test_fsdp_cpu_offloading(self):\n \n     @require_torch_multi_accelerator\n     @slow\n-    @require_fsdp\n     @require_fsdp_v2_version\n     @require_accelerate_fsdp2\n     def test_accelerate_fsdp2_integration(self):"
        },
        {
            "sha": "a6aac8e3829a92492eaa6f11692254a505bcfac1",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -510,7 +510,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "e639f31073a4abca4cb723c1b68b7cd5d1709985",
            "filename": "tests/models/bert_generation/test_modeling_bert_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -273,7 +273,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "bdab0f73b653b0df13180188f061a666889e6885",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -506,7 +506,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "520ff2af3dd92527d4c7ab276a4a89c042dd26f7",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -354,7 +354,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "acb18b3d8e89bb5f87bfda8aaa24a3fa926fca95",
            "filename": "tests/models/data2vec/test_modeling_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -409,7 +409,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "7e99ba8e81db2c0acfb8d27d622745c24cbac79a",
            "filename": "tests/models/ernie/test_modeling_ernie.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -492,7 +492,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "33c79f2a7b12ad2a744b9f971b5ee1de477c0d69",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -306,7 +306,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(config, input_ids, input_mask)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         config, input_ids, input_mask, token_labels = self.model_tester.prepare_config_and_inputs_for_decoder()\n \n         input_mask = None"
        },
        {
            "sha": "168a0f2eebfbe5cf4cf01c0192135c58aff3f961",
            "filename": "tests/models/gpt_neox_japanese/test_modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -223,7 +223,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(config, input_ids, input_mask)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         config, input_ids, input_mask, token_labels = self.model_tester.prepare_config_and_inputs_for_decoder()\n \n         input_mask = None"
        },
        {
            "sha": "c4e65cfa5c6b8282559772a20fc1a62e78917048",
            "filename": "tests/models/hubert/test_modeling_hubert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -23,7 +23,6 @@\n \n from transformers import HubertConfig, is_torch_available\n from transformers.testing_utils import require_soundfile, require_torch, slow, torch_device\n-from transformers.utils import is_torch_fx_available\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n@@ -48,8 +47,7 @@\n     )\n     from transformers.models.hubert.modeling_hubert import _compute_mask_indices\n \n-if is_torch_fx_available():\n-    from transformers.utils.fx import symbolic_trace\n+from transformers.utils.fx import symbolic_trace\n \n \n class HubertModelTester:\n@@ -438,8 +436,8 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n         # TODO: fix it\n         self.skipTest(reason=\"torch 2.1 breaks torch fx tests for wav2vec2/hubert.\")\n \n-        if not is_torch_fx_available() or not self.fx_compatible:\n-            self.skipTest(reason=\"torch fx is not available or not compatible with this model\")\n+        if not self.fx_compatible:\n+            self.skipTest(reason=\"torch fx is not compatible with this model\")\n \n         configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n         configs_no_init.return_dict = False"
        },
        {
            "sha": "7da319e1963cdc419caa04b56df7ce2c566da7f5",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -27,17 +27,14 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_torch_fx_available\n+from transformers.utils.fx import symbolic_trace\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n-if is_torch_fx_available():\n-    from transformers.utils.fx import symbolic_trace\n-\n if is_torch_available():\n     import torch\n     import torch.nn.functional as F\n@@ -598,8 +595,8 @@ def is_pipeline_test_to_skip(\n         return False\n \n     def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        if not is_torch_fx_available() or not self.fx_compatible:\n-            self.skipTest(reason=\"torch.fx is not available or not compatible with this model\")\n+        if not self.fx_compatible:\n+            self.skipTest(reason=\"torch.fx is not compatible with this model\")\n \n         configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n         configs_no_init.return_dict = False"
        },
        {
            "sha": "e7804c59799bf45e14078b02505dd66fe4f1e962",
            "filename": "tests/models/rembert/test_modeling_rembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -416,7 +416,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "4f4d93b07f4dad1c03e896cd12952ee5c4c6863f",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -417,7 +417,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "7bb0de874d9e2fae3c60c38c0a2db2776ebdd830",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -421,7 +421,6 @@ def test_model_as_decoder(self):\n \n     # Copied from tests.models.roberta.test_modeling_roberta.RobertaModelTest.test_model_as_decoder_with_default_input_mask\n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "55babab54b51582650aaa6774d1d288b0cc3a089",
            "filename": "tests/models/roc_bert/test_modeling_roc_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -664,7 +664,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "3b94cac79eab8a19f93661c67ff2fc1d8d86dcae",
            "filename": "tests/models/roformer/test_modeling_roformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -433,7 +433,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "c608a2dbdd52384d31a4303a9fc7fdd5b199d3ae",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -32,18 +32,15 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import cached_property, is_torch_fx_available\n+from transformers.utils import cached_property\n+from transformers.utils.fx import symbolic_trace\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n-if is_torch_fx_available():\n-    from transformers.utils.fx import symbolic_trace\n-\n-\n if is_torch_available():\n     import torch\n     import torch.nn.functional as F\n@@ -603,8 +600,8 @@ def is_pipeline_test_to_skip(\n         return False\n \n     def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        if not is_torch_fx_available() or not self.fx_compatible:\n-            self.skipTest(reason=\"torch.fx is not available or not compatible with this model\")\n+        if not self.fx_compatible:\n+            self.skipTest(reason=\"torch.fx is not compatible with this model\")\n \n         configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n         configs_no_init.return_dict = False"
        },
        {
            "sha": "46a263cf799c6708cd7f8e95db577d02f278c6dd",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -27,18 +27,14 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_torch_fx_available\n+from transformers.utils.fx import symbolic_trace\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n-if is_torch_fx_available():\n-    from transformers.utils.fx import symbolic_trace\n-\n-\n if is_torch_available():\n     import torch\n     import torch.nn.functional as F\n@@ -300,8 +296,8 @@ def is_pipeline_test_to_skip(\n         return False\n \n     def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        if not is_torch_fx_available() or not self.fx_compatible:\n-            self.skipTest(reason=\"torch fx is not available or not compatible with this model\")\n+        if not self.fx_compatible:\n+            self.skipTest(reason=\"torch fx is not compatible with this model\")\n \n         configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n         configs_no_init.return_dict = False"
        },
        {
            "sha": "c47d4855ccd9e1279fd76b81c2ee3e038527c7e7",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -42,7 +42,6 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_torch_fx_available\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n@@ -90,8 +89,7 @@\n     from transformers.models.wav2vec2_with_lm import processing_wav2vec2_with_lm\n \n \n-if is_torch_fx_available():\n-    from transformers.utils.fx import symbolic_trace\n+from transformers.utils.fx import symbolic_trace\n \n \n def _test_wav2vec2_with_lm_invalid_pool(in_queue, out_queue, timeout):\n@@ -716,8 +714,8 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n         # TODO: fix it\n         self.skipTest(reason=\"torch 2.1 breaks torch fx tests for wav2vec2/hubert.\")\n \n-        if not is_torch_fx_available() or not self.fx_compatible:\n-            self.skipTest(reason=\"torch fx not available or not compatible with this model\")\n+        if not self.fx_compatible:\n+            self.skipTest(reason=\"torch fx is not compatible with this model\")\n \n         configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n         configs_no_init.return_dict = False"
        },
        {
            "sha": "543f4ef841de6d454b1de8327adecba8e29b8939",
            "filename": "tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -425,7 +425,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "8a0c90cd1fce80a715ef5c4ee2b1f1046c1fbfe7",
            "filename": "tests/models/xmod/test_modeling_xmod.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -420,7 +420,6 @@ def test_model_as_decoder(self):\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n     def test_model_as_decoder_with_default_input_mask(self):\n-        # This regression test was failing with PyTorch < 1.3\n         (\n             config,\n             input_ids,"
        },
        {
            "sha": "9bea39cecd5e98adb2690ba6d8acde051157a0f1",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -101,7 +101,6 @@\n     is_accelerate_available,\n     is_torch_bf16_available_on_device,\n     is_torch_fp16_available_on_device,\n-    is_torch_fx_available,\n     is_torch_sdpa_available,\n )\n from transformers.utils.generic import ContextManagers\n@@ -125,8 +124,8 @@\n     from transformers.modeling_utils import load_state_dict, no_init_weights\n     from transformers.pytorch_utils import id_tensor_storage\n \n-if is_torch_fx_available():\n-    from transformers.utils.fx import _FX_SUPPORTED_MODELS_WITH_KV_CACHE, symbolic_trace\n+from transformers.utils.fx import _FX_SUPPORTED_MODELS_WITH_KV_CACHE, symbolic_trace\n+\n \n if is_deepspeed_available():\n     import deepspeed\n@@ -1190,10 +1189,8 @@ def test_torch_fx_output_loss(self):\n         self._create_and_check_torch_fx_tracing(config, inputs_dict, output_loss=True)\n \n     def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=False):\n-        if not is_torch_fx_available() or not self.fx_compatible:\n-            self.skipTest(\n-                f\"Either torch.fx is not available, or the model type {config.model_type} is not compatible with torch.fx\"\n-            )\n+        if not self.fx_compatible:\n+            self.skipTest(f\"The model type {config.model_type} is not compatible with torch.fx\")\n \n         configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n         configs_no_init.return_dict = False"
        },
        {
            "sha": "e9f11bf72945d973bd9a2c870fbb47a9e6a51f75",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 10,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -99,7 +99,6 @@\n     require_torch_tensorrt_fx,\n     require_torch_tf32,\n     require_torch_up_to_2_accelerators,\n-    require_torchdynamo,\n     require_vision,\n     require_wandb,\n     run_first,\n@@ -3994,10 +3993,9 @@ def test_fp16_full_eval(self):\n \n     @require_non_xpu\n     @require_torch_non_multi_gpu\n-    @require_torchdynamo\n     @require_torch_tensorrt_fx\n     def test_torchdynamo_full_eval(self):\n-        import torchdynamo\n+        from torch import _dynamo as torchdynamo\n \n         # torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu\n         n_gpus = get_gpu_count()\n@@ -4017,30 +4015,35 @@ def test_torchdynamo_full_eval(self):\n             del trainer\n \n             # 2. TorchDynamo eager\n-            trainer = get_regression_trainer(a=a, b=b, eval_len=eval_len, torchdynamo=\"eager\", output_dir=tmp_dir)\n+            trainer = get_regression_trainer(\n+                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"eager\", output_dir=tmp_dir\n+            )\n             metrics = trainer.evaluate()\n             self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n             del trainer\n             torchdynamo.reset()\n \n             # 3. TorchDynamo nvfuser\n-            trainer = get_regression_trainer(a=a, b=b, eval_len=eval_len, torchdynamo=\"nvfuser\", output_dir=tmp_dir)\n+            trainer = get_regression_trainer(\n+                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"nvfuser\", output_dir=tmp_dir\n+            )\n             metrics = trainer.evaluate()\n             self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n             torchdynamo.reset()\n \n             # 4. TorchDynamo fx2trt\n-            trainer = get_regression_trainer(a=a, b=b, eval_len=eval_len, torchdynamo=\"fx2trt\", output_dir=tmp_dir)\n+            trainer = get_regression_trainer(\n+                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"fx2trt\", output_dir=tmp_dir\n+            )\n             metrics = trainer.evaluate()\n             self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n             torchdynamo.reset()\n \n-    @unittest.skip(reason=\"torch 2.0.0 gives `ModuleNotFoundError: No module named 'torchdynamo'`.\")\n     @require_torch_non_multi_gpu\n-    @require_torchdynamo\n+    @require_torch_gpu\n     def test_torchdynamo_memory(self):\n         # torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu\n-        import torchdynamo\n+        from torch import _dynamo as torchdynamo\n \n         class CustomTrainer(Trainer):\n             def compute_loss(self, model, inputs, return_outputs=False):\n@@ -4085,7 +4088,7 @@ def forward(self, x):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             a = torch.ones(1024, 1024, device=\"cuda\", requires_grad=True)\n             a.grad = None\n-            args = TrainingArguments(output_dir=tmp_dir, torchdynamo=\"nvfuser\")\n+            args = TrainingArguments(output_dir=tmp_dir, torch_compile_backend=\"nvfuser\")\n             trainer = CustomTrainer(model=mod, args=args)\n             # warmup\n             for _ in range(10):"
        },
        {
            "sha": "690ebd9d80df5d758efbf178c977df9e1a3a2743",
            "filename": "tests/trainer/test_trainer_fsdp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_fsdp.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -21,7 +21,6 @@\n     get_torch_dist_unique_port,\n     require_accelerate,\n     require_fp8,\n-    require_fsdp,\n     require_torch_multi_accelerator,\n     run_first,\n     torch_device,\n@@ -68,7 +67,6 @@ def __getitem__(self, i: int) -> str:\n class TestFSDPTrainer(TestCasePlus):\n     @require_torch_multi_accelerator\n     @require_accelerate\n-    @require_fsdp\n     @run_first\n     def test_trainer(self):\n         output_dir = self.get_auto_remove_tmp_dir()\n@@ -95,7 +93,6 @@ def test_trainer(self):\n class TestFSDPTrainerFP8(TestCasePlus):\n     @require_torch_multi_accelerator\n     @require_accelerate\n-    @require_fsdp\n     @require_fp8\n     @run_first\n     def test_trainer(self):\n@@ -125,7 +122,6 @@ def test_trainer(self):\n class TestFSDPTrainerWrap(TestCasePlus):\n     @require_torch_multi_accelerator\n     @require_accelerate\n-    @require_fsdp\n     @run_first\n     def test_trainer(self):\n         output_dir = self.get_auto_remove_tmp_dir()"
        },
        {
            "sha": "b96678f114d26921e7b901b75410471215f5ac0a",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -81,7 +81,6 @@\n     is_tf_available,\n     is_torch_npu_available,\n     is_torch_sdpa_available,\n-    is_torchdynamo_available,\n )\n \n \n@@ -1483,8 +1482,6 @@ def test_warn_if_padding_and_no_attention_mask(self):\n                     model.warn_if_padding_and_no_attention_mask(input_ids, attention_mask=None)\n             self.assertIn(\"You may ignore this warning if your `pad_token_id`\", cl.out)\n \n-        if not is_torchdynamo_available():\n-            self.skipTest(reason=\"torchdynamo is not available\")\n         with self.subTest(\"Ensure that the warning code is skipped when compiling with torchdynamo.\"):\n             logger.warning_once.cache_clear()\n             from torch._dynamo import config, testing"
        },
        {
            "sha": "c485c4e84bc7557175da86f14a10da2940dc693d",
            "filename": "tests/utils/test_versions_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Futils%2Ftest_versions_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c44d0efb2ac552bbfed9a65baea4d4d83747c/tests%2Futils%2Ftest_versions_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_versions_utils.py?ref=371c44d0efb2ac552bbfed9a65baea4d4d83747c",
            "patch": "@@ -86,7 +86,7 @@ def test_core(self):\n \n     def test_python(self):\n         # matching requirement\n-        require_version(\"python>=3.6.0\")\n+        require_version(\"python>=3.9.0\")\n \n         # not matching requirements\n         for req in [\"python>9.9.9\", \"python<3.0.0\"]:"
        }
    ],
    "stats": {
        "total": 231,
        "additions": 53,
        "deletions": 178
    }
}