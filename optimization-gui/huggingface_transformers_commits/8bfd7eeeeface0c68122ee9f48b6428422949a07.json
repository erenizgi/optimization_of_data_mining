{
    "author": "alexrs-cohere",
    "message": "Add Cohere2 docs details (#35294)\n\n* Add Cohere2 docs details\n\n* Update docs/source/en/model_doc/cohere2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "8bfd7eeeeface0c68122ee9f48b6428422949a07",
    "files": [
        {
            "sha": "33e67d48fb0e8b6c18da3a32ffcbd37680d6fd02",
            "filename": "docs/source/en/model_doc/cohere2.md",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bfd7eeeeface0c68122ee9f48b6428422949a07/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bfd7eeeeface0c68122ee9f48b6428422949a07/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md?ref=8bfd7eeeeface0c68122ee9f48b6428422949a07",
            "patch": "@@ -1,5 +1,12 @@\n # Cohere\n \n+## Overview\n+[C4AI Command R7B](https://cohere.com/blog/command-r7b) is an open weights research release of a 7B billion parameter model developed by Cohere and Cohere For AI. It has advanced capabilities optimized for various use cases, including reasoning, summarization, question answering, and code. The model is trained to perform sophisticated tasks including Retrieval Augmented Generation (RAG) and tool use. The model also has powerful agentic capabilities that can use and combine multiple tools over multiple steps to accomplish more difficult tasks. It obtains top performance on enterprise-relevant code use cases. C4AI Command R7B is a multilingual model trained on 23 languages.\n+\n+The model features three layers with sliding window attention (window size 4096) and ROPE for efficient local context modeling and relative positional encoding. A fourth layer uses global attention without positional embeddings, enabling unrestricted token interactions across the entire sequence.\n+\n+The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\n+\n ## Usage tips\n The model and tokenizer can be loaded via:\n \n@@ -20,7 +27,7 @@ gen_tokens = model.generate(\n     max_new_tokens=100,\n     do_sample=True,\n     temperature=0.3,\n-    )\n+)\n \n gen_text = tokenizer.decode(gen_tokens[0])\n print(gen_text)"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 8,
        "deletions": 1
    }
}