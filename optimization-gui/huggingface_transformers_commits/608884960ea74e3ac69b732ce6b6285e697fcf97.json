{
    "author": "Cyrilvallez",
    "message": "add default mapping to peft integration",
    "sha": "608884960ea74e3ac69b732ce6b6285e697fcf97",
    "files": [
        {
            "sha": "7c9e37c2786ead603e1ea6d6dc00b3024b51571c",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/608884960ea74e3ac69b732ce6b6285e697fcf97/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/608884960ea74e3ac69b732ce6b6285e697fcf97/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=608884960ea74e3ac69b732ce6b6285e697fcf97",
            "patch": "@@ -28,6 +28,7 @@\n     is_torch_available,\n     logging,\n )\n+from ..modeling_utils import VLMS\n \n \n if is_torch_available():\n@@ -151,6 +152,8 @@ def load_adapter(\n         # peft only supports low_cpu_mem_usage starting from v0.13.0\n         peft_load_kwargs = {}\n         key_mapping = adapter_kwargs.pop(\"key_mapping\", None) if adapter_kwargs is not None else None\n+        if key_mapping is None and any(allowed_name in self.__class__.__name__.lower() for allowed_name in VLMS):\n+            key_mapping = self._checkpoint_conversion_mapping\n         if low_cpu_mem_usage:\n             min_version_lcmu = \"0.13.0\"\n             if version.parse(importlib.metadata.version(\"peft\")) >= version.parse(min_version_lcmu):"
        },
        {
            "sha": "a0f5180ac8b1bc54d2def7cc97978722133daeab",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/608884960ea74e3ac69b732ce6b6285e697fcf97/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/608884960ea74e3ac69b732ce6b6285e697fcf97/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=608884960ea74e3ac69b732ce6b6285e697fcf97",
            "patch": "@@ -4251,11 +4251,10 @@ def from_pretrained(\n         device_mesh = kwargs.pop(\"device_mesh\", None)\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n \n-        # Load models with hardcoded key mapping on class for VLMs only,  to keep BC and standardize model\n-        if any(allowed_name in cls.__name__.lower() for allowed_name in VLMS):\n-            key_mapping = kwargs.pop(\"key_mapping\", cls._checkpoint_conversion_mapping)\n-        else:\n-            key_mapping = kwargs.pop(\"key_mapping\", None)\n+        key_mapping = kwargs.pop(\"key_mapping\", None)\n+        # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model\n+        if key_mapping is None and any(allowed_name in cls.__name__.lower() for allowed_name in VLMS):\n+            key_mapping = cls._checkpoint_conversion_mapping\n \n         # Not used anymore -- remove them from the kwargs\n         _ = kwargs.pop(\"resume_download\", None)"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 7,
        "deletions": 5
    }
}