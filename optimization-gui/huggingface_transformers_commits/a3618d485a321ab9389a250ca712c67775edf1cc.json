{
    "author": "qubvel",
    "message": "RotaryEmbeddings change `is not None` -> `isinstance(..., dict)` (#39145)\n\nis None -> isinstance dict",
    "sha": "a3618d485a321ab9389a250ca712c67775edf1cc",
    "files": [
        {
            "sha": "eb952faf11b4fff3965ce3b7d83bad0591e8435b",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -89,7 +89,7 @@ class ArceeRotaryEmbedding(nn.Module):\n     def __init__(self, config: ArceeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "1ed6af9b45f9f7aec6d2205bea6ee1805438f507",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -690,7 +690,7 @@ class AriaTextRotaryEmbedding(nn.Module):\n     def __init__(self, config: AriaTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "73d64bcdf3c0fa690141dab90ee1da8c6f5d5c8b",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -145,7 +145,7 @@ class BambaRotaryEmbedding(nn.Module):\n     def __init__(self, config: BambaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "8fe4dfe86f07f075405be1f1a3a6d29cd9ba1539",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -271,7 +271,7 @@ class BitNetRotaryEmbedding(nn.Module):\n     def __init__(self, config: BitNetConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "749070a26defa1a79128b13760b85133f97822d5",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -68,7 +68,7 @@ class CohereRotaryEmbedding(nn.Module):\n     def __init__(self, config: CohereConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "bfd20cd032f84989664a71096bcd6ed1a4d7b5e8",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -44,7 +44,7 @@ class Cohere2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Cohere2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "4efd1970e7d9487578f502c90a49b538613cc86b",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -121,7 +121,7 @@ class CsmRotaryEmbedding(nn.Module):\n     def __init__(self, config: CsmConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "8a0496a583b706d93482b16febe2ba22326d996a",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -52,7 +52,7 @@ class DeepseekV3RotaryEmbedding(nn.Module):\n     def __init__(self, config: DeepseekV3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "6c5b52655d8fbe55f358b47923b7bb97e25e3a74",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -157,7 +157,7 @@ class DiaRotaryEmbedding(nn.Module):\n     def __init__(self, config: DiaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "bbdfa730826648bcf226f339a32f8d12bee0cd3c",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -565,7 +565,7 @@ class DiffLlamaRotaryEmbedding(nn.Module):\n     def __init__(self, config: DiffLlamaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "2fe5d47f0c26d2cd3934ddbc50dd0022413ffa1e",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -65,7 +65,7 @@ class Dots1RotaryEmbedding(nn.Module):\n     def __init__(self, config: Dots1Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "efc3e4c7ce54daa842d8fced5020327993dca516",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -1122,7 +1122,7 @@ class Emu3RotaryEmbedding(nn.Module):\n     def __init__(self, config: Emu3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "593515ed23b2f1c2ebc8de17986c19fb3bcf8ae2",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -104,7 +104,7 @@ class FalconRotaryEmbedding(nn.Module):\n     def __init__(self, config: FalconConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "5a88344739bf840777ec9154a3ba04dc8beb41ec",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -225,7 +225,7 @@ class FalconH1RotaryEmbedding(nn.Module):\n     def __init__(self, config: FalconH1Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "b65022b5dbef7d1b1d50982bb075b21143b04af0",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -86,7 +86,7 @@ class GemmaRotaryEmbedding(nn.Module):\n     def __init__(self, config: GemmaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "4af5ad0be9bae458bd6c7941d38c92f392e3ccb7",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -303,7 +303,7 @@ class Gemma2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Gemma2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "3ff15f3d71dcaaef03c8c902702bdf35bb6f2ada",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -161,7 +161,7 @@ class Gemma3RotaryEmbedding(nn.Module):\n     def __init__(self, config: Gemma3TextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "b85471ba8d347c7bf27737ff5c59599128db6120",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -1153,7 +1153,7 @@ class Gemma3nTextRotaryEmbedding(nn.Module):\n     def __init__(self, config: Gemma3nTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "f96e91221a5f6b15d62dc36680ee3b43dbe23e10",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -245,7 +245,7 @@ class GlmRotaryEmbedding(nn.Module):\n     def __init__(self, config: GlmConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "64203fba5f03ca8e5d1d6c4fbbcc68c69847a888",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -294,7 +294,7 @@ class Glm4RotaryEmbedding(nn.Module):\n     def __init__(self, config: Glm4Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "49f6dfb45e24f159eba1fbf81675a370cdf3271b",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -257,7 +257,7 @@ class GPTNeoXRotaryEmbedding(nn.Module):\n     def __init__(self, config: GPTNeoXConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "f1866b4977788481d566560e4247449bfa3520ce",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -228,7 +228,7 @@ class GPTNeoXJapaneseRotaryEmbedding(nn.Module):\n     def __init__(self, config: GPTNeoXJapaneseConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "9fbdc216806fd0577bab694c78b4394e0f092215",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -335,7 +335,7 @@ class GraniteRotaryEmbedding(nn.Module):\n     def __init__(self, config: GraniteConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "516200115ef52ef202659f16f8186ac0a4a182b5",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -149,7 +149,7 @@ class GraniteMoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: GraniteMoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "1cb2e357a0832a85875ac5c03046b2a3d169c0e3",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -1202,7 +1202,7 @@ class GraniteMoeHybridRotaryEmbedding(nn.Module):\n     def __init__(self, config: GraniteMoeHybridConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "5996368649b5cade4ab535282b23c9db5a98930f",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -535,7 +535,7 @@ class GraniteMoeSharedRotaryEmbedding(nn.Module):\n     def __init__(self, config: GraniteMoeSharedConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "9849ef8b85aa95fd6a710097f2e26ee0531092b6",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -68,7 +68,7 @@ class HeliumRotaryEmbedding(nn.Module):\n     def __init__(self, config: HeliumConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "5112066f8eaa067a9a804d190118f5d8c4e52aae",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -386,7 +386,7 @@ class JetMoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: JetMoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "cfdf75b5e5e9ff9eca3da52284e4fc123aaa2c45",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -270,7 +270,7 @@ class KyutaiSpeechToTextRotaryEmbedding(nn.Module):\n     def __init__(self, config: KyutaiSpeechToTextConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "7ec9b5644ac2a8844d2ec41d0f984f9feff11896",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -71,7 +71,7 @@ class LlamaRotaryEmbedding(nn.Module):\n     def __init__(self, config: LlamaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "1109c75e5a038223c47125c4da7f0c9eabac8eb9",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -506,7 +506,7 @@ class MimiRotaryEmbedding(nn.Module):\n     def __init__(self, config: MimiConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "8d17bbac83a8505c424b6e315b8c26ecf9427962",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -622,7 +622,7 @@ class MiniMaxRotaryEmbedding(nn.Module):\n     def __init__(self, config: MiniMaxConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "9a1e6b356fe2fef6a61af8e123eefe1d45edd44f",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -286,7 +286,7 @@ class MistralRotaryEmbedding(nn.Module):\n     def __init__(self, config: MistralConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "589417cc3d8a043a0799039376205f3f35a388ab",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -382,7 +382,7 @@ class MixtralRotaryEmbedding(nn.Module):\n     def __init__(self, config: MixtralConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "7e14914ab33de0fafac5c533c4dcc9ed628e6044",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -244,7 +244,7 @@ class ModernBertRotaryEmbedding(nn.Module):\n     def __init__(self, config: ModernBertConfig, dim: int, base: float, device: Optional[torch.device] = None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "c8cfde30b4b3064c6237865ba6ac17bf7458241d",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -297,7 +297,7 @@ class MoonshineRotaryEmbedding(nn.Module):\n     def __init__(self, config: MoonshineConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "b6a71bb6fdf16d13d7761ac741afa6f7f2913d88",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -276,7 +276,7 @@ class MoshiRotaryEmbedding(nn.Module):\n     def __init__(self, config: MoshiConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "845ea93fb7e8d3c54ae1faaa18b29cb68a7580a4",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -254,7 +254,7 @@ class OlmoRotaryEmbedding(nn.Module):\n     def __init__(self, config: OlmoConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "82fb9618b9c21c6586c5776c58fe9b73ad944034",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -259,7 +259,7 @@ class Olmo2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Olmo2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "d3fccd6206026385a59010bbcb9ab3c48ff96b6c",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -147,7 +147,7 @@ class OlmoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: OlmoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "d04b04a51908fc5f148d3bcaa485839cd4889974",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -59,7 +59,7 @@ class PersimmonRotaryEmbedding(nn.Module):\n     def __init__(self, config: PersimmonConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "1f925f0262cc9aa0e8f7a19f6de7dc9ed255cebe",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -258,7 +258,7 @@ class PhiRotaryEmbedding(nn.Module):\n     def __init__(self, config: PhiConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "1c20b9a5ba0451b63459249e02a68cd367d616c1",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -318,7 +318,7 @@ class Phi3RotaryEmbedding(nn.Module):\n     def __init__(self, config: Phi3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "3aa8669d33aeb3d98c7f8e84cd10f35465bed2fc",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -1552,7 +1552,7 @@ class Phi4MultimodalRotaryEmbedding(nn.Module):\n     def __init__(self, config: Phi4MultimodalConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "7f2879cbfaa5a06fdfdb3963b1e1abf1492bbef9",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -289,7 +289,7 @@ class Qwen2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Qwen2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "c5f01da0f530c4c9f188f7716f30d22b3cbd609b",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -168,7 +168,7 @@ class Qwen2MoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: Qwen2MoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "580cbf7ae2e32ffd3dda8d698d18c4338fd066a0",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -315,7 +315,7 @@ class Qwen3RotaryEmbedding(nn.Module):\n     def __init__(self, config: Qwen3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "b6734d029cbc8a349dd67c02b7466be42042452b",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -388,7 +388,7 @@ class Qwen3MoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: Qwen3MoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "2a1d40cf9b12883034b8174f5c9fb002c3841e2e",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -319,7 +319,7 @@ class SmolLM3RotaryEmbedding(nn.Module):\n     def __init__(self, config: SmolLM3Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "b88000499cb73bdf618cd5e4664470da0a82f65f",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -64,7 +64,7 @@ class StableLmRotaryEmbedding(nn.Module):\n     def __init__(self, config: StableLmConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "34528e2118ef5be6ad7fc90f20f8ac9d5acfa11b",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -256,7 +256,7 @@ class Starcoder2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Starcoder2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "bdfceff99559bf6018c3e585c86ef00cf20bc008",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -93,7 +93,7 @@ class T5GemmaRotaryEmbedding(nn.Module):\n     def __init__(self, config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        },
        {
            "sha": "6e0c1d8db023b7df8bd912615b4c33cabfb036b4",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3618d485a321ab9389a250ca712c67775edf1cc/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=a3618d485a321ab9389a250ca712c67775edf1cc",
            "patch": "@@ -213,7 +213,7 @@ def __init__(\n     ):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\""
        }
    ],
    "stats": {
        "total": 106,
        "additions": 53,
        "deletions": 53
    }
}