{
    "author": "vasqu",
    "message": "[`Ernie 4.5 Moe`] Fix Moe and offloading (#41385)\n\nfix",
    "sha": "baecdb8a97a456ca475c832a11dac1b05ef72e1f",
    "files": [
        {
            "sha": "6657ad1edd08cf67b6a73f19b87f5626dbc50780",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 39,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/baecdb8a97a456ca475c832a11dac1b05ef72e1f/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/baecdb8a97a456ca475c832a11dac1b05ef72e1f/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=baecdb8a97a456ca475c832a11dac1b05ef72e1f",
            "patch": "@@ -286,37 +286,12 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n-class Ernie4_5_MoeRouter(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.top_k = config.moe_k\n-        self.num_experts = config.moe_num_experts\n-        self.norm_min = config.moe_norm_min\n-        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n-        self.moe_statics = Ernie4_5_MoeStatics(config)\n-\n-    def forward(\n-        self, hidden_states: torch.Tensor, device_type: str\n-    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            router_logits = self.gate(hidden_states.float())\n-            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            routing_bias = self.moe_statics.e_score_correction_bias.squeeze()\n-            _, selected_experts = torch.topk(routing_weights + routing_bias, self.top_k, dim=-1)\n-            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n-            routing_weights = routing_weights / torch.clamp(\n-                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n-            )\n-        routing_weights = routing_weights.to(router_logits.dtype)\n-        return router_logits, selected_experts, routing_weights\n-\n-\n class Ernie4_5_MoeExperts(nn.ModuleList):\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.moe_num_experts\n         for _ in range(self.num_experts):\n-            self.append(Ernie4_5_MoeMLP(config))\n+            self.append(Ernie4_5_MoeMLP(config, config.moe_intermediate_size))\n \n     def forward(\n         self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n@@ -349,38 +324,33 @@ def __init__(self, config):\n         if config.moe_num_shared_experts > 0:\n             self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n \n-    def route_tokens_to_experts(self, hidden_states, router_logits):\n+    def route_tokens_to_experts(self, hidden_states):\n         device_type = (\n             hidden_states.device.type\n             if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n             else \"cpu\"\n         )\n \n         with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            router_logits = self.gate(hidden_states.float())\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            routing_bias = self.moe_statics.e_score_correction_bias.squeeze()\n-            _, selected_experts = torch.topk(routing_weights + routing_bias, self.top_k, dim=-1)\n+            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n             routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n         routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, _ = hidden_states.shape\n-        hidden_states_reshaped = hidden_states.view(-1, self.hidden_dim)\n+        hidden_states = hidden_states.view(-1, self.hidden_dim)\n \n         if self.shared_experts is not None:\n-            shared_output = self.shared_experts(hidden_states_reshaped)\n-\n-        router_logits = self.gate(hidden_states_reshaped.float())\n-        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states_reshaped, router_logits)\n+            shared_output = self.shared_experts(hidden_states)\n \n-        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states)\n+        final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n \n         if self.shared_experts is not None:\n             final_hidden_states = final_hidden_states + shared_output"
        },
        {
            "sha": "b12958b785b77ef49bf08f867a5e5ea15644b03e",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 39,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/baecdb8a97a456ca475c832a11dac1b05ef72e1f/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/baecdb8a97a456ca475c832a11dac1b05ef72e1f/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=baecdb8a97a456ca475c832a11dac1b05ef72e1f",
            "patch": "@@ -96,37 +96,12 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n-class Ernie4_5_MoeRouter(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.top_k = config.moe_k\n-        self.num_experts = config.moe_num_experts\n-        self.norm_min = config.moe_norm_min\n-        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n-        self.moe_statics = Ernie4_5_MoeStatics(config)\n-\n-    def forward(\n-        self, hidden_states: torch.Tensor, device_type: str\n-    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            router_logits = self.gate(hidden_states.float())\n-            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            routing_bias = self.moe_statics.e_score_correction_bias.squeeze()\n-            _, selected_experts = torch.topk(routing_weights + routing_bias, self.top_k, dim=-1)\n-            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n-            routing_weights = routing_weights / torch.clamp(\n-                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n-            )\n-        routing_weights = routing_weights.to(router_logits.dtype)\n-        return router_logits, selected_experts, routing_weights\n-\n-\n class Ernie4_5_MoeExperts(nn.ModuleList):\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.moe_num_experts\n         for _ in range(self.num_experts):\n-            self.append(Ernie4_5_MoeMLP(config))\n+            self.append(Ernie4_5_MoeMLP(config, config.moe_intermediate_size))\n \n     def forward(\n         self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n@@ -159,38 +134,33 @@ def __init__(self, config):\n         if config.moe_num_shared_experts > 0:\n             self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n \n-    def route_tokens_to_experts(self, hidden_states, router_logits):\n+    def route_tokens_to_experts(self, hidden_states):\n         device_type = (\n             hidden_states.device.type\n             if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n             else \"cpu\"\n         )\n \n         with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            router_logits = self.gate(hidden_states.float())\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            routing_bias = self.moe_statics.e_score_correction_bias.squeeze()\n-            _, selected_experts = torch.topk(routing_weights + routing_bias, self.top_k, dim=-1)\n+            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n             routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n         routing_weights = routing_weights.to(router_logits.dtype)\n         return selected_experts, routing_weights\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, _ = hidden_states.shape\n-        hidden_states_reshaped = hidden_states.view(-1, self.hidden_dim)\n+        hidden_states = hidden_states.view(-1, self.hidden_dim)\n \n         if self.shared_experts is not None:\n-            shared_output = self.shared_experts(hidden_states_reshaped)\n-\n-        router_logits = self.gate(hidden_states_reshaped.float())\n-        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states_reshaped, router_logits)\n+            shared_output = self.shared_experts(hidden_states)\n \n-        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states)\n+        final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n \n         if self.shared_experts is not None:\n             final_hidden_states = final_hidden_states + shared_output"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 18,
        "deletions": 78
    }
}