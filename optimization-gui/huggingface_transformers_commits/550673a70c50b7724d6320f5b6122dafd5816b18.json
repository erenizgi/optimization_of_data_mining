{
    "author": "unknown",
    "message": "Remove `logits.float()` (#33902)\n\n* Remove logits.float() if not computing loss\r\n\r\n* Remove warning about 4.46 logits dtype change if not computing loss",
    "sha": "550673a70c50b7724d6320f5b6122dafd5816b18",
    "files": [
        {
            "sha": "9f9dcd5e4cd99a77d374cfe6e9a77952169b89dc",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -46,7 +46,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1169,13 +1168,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n         logits = logits * self.logit_scale\n \n         loss = None"
        },
        {
            "sha": "0d491b76cae747918b91c74f6dcd52ea4f0960e6",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -43,7 +43,6 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1081,13 +1080,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "f3ac0854313d1bcaced02447927a587e1b7a7b75",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -27,7 +27,7 @@\n from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n-from ...utils import is_torchdynamo_compiling, logging\n+from ...utils import logging\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n     LlamaFlashAttention2,\n@@ -833,13 +833,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "6080e541763c80e8eef43421a4c2dcf3073ddd46",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -42,7 +42,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1058,19 +1057,13 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n         if self.config.final_logit_softcapping is not None:\n             logits = logits / self.config.final_logit_softcapping\n             logits = torch.tanh(logits)\n             logits = logits * self.config.final_logit_softcapping\n \n-        # TODO: remove the float() operation in v4.46\n-        logits = logits.float()\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues"
        },
        {
            "sha": "f919731fc1476257dd63a6cbe18f36add2bd0530",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -31,7 +31,6 @@\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from ..gemma.modeling_gemma import (\n@@ -800,19 +799,13 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n         if self.config.final_logit_softcapping is not None:\n             logits = logits / self.config.final_logit_softcapping\n             logits = torch.tanh(logits)\n             logits = logits * self.config.final_logit_softcapping\n \n-        # TODO: remove the float() operation in v4.46\n-        logits = logits.float()\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues"
        },
        {
            "sha": "854a0b62f8210a23bd9040c363b4bf2581a2b955",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -34,7 +34,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1617,13 +1616,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "dd79043141b8dc4314624d6d0e25c0abfa2a0714",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -51,7 +51,6 @@\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n     is_mamba_ssm_available,\n-    is_torchdynamo_compiling,\n )\n from .configuration_jamba import JambaConfig\n \n@@ -1543,12 +1542,6 @@ def forward(\n             logits = self.lm_head(hidden_states)\n         else:\n             logits = self.lm_head(hidden_states[..., -num_logits_to_keep:, :])\n-        if labels is None and not is_torchdynamo_compiling:\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n-        # TODO: remove the float() operations in v4.46\n-        logits = logits.float()\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "e3899366886260db47a7ea4dea2bc3669311d451",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -38,7 +38,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1302,13 +1301,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "7a8f2c82c6467447b72ca354bfa6898761679544",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -45,7 +45,6 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1206,13 +1205,8 @@ def forward(\n             logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n             logits = torch.cat(logits, dim=-1)\n         else:\n-            if labels is None and not is_torchdynamo_compiling():\n-                logger.warning_once(\n-                    \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-                )\n             # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-            # TODO: remove the float() operation in v4.46\n-            logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+            logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "a099c60c5ba933aa07ef3b57e1dc12626d1b3863",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -43,7 +43,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1050,13 +1049,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "dd060a55f0fc86e0f8f5969fa04e9e1ff6d6bc20",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -44,7 +44,6 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1302,13 +1301,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "c3cccd56108ef2e124cf0a38c17c0a75bbf37b82",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -43,7 +43,6 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1081,14 +1080,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n-        # TODO: remove the float() operation in v4.46\n-        logits = logits.float()\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "894adeb44ef1715c09e69bbf63fe2b872c201367",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -43,7 +43,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1124,13 +1123,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "1ae4a5c5c1dbe69d1ea55041449e1d14355c2222",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -43,7 +43,6 @@\n     get_torch_version,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1247,13 +1246,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "20d023311548cc4230dab4c7767d18adb8614d59",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -41,7 +41,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1277,13 +1276,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "81a310bd26e74dc24e72c0e92732c78dec6a70cc",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -44,7 +44,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1180,13 +1179,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "7eba44c944409127c3b7ee853356b2f92e69b203",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -45,7 +45,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1368,13 +1367,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "0f196118de56837f8812a47bc3603f002bf4252c",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/550673a70c50b7724d6320f5b6122dafd5816b18/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=550673a70c50b7724d6320f5b6122dafd5816b18",
            "patch": "@@ -44,7 +44,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1155,13 +1154,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        }
    ],
    "stats": {
        "total": 141,
        "additions": 15,
        "deletions": 126
    }
}