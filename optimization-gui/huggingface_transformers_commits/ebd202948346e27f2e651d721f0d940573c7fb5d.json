{
    "author": "zhanluxianshen",
    "message": "Change GPUS to GPUs (#36945)\n\nSigned-off-by: zhanluxianshen <zhanluxianshen@163.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "ebd202948346e27f2e651d721f0d940573c7fb5d",
    "files": [
        {
            "sha": "3b4e587a6d12848851595521107021f4b5ec07e5",
            "filename": "ISSUES.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebd202948346e27f2e651d721f0d940573c7fb5d/ISSUES.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebd202948346e27f2e651d721f0d940573c7fb5d/ISSUES.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/ISSUES.md?ref=ebd202948346e27f2e651d721f0d940573c7fb5d",
            "patch": "@@ -263,9 +263,9 @@ You are not required to read the following guidelines before opening an issue. H\n     But if you're replying to a comment that happened some comments back it's always a good practice to quote just the relevant lines you're replying it. The `>` is used for quoting, or you can always use the menu to do so. For example your editor box will look like:\n \n     ```\n-    > How big is your gpu cluster?\n+    > How big is your GPU cluster?\n \n-    Our cluster is made of 256 gpus.\n+    Our cluster is made of 256 GPUs.\n     ```\n \n     If you are addressing multiple comments, quote the relevant parts of each before your answer. Some people use the same comment to do multiple replies, others separate them into separate comments. Either way works. The latter approach helps for linking to a specific comment."
        },
        {
            "sha": "741d8b5dd54b38c3ff0db0f51de5fa9249f5ec73",
            "filename": "examples/legacy/seq2seq/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebd202948346e27f2e651d721f0d940573c7fb5d/examples%2Flegacy%2Fseq2seq%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebd202948346e27f2e651d721f0d940573c7fb5d/examples%2Flegacy%2Fseq2seq%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2FREADME.md?ref=ebd202948346e27f2e651d721f0d940573c7fb5d",
            "patch": "@@ -209,7 +209,7 @@ th 56 \\\n ```\n \n ### Multi-GPU Evaluation\n-here is a command to run xsum evaluation on 8 GPUS. It is more than linearly faster than run_eval.py in some cases\n+here is a command to run xsum evaluation on 8 GPUs. It is more than linearly faster than run_eval.py in some cases\n because it uses SortishSampler to minimize padding. You can also use it on 1 GPU. `data_dir` must have\n `{type_path}.source` and `{type_path}.target`. Run `./run_distributed_eval.py --help` for all clargs.\n "
        },
        {
            "sha": "b1453f85eaea2f573676458c3e460f7c58bd57bb",
            "filename": "tests/quantization/eetq_integration/test_eetq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py?ref=ebd202948346e27f2e651d721f0d940573c7fb5d",
            "patch": "@@ -158,7 +158,7 @@ def test_save_pretrained(self):\n     def test_quantized_model_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUS\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n         quantization_config = EetqConfig()"
        },
        {
            "sha": "3efff115ba7ab6321a3bfc61cdb10c97ff8d5db3",
            "filename": "tests/quantization/fbgemm_fp8/test_fbgemm_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py?ref=ebd202948346e27f2e651d721f0d940573c7fb5d",
            "patch": "@@ -215,7 +215,7 @@ def test_change_loading_attributes(self):\n     def test_quantized_model_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUS\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n         quantization_config = FbgemmFp8Config()"
        },
        {
            "sha": "e59c2068cde7f09ea8e8a7db7478f9c1e7c614c2",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=ebd202948346e27f2e651d721f0d940573c7fb5d",
            "patch": "@@ -193,7 +193,7 @@ def test_block_size(self):\n     def test_quantized_model_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUS\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device_map)\n         quantization_config = FineGrainedFP8Config()"
        },
        {
            "sha": "687a4ab22f21d79593c237d6c06a2ba2651e9f4d",
            "filename": "tests/quantization/higgs/test_higgs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py?ref=ebd202948346e27f2e651d721f0d940573c7fb5d",
            "patch": "@@ -156,7 +156,7 @@ def test_save_pretrained(self):\n     def test_quantized_model_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUS\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n         quantization_config = HiggsConfig()"
        },
        {
            "sha": "55d57d986c233ad226554ea14422f85011419c9d",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=ebd202948346e27f2e651d721f0d940573c7fb5d",
            "patch": "@@ -255,7 +255,7 @@ def test_int4wo_offload(self):\n     def test_int4wo_quant_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model int4 weight only is working properly with multiple GPUs\n-        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUS\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n         \"\"\"\n \n         quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)"
        },
        {
            "sha": "70dc301f9c33b2c976af3b3835e7e27c10cb340e",
            "filename": "tests/sagemaker/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fsagemaker%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebd202948346e27f2e651d721f0d940573c7fb5d/tests%2Fsagemaker%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2FREADME.md?ref=ebd202948346e27f2e651d721f0d940573c7fb5d",
            "patch": "@@ -138,7 +138,7 @@ images:\n \n ## Current Tests\n \n-| ID                                  | Description                                                       | Platform                   | #GPUS | Collected & evaluated metrics            |\n+| ID                                  | Description                                                       | Platform                   | #GPUs | Collected & evaluated metrics            |\n |-------------------------------------|-------------------------------------------------------------------|-----------------------------|-------|------------------------------------------|\n | pytorch-transfromers-test-single    | test bert finetuning using BERT fromtransformerlib+PT             | SageMaker createTrainingJob | 1     | train_runtime, eval_accuracy & eval_loss |\n | pytorch-transfromers-test-2-ddp     | test bert finetuning using BERT from transformer lib+ PT DPP      | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 9,
        "deletions": 9
    }
}