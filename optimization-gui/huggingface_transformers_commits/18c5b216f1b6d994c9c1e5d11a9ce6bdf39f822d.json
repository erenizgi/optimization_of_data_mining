{
    "author": "xenova",
    "message": "Fix ViT-MAE decoder interpolate (#33330)\n\n* Fix ViT-MAE decoder interpolate\r\n\r\n* Add unit test for `interpolate_pos_encoding` w/ custom sizes\r\n\r\n* [run_slow] vit_mae",
    "sha": "18c5b216f1b6d994c9c1e5d11a9ce6bdf39f822d",
    "files": [
        {
            "sha": "0be169a51b276ddf8ba68e026c40abb635948b66",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/18c5b216f1b6d994c9c1e5d11a9ce6bdf39f822d/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18c5b216f1b6d994c9c1e5d11a9ce6bdf39f822d/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=18c5b216f1b6d994c9c1e5d11a9ce6bdf39f822d",
            "patch": "@@ -841,21 +841,20 @@ def __init__(self, config, num_patches):\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n-        This method is a modified version of the interpolation function for ViT-mae model at the deocder, that\n+        This method is a modified version of the interpolation function for ViT-mae model at the decoder, that\n         allows to interpolate the pre-trained decoder position encodings, to be able to use the model on higher\n         resolution images.\n \n-        Source:\n+        Adapted from:\n         https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n         \"\"\"\n \n         # -1 removes the class dimension since we later append it without interpolation\n         embeddings_positions = embeddings.shape[1] - 1\n-        num_positions = self.decoder_pos_embed.shape[1] - 1\n \n         # Separation of class token and patch tokens\n-        class_pos_embed = self.decoder_pos_embed[:, 0, :]\n-        patch_pos_embed = self.decoder_pos_embed[:, 1:, :]\n+        class_pos_embed = self.decoder_pos_embed[:, :1]\n+        patch_pos_embed = self.decoder_pos_embed[:, 1:]\n \n         # To retain the final 3d tensor with the required dimensions\n         dim = self.decoder_pos_embed.shape[-1]\n@@ -867,18 +866,18 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor) -> torch.Tensor:\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n \n         # Interpolating the decoder position embeddings shape wrt embeddings shape i.e (x).\n-        # 1 keeps the other dimension constant\n+        # we keep the second last dimension constant\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(1, embeddings_positions / num_positions),\n+            size=(patch_pos_embed.shape[-2], embeddings_positions),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n \n         # Converting back to the original shape\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n         # Adding the class token back\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def initialize_weights(self, num_patches):\n         # initialize (and freeze) position embeddings by sin-cos embedding"
        },
        {
            "sha": "5cff9616e004f56d1f1b591281e42b91cc808002",
            "filename": "tests/models/vit_mae/test_modeling_vit_mae.py",
            "status": "modified",
            "additions": 35,
            "deletions": 7,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/18c5b216f1b6d994c9c1e5d11a9ce6bdf39f822d/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18c5b216f1b6d994c9c1e5d11a9ce6bdf39f822d/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py?ref=18c5b216f1b6d994c9c1e5d11a9ce6bdf39f822d",
            "patch": "@@ -298,12 +298,16 @@ class ViTMAEModelIntegrationTest(unittest.TestCase):\n     def default_image_processor(self):\n         return ViTImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n \n+    @cached_property\n+    def default_model(self):\n+        return ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\").to(torch_device)\n+\n     @slow\n     def test_inference_for_pretraining(self):\n         # make random mask reproducible across the PT and TF model\n         np.random.seed(2)\n \n-        model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\").to(torch_device)\n+        model = self.default_model\n \n         image_processor = self.default_image_processor\n         image = prepare_img()\n@@ -313,11 +317,11 @@ def test_inference_for_pretraining(self):\n         # (this way we can ensure that the PT and TF models operate on the same inputs)\n         vit_mae_config = ViTMAEConfig()\n         num_patches = int((vit_mae_config.image_size // vit_mae_config.patch_size) ** 2)\n-        noise = np.random.uniform(size=(1, num_patches))\n+        noise = torch.from_numpy(np.random.uniform(size=(1, num_patches))).to(device=torch_device)\n \n         # forward pass\n         with torch.no_grad():\n-            outputs = model(**inputs, noise=torch.from_numpy(noise).to(device=torch_device))\n+            outputs = model(**inputs, noise=noise)\n \n         # verify the logits\n         expected_shape = torch.Size((1, 196, 768))\n@@ -339,7 +343,7 @@ def test_inference_interpolate_pos_encoding(self):\n         # make random mask reproducible across the PT and TF model\n         np.random.seed(2)\n \n-        model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\").to(torch_device)\n+        model = self.default_model\n \n         image_processor = self.default_image_processor\n         image = prepare_img()\n@@ -349,14 +353,38 @@ def test_inference_interpolate_pos_encoding(self):\n         # (this way we can ensure that the PT and TF models operate on the same inputs)\n         vit_mae_config = ViTMAEConfig()\n         num_patches = (image.height // vit_mae_config.patch_size) * (image.width // vit_mae_config.patch_size)\n-        noise = np.random.uniform(size=(1, num_patches))\n+        noise = torch.from_numpy(np.random.uniform(size=(1, num_patches))).to(device=torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, noise=noise, interpolate_pos_encoding=True)\n+\n+        # verify the logits\n+        expected_shape = torch.Size((1, 1200, 768))\n+        self.assertEqual(outputs.logits.shape, expected_shape)\n+\n+    @slow\n+    def test_inference_interpolate_pos_encoding_custom_sizes(self):\n+        # Ensure custom sizes are correctly handled when interpolating the position embeddings\n+\n+        # make random mask reproducible across the PT and TF model\n+        np.random.seed(2)\n+\n+        model = self.default_model\n+        image_processor = self.default_image_processor\n+\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\", size={\"height\": 256, \"width\": 256}).to(\n+            torch_device\n+        )\n \n         # forward pass\n         with torch.no_grad():\n             outputs = model(\n-                **inputs, noise=torch.from_numpy(noise).to(device=torch_device), interpolate_pos_encoding=True\n+                **inputs,\n+                interpolate_pos_encoding=True,\n             )\n \n         # verify the logits\n-        expected_shape = torch.Size((1, 1200, 768))\n+        expected_shape = torch.Size((1, 256, 768))\n         self.assertEqual(outputs.logits.shape, expected_shape)"
        }
    ],
    "stats": {
        "total": 57,
        "additions": 42,
        "deletions": 15
    }
}