{
    "author": "remi-or",
    "message": "[CB] Easy optimizations for continuous batching (#42839)\n\n* Cb example more args\n\n* Remove useless sync\n\n* Better new tokens, and no more BS1 on outputs\n\n* Add dynamic to compile to avoid many graphs\n\n* Sort prefix to maximize cache hits\n\n* More robust ways to retrieve results in test\n\n* Style\n\n* Update src/transformers/generation/continuous_batching/continuous_api.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "f3d5f2558b75369416ffe255a87a34358d08e429",
    "files": [
        {
            "sha": "c1408401b625831763295dcca51d74dd3bdeafd7",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f3d5f2558b75369416ffe255a87a34358d08e429/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f3d5f2558b75369416ffe255a87a34358d08e429/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=f3d5f2558b75369416ffe255a87a34358d08e429",
            "patch": "@@ -182,13 +182,16 @@ def batch_generate(\n \n     # Benchmark parameters\n     parser.add_argument(\"--samples\", type=int, default=500, help=\"Number of samples to generate\")\n+    parser.add_argument(\n+        \"--input-length\", type=int, default=None, help=\"Length of input sequences. Leave to None to mimic real eval.\"\n+    )\n     parser.add_argument(\"--max-new-tokens\", type=int, default=512, help=\"Maximum number of new tokens to generate\")\n+    parser.add_argument(\"--force-max-length\", action=\"store_true\", help=\"Force generation to stop at max length\")\n \n     parser.add_argument(\"--add-prefix\", action=\"store_true\", help=\"Add a prefix to the samples\")\n     parser.add_argument(\"--compare\", action=\"store_true\", help=\"Compare CB generation with classic generate\")\n     parser.add_argument(\"--profile\", type=str, default=None)\n     parser.add_argument(\"--metrics\", action=\"store_true\")\n-    parser.add_argument(\"--force-max-length\", action=\"store_true\", help=\"Force generation to stop at max length\")\n \n     # Display parameters\n     parser.add_argument(\"--displayed\", type=int, default=0, help=\"Number of samples to display\")\n@@ -251,6 +254,12 @@ def batch_generate(\n     else:\n         possible_prefixes = [None]\n \n+    tokenizer_kwargs = {\"add_generation_prompt\": True}\n+    if args.input_length is not None:\n+        tokenizer_kwargs[\"max_length\"] = args.input_length\n+        tokenizer_kwargs[\"truncation\"] = True\n+        tokenizer_kwargs[\"padding\"] = True\n+\n     batched_inputs = []\n     for item, prefix in zip(dataset, cycle(possible_prefixes)):\n         messages = []\n@@ -261,7 +270,7 @@ def batch_generate(\n             else:\n                 question = prefix + \"\\n\\n\" + question\n         messages.append({\"role\": \"user\", \"content\": question})\n-        inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n+        inputs = tokenizer.apply_chat_template(messages, **tokenizer_kwargs)\n         inputs = inputs if isinstance(inputs, list) else inputs[\"input_ids\"]\n         batched_inputs.append(inputs)\n \n@@ -283,6 +292,7 @@ def batch_generate(\n         generation_cfg.compile_config = CompileConfig(\n             fullgraph=True,\n             mode=\"max-autotune-no-cudagraphs\",\n+            dynamic=True,  # FIXME: if we warmup all graphs, this is not needed anymore\n         )\n \n     # If we need to compare, we need to generate the reference outputs"
        },
        {
            "sha": "ee8a4370a76bde3b44f61c3ca4ee6302f5eae1e8",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 17,
            "deletions": 21,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/f3d5f2558b75369416ffe255a87a34358d08e429/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f3d5f2558b75369416ffe255a87a34358d08e429/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=f3d5f2558b75369416ffe255a87a34358d08e429",
            "patch": "@@ -259,7 +259,7 @@ def setup_static_tensors(self, num_groups: int) -> None:\n         self.cumulative_seqlens_q = torch.empty((self.max_batch_tokens + 1,), **self.tensor_metadata)\n         self.max_seqlen_q = 0\n         self.logits_indices = torch.empty((self.max_batch_tokens,), **self.tensor_metadata)\n-        self.output_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n+        self.output_ids = torch.empty((self.max_batch_tokens,), **self.tensor_metadata)\n \n         # For some kwargs, we have a dict of tensors with as many items as there are attention types\n         layer_types = getattr(self.config, \"layer_types\", None)\n@@ -311,7 +311,7 @@ def reset_static_tensors(self, full_reset: bool = False) -> None:\n         self.cumulative_seqlens_q[: b_size + 1].zero_()\n         self.max_seqlen_q = 0\n         self.logits_indices[:q_len].fill_(-1)\n-        self.output_ids[:, :q_len].fill_(-1)\n+        self.output_ids[:q_len].fill_(-1)\n \n         # Reset the attributes that are either tensors or dict of tensors\n         for layer_type in self.cumulative_seqlens_k:\n@@ -447,7 +447,7 @@ def prepare_next_batch(self) -> bool:\n         self.metrics.record_batch_metrics(self.requests_in_batch)\n \n         # Reset the static tensors used for storage\n-        self.reset_static_tensors()  # TODO: this might be unnecessary\n+        self.reset_static_tensors()  # FIXME: why does this make the generation faster?\n \n         # Prepare accumulators\n         self.actual_query_length = 0\n@@ -557,13 +557,10 @@ def _build_tensors(\n             self.actual_index_sizes[i] = (len(group_read_indices), len(group_write_indices))\n \n     @traced\n-    def _sync(self) -> list[int]:\n-        if self.output_ids is not None:\n-            try:\n-                return self.output_ids.tolist()[0]\n-            except Exception:\n-                return [0, 1]\n-        return [0, 0]\n+    def _get_new_tokens(self, num_new_tokens: int) -> list[int]:\n+        indices = self.logits_indices[:num_new_tokens]\n+        new_tokens = self.output_ids[indices]\n+        return new_tokens.tolist()\n \n     @traced\n     def _maybe_send_output(self, state: RequestState) -> None:\n@@ -574,13 +571,13 @@ def _maybe_send_output(self, state: RequestState) -> None:\n     @traced\n     def update_batch(self) -> None:\n         \"\"\"Update request states based on generated tokens.\"\"\"\n-        out_tokens = self._sync()\n+        new_tokens = self._get_new_tokens(len(self.requests_in_batch))\n         for i, state in enumerate(self.requests_in_batch):\n             # If the request has no remaining prompt ids, it means prefill has already ended or just finished\n             if len(state.remaining_prefill_tokens) == 0:\n                 self.metrics.record_ttft_metric(state.created_time, state.request_id)\n                 state.status = RequestStatus.DECODING\n-                token = out_tokens[self.logits_indices[i]]\n+                token = new_tokens[i]\n                 state.tokens_to_process = [token]\n                 # Update the request and stop if it is complete\n                 is_finished = state.update_and_check_completion(token)\n@@ -727,12 +724,11 @@ def _sample(self, probs: torch.Tensor, do_sample: bool) -> None:\n             probs = nn.functional.softmax(probs, dim=-1)\n             # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n             next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n-            # Add batch dimension back to match argmax output\n-            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n         else:\n-            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n-        tokens = next_tokens.size(1)  # Get seq_len dimension\n-        self.output_ids[:, :tokens].copy_(next_tokens)\n+            next_tokens = torch.argmax(probs, dim=-1)  # shape is [1, seq_len]\n+            next_tokens = next_tokens.squeeze(0)  # shape is [seq_len]\n+        tokens = next_tokens.size(0)  # Get seq_len dimension\n+        self.output_ids[:tokens].copy_(next_tokens)\n \n \n # Manager Class (User Interface)\n@@ -950,6 +946,10 @@ def add_requests(\n         streaming: bool = False,\n         record_timestamps: bool = False,\n     ) -> None:\n+        # If there is prefix sharing, we sort the inputs to maximize cache hits\n+        if self._allow_prefix_sharing:\n+            inputs = sorted(inputs, reverse=True)\n+        # Add requests in order\n         for input_ids in inputs:\n             self.add_request(\n                 input_ids, max_new_tokens=max_new_tokens, streaming=streaming, record_timestamps=record_timestamps\n@@ -1080,10 +1080,6 @@ def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor) -> N\n             )\n \n         self._generation_step()\n-\n-        if torch.cuda.is_available():\n-            torch.cuda.synchronize()  # FIXME: why is this needed?\n-        # Processor updates the batch after generation step is truly over\n         batch_processor.update_batch()\n \n     @traced"
        },
        {
            "sha": "e3ac8d6b911aff4731e208b0aaf9e011d80dad1b",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f3d5f2558b75369416ffe255a87a34358d08e429/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f3d5f2558b75369416ffe255a87a34358d08e429/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=f3d5f2558b75369416ffe255a87a34358d08e429",
            "patch": "@@ -251,7 +251,17 @@ def _test_continuous_batching_parity(\n         generate_outputs = model.generate(**inputs.to(torch_device), generation_config=model.generation_config)\n \n         for i, user_message in enumerate(user_messages):\n-            continuous_batching_output = continuous_batching_outputs[f\"req_{i}\"].generated_tokens\n+            # Find the corresponding request in the continuous batching outputs\n+            input_tokens = inputs.input_ids[i][inputs.attention_mask[i] == 1].tolist()\n+            key_to_pop = None\n+            for key, state in continuous_batching_outputs.items():\n+                if state.prompt_ids == input_tokens:\n+                    key_to_pop = key\n+                    break\n+            if key_to_pop is None:\n+                self.fail(f\"Request {i} not found in continuous batching outputs\")\n+            continuous_batching_output = continuous_batching_outputs.pop(key_to_pop).generated_tokens\n+\n             generate_output = generate_outputs[i][num_input_tokens:].tolist()\n             while generate_output[-1] == model.generation_config.pad_token_id:\n                 generate_output.pop()"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 40,
        "deletions": 24
    }
}