{
    "author": "paulpak58",
    "message": "[Cache] lfm2 cache: allocate empty kv layers during init (#41396)\n\n* [Cache] lfm2 cache: allocate empty kv layers during init\n\nSigned-off-by: Paul Pak <paulpak58@gmail.com>\n\n* [Cache] lfm2_cache: update modular file\n\nSigned-off-by: Paul Pak <paulpak58@gmail.com>\n\n---------\n\nSigned-off-by: Paul Pak <paulpak58@gmail.com>",
    "sha": "0464d9eb37b7e384f8d96b7c9bb9cfb61fcdfd1e",
    "files": [
        {
            "sha": "07aced67f65062680e6a131d62e7a4b330b4b317",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 23,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/0464d9eb37b7e384f8d96b7c9bb9cfb61fcdfd1e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0464d9eb37b7e384f8d96b7c9bb9cfb61fcdfd1e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=0464d9eb37b7e384f8d96b7c9bb9cfb61fcdfd1e",
            "patch": "@@ -165,6 +165,8 @@ def __init__(\n             )\n             torch._dynamo.mark_static_address(conv_state)\n             self.conv_cache.append(conv_state)\n+            self.key_cache.append(torch.tensor([]))\n+            self.value_cache.append(torch.tensor([]))\n \n     def update(\n         self,\n@@ -190,35 +192,27 @@ def update(\n             A tuple containing the updated key and value states.\n         \"\"\"\n         # Update the cache\n-        if key_states is not None:\n-            if len(self.key_cache) <= layer_idx:\n-                # There may be skipped layers, fill them with empty lists\n-                for _ in range(len(self.key_cache), layer_idx):\n-                    self.key_cache.append(torch.tensor([]))\n-                    self.value_cache.append(torch.tensor([]))\n-                self.key_cache.append(key_states)\n-                self.value_cache.append(value_states)\n-            elif (\n-                not self.key_cache[layer_idx].numel()  # prefers not t.numel() to len(t) == 0 to export the model\n-            ):  # fills previously skipped layers; checking for tensor causes errors\n-                self.key_cache[layer_idx] = key_states\n-                self.value_cache[layer_idx] = value_states\n-            else:\n-                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n-                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n+        if self.key_cache[layer_idx].numel() == 0:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n \n         return self.key_cache[layer_idx], self.value_cache[layer_idx]\n \n     def reorder_cache(self, beam_idx: torch.LongTensor):\n         \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n         for layer_idx in range(len(self.key_cache)):\n-            device = self.key_cache[layer_idx].device\n-            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.value_cache[layer_idx].device\n-            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n-\n-            device = self.conv_cache[layer_idx].device\n-            self.conv_cache[layer_idx] = self.conv_cache[layer_idx].index_select(0, beam_idx.to(device))\n+            if self.key_cache[layer_idx].numel():\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            if self.conv_cache[layer_idx].numel():\n+                device = self.conv_cache[layer_idx].device\n+                self.conv_cache[layer_idx] = self.conv_cache[layer_idx].index_select(0, beam_idx.to(device))\n \n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\""
        },
        {
            "sha": "16a69fa0dc065f58aac1ca1abe6aef1cd5898108",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 23,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/0464d9eb37b7e384f8d96b7c9bb9cfb61fcdfd1e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0464d9eb37b7e384f8d96b7c9bb9cfb61fcdfd1e/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=0464d9eb37b7e384f8d96b7c9bb9cfb61fcdfd1e",
            "patch": "@@ -123,6 +123,8 @@ def __init__(\n             )\n             torch._dynamo.mark_static_address(conv_state)\n             self.conv_cache.append(conv_state)\n+            self.key_cache.append(torch.tensor([]))\n+            self.value_cache.append(torch.tensor([]))\n \n     def update(\n         self,\n@@ -148,35 +150,27 @@ def update(\n             A tuple containing the updated key and value states.\n         \"\"\"\n         # Update the cache\n-        if key_states is not None:\n-            if len(self.key_cache) <= layer_idx:\n-                # There may be skipped layers, fill them with empty lists\n-                for _ in range(len(self.key_cache), layer_idx):\n-                    self.key_cache.append(torch.tensor([]))\n-                    self.value_cache.append(torch.tensor([]))\n-                self.key_cache.append(key_states)\n-                self.value_cache.append(value_states)\n-            elif (\n-                not self.key_cache[layer_idx].numel()  # prefers not t.numel() to len(t) == 0 to export the model\n-            ):  # fills previously skipped layers; checking for tensor causes errors\n-                self.key_cache[layer_idx] = key_states\n-                self.value_cache[layer_idx] = value_states\n-            else:\n-                self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n-                self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n+        if self.key_cache[layer_idx].numel() == 0:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n \n         return self.key_cache[layer_idx], self.value_cache[layer_idx]\n \n     def reorder_cache(self, beam_idx: torch.LongTensor):\n         \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n         for layer_idx in range(len(self.key_cache)):\n-            device = self.key_cache[layer_idx].device\n-            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.value_cache[layer_idx].device\n-            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n-\n-            device = self.conv_cache[layer_idx].device\n-            self.conv_cache[layer_idx] = self.conv_cache[layer_idx].index_select(0, beam_idx.to(device))\n+            if self.key_cache[layer_idx].numel():\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            if self.conv_cache[layer_idx].numel():\n+                device = self.conv_cache[layer_idx].device\n+                self.conv_cache[layer_idx] = self.conv_cache[layer_idx].index_select(0, beam_idx.to(device))\n \n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\""
        }
    ],
    "stats": {
        "total": 80,
        "additions": 34,
        "deletions": 46
    }
}