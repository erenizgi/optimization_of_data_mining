{
    "author": "eu90h",
    "message": "Fixed markdown for BertTokenizer's '[CLS]' token. (#38506)",
    "sha": "3620b32cc853593abb629f0e680d79125e6ece4a",
    "files": [
        {
            "sha": "7dc1ac124457a7e533866493bf43f309995a9bb5",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3620b32cc853593abb629f0e680d79125e6ece4a/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3620b32cc853593abb629f0e680d79125e6ece4a/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=3620b32cc853593abb629f0e680d79125e6ece4a",
            "patch": "@@ -911,7 +911,7 @@ def add_special_tokens(\n           makes it easy to develop model-agnostic training and fine-tuning scripts.\n \n         When possible, special tokens are already registered for provided pretrained models (for instance\n-        [`BertTokenizer`] `cls_token` is already registered to be :obj*'[CLS]'* and XLM's one is also registered to be\n+        [`BertTokenizer`] `cls_token` is already registered to be `'[CLS]'` and XLM's one is also registered to be\n         `'</s>'`).\n \n         Args:"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}