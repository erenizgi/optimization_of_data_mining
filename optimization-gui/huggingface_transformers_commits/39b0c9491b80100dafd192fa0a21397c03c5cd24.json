{
    "author": "cyyever",
    "message": "Remove unused function patameters (#41358)\n\nRemove unused arguments\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "39b0c9491b80100dafd192fa0a21397c03c5cd24",
    "files": [
        {
            "sha": "bc50ebc0a9b7c71b951d5b7ae0d7d9ad661da1ed",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/39b0c9491b80100dafd192fa0a21397c03c5cd24/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/39b0c9491b80100dafd192fa0a21397c03c5cd24/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=39b0c9491b80100dafd192fa0a21397c03c5cd24",
            "patch": "@@ -1413,7 +1413,7 @@ def converted(self) -> Tokenizer:\n class MoshiConverter(SpmConverter):\n     handle_byte_fallback = True\n \n-    def __init__(self, vocab_file, model_max_length=None, **kwargs):\n+    def __init__(self, vocab_file, **kwargs):\n         requires_backends(self, \"protobuf\")\n \n         Converter.__init__(self, vocab_file)"
        },
        {
            "sha": "6da149d7c79519236c1d713c680349978e4e9d7e",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/39b0c9491b80100dafd192fa0a21397c03c5cd24/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/39b0c9491b80100dafd192fa0a21397c03c5cd24/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=39b0c9491b80100dafd192fa0a21397c03c5cd24",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n from collections import deque\n from math import floor, gcd, sqrt\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n \n@@ -123,7 +123,6 @@ def __init__(\n         generation_config: GenerationConfig,\n         device: torch.device,\n         dtype: torch.dtype = torch.float16,\n-        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n         tp_size: Optional[int] = None,\n     ) -> None:\n         \"\"\"Initialize a paged attention cache for efficient memory usage.\n@@ -133,7 +132,6 @@ def __init__(\n             generation_config: Generation configuration containing cache parameters\n             device: Device for the cache tensors\n             dtype: Data type of the cache\n-            layer_device_map: Optional mapping of layer indices to devices\n             tp_size: Tensor parallelism size\n         \"\"\"\n         self.config = config"
        },
        {
            "sha": "e5f9f364b639a7a088ce9f98c65e5d331f26aab8",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/39b0c9491b80100dafd192fa0a21397c03c5cd24/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/39b0c9491b80100dafd192fa0a21397c03c5cd24/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=39b0c9491b80100dafd192fa0a21397c03c5cd24",
            "patch": "@@ -3241,11 +3241,11 @@ def _get_resized_embeddings(\n \n                 with deepspeed.zero.GatheredParameters([old_embeddings.weight], modifier_rank=None):\n                     self._init_added_embeddings_weights_with_mean(\n-                        old_embeddings, new_embeddings, old_embedding_dim, old_num_tokens, added_num_tokens\n+                        old_embeddings, new_embeddings, old_num_tokens, added_num_tokens\n                     )\n             else:\n                 self._init_added_embeddings_weights_with_mean(\n-                    old_embeddings, new_embeddings, old_embedding_dim, old_num_tokens, added_num_tokens\n+                    old_embeddings, new_embeddings, old_num_tokens, added_num_tokens\n                 )\n \n         # Copy token embeddings from the previous weights\n@@ -3415,7 +3415,7 @@ def _get_resized_lm_head(\n         return new_lm_head\n \n     def _init_added_embeddings_weights_with_mean(\n-        self, old_embeddings, new_embeddings, old_embedding_dim, old_num_tokens, added_num_tokens\n+        self, old_embeddings, new_embeddings, old_num_tokens, added_num_tokens\n     ):\n         old_embeddings_weight = old_embeddings.weight.data.to(torch.float32)\n         mean_embeddings = torch.mean(old_embeddings_weight, axis=0)\n@@ -3454,9 +3454,7 @@ def _init_added_lm_head_weights_with_mean(\n             old_lm_head.weight.data = old_lm_head.weight.data.T\n \n         # The same initialization logic as Embeddings.\n-        self._init_added_embeddings_weights_with_mean(\n-            old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens\n-        )\n+        self._init_added_embeddings_weights_with_mean(old_lm_head, new_lm_head, old_num_tokens, added_num_tokens)\n \n         if transposed:\n             # Transpose again to the correct shape."
        }
    ],
    "stats": {
        "total": 16,
        "additions": 6,
        "deletions": 10
    }
}