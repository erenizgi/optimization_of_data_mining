{
    "author": "muellerzr",
    "message": "Fix model kwargs (#35875)\n\n* Save state\r\n\r\n* Make a failing test\r\n\r\n* Better test\r\n\r\n* mpt -> done, many more to go\r\n\r\n* Rm extranious\r\n\r\n* Bamba\r\n\r\n* Bert\r\n\r\n* big_bird\r\n\r\n* biogpt\r\n\r\n* bloom\r\n\r\n* codegen\r\n\r\n* ctrl\r\n\r\n* data2vec\r\n\r\n* dbrx\r\n\r\n* Through up to Dbrx\r\n\r\n* electra\r\n\r\n* ernie\r\n\r\n* falcon\r\n\r\n* Fuyu/persimmon\r\n\r\n* Include noop kwargs to base models\r\n\r\n* Rebase\r\n\r\n* Skip musigen\r\n\r\n* Refactor/skip mllama\r\n\r\n* Revert makefile\r\n\r\n* Rm file\r\n\r\n* Fix PT failing, need to modify rest of loss funcs to not resize\r\n\r\n* Propagate some\r\n\r\n* Continue\r\n\r\n* More\r\n\r\n* More options\r\n\r\n* Mostly fixed\r\n\r\n* Proved that it's the same\r\n\r\n* Bloom is good\r\n\r\n* Make ability to override loss func possible\r\n\r\n* Fixup\r\n\r\n* Clean\r\n\r\n* Fix xglm\r\n\r\n* Quality tests\r\n\r\n* Skip OCR2\r\n\r\n* Make specific loss for xglm\r\n\r\n* Make order the same/line up 1:1\r\n\r\n* xglm\r\n\r\n* Skip fx output loss bloom model\r\n\r\n* Didn't pass in pad_token_id\r\n\r\n* Fix quality",
    "sha": "28f73bc3072bd298377b9d473cf2d62a4e4f442b",
    "files": [
        {
            "sha": "e292b1061a28cc0e69c2f4cf5c390aba6a460368",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -5198,6 +5198,9 @@ def tplize(mod: torch.nn.Module) -> None:\n \n     @property\n     def loss_function(self):\n+        if hasattr(self, \"_loss_function\"):\n+            return self._loss_function\n+\n         loss_type = getattr(self, \"loss_type\", None)\n \n         if loss_type is None or loss_type not in LOSS_MAPPING:\n@@ -5208,6 +5211,10 @@ def loss_function(self):\n             loss_type = \"ForCausalLM\"\n         return LOSS_MAPPING[loss_type]\n \n+    @loss_function.setter\n+    def loss_function(self, value):\n+        self._loss_function = value\n+\n     def get_compiled_call(self, compile_config: CompileConfig):\n         \"\"\"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n         non-compiled/compiled `forward` during inference, especially to switch between prefill (where we don't"
        },
        {
            "sha": "41ba1c5b26e0f45d9b7185dcf341d5bf1433cf62",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1208,6 +1208,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "3972d25b51b923ad1f6c1c83146484328f8828f5",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -949,6 +949,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "f011550ff340c04d03492e55f126eedc3ed6489f",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n@@ -734,6 +733,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -901,6 +901,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -963,18 +964,20 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n         prediction_scores = self.lm_head(sequence_output)\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[1:]"
        },
        {
            "sha": "4ddce6e9fe4b3775569d3541c8d20e2cf2996cef",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1983,6 +1983,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,  # NOOP kwargs, for now\n     ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -2540,6 +2541,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[CausalLMOutputWithCrossAttentions, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -2580,18 +2582,20 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n         prediction_scores = self.cls(sequence_output)\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "51f298098ba1d64d22154bc6cf6e5e89e4edb479",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -588,6 +588,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -757,6 +758,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -783,11 +785,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[1:]"
        },
        {
            "sha": "ebe506cd362dc2cb5ed5636d29e30e5cd6b3081c",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -967,6 +967,8 @@ def forward(\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n+        # Bloom has deprecated kwargs, so we need to pop num_items_in_batch explicitly\n+        num_items_in_batch = deprecated_arguments.pop(\"num_items_in_batch\", None)\n         if deprecated_arguments.pop(\"position_ids\", False) is not False:\n             # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`\n             warnings.warn(\n@@ -999,14 +1001,12 @@ def forward(\n         if labels is not None:\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(lm_logits.device)\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            batch_size, seq_length, vocab_size = shift_logits.shape\n             # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                num_items_in_batch=num_items_in_batch,\n             )\n \n         if not return_dict:"
        },
        {
            "sha": "e44ef805531ed46e7fd20bc78f701e46ebce42df",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1584,6 +1584,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1655,11 +1656,12 @@ def forward(\n         if labels is not None:\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(prediction_scores.device)\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "d09ab5612263c6b2b1cf9f9862ed67072884dd10",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -19,7 +19,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -450,6 +449,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -741,6 +741,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -775,12 +776,13 @@ def forward(\n         if labels is not None:\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(lm_logits.device)\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n             # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n             loss = loss.to(hidden_states.dtype)\n "
        },
        {
            "sha": "f530b702870f37c159d6a74ab20b777a2caa2d6a",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -360,6 +360,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPast]:\n         r\"\"\"\n         Returns:\n@@ -537,6 +538,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -593,12 +595,12 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (lm_logits,) + transformer_outputs[1:]"
        },
        {
            "sha": "5f84eca754e85b1c458b2d581d87021d62d964e3",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -906,6 +906,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -975,13 +976,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-\n-            labels = labels.to(shifted_prediction_scores.device)\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "e3622912e7a786281270c652a73fc5e0f41b564c",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -980,6 +980,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, MoeModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1278,6 +1279,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"Forward function for causal language modeling.\n \n@@ -1344,16 +1346,12 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         aux_loss = None\n         if output_router_logits:"
        },
        {
            "sha": "f2138ac0f68386c6219c1a34421ac1947b6e58df",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1564,6 +1564,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1633,11 +1634,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[1:]"
        },
        {
            "sha": "2ab1521f19a74e8a0eade5df8dfb3322ef7add85",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1130,6 +1130,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1181,11 +1182,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "4894fdde0b362bcb7f8f666a36c6fc38f25077a8",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1199,6 +1199,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1236,14 +1237,11 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            batch_size, seq_length, vocab_size = shift_logits.shape\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n             )\n \n         if not return_dict:"
        },
        {
            "sha": "d19c48b76293953daa978943f47f0b504f905877",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -239,6 +239,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -327,6 +328,7 @@ def forward(\n             labels=labels,\n             use_cache=use_cache,\n             return_dict=return_dict,\n+            **kwargs,\n         )\n \n         return outputs"
        },
        {
            "sha": "047516a4b162dd1d6462857ab8f1aa3d24bea0c4",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -524,6 +524,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,  # NOOP kwarg for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "540ce2b87c15139d9481889d02de9b2053230a06",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -374,6 +374,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,  # NOOP kwarg for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "b3a88545fa37ed43250b51489647d19eb8375a63",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -22,7 +22,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n@@ -1426,6 +1425,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1591,8 +1591,12 @@ def forward(\n             num_image_tokens = self.git.encoder.layer[0].attention.self.image_patch_tokens\n             shifted_logits = logits[:, num_image_tokens:-1, :].contiguous()\n             labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shifted_logits.view(-1, self.config.vocab_size), labels.view(-1))\n+            loss = self.loss_function(\n+                shifted_logits.view(-1, self.config.vocab_size),\n+                labels.view(-1),\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "884e05da322930697fad5d473d2b3aeffb1b1d18",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1049,6 +1049,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1084,14 +1085,13 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(lm_logits.device)\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n             # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (lm_logits,) + transformer_outputs[1:]"
        },
        {
            "sha": "73868cce2aa296f4cc9908c606b8df2262cbd147",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1148,6 +1148,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1178,12 +1179,12 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous().to(shift_logits.device)\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (lm_logits,) + transformer_outputs[1:]"
        },
        {
            "sha": "7037ef83dc6354520a8b4b345773c53cfbc9c269",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -951,6 +951,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -986,12 +987,13 @@ def forward(\n             # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n             lm_logits = lm_logits.to(torch.float32)\n \n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n             # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n             lm_logits = lm_logits.to(hidden_states.dtype)\n             loss = loss.to(hidden_states.dtype)"
        },
        {
            "sha": "a219d3c28521bde75f6da65fa1bb3381df07fee7",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import Tensor, nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -816,6 +815,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -866,11 +866,12 @@ def forward(\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(lm_logits.device)\n \n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shift_logits = lm_logits[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (lm_logits,) + outputs[1:]"
        },
        {
            "sha": "82bcbb69b5cd81acdeb78124b1f1f6f6cc0e2831",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1085,6 +1085,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1124,12 +1125,13 @@ def forward(\n         if labels is not None:\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(lm_logits.device)\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n             # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n             loss = loss.to(hidden_states.dtype)\n "
        },
        {
            "sha": "4da0fbcbb909b719c05ca41ebeadc2af45231107",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -19,7 +19,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1295,6 +1294,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1353,16 +1353,13 @@ def forward(\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues\n             logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n             # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         aux_loss = None\n         if output_router_logits:"
        },
        {
            "sha": "6c47a57291f61eac272121ae1ba1d2b3341b5dfc",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n from torch.nn import functional as F\n \n from ...activations import ACT2FN\n@@ -1295,6 +1294,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1350,8 +1350,12 @@ def forward(\n             shift_labels = shift_labels.view(-1)\n             # Ensure tensors are on the same device\n             shift_labels = shift_labels.to(shift_logits.device)\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(\n+                shift_logits,\n+                shift_labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         aux_loss = None\n         if output_router_logits:"
        },
        {
            "sha": "dba31a7b85fa01c073928fe566b9eb11215433c2",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1151,6 +1151,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1217,11 +1218,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "50f9a56a8a5ef1d2d355695c9758fa3e42d4c883",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1806,6 +1806,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[Tuple, MoshiCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1876,12 +1877,16 @@ def forward(\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n             # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n             shift_logits = shift_logits.view(-1, self.config.vocab_size)\n             shift_labels = shift_labels.view(-1)\n             # Enable model parallelism\n             shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(\n+                shift_logits,\n+                shift_labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = ("
        },
        {
            "sha": "f9bfdf7c1ddad51072957e52cec13e1f854fe269",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -392,6 +392,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -535,6 +536,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -562,14 +564,12 @@ def forward(\n         if labels is not None:\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(lm_logits.device)\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            batch_size, seq_length, vocab_size = shift_logits.shape\n             # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(batch_size * seq_length, vocab_size), shift_labels.view(batch_size * seq_length)\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n             )\n \n         if not return_dict:"
        },
        {
            "sha": "cab950995a97558af23c1a5cf856be4f568d3f1c",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1259,6 +1259,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks)`, *optional*):"
        },
        {
            "sha": "a86b86bdcac6fe5e1a6e7d8f4f2183a3a638e32f",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -560,6 +560,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -585,12 +586,13 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            shift_logits = lm_logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n             # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                lm_logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (lm_logits,) + transformer_outputs[1:]"
        },
        {
            "sha": "bf7e33c643abe32fbae035cbb813ae6cc0b0b172",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1085,6 +1085,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1191,12 +1192,12 @@ def forward(\n         if labels is not None:\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(logits.device)\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "568f380c82b002fff07a2fb87cf77d14c63aad87",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -25,7 +25,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -848,6 +847,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -909,16 +909,12 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "e2014079f9361fdecf52a25c6eaa835649874c47",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -21,7 +21,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n@@ -819,6 +818,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutput]:\n         r\"\"\"\n         Args:\n@@ -873,16 +873,12 @@ def forward(\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues\n             logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "ab6c15f5174e491ae51f044e1bdf18320cf8ff1c",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -2232,6 +2232,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -2260,12 +2261,12 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (logits,) + reformer_outputs[1:]"
        },
        {
            "sha": "66ba88b40d7a0826943956fe0d45eddda2ef9a24",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1042,6 +1042,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1107,11 +1108,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "0425b8d1978da8efa30c982dacacd4411652defb",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1043,6 +1043,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1114,11 +1115,12 @@ def forward(\n         if labels is not None:\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(prediction_scores.device)\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "e8c5156d3cc5c5a9f16fc50ca5813b97998c877a",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -897,6 +897,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -968,11 +969,12 @@ def forward(\n         if labels is not None:\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(prediction_scores.device)\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "d9716db14d369ab9d25cc6b629ea7d77b1fb88fe",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1449,6 +1449,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1524,11 +1525,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "8dbf17ea46d50c354d8311c53a1645872b7e1b61",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1074,6 +1074,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[CausalLMOutputWithCrossAttentions, Tuple[torch.Tensor]]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1138,11 +1139,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[1:]"
        },
        {
            "sha": "10aea72223203a5b60fa0686e2217e8293cd44af",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -23,7 +23,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...generation import GenerationMixin\n from ...modeling_utils import PreTrainedModel\n@@ -803,6 +802,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, RwkvCausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -827,14 +827,12 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (logits,) + rwkv_outputs[1:]"
        },
        {
            "sha": "eecfa7051d844d67c06a6e7847a0ba3ec42bf398",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -25,7 +25,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -1105,6 +1104,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1165,16 +1165,12 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "cff5eae2f683b930f0151304d7647c3bbc64cec2",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 37,
            "deletions": 8,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n@@ -690,6 +689,33 @@ def forward(\n         )\n \n \n+def xglm_cross_entropy_loss(\n+    logits,\n+    labels,\n+    num_items_in_batch: int = None,\n+    ignore_index: int = -100,\n+    pad_token_id: int = -100,\n+    vocab_size: int = None,\n+):\n+    \"\"\"\n+    Loss function for XGLM that takes into account `num_items_in_batch`\n+    \"\"\"\n+    shift_labels = labels.new_zeros(labels.shape)\n+    shift_labels[:, :-1] = labels[:, 1:].clone()\n+    shift_labels[:, -1] = pad_token_id\n+    # move labels to correct device to enable model parallelism\n+    labels = labels.float().to(logits.device)\n+\n+    logits = logits.view(-1, vocab_size).float()\n+    shift_labels = shift_labels.view(-1)\n+\n+    reduction = \"sum\" if num_items_in_batch is not None else \"mean\"\n+    loss = nn.functional.cross_entropy(logits, shift_labels, ignore_index=ignore_index, reduction=reduction)\n+    if reduction == \"sum\":\n+        loss = loss / num_items_in_batch\n+    return loss\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     The XGLM Model transformer with a language modeling head on top (linear layer with weights tied to the input\n@@ -709,6 +735,8 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+        self._loss_function = xglm_cross_entropy_loss\n+\n     def get_input_embeddings(self):\n         return self.model.embed_tokens\n \n@@ -743,6 +771,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -778,13 +807,13 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # shift labels and add a pad token to the end\n-            shift_labels = labels.new_zeros(labels.shape)\n-            shift_labels[:, :-1] = labels[:, 1:].clone()\n-            shift_labels[:, -1] = self.config.pad_token_id\n-\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                pad_token_id=self.config.pad_token_id,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "0ffa3319081d610afa7194a1df5ce023e3873cf8",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -486,6 +486,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,  # Dummy kwargs for now\n     ) -> Union[Tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -712,6 +713,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -734,6 +736,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            **kwargs,\n         )\n \n         output = transformer_outputs[0]"
        },
        {
            "sha": "07800804c1bfc6517072186e5679ac4fedf510d7",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1046,6 +1046,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1117,11 +1118,12 @@ def forward(\n         if labels is not None:\n             # move labels to correct device to enable model parallelism\n             labels = labels.to(prediction_scores.device)\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "014480ecd82eb9eeeed4d9311550db906c80ff5d",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -1026,6 +1026,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1092,11 +1093,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "a3bde4c2b59d758c29ddac819f9c95783d9075c7",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -999,6 +999,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1070,11 +1071,12 @@ def forward(\n \n         lm_loss = None\n         if labels is not None:\n-            # we are doing next-token prediction; shift prediction scores and input ids by one\n-            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n-            labels = labels[:, 1:].contiguous()\n-            loss_fct = CrossEntropyLoss()\n-            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n \n         if not return_dict:\n             output = (prediction_scores,) + outputs[2:]"
        },
        {
            "sha": "964595a0cd046adc6f22b6ed4efc3b58d2faa449",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/28f73bc3072bd298377b9d473cf2d62a4e4f442b/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/28f73bc3072bd298377b9d473cf2d62a4e4f442b/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=28f73bc3072bd298377b9d473cf2d62a4e4f442b",
            "patch": "@@ -922,6 +922,42 @@ def test_training(self):\n             loss = model(**inputs).loss\n             loss.backward()\n \n+    def test_causal_lm_can_accept_kwargs(self):\n+        if not getattr(self.model_tester, \"is_training\", False):\n+            self.skipTest(reason=\"ModelTester is not configured to run training tests\")\n+\n+        valid_model_class = False\n+        incompatible_models = (\n+            \"MusicgenForCausalLM\",\n+            \"MusicgenMelodyForCausalLM\",\n+            \"MllamaForCausalLM\",\n+            \"CpmAntForCausalLM\",\n+            \"GotOcr2ForConditionalGeneration\",\n+        )\n+        for model_class in self.all_model_classes:\n+            if (\n+                model_class.__name__ in get_values(MODEL_FOR_CAUSAL_LM_MAPPING_NAMES)\n+                and model_class.__name__ not in incompatible_models\n+            ):\n+                valid_model_class = True\n+        if not valid_model_class:\n+            self.skipTest(reason=\"No causal lm model classes found\")\n+        for model_class in self.all_model_classes:\n+            model_name = model_class.__name__\n+            if model_name in get_values(MODEL_FOR_CAUSAL_LM_MAPPING_NAMES) and model_name not in incompatible_models:\n+                config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+                with tempfile.TemporaryDirectory() as tmpdir:\n+                    with torch.device(torch_device):\n+                        model_eager = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float32)\n+\n+                    model_eager.save_pretrained(tmpdir)\n+                    with torch.device(torch_device):\n+                        model = AutoModelForCausalLM.from_pretrained(tmpdir, torch_dtype=torch.float32)\n+                        inputs_dict[\"num_items_in_batch\"] = inputs_dict[\"input_ids\"].shape[0]\n+                        inputs_dict[\"labels\"] = inputs_dict[\"input_ids\"]\n+                        _ = model(**inputs_dict, return_dict=False)\n+\n     def test_training_gradient_checkpointing(self):\n         # Scenario - 1 default behaviour\n         self.check_training_gradient_checkpointing()\n@@ -1236,6 +1272,8 @@ def test_torch_fx(self):\n         self._create_and_check_torch_fx_tracing(config, inputs_dict)\n \n     def test_torch_fx_output_loss(self):\n+        if self.all_model_classes[0].__name__ == \"BloomModel\":\n+            self.skipTest(reason=\"Bloom currently has issues, @michaelbenayoun\")\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         self._create_and_check_torch_fx_tracing(config, inputs_dict, output_loss=True)\n "
        }
    ],
    "stats": {
        "total": 606,
        "additions": 365,
        "deletions": 241
    }
}