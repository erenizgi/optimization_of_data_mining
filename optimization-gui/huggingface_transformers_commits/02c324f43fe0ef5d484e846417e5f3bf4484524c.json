{
    "author": "RyanMullins",
    "message": "Fix: Gemma3TextConfig rope scaling assignments (#41934)\n\n* Fix: Gemma3TextConfig rope scaling assignments\n\n* Fix: type annotation for rope_parameters",
    "sha": "02c324f43fe0ef5d484e846417e5f3bf4484524c",
    "files": [
        {
            "sha": "d549bfffddf10fd0702f15892576687b787d7565",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/02c324f43fe0ef5d484e846417e5f3bf4484524c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/02c324f43fe0ef5d484e846417e5f3bf4484524c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=02c324f43fe0ef5d484e846417e5f3bf4484524c",
            "patch": "@@ -156,7 +156,7 @@ def __init__(\n         layer_types: Optional[list[str]] = None,\n         final_logit_softcapping: Optional[float] = None,\n         attn_logit_softcapping: Optional[float] = None,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_bidirectional_attention: Optional[bool] = False,\n         **kwargs,\n     ):\n@@ -186,10 +186,16 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n+\n         # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        if rope_scaling is not None:\n-            rope_parameters = {\"sliding_attention\": {\"rope_type\": \"default\"}, \"full_attention\": rope_scaling}\n+        if (rope_scaling := kwargs.pop(\"rope_scaling\", None)) is not None:\n+            if rope_parameters is None:\n+                rope_parameters = {\"sliding_attention\": {\"rope_type\": \"default\"}, \"full_attention\": rope_scaling}\n+            elif \"full_attention\" in rope_parameters:\n+                rope_parameters[\"full_attention\"].update(rope_scaling)\n+            else:\n+                rope_parameters.update(rope_scaling)\n+\n         self.rope_parameters = rope_parameters\n         self.use_bidirectional_attention = use_bidirectional_attention\n         if use_bidirectional_attention:"
        },
        {
            "sha": "f4b4ce22381ea2bfb815c0cf27da840ac5f94654",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/02c324f43fe0ef5d484e846417e5f3bf4484524c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/02c324f43fe0ef5d484e846417e5f3bf4484524c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=02c324f43fe0ef5d484e846417e5f3bf4484524c",
            "patch": "@@ -171,7 +171,7 @@ def __init__(\n         layer_types: Optional[list[str]] = None,\n         final_logit_softcapping: Optional[float] = None,\n         attn_logit_softcapping: Optional[float] = None,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         use_bidirectional_attention: Optional[bool] = False,\n         **kwargs,\n     ):\n@@ -201,10 +201,16 @@ def __init__(\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n+\n         # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        if rope_scaling is not None:\n-            rope_parameters = {\"sliding_attention\": {\"rope_type\": \"default\"}, \"full_attention\": rope_scaling}\n+        if (rope_scaling := kwargs.pop(\"rope_scaling\", None)) is not None:\n+            if rope_parameters is None:\n+                rope_parameters = {\"sliding_attention\": {\"rope_type\": \"default\"}, \"full_attention\": rope_scaling}\n+            elif \"full_attention\" in rope_parameters:\n+                rope_parameters[\"full_attention\"].update(rope_scaling)\n+            else:\n+                rope_parameters.update(rope_scaling)\n+\n         self.rope_parameters = rope_parameters\n         self.use_bidirectional_attention = use_bidirectional_attention\n         if use_bidirectional_attention:"
        }
    ],
    "stats": {
        "total": 28,
        "additions": 20,
        "deletions": 8
    }
}