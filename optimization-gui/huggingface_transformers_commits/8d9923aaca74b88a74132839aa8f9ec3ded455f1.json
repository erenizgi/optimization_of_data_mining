{
    "author": "BenjaminBossan",
    "message": "ðŸš¨ [v5][PEFT] Bump min version requirement of PEFT to  0.18.0 (#41889)\n\nPEFT is an optional dependency of transformers with a min version of\n0.5.0. However, starting with transformers v5, older PEFT versions will\nnot work anymore (see #41406). The minimum PEFT version will be 0.18.0.\n\nThis PR updates the PEFT integration to require PEFT 0.18.0. This allows\nus to eliminate some obsolete checks and tests that were required for\nbackwards compatibility.",
    "sha": "8d9923aaca74b88a74132839aa8f9ec3ded455f1",
    "files": [
        {
            "sha": "d7216f52bd0dee9e6c8a36cdccf114cd912f9c0b",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d9923aaca74b88a74132839aa8f9ec3ded455f1/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d9923aaca74b88a74132839aa8f9ec3ded455f1/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=8d9923aaca74b88a74132839aa8f9ec3ded455f1",
            "patch": "@@ -132,6 +132,7 @@\n     \"pandas<2.3.0\",  # `datasets` requires `pandas` while `pandas==2.3.0` has issues with CircleCI on 2025/06/05\n     \"packaging>=20.0\",\n     \"parameterized>=0.9\",  # older version of parameterized cause pytest collection to fail on .expand\n+    \"peft>=0.18.0\",\n     \"phonemizer\",\n     \"protobuf\",\n     \"psutil\","
        },
        {
            "sha": "b9d579b25fbfcfa02ffd357d5be2bcd04cf56810",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d9923aaca74b88a74132839aa8f9ec3ded455f1/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d9923aaca74b88a74132839aa8f9ec3ded455f1/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=8d9923aaca74b88a74132839aa8f9ec3ded455f1",
            "patch": "@@ -42,6 +42,7 @@\n     \"pandas\": \"pandas<2.3.0\",\n     \"packaging\": \"packaging>=20.0\",\n     \"parameterized\": \"parameterized>=0.9\",\n+    \"peft\": \"peft>=0.18.0\",\n     \"phonemizer\": \"phonemizer\",\n     \"protobuf\": \"protobuf\",\n     \"psutil\": \"psutil\","
        },
        {
            "sha": "f52b5cf1d58e38728f1950e5aa4f623f5d8c7bd6",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 7,
            "deletions": 69,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d9923aaca74b88a74132839aa8f9ec3ded455f1/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d9923aaca74b88a74132839aa8f9ec3ded455f1/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=8d9923aaca74b88a74132839aa8f9ec3ded455f1",
            "patch": "@@ -12,15 +12,12 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import importlib.metadata\n import inspect\n import json\n import os\n import re\n from typing import Any, Literal\n \n-from packaging import version\n-\n from ..utils import (\n     CONFIG_NAME,\n     cached_file,\n@@ -43,7 +40,7 @@\n     from accelerate.utils import get_balanced_memory, infer_auto_device_map\n \n # Minimum PEFT version supported for the integration\n-MIN_PEFT_VERSION = \"0.5.0\"\n+MIN_PEFT_VERSION = \"0.18.0\"\n \n \n logger = logging.get_logger(__name__)\n@@ -79,7 +76,7 @@ class PeftAdapterMixin:\n     prompt tuning, prompt learning are out of scope as these adapters are not \"injectable\" into a torch module. For\n     using these methods, please refer to the usage guide of PEFT library.\n \n-    With this mixin, if the correct PEFT version is installed, it is possible to:\n+    With this mixin, if the correct PEFT version is installed (>= 0.18.0), it is possible to:\n \n     - Load an adapter stored on a local path or in a remote Hub repository, and inject it in the model\n     - Attach new adapters in the model and train them with Trainer or by your own.\n@@ -157,7 +154,6 @@ def load_adapter(\n                 dicts.\n             low_cpu_mem_usage (`bool`, *optional*, defaults to `False`):\n                 Reduce memory usage while loading the PEFT adapter. This should also speed up the loading process.\n-                Requires PEFT version 0.13.0 or higher.\n             is_trainable (`bool`, *optional*, defaults to `False`):\n                 Whether the adapter should be trainable or not. If `False`, the adapter will be frozen and can only be\n                 used for inference.\n@@ -208,9 +204,6 @@ def load_adapter(\n             hotswap = hotswap_enabled and not_first_adapter\n \n         if hotswap:\n-            min_version_hotswap = \"0.15.0\"\n-            if version.parse(importlib.metadata.version(\"peft\")) < version.parse(min_version_hotswap):\n-                raise ValueError(f\"To hotswap the adapter, you need PEFT >= v{min_version_hotswap}.\")\n             if (not self._hf_peft_config_loaded) or (adapter_name not in self.peft_config):\n                 raise ValueError(\n                     \"To hotswap an adapter, there must already be an existing adapter with the same adapter name.\"\n@@ -223,15 +216,7 @@ def load_adapter(\n         key_mapping = adapter_kwargs.pop(\"key_mapping\", None) if adapter_kwargs is not None else None\n         if key_mapping is None and any(allowed_name in self.__class__.__name__.lower() for allowed_name in VLMS):\n             key_mapping = self._checkpoint_conversion_mapping\n-        if low_cpu_mem_usage:\n-            min_version_lcmu = \"0.13.0\"\n-            if version.parse(importlib.metadata.version(\"peft\")) >= version.parse(min_version_lcmu):\n-                peft_load_kwargs[\"low_cpu_mem_usage\"] = low_cpu_mem_usage\n-            else:\n-                raise ValueError(\n-                    \"The version of PEFT you are using does not support `low_cpu_mem_usage` yet, \"\n-                    f\"please install PEFT >= {min_version_lcmu}.\"\n-                )\n+        peft_load_kwargs[\"low_cpu_mem_usage\"] = low_cpu_mem_usage\n \n         adapter_name = adapter_name if adapter_name is not None else \"default\"\n         if adapter_kwargs is None:\n@@ -427,10 +412,6 @@ def enable_peft_hotswap(\n                   - \"warn\": issue a warning\n                   - \"ignore\": do nothing\n         \"\"\"\n-        min_version_hotswap = \"0.15.0\"\n-        if version.parse(importlib.metadata.version(\"peft\")) < version.parse(min_version_hotswap):\n-            raise ValueError(f\"To hotswap the adapter, you need PEFT >= v{min_version_hotswap}.\")\n-\n         if getattr(self, \"peft_config\", {}):\n             if check_compiled == \"error\":\n                 raise RuntimeError(\"Call `enable_peft_hotswap` before loading the first adapter.\")\n@@ -519,11 +500,7 @@ def set_adapter(self, adapter_name: list[str] | str) -> None:\n \n         for _, module in self.named_modules():\n             if isinstance(module, (BaseTunerLayer, ModulesToSaveWrapper)):\n-                # For backward compatibility with previous PEFT versions\n-                if hasattr(module, \"set_adapter\"):\n-                    module.set_adapter(adapter_name)\n-                else:\n-                    module.active_adapter = adapter_name\n+                module.set_adapter(adapter_name)\n                 _adapters_has_been_set = True\n \n         if not _adapters_has_been_set:\n@@ -548,11 +525,7 @@ def disable_adapters(self) -> None:\n \n         for _, module in self.named_modules():\n             if isinstance(module, (BaseTunerLayer, ModulesToSaveWrapper)):\n-                # The recent version of PEFT need to call `enable_adapters` instead\n-                if hasattr(module, \"enable_adapters\"):\n-                    module.enable_adapters(enabled=False)\n-                else:\n-                    module.disable_adapters = True\n+                module.enable_adapters(enabled=False)\n \n     def enable_adapters(self) -> None:\n         \"\"\"\n@@ -570,11 +543,7 @@ def enable_adapters(self) -> None:\n \n         for _, module in self.named_modules():\n             if isinstance(module, BaseTunerLayer):\n-                # The recent version of PEFT need to call `enable_adapters` instead\n-                if hasattr(module, \"enable_adapters\"):\n-                    module.enable_adapters(enabled=True)\n-                else:\n-                    module.disable_adapters = False\n+                module.enable_adapters(enabled=True)\n \n     def active_adapters(self) -> list[str]:\n         \"\"\"\n@@ -589,9 +558,6 @@ def active_adapters(self) -> list[str]:\n         \"\"\"\n         check_peft_version(min_version=MIN_PEFT_VERSION)\n \n-        if not is_peft_available():\n-            raise ImportError(\"PEFT is not available. Please install PEFT to use this function: `pip install peft`.\")\n-\n         if not self._hf_peft_config_loaded:\n             raise ValueError(\"No adapter loaded. Please load an adapter first.\")\n \n@@ -703,39 +669,11 @@ def delete_adapter(self, adapter_names: list[str] | str) -> None:\n         \"\"\"\n \n         check_peft_version(min_version=MIN_PEFT_VERSION)\n-        min_version_delete_adapter = \"0.18.0\"\n \n         if not self._hf_peft_config_loaded:\n             raise ValueError(\"No adapter loaded. Please load an adapter first.\")\n \n-        # TODO: delete old version once support for PEFT < 0.18.0 is dropped\n-        def old_delete_adapter(model, adapter_name, prefix=None):\n-            from peft.tuners.tuners_utils import BaseTunerLayer\n-            from peft.utils import ModulesToSaveWrapper\n-\n-            has_modules_to_save = False\n-            for module in model.modules():\n-                if isinstance(module, ModulesToSaveWrapper):\n-                    has_modules_to_save |= True\n-                    continue\n-                if isinstance(module, BaseTunerLayer):\n-                    if hasattr(module, \"delete_adapter\"):\n-                        module.delete_adapter(adapter_name)\n-                    else:\n-                        raise ValueError(\n-                            \"The version of PEFT you are using is not compatible, please use a version that is greater than 0.6.1\"\n-                        )\n-\n-            if has_modules_to_save:\n-                logger.warning(\n-                    \"The deleted adapter contains modules_to_save, which could not be deleted. For this to work, PEFT version \"\n-                    f\">= {min_version_delete_adapter} is required.\"\n-                )\n-\n-        if version.parse(importlib.metadata.version(\"peft\")) >= version.parse(min_version_delete_adapter):\n-            from peft.functional import delete_adapter\n-        else:\n-            delete_adapter = old_delete_adapter\n+        from peft.functional import delete_adapter\n \n         if isinstance(adapter_names, str):\n             adapter_names = [adapter_names]"
        },
        {
            "sha": "709553868d46c9b51cdf3fd9fe74078f31ba972b",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 34,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d9923aaca74b88a74132839aa8f9ec3ded455f1/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d9923aaca74b88a74132839aa8f9ec3ded455f1/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=8d9923aaca74b88a74132839aa8f9ec3ded455f1",
            "patch": "@@ -63,6 +63,7 @@\n from .hyperparameter_search import ALL_HYPERPARAMETER_SEARCH_BACKENDS, default_hp_search_backend\n from .image_processing_utils import BaseImageProcessor\n from .integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available\n+from .integrations.peft import MIN_PEFT_VERSION\n from .integrations.tpu import tpu_spmd_dataloader\n from .modelcard import TrainingSummary\n from .modeling_utils import PreTrainedModel, unwrap_model\n@@ -205,7 +206,7 @@\n     from .trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n \n if is_peft_available():\n-    from peft import PeftModel\n+    from peft import PeftMixedModel, PeftModel\n \n if is_accelerate_available():\n     from accelerate import Accelerator, skip_first_batches\n@@ -228,12 +229,7 @@\n \n def _is_peft_model(model):\n     if is_peft_available():\n-        classes_to_check = (PeftModel,)\n-        # Here we also check if the model is an instance of `PeftMixedModel` introduced in peft>=0.7.0: https://github.com/huggingface/transformers/pull/28321\n-        if version.parse(importlib.metadata.version(\"peft\")) >= version.parse(\"0.7.0\"):\n-            from peft import PeftMixedModel\n-\n-            classes_to_check = (*classes_to_check, PeftMixedModel)\n+        classes_to_check = (PeftModel, PeftMixedModel)\n         return isinstance(model, classes_to_check)\n     return False\n \n@@ -2807,20 +2803,13 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n \n         # Load adapters following PR # 24096\n         elif _is_peft_model(model):\n-            # If train a model using PEFT & LoRA, assume that adapter have been saved properly.\n-            # TODO: in the future support only specific min PEFT versions\n-            if (hasattr(model, \"active_adapter\") or hasattr(model, \"active_adapters\")) and hasattr(\n-                model, \"load_adapter\"\n-            ):\n+            # If training a model using PEFT, assume that adapter have been saved properly.\n+            if hasattr(model, \"active_adapters\") and hasattr(model, \"load_adapter\"):\n                 if os.path.exists(resume_from_checkpoint):\n-                    # For BC for older PEFT versions\n-                    if hasattr(model, \"active_adapters\"):\n-                        active_adapters = model.active_adapters\n-                        if len(active_adapters) > 1:\n-                            logger.warning(\"Multiple active adapters detected will only consider the first adapter\")\n-                        active_adapter = active_adapters[0]\n-                    else:\n-                        active_adapter = model.active_adapter\n+                    active_adapters = model.active_adapters\n+                    if len(active_adapters) > 1:\n+                        logger.warning(\"Multiple active adapters detected will only consider the first adapter\")\n+                    active_adapter = active_adapters[0]\n \n                     if adapter_subdirs:\n                         for subdir_name in adapter_subdirs:\n@@ -2836,7 +2825,7 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                         \"Check some examples here: https://github.com/huggingface/peft/issues/96\"\n                     )\n             else:\n-                logger.warning(\"Could not load adapter model, make sure to have `peft>=0.3.0` installed\")\n+                logger.warning(f\"Could not load adapter model, make sure to have PEFT >= {MIN_PEFT_VERSION} installed\")\n         else:\n             # We load the sharded checkpoint\n             load_result = load_sharded_checkpoint(\n@@ -2883,18 +2872,11 @@ def _load_best_model(self):\n                 )\n             else:\n                 if _is_peft_model(model):\n-                    # If train a model using PEFT & LoRA, assume that adapter have been saved properly.\n-                    # TODO: in the future support only specific min PEFT versions\n-                    if (hasattr(model, \"active_adapter\") or hasattr(model, \"active_adapters\")) and hasattr(\n-                        model, \"load_adapter\"\n-                    ):\n-                        # For BC for older PEFT versions\n-                        if hasattr(model, \"active_adapters\"):\n-                            active_adapter = model.active_adapters[0]\n-                            if len(model.active_adapters) > 1:\n-                                logger.warning(\"Detected multiple active adapters, will only consider the first one\")\n-                        else:\n-                            active_adapter = model.active_adapter\n+                    # If training a model using PEFT, assume that adapter have been saved properly.\n+                    if hasattr(model, \"active_adapters\") and hasattr(model, \"load_adapter\"):\n+                        active_adapter = model.active_adapters[0]\n+                        if len(model.active_adapters) > 1:\n+                            logger.warning(\"Detected multiple active adapters, will only consider the first one\")\n \n                         if os.path.exists(best_adapter_model_path) or os.path.exists(best_safe_adapter_model_path):\n                             try:\n@@ -2925,7 +2907,9 @@ def _load_best_model(self):\n                             )\n                             has_been_loaded = False\n                     else:\n-                        logger.warning(\"Could not load adapter model, make sure to have `peft>=0.3.0` installed\")\n+                        logger.warning(\n+                            f\"Could not load adapter model, make sure to have PEFT >= {MIN_PEFT_VERSION} installed\"\n+                        )\n                         has_been_loaded = False\n                 else:\n                     # We load the model state dict on the CPU to avoid an OOM error."
        },
        {
            "sha": "99062bf6502f8f2cf94bbfc3d53f023f41161afc",
            "filename": "src/transformers/utils/peft_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d9923aaca74b88a74132839aa8f9ec3ded455f1/src%2Ftransformers%2Futils%2Fpeft_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d9923aaca74b88a74132839aa8f9ec3ded455f1/src%2Ftransformers%2Futils%2Fpeft_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fpeft_utils.py?ref=8d9923aaca74b88a74132839aa8f9ec3ded455f1",
            "patch": "@@ -113,7 +113,4 @@ def check_peft_version(min_version: str) -> None:\n     is_peft_version_compatible = version.parse(importlib.metadata.version(\"peft\")) >= version.parse(min_version)\n \n     if not is_peft_version_compatible:\n-        raise ValueError(\n-            f\"The version of PEFT you are using is not compatible, please use a version that is greater\"\n-            f\" than {min_version}\"\n-        )\n+        raise ValueError(f\"The version of PEFT you are using is not compatible, please use a version >= {min_version}\")"
        },
        {
            "sha": "1e0e2335067da6d8842a860966f0460a8e8caa83",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 8,
            "deletions": 62,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d9923aaca74b88a74132839aa8f9ec3ded455f1/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d9923aaca74b88a74132839aa8f9ec3ded455f1/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=8d9923aaca74b88a74132839aa8f9ec3ded455f1",
            "patch": "@@ -12,15 +12,13 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import gc\n-import importlib\n import os\n import re\n import tempfile\n import unittest\n \n from datasets import Dataset, DatasetDict\n from huggingface_hub import hf_hub_download\n-from packaging import version\n from torch import nn\n \n from transformers import (\n@@ -432,10 +430,6 @@ def test_delete_adapter_with_modules_to_save(self):\n         \"\"\"\n         Ensure that modules_to_save is accounted for when deleting an adapter.\n         \"\"\"\n-        min_version_delete_adapter = \"0.18.0\"\n-        if version.parse(importlib.metadata.version(\"peft\")) < version.parse(min_version_delete_adapter):\n-            self.skipTest(\"Correctly deleting modules_to_save only works with PEFT >= 0.18.0\")\n-\n         from peft import LoraConfig\n \n         # the test assumes a specific model architecture, so only test this one:\n@@ -456,40 +450,6 @@ def test_delete_adapter_with_modules_to_save(self):\n         self.assertFalse(\"adapter_1\" in model.lm_head.modules_to_save)\n         self.assertFalse(model.lm_head.modules_to_save)  # i.e. empty ModuleDict\n \n-    def test_delete_adapter_with_modules_to_save_old_peft_warns(self):\n-        \"\"\"\n-        When PEFT < 0.18.0 is being used, modules_to_save are not deleted but the user should get a warning.\n-        \"\"\"\n-        from peft import LoraConfig\n-\n-        peft_ge_018 = version.parse(importlib.metadata.version(\"peft\")) >= version.parse(\"0.18.0\")\n-        logger = logging.get_logger(\"transformers.integrations.peft\")\n-        warn_msg = \"The deleted adapter contains modules_to_save\"\n-        # the test assumes a specific model architecture, so only test this one:\n-        model_id = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n-\n-        # first a sanity check: when there is no modules_to_save, there is also no warning\n-        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n-        peft_config_0 = LoraConfig(init_lora_weights=False)\n-        model.add_adapter(peft_config_0, adapter_name=\"adapter_1\")\n-        with CaptureLogger(logger) as cl:\n-            model.delete_adapter(\"adapter_1\")\n-        assert warn_msg not in cl.out\n-\n-        # now test a model with modules_to_save\n-        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n-        peft_config_1 = LoraConfig(init_lora_weights=False, modules_to_save=[\"lm_head\"])\n-        model.add_adapter(peft_config_1, adapter_name=\"adapter_1\")\n-        with CaptureLogger(logger) as cl:\n-            model.delete_adapter(\"adapter_1\")\n-\n-        if peft_ge_018:\n-            self.assertTrue(\"adapter_1\" not in model.lm_head.modules_to_save)\n-            assert warn_msg not in cl.out\n-        else:\n-            self.assertTrue(\"adapter_1\" in model.lm_head.modules_to_save)\n-            assert warn_msg in cl.out\n-\n     @require_torch_accelerator\n     @require_bitsandbytes\n     def test_peft_from_pretrained_kwargs(self):\n@@ -653,9 +613,6 @@ def test_peft_add_adapter_with_state_dict_low_cpu_mem_usage(self):\n         \"\"\"\n         from peft import LoraConfig\n \n-        min_version_lcmu = \"0.13.0\"\n-        is_lcmu_supported = version.parse(importlib.metadata.version(\"peft\")) >= version.parse(min_version_lcmu)\n-\n         for model_id, peft_model_id in zip(self.transformers_test_model_ids, self.peft_test_model_ids):\n             for transformers_class in self.transformers_test_model_classes:\n                 model = transformers_class.from_pretrained(model_id).to(torch_device)\n@@ -670,25 +627,14 @@ def test_peft_add_adapter_with_state_dict_low_cpu_mem_usage(self):\n                     adapter_state_dict=dummy_state_dict, peft_config=peft_config, low_cpu_mem_usage=False\n                 )\n \n-                if is_lcmu_supported:\n-                    # if supported, this should not raise an error\n-                    model.load_adapter(\n-                        adapter_state_dict=dummy_state_dict,\n-                        adapter_name=\"other\",\n-                        peft_config=peft_config,\n-                        low_cpu_mem_usage=True,\n-                    )\n-                    # after loading, no meta device should be remaining\n-                    self.assertFalse(any((p.device.type == \"meta\") for p in model.parameters()))\n-                else:\n-                    err_msg = r\"The version of PEFT you are using does not support `low_cpu_mem_usage` yet\"\n-                    with self.assertRaisesRegex(ValueError, err_msg):\n-                        model.load_adapter(\n-                            adapter_state_dict=dummy_state_dict,\n-                            adapter_name=\"other\",\n-                            peft_config=peft_config,\n-                            low_cpu_mem_usage=True,\n-                        )\n+                model.load_adapter(\n+                    adapter_state_dict=dummy_state_dict,\n+                    adapter_name=\"other\",\n+                    peft_config=peft_config,\n+                    low_cpu_mem_usage=True,\n+                )\n+                # after loading, no meta device should be remaining\n+                self.assertFalse(any((p.device.type == \"meta\") for p in model.parameters()))\n \n     def test_peft_from_pretrained_hub_kwargs(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 205,
        "additions": 36,
        "deletions": 169
    }
}