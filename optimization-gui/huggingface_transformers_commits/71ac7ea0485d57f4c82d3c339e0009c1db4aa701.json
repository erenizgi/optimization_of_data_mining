{
    "author": "gante",
    "message": "[tests] update `test_past_key_values_format` and delete overwrites (#40701)\n\n* tmp\n\n* rm some overwrites",
    "sha": "71ac7ea0485d57f4c82d3c339e0009c1db4aa701",
    "files": [
        {
            "sha": "37bbf18b4e1dfd5fadb4613d7160486b82924204",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 6,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=71ac7ea0485d57f4c82d3c339e0009c1db4aa701",
            "patch": "@@ -1239,14 +1239,30 @@ def get_text_config(self, decoder=None, encoder=None) -> \"PretrainedConfig\":\n         if not return_both and len(valid_text_config_names) == 0 and config_to_return.is_encoder_decoder:\n             config_to_return = copy.deepcopy(config_to_return)\n             prefix_to_discard = \"encoder\" if decoder else \"decoder\"\n+            prefix_to_keep = \"decoder\" if decoder else \"encoder\"\n             for key in config_to_return.to_dict():\n-                if key.startswith(prefix_to_discard):\n+                # NOTE: We don't want to discard the key if it is mapped from a different attribute name at read time\n+                if key.startswith(prefix_to_discard) and key not in config_to_return.attribute_map.values():\n                     delattr(config_to_return, key)\n-            # old encoder/decoder models may use \"encoder_layers\"/\"decoder_layers\" instead of \"num_hidden_layers\"\n-            if decoder and hasattr(config_to_return, \"decoder_layers\"):\n-                config_to_return.num_hidden_layers = config_to_return.decoder_layers\n-            elif encoder and hasattr(config_to_return, \"encoder_layers\"):\n-                config_to_return.num_hidden_layers = config_to_return.encoder_layers\n+                if key.startswith(prefix_to_keep):\n+                    # [encoder/decoder]_layers -> num_hidden_layers\n+                    if key == prefix_to_keep + \"_layers\":\n+                        new_key = \"num_hidden_layers\"\n+                    # [encoder/decoder]_attention_heads -> num_attention_heads\n+                    elif key == prefix_to_keep + \"_attention_heads\":\n+                        new_key = \"num_attention_heads\"\n+                    # e.g. encoder_hidden_act -> hidden_act\n+                    else:\n+                        new_key = key[len(prefix_to_keep) + 1 :]\n+\n+                    # Does the class map the new key into a different attribute name at read time? if so, let's write\n+                    # into that attribute instead\n+                    if new_key in config_to_return.attribute_map:\n+                        new_key = config_to_return.attribute_map[new_key]\n+\n+                    value = getattr(config_to_return, key)\n+                    delattr(config_to_return, key)\n+                    setattr(config_to_return, new_key, value)\n \n         return config_to_return\n "
        },
        {
            "sha": "6d5613dd2aedf2d52904d296e5aef538197efecd",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 30,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=71ac7ea0485d57f4c82d3c339e0009c1db4aa701",
            "patch": "@@ -1014,7 +1014,8 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n             config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n \n             # 1. If it doesn't support cache, skip the test\n-            if not hasattr(config.get_text_config(), \"use_cache\"):\n+            decoder_config = config.get_text_config(decoder=True)\n+            if not hasattr(decoder_config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             model = model_class(config).to(torch_device)\n@@ -1030,39 +1031,19 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n             past_kv = outputs[\"past_key_values\"]\n             is_legacy_cache = not isinstance(past_kv, Cache)\n \n-            text_config = config.get_text_config()\n-            num_decoder_layers = (\n-                getattr(text_config, \"decoder_layers\", None)\n-                or getattr(text_config, \"num_decoder_layers\", None)\n-                or text_config.num_hidden_layers\n-            )\n-\n+            num_decoder_layers = decoder_config.num_hidden_layers\n             if custom_all_cache_shapes is None:\n-                num_query_attention_heads = getattr(\n-                    text_config, \"decoder_attention_heads\", text_config.num_attention_heads\n-                )\n-                embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n-                per_head_embed_dim = embed_dim // num_query_attention_heads\n-                num_key_value_heads = (\n-                    text_config.num_key_value_heads\n-                    if getattr(text_config, \"num_key_value_heads\", None) is not None\n-                    else num_query_attention_heads\n+                num_query_attention_heads = decoder_config.num_attention_heads\n+                embed_dim = getattr(decoder_config, \"d_model\", decoder_config.hidden_size)\n+                per_head_embed_dim = (\n+                    getattr(decoder_config, \"head_dim\", None) or embed_dim // num_query_attention_heads\n                 )\n+                num_key_value_heads = getattr(decoder_config, \"num_key_value_heads\", None) or num_query_attention_heads\n                 if config.is_encoder_decoder:\n-                    encoder_num_attention_heads = (\n-                        text_config.encoder_attention_heads\n-                        if hasattr(text_config, \"encoder_attention_heads\")\n-                        else text_config.num_attention_heads\n-                    )\n-                    encoder_per_head_embed_dim = embed_dim // encoder_num_attention_heads\n                     batch_size, seq_length = inputs[\"decoder_input_ids\"].shape[:2]\n                     # The sequence length for the encoder K V depends on the model. Since it is not manipulated in\n                     # autoregressive generation, we're keeping the test general and not checking the 3rd dim\n-                    default_cross_attention_shape = (\n-                        batch_size,\n-                        encoder_num_attention_heads,\n-                        encoder_per_head_embed_dim,\n-                    )\n+                    default_cross_attention_shape = (batch_size, num_key_value_heads, per_head_embed_dim)\n                     default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n                     all_cache_shapes = [\n                         [\n@@ -1118,9 +1099,13 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n             # 3.2. Decoder-only checks\n             else:\n                 num_cache_decoder_layers = len(past_kv)\n-                self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n+                self.assertEqual(\n+                    # we may have skipped layers\n+                    num_cache_decoder_layers + getattr(decoder_config, \"num_kv_shared_layers\", 0),\n+                    num_decoder_layers,\n+                )\n \n-                for i in range(num_decoder_layers):\n+                for i in range(num_cache_decoder_layers):\n                     if is_legacy_cache:\n                         self.assertEqual(len(past_kv[0]), 2)  # legacy check: confirm number of elements in tuple\n "
        },
        {
            "sha": "900bb0cef73d714aee63cd6fd19f25086eedd811",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=71ac7ea0485d57f4c82d3c339e0009c1db4aa701",
            "patch": "@@ -517,10 +517,6 @@ def test_generate_continue_from_past_key_values(self):\n                         )\n                     )\n \n-    @unittest.skip(reason=\"Indirectly checked in Dia through the generate methods.\")\n-    def test_past_key_values_format(self, custom_all_cache_shapes=None):\n-        pass\n-\n     @unittest.skip(reason=\"Indirectly checked in Dia through the generate methods.\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "ccfe8ec6a36511aeead8eca1a2fe2f353d242b70",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=71ac7ea0485d57f4c82d3c339e0009c1db4aa701",
            "patch": "@@ -28,7 +28,6 @@\n     AutoModelForCausalLM,\n     AutoProcessor,\n     AutoTokenizer,\n-    Cache,\n     Gemma3nAudioConfig,\n     Gemma3nAudioFeatureExtractor,\n     Gemma3nConfig,\n@@ -551,141 +550,6 @@ def test_generate_with_static_cache(self):\n                 dynamic_cache_generation = model.generate(**generation_kwargs, **inputs_dict)\n                 self.assertTrue(has_similar_generate_outputs(dynamic_cache_generation, static_cache_generation))\n \n-    @pytest.mark.generate\n-    def test_past_key_values_format(self, custom_all_cache_shapes=None):\n-        \"\"\"\n-        Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test, or pass the\n-        expected cache shapes.\n-        Having a standard KV cache format is important for a consistent API (and for advanced generation methods).\n-        \"\"\"\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            # 1. If it doesn't support cache, skip the test\n-            if not hasattr(config.get_text_config(), \"use_cache\"):\n-                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n-\n-            model = model_class(config).to(torch_device)\n-            model = model.eval()\n-            if \"use_cache\" not in inputs:\n-                inputs[\"use_cache\"] = True\n-            outputs = model(**inputs)\n-\n-            if \"past_key_values\" not in outputs:\n-                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n-\n-            # 2. retrieve the KV cache and compute its default expected shapes (if no custom shapes are provided)\n-            past_kv = outputs[\"past_key_values\"]\n-            is_legacy_cache = not isinstance(past_kv, Cache)\n-\n-            text_config = config.get_text_config()\n-            num_decoder_layers = (\n-                getattr(text_config, \"decoder_layers\", None)\n-                or getattr(text_config, \"num_decoder_layers\", None)\n-                or text_config.num_hidden_layers\n-            )\n-\n-            if custom_all_cache_shapes is None:\n-                num_query_attention_heads = getattr(\n-                    text_config, \"decoder_attention_heads\", text_config.num_attention_heads\n-                )\n-                embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n-                per_head_embed_dim = embed_dim // num_query_attention_heads\n-                num_key_value_heads = (\n-                    text_config.num_key_value_heads\n-                    if getattr(text_config, \"num_key_value_heads\", None) is not None\n-                    else num_query_attention_heads\n-                )\n-                if config.is_encoder_decoder:\n-                    encoder_num_attention_heads = (\n-                        text_config.encoder_attention_heads\n-                        if hasattr(text_config, \"encoder_attention_heads\")\n-                        else text_config.num_attention_heads\n-                    )\n-                    encoder_per_head_embed_dim = embed_dim // encoder_num_attention_heads\n-                    batch_size, seq_length = inputs[\"decoder_input_ids\"].shape[:2]\n-                    # The sequence length for the encoder K V depends on the model. Since it is not manipulated in\n-                    # autoregressive generation, we're keeping the test general and not checking the 3rd dim\n-                    default_cross_attention_shape = (\n-                        batch_size,\n-                        encoder_num_attention_heads,\n-                        encoder_per_head_embed_dim,\n-                    )\n-                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n-                    all_cache_shapes = [\n-                        [\n-                            default_self_attention_shape,\n-                            default_self_attention_shape,\n-                            default_cross_attention_shape,\n-                            default_cross_attention_shape,\n-                        ]\n-                        for _ in range(num_decoder_layers)\n-                    ]\n-                else:\n-                    batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n-                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n-                    all_cache_shapes = [\n-                        [default_self_attention_shape, default_self_attention_shape] for _ in range(num_decoder_layers)\n-                    ]\n-\n-            else:\n-                all_cache_shapes = custom_all_cache_shapes\n-\n-            # 3. Check cache shapes\n-            # 3.1. Encoder-Decoder checks\n-            if config.is_encoder_decoder:\n-                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv.self_attention_cache)\n-                self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n-\n-                for i in range(num_decoder_layers):\n-                    if is_legacy_cache:\n-                        self.assertEqual(len(past_kv[0]), 4)  # legacy check: confirm number of elements in tuple\n-\n-                    # Self attention\n-                    self_attention_layer_keys = (\n-                        past_kv[i][0] if is_legacy_cache else past_kv.self_attention_cache.layers[i].keys\n-                    )\n-                    self_attention_layer_values = (\n-                        past_kv[i][1] if is_legacy_cache else past_kv.self_attention_cache.layers[i].values\n-                    )\n-                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n-\n-                    # Cross attention (ignore 3rd dim, see default shape preparation)\n-                    cross_attention_layer_keys = (\n-                        past_kv[i][2] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].keys\n-                    )\n-                    cross_attention_layer_values = (\n-                        past_kv[i][3] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].values\n-                    )\n-                    cross_attention_layer_keys = cross_attention_layer_keys[:, :, 0, :]\n-                    cross_attention_layer_values = cross_attention_layer_values[:, :, 0, :]\n-                    self.assertEqual(cross_attention_layer_keys.shape, all_cache_shapes[i][2])\n-                    self.assertEqual(cross_attention_layer_values.shape, all_cache_shapes[i][3])\n-\n-            # 3.2. Decoder-only checks\n-            else:\n-                num_cache_decoder_layers = len(past_kv)\n-                self.assertEqual(num_cache_decoder_layers, num_decoder_layers - text_config.num_kv_shared_layers)\n-\n-                for i in range(num_decoder_layers - text_config.num_kv_shared_layers):\n-                    if is_legacy_cache:\n-                        self.assertEqual(len(past_kv[0]), 2)  # legacy check: confirm number of elements in tuple\n-\n-                    # Self attention\n-                    if is_legacy_cache:\n-                        self_attention_layer_keys = past_kv[i][0]\n-                        self_attention_layer_values = past_kv[i][1]\n-                    elif getattr(past_kv, \"layers\", None) is None:\n-                        # Cache is lot layered (i.e, Mamba derivatives)\n-                        self_attention_layer_keys = past_kv.key_cache[i]\n-                        self_attention_layer_values = past_kv.value_cache[i]\n-                    else:\n-                        self_attention_layer_keys = past_kv.layers[i].keys\n-                        self_attention_layer_values = past_kv.layers[i].values\n-                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n-\n \n class Gemma3nVision2TextModelTester:\n     text_config = {\"activation_sparsity_pattern\": None}"
        },
        {
            "sha": "59577106b06917b009cf76d2d36b9f93c7f7465f",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=71ac7ea0485d57f4c82d3c339e0009c1db4aa701",
            "patch": "@@ -177,12 +177,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    @unittest.skip(\n-        reason=\"GotOcr2's language backbone is Qwen2 which uses GQA so the KV cache is a non standard format\"\n-    )\n-    def test_past_key_values_format(self):\n-        pass\n-\n \n @require_torch\n class GotOcr2IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "3b7ce7954f33bc291fe5f1946960bb6e939ca4e8",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=71ac7ea0485d57f4c82d3c339e0009c1db4aa701",
            "patch": "@@ -20,7 +20,6 @@\n \n from transformers import SpeechT5Config, SpeechT5HifiGanConfig\n from transformers.testing_utils import (\n-    is_flaky,\n     is_torch_available,\n     require_deterministic_for_xpu,\n     require_sentencepiece,\n@@ -728,10 +727,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @is_flaky(max_attempts=5, description=\"Flaky for some input configurations.\")\n-    def test_past_key_values_format(self):\n-        super().test_past_key_values_format()\n-\n     # overwrite from test_modeling_common\n     def _mock_init_weights(self, module):\n         if hasattr(module, \"weight\") and module.weight is not None:"
        },
        {
            "sha": "b44cc9f79054b6dcffe5a455d23ae13a8cb9cc59",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 121,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71ac7ea0485d57f4c82d3c339e0009c1db4aa701/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=71ac7ea0485d57f4c82d3c339e0009c1db4aa701",
            "patch": "@@ -45,7 +45,6 @@\n         T5GemmaForTokenClassification,\n         T5GemmaModel,\n     )\n-    from transformers.cache_utils import Cache\n \n \n class T5GemmaModelTester:\n@@ -992,126 +991,6 @@ def test_attention_outputs(self):\n                     [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n                 )\n \n-    # Based on tests.generation.test_utils.GenerationTesterMixin.test_past_key_values_format\n-    # Adjust encoder attention number for cross-attention caching and update attention head dimension\n-    @pytest.mark.generate\n-    def test_past_key_values_format(self, custom_all_cache_shapes=None):\n-        \"\"\"\n-        Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test, or pass the\n-        expected cache shapes.\n-        Having a standard KV cache format is important for a consistent API (and for advanced generation methods).\n-        \"\"\"\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            # 1. If it doesn't support cache, skip the test\n-            if not hasattr(config.get_text_config(), \"use_cache\"):\n-                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n-\n-            model = model_class(config).to(torch_device)\n-            model = model.eval()\n-            if \"use_cache\" not in inputs:\n-                inputs[\"use_cache\"] = True\n-            outputs = model(**inputs)\n-\n-            if \"past_key_values\" not in outputs:\n-                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n-\n-            # 2. retrieve the KV cache and compute its default expected shapes (if no custom shapes are provided)\n-            past_kv = outputs[\"past_key_values\"]\n-            is_legacy_cache = not isinstance(past_kv, Cache)\n-\n-            text_config = config.get_text_config().decoder\n-            num_decoder_layers = text_config.num_hidden_layers\n-\n-            if custom_all_cache_shapes is None:\n-                num_query_attention_heads = getattr(\n-                    text_config, \"decoder_attention_heads\", text_config.num_attention_heads\n-                )\n-                per_head_embed_dim = text_config.head_dim\n-                num_key_value_heads = (\n-                    text_config.num_key_value_heads\n-                    if getattr(text_config, \"num_key_value_heads\", None) is not None\n-                    else num_query_attention_heads\n-                )\n-                if config.is_encoder_decoder:\n-                    encoder_num_attention_heads = num_key_value_heads\n-                    encoder_per_head_embed_dim = per_head_embed_dim\n-                    batch_size, seq_length = inputs[\"decoder_input_ids\"].shape[:2]\n-                    # The sequence length for the encoder K V depends on the model. Since it is not manipulated in\n-                    # autoregressive generation, we're keeping the test general and not checking the 3rd dim\n-                    default_cross_attention_shape = (\n-                        batch_size,\n-                        encoder_num_attention_heads,\n-                        encoder_per_head_embed_dim,\n-                    )\n-                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n-                    all_cache_shapes = [\n-                        [\n-                            default_self_attention_shape,\n-                            default_self_attention_shape,\n-                            default_cross_attention_shape,\n-                            default_cross_attention_shape,\n-                        ]\n-                        for _ in range(num_decoder_layers)\n-                    ]\n-                else:\n-                    batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n-                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n-                    all_cache_shapes = [\n-                        [default_self_attention_shape, default_self_attention_shape] for _ in range(num_decoder_layers)\n-                    ]\n-\n-            else:\n-                all_cache_shapes = custom_all_cache_shapes\n-\n-            # 3. Check cache shapes\n-            # 3.1. Encoder-Decoder checks\n-            if config.is_encoder_decoder:\n-                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv.self_attention_cache)\n-                self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n-\n-                for i in range(num_decoder_layers):\n-                    if is_legacy_cache:\n-                        self.assertEqual(len(past_kv[0]), 5)  # legacy check: confirm number of elements in tuple\n-\n-                    # Self attention\n-                    self_attention_layer_keys = (\n-                        past_kv[i][0] if is_legacy_cache else past_kv.self_attention_cache.layers[i].keys\n-                    )\n-                    self_attention_layer_values = (\n-                        past_kv[i][1] if is_legacy_cache else past_kv.self_attention_cache.layers[i].values\n-                    )\n-                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n-\n-                    # Cross attention (ignore 3rd dim, see default shape preparation)\n-                    cross_attention_layer_keys = (\n-                        past_kv[i][2] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].keys\n-                    )\n-                    cross_attention_layer_values = (\n-                        past_kv[i][3] if is_legacy_cache else past_kv.cross_attention_cache.layers[i].values\n-                    )\n-                    cross_attention_layer_keys = cross_attention_layer_keys[:, :, 0, :]\n-                    cross_attention_layer_values = cross_attention_layer_values[:, :, 0, :]\n-                    self.assertEqual(cross_attention_layer_keys.shape, all_cache_shapes[i][2])\n-                    self.assertEqual(cross_attention_layer_values.shape, all_cache_shapes[i][3])\n-\n-            # 3.2. Decoder-only checks\n-            else:\n-                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv)\n-                self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n-\n-                for i in range(num_decoder_layers):\n-                    if is_legacy_cache:\n-                        self.assertEqual(len(past_kv[0]), 2)  # legacy check: confirm number of elements in tuple\n-\n-                    # Self attention\n-                    self_attention_layer_keys = past_kv[i][0] if is_legacy_cache else past_kv.layers[i].keys\n-                    self_attention_layer_values = past_kv[i][1] if is_legacy_cache else past_kv.layers[i].values\n-                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n-\n     @unittest.skip(\"Mismatch issue doesn't exist in T5Gemma.\")\n     def test_load_with_mismatched_shapes(self):\n         pass"
        }
    ],
    "stats": {
        "total": 345,
        "additions": 37,
        "deletions": 308
    }
}