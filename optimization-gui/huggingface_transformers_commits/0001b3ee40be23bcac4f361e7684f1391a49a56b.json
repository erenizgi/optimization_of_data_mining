{
    "author": "MekkCyber",
    "message": "[kernels] Add user_agent to track kernels metrics (#41689)\n\n* add wrapper\n\n* fix style\n\n* change the name to get_kernel\n\n* nit",
    "sha": "0001b3ee40be23bcac4f361e7684f1391a49a56b",
    "files": [
        {
            "sha": "98f901c79cb9554f0f3e57d51c861c39b80a0cc3",
            "filename": "src/transformers/integrations/eetq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feetq.py?ref=0001b3ee40be23bcac4f361e7684f1391a49a56b",
            "patch": "@@ -97,7 +97,7 @@ def replace_with_eetq_linear(model, modules_to_not_convert: list[str] | None = N\n             Names of the modules to not convert in `EetqLinear`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n     \"\"\"\n-    from kernels import get_kernel\n+    from .hub_kernels import get_kernel\n \n     global eetq_kernels_hub\n     eetq_kernels_hub = get_kernel(\"kernels-community/quantization-eetq\")"
        },
        {
            "sha": "8209496f09916ea0ece43153150c50d88c225ac1",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=0001b3ee40be23bcac4f361e7684f1391a49a56b",
            "patch": "@@ -257,7 +257,7 @@ def forward(self, hidden_states):\n @lru_cache(maxsize=1)\n def get_quantize_fp8_per_row():\n     if _is_torch_xpu_available:\n-        from kernels import get_kernel\n+        from .hub_kernels import get_kernel\n \n         return get_kernel(\"kernels-community/fp8-fbgemm\").quantize_fp8_per_row\n     return torch.ops.fbgemm.quantize_fp8_per_row"
        },
        {
            "sha": "54efc5181430a34ae6eea8bcf52fb7f3f857ed8f",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 22,
            "deletions": 4,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=0001b3ee40be23bcac4f361e7684f1391a49a56b",
            "patch": "@@ -11,11 +11,14 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import importlib.metadata\n import os\n import re\n from collections.abc import Callable\n from types import ModuleType\n \n+from packaging import version as pkg_version\n+\n from ..utils import ENV_VARS_TRUE_VALUES, logging\n from ..utils.import_utils import is_kernels_available\n from .flash_attention import flash_attention_forward\n@@ -28,10 +31,12 @@\n         Device,\n         LayerRepository,\n         Mode,\n-        get_kernel,\n         register_kernel_mapping,\n         replace_kernel_forward_from_hub,\n     )\n+    from kernels import (\n+        get_kernel as get_kernel_hub,\n+    )\n     from kernels import (\n         use_kernel_forward_from_hub as _kernels_use_kernel_forward_from_hub,\n     )\n@@ -340,8 +345,6 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n         mapping[kernel_name] = None\n         return None\n     if _kernels_available:\n-        from kernels import get_kernel\n-\n         try:\n             repo_id = _HUB_KERNEL_MAPPING[kernel_name][\"repo_id\"]\n             revision = _HUB_KERNEL_MAPPING[kernel_name].get(\"revision\", None)\n@@ -381,6 +384,20 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n     return mapping[kernel_name]\n \n \n+def get_kernel(kernel_name: str, revision: str | None = None, version: str | None = None) -> ModuleType:\n+    from .. import __version__\n+\n+    user_agent = {\"framework\": \"transformers\", \"version\": __version__, \"repo_id\": kernel_name}\n+    if _kernels_available:\n+        kernels_version = importlib.metadata.version(\"kernels\")\n+        if pkg_version.parse(kernels_version) >= pkg_version.parse(\"0.10.4\"):\n+            return get_kernel_hub(kernel_name, revision=revision, version=version, user_agent=user_agent)\n+        else:\n+            return get_kernel_hub(kernel_name, revision=revision)\n+    else:\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+\n+\n def use_kernelized_func(module_names: list[Callable] | Callable):\n     \"\"\"\n     This decorator attaches the target function as an attribute of the module.\n@@ -415,5 +432,6 @@ def new_init(self, *args, **kwargs):\n     \"register_kernel_mapping_transformers\",\n     \"replace_kernel_forward_from_hub\",\n     \"lazy_load_kernel\",\n+    \"get_kernel\",\n     \"use_kernelized_func\",\n-]\n+]  # type: ignore"
        },
        {
            "sha": "9923af56d1a36d4dee4b163f6cca81938f2c8a1c",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=0001b3ee40be23bcac4f361e7684f1391a49a56b",
            "patch": "@@ -610,7 +610,7 @@ def replace_with_mxfp4_linear(model, quantization_config=None, modules_to_not_co\n     if quantization_config.dequantize:\n         return model\n \n-    from kernels import get_kernel\n+    from .hub_kernels import get_kernel\n \n     global triton_kernels_hub\n     triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")"
        },
        {
            "sha": "6705b9b46577482be5c34a12eea007f85858dc3b",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=0001b3ee40be23bcac4f361e7684f1391a49a56b",
            "patch": "@@ -54,7 +54,7 @@ def load_cuda_kernels():\n     global mra_cuda_kernel\n     if not is_kernels_available():\n         raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n-    from kernels import get_kernel\n+    from ...integrations.hub_kernels import get_kernel\n \n     mra_cuda_kernel = get_kernel(\"kernels-community/mra\")\n "
        },
        {
            "sha": "29afe840925422781136563a281ff555061a8a61",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=0001b3ee40be23bcac4f361e7684f1391a49a56b",
            "patch": "@@ -49,7 +49,7 @@ def load_wkv_cuda_kernel(context_length):\n     if not is_kernels_available():\n         raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n \n-    from kernels import get_kernel\n+    from ...integrations.hub_kernels import get_kernel\n \n     rwkv_cuda_kernel = get_kernel(\"kernels-community/rwkv\")\n     rwkv_cuda_kernel.max_seq_length = context_length"
        },
        {
            "sha": "6265cf71b7fbf63664e4d6bfbca07871662f4dc3",
            "filename": "src/transformers/models/sam3_video/modeling_sam3_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py?ref=0001b3ee40be23bcac4f361e7684f1391a49a56b",
            "patch": "@@ -33,7 +33,7 @@\n \n \n if is_kernels_available():\n-    from kernels import get_kernel\n+    from ...integrations.hub_kernels import get_kernel\n \n logger = logging.get_logger(__name__)\n "
        },
        {
            "sha": "79ae3f9d996e2ec941fcb599f58cde51252ac53d",
            "filename": "src/transformers/models/yoso/modeling_yoso.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py?ref=0001b3ee40be23bcac4f361e7684f1391a49a56b",
            "patch": "@@ -54,7 +54,7 @@ def load_cuda_kernels():\n     global lsh_cumulation\n     if not is_kernels_available():\n         raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n-    from kernels import get_kernel\n+    from ...integrations.hub_kernels import get_kernel\n \n     yoso = get_kernel(\"kernels-community/yoso\")\n     lsh_cumulation = yoso.lsh_cumulation"
        },
        {
            "sha": "36fbea9c549ab9b26f1463d5203ea214dc735e38",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0001b3ee40be23bcac4f361e7684f1391a49a56b/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=0001b3ee40be23bcac4f361e7684f1391a49a56b",
            "patch": "@@ -53,7 +53,7 @@ def _lazy_import_kernels(self):\n         \"\"\"Lazy import and initialize kernels only when needed\"\"\"\n         if self.triton_kernels_hub is None:\n             try:\n-                from kernels import get_kernel\n+                from ..integrations.hub_kernels import get_kernel\n \n                 self.triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")\n             except ImportError:"
        }
    ],
    "stats": {
        "total": 42,
        "additions": 30,
        "deletions": 12
    }
}