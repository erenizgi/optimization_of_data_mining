{
    "author": "D15M4S",
    "message": "ğŸŒ [i18n-KO] Translated `perf_train_gpu_one.md` to Korean (#39552)\n\n* docs: ko: perf_train_gpu_one.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* fix: Manually added missing backticks\n\n* Update docs/source/ko/perf_train_gpu_one.md\r\n\r\nfix: remove space between heading and GPU anchor\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Update docs/source/ko/perf_train_gpu_one.md\r\n\r\nfix: clarify table headers to indicate training speed boost and memory savings\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Update docs/source/ko/perf_train_gpu_one.md\r\n\r\nfix: improve readability\n\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\n\n* Update docs/source/ko/perf_train_gpu_one.md\r\n\r\nfix : rephrase explanation of data preloading to improve readability\n\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\n\n---------\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>",
    "sha": "43fe41c0a85a8576c9d9d74ad648d11c0469d8ab",
    "files": [
        {
            "sha": "b677145a0f15fdf085e949d76d24f911411ebb45",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/43fe41c0a85a8576c9d9d74ad648d11c0469d8ab/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/43fe41c0a85a8576c9d9d74ad648d11c0469d8ab/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=43fe41c0a85a8576c9d9d74ad648d11c0469d8ab",
            "patch": "@@ -135,8 +135,8 @@\n       title: ë‹¤ì¤‘ GPUì—ì„œ í•™ìŠµ ì§„í–‰í•˜ê¸°\n     title: ë¶„ì‚° í•™ìŠµ(Distributed training)\n   - sections:\n-    - local: in_translation\n-      title: (ë²ˆì—­ì¤‘) GPU\n+    - local: perf_train_gpu_one\n+      title: GPU\n     - local: perf_train_cpu\n       title: CPUì—ì„œ í›ˆë ¨\n     - local: perf_train_special"
        },
        {
            "sha": "656da59adbcde0f115a86663861d1699bb8cfc9c",
            "filename": "docs/source/ko/perf_train_gpu_one.md",
            "status": "added",
            "additions": 294,
            "deletions": 0,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/43fe41c0a85a8576c9d9d74ad648d11c0469d8ab/docs%2Fsource%2Fko%2Fperf_train_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/43fe41c0a85a8576c9d9d74ad648d11c0469d8ab/docs%2Fsource%2Fko%2Fperf_train_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_train_gpu_one.md?ref=43fe41c0a85a8576c9d9d74ad648d11c0469d8ab",
            "patch": "@@ -0,0 +1,294 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# GPU[[gpu]]\n+\n+GPUëŠ” ë†’ì€ ë©”ëª¨ë¦¬ ëŒ€ì—­í­ê³¼ ë³‘ë ¬ ì²˜ë¦¬ ëŠ¥ë ¥ ë•ë¶„ì— ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. GPU ì‚¬ì–‘ê³¼ ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ìˆ˜ì‹­ì–µ ê°œ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë¸ë„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•µì‹¬ì€ GPU ë©”ëª¨ë¦¬ í™œìš©ë„(ë°ì´í„° ì²˜ë¦¬ëŸ‰/í•™ìŠµ ì‹œê°„)ì™€ í•™ìŠµ ì†ë„ ì‚¬ì´ì—ì„œ ìµœì ì˜ ê· í˜•ì„ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.\n+\n+ì´ ê°€ì´ë“œëŠ” Transformersì™€ PyTorchì—ì„œ GPUë¥¼ í™œìš©í•´ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•´ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì„ ì†Œê°œí•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê²½ìš°, ì´ ê¸°ëŠ¥ë“¤ì„ ì¡°í•©í•´ì„œ í•™ìŠµì„ ìµœì í™”í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n+\n+ì•„ë˜ í‘œë¥¼ ì°¸ê³ í•˜ë©´ ìì‹ ì˜ í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì— ì í•©í•œ ê¸°ëŠ¥ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+| ê¸°ëŠ¥                        | í•™ìŠµ ì†ë„ ê°€ì† | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ˆì•½ |\n+| --------------------------- | --------- | ------------- |\n+| ë°°ì¹˜ í¬ê¸°                   | ì˜ˆ        | ì˜ˆ            |\n+| ê·¸ë ˆì´ë””ì–¸íŠ¸ ëˆ„ì            | ì•„ë‹ˆìš”    | ì˜ˆ            |\n+| ê·¸ë ˆì´ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…     | ì•„ë‹ˆìš”    | ì˜ˆ            |\n+| í˜¼í•© ì •ë°€ë„                 | ì˜ˆ        | ì¡°ê±´ë¶€        |\n+| ì˜µí‹°ë§ˆì´ì €                  | ì˜ˆ        | ì˜ˆ            |\n+| ë°ì´í„° ì‚¬ì „ ì ì¬            | ì˜ˆ        | ì•„ë‹ˆìš”        |\n+| torch_empty_cache_steps     | ì•„ë‹ˆìš”    | ì˜ˆ            |\n+| torch.compile               | ì˜ˆ        | ì•„ë‹ˆìš”        |\n+| ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜ (SDPA) | ì˜ˆ        | ì˜ˆ            |\n+\n+## Trainer[[trainer]]\n+\n+TrainerëŠ” [`TrainingArguments`]ë¡œ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ í•™ìŠµ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë²ˆ ì„¹ì…˜ì—ì„œëŠ” í•™ìŠµ ìµœì í™”ì— íŠ¹íˆ ìœ ìš©í•œ ì£¼ìš” ê¸°ëŠ¥ ëª‡ ê°€ì§€ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤.\n+\n+### ë°°ì¹˜ í¬ê¸°[[batch-size]]\n+\n+ë°°ì¹˜ í¬ê¸°ëŠ” GPU í•™ìŠµ íš¨ìœ¨ì„ ì¢Œìš°í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ë¡œ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ í•™ìŠµ ì†ë„ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ì¤ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ í¬ê²Œ í•˜ë©´ GPUì˜ ë³‘ë ¬ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ê·¹ëŒ€í™”í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ 8, 64, 128, 256, 512ì²˜ëŸ¼ 2ì˜ ê±°ë“­ì œê³± ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°ëŠ” GPU ì‚¬ì–‘ê³¼ ëª¨ë¸ì˜ ë°ì´í„° íƒ€ì…ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n+\n+ë°°ì¹˜ í¬ê¸°ëŠ” [`TrainingArguments`]ì˜ [`~TrainingArguments.per_device_train_batch_size`] ì˜µì…˜ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+args = TrainingArguments(\n+    per_device_train_batch_size=256,\n+    per_device_eval_batch_size=256,\n+)\n+```\n+\n+ì„±ëŠ¥, ì…ë ¥ í”¼ì²˜ ìˆ˜ì™€ ì¶œë ¥ ë‰´ëŸ° ìˆ˜, ë°°ì¹˜ í¬ê¸°ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ì„œëŠ” NVIDIA [Performance](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features) ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”. ì´ ë§¤ê°œë³€ìˆ˜ë“¤ì€ GPUì—ì„œ ì‹¤í–‰ë˜ëŠ” General Matrix Multiplications(GEMMs)ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ê°€ í´ìˆ˜ë¡ ë³‘ë ¬í™”ì™€ íš¨ìœ¨ì„±ì´ í–¥ìƒë©ë‹ˆë‹¤.\n+\n+ë°ì´í„° íƒ€ì…ê³¼ GPUì— ë”°ë¥¸ ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ì„ íƒí•´ í…ì„œ ê³±ì…ˆ ì†ë„ë¥¼ ê·¹ëŒ€í™”í•˜ë ¤ë©´, [Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) ì„¹ì…˜ì„ ì°¸ê³ í•˜ëŠ” ê²ƒì´ ìœ ìš©í•©ë‹ˆë‹¤. ê·¸ ì˜ˆì‹œë¡œ, fp16ì—ì„œëŠ” 8ì˜ ë°°ìˆ˜ê°€ ê¶Œì¥ë˜ì§€ë§Œ, A100 GPUì—ì„œëŠ” 64ì˜ ë°°ìˆ˜ê°€ ë” ì í•©í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ë§ˆì§€ë§‰ìœ¼ë¡œ, ì‘ì€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization)ë¥¼ ê³ ë ¤í•˜ì„¸ìš”. í–‰ë ¬ ì°¨ì›ì´ GPU ìŠ¤ë ˆë“œ ë¸”ë¡ì˜ íƒ€ì¼ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ì§€ì§€ ì•Šìœ¼ë©´ íƒ€ì¼ ì–‘ìí™”ê°€ ë°œìƒí•˜ì—¬ GPU ìì›ì„ ì¶©ë¶„íˆ í™œìš©í•˜ì§€ ëª»í•©ë‹ˆë‹¤. í–‰ë ¬ì´ íƒ€ì¼ í¬ê¸°ë¡œ ì •í™•íˆ ë‚˜ë‰˜ë„ë¡ ì˜¬ë°”ë¥¸ ë°°ì¹˜ í¬ê¸° ë°°ìˆ˜ë¥¼ ì„ íƒí•˜ë©° í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.\n+\n+### ê·¸ë ˆì´ë””ì–¸íŠ¸ ëˆ„ì [[gradient-accumulation]]\n+\n+ê·¸ë ˆì´ë””ì–¸íŠ¸ ëˆ„ì ì€ ë©”ëª¨ë¦¬ ì œì•½ì„ ê·¹ë³µí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë‹¨ì¼ GPUì— ë§ì§€ ì•ŠëŠ” ë§¤ìš° í° ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ì „ì— ì—¬ëŸ¬ ë¯¸ë‹ˆ ë°°ì¹˜ì— ê±¸ì³ ê·¸ë ˆì´ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ê·¸ ê²°ê³¼, ì €ì¥í•´ì•¼ í•˜ëŠ” ê·¸ë ˆì´ë””ì–¸íŠ¸ ìˆ˜ê°€ ì¤„ì–´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¤„ì–´ë“¤ê³ , ì¼ë°˜ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ë°°ì¹˜ì—ì„œë§Œ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ëŠ” ë°©ì‹ë³´ë‹¤ ë” í° ìœ íš¨ ë°°ì¹˜ í¬ê¸°ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì¶”ê°€ì ì¸ ìˆœì „íŒŒì™€ ì—­ì „íŒŒê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ê·¸ë ˆì´ë””ì–¸íŠ¸ ëˆ„ì ì„ í™œì„±í™”í•˜ë ¤ë©´ [`TrainingArguments`]ì—ì„œ [`TrainingArguments.per_device_train_batch_size`] ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+# íš¨ìœ¨ì ì¸ ë°°ì¹˜ í¬ê¸° 64\n+args = TrainingArguments(\n+    per_device_train_batch_size=4,\n+    gradient_accumulation_steps=16,\n+)\n+```\n+\n+í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê·¸ë ˆì´ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ë¥¼ ë„ˆë¬´ í¬ê²Œ ì„¤ì •í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì„¸ìš”, GPUì— ë‹´ì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ ë°°ì¹˜ í¬ê¸°ê°€ 4ë¼ë©´ GPUì˜ íš¨ìœ¨ì ì¸ ì‚¬ìš©ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ 4ë¡œ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n+\n+| ë°°ì¹˜ í¬ê¸° | ê·¸ë ˆì´ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ | íš¨ìœ¨ì ì¸ ë°°ì¹˜ í¬ê¸° |     |\n+| --------- | ---------------------- | ------------------ | --- |\n+| 1         | 64                     | 64                 | ğŸ‘  |\n+| 4         | 16                     | 64                 | ğŸ‘  |\n+\n+### ê·¸ë ˆì´ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…[[gradient-checkpointing]]\n+\n+ê·¸ë ˆì´ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ì€ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì¼ë¶€ ì¤‘ê°„ í™œì„±í™” ê°’ë§Œ ì €ì¥í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë‹¤ì‹œ ê³„ì‚°í•´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìˆœì „íŒŒ ê³¼ì •ì—ì„œ ëª¨ë“  ì¤‘ê°„ í™œì„±í™” ê°’ì„ ì €ì¥í•˜ì§€ ì•Šì•„ë„ ë˜ì–´ ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, í•™ìŠµ ì†ë„ê°€ ì•½ 20% ëŠë ¤ì§€ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.\n+\n+ê·¸ë ˆì´ë””ì–¸íŠ¸ ëˆ„ì ì„ í™œì„±í™”í•˜ë ¤ë©´ [`TrainingArguments`]ì—ì„œ [`~TrainingArguments.gradient_checkpointing`] ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+args = TrainingArguments(\n+    per_device_train_batch_size=4,\n+    gradient_accumulation_steps=16,\n+    gradient_checkpointing=True,\n+)\n+```\n+\n+### í˜¼í•© ì •ë°€ë„[[mixed-precision]]\n+\n+í˜¼í•© ì •ë°€ë„ëŠ” ì¼ë¶€ ê³„ì‚°ì„ ë°˜ì •ë°€ë„(fp16)ë¡œ, ë‚˜ë¨¸ì§€ë¥¼ ì „ì •ë°€ë„(fp32)ë¡œ ìˆ˜í–‰í•´ í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ë°˜ì •ë°€ë„ ê³„ì‚°ì€ ì „ì •ë°€ë„ë³´ë‹¤ ê³„ì‚°ëŸ‰ì´ ì ì–´ ë” ë¹ ë¥´ê²Œ ìˆ˜í–‰ë©ë‹ˆë‹¤. í•œí¸, ì „ì •ë°€ë„ë¡œ ì¼ë¶€ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ë©´ ì •í™•ë„ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+í˜¼í•© ì •ë°€ë„ í•™ìŠµì„ ìœ„í•´ ì—¬ëŸ¬ ìë£Œí˜•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+<hfoptions id=\"mixed-precision\">\n+<hfoption id=\"fp16\">\n+\n+í˜¼í•© ì •ë°€ë„ í•™ìŠµì˜ ì£¼ìš” ì¥ì ì€ í™œì„±í™” ê°’ì„ fp16ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\n+\n+fp16 ìë£Œí˜•ìœ¼ë¡œ í˜¼í•© ì •ë°€ë„ í•™ìŠµì„ í™œì„±í™”í•˜ë ¤ë©´ [`TrainingArguments`]ì—ì„œ [`~TrainingArguments.fp16`] ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+args = TrainingArguments(\n+    per_device_train_batch_size=4,\n+    gradient_accumulation_steps=16,\n+    gradient_checkpointing=True,\n+    fp16=True.\n+)\n+```\n+\n+fp16ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ì— ìµœì í™”ëœ ë°©ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ì´ëŠ” fp16ìœ¼ë¡œ ê³„ì‚°ëœ ê·¸ë ˆì´ë””ì–¸íŠ¸ê°€ ìµœì í™” ë‹¨ê³„ì—ì„œ fp32ë¡œ ë‹¤ì‹œ ë³€í™˜ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. íŠ¹íˆ ë°°ì¹˜ í¬ê¸°ê°€ ì‘ì„ ë•ŒëŠ”, GPUì— ë‘ ê°€ì§€ ìë£Œí˜•(fp16, fp32)ì´ ì ì¬ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë” ë§ì€ GPU ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.\n+</hfoption>\n+<hfoption id=\"bf16\">\n+\n+[bf16](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus)ì€ ì¼ë¶€ ì •ë°€ë„ë¥¼ í¬ê¸°í•˜ëŠ” ëŒ€ì‹ , í›¨ì”¬ ë” ë„“ì€ ë™ì  ë²”ìœ„ë¥¼ ì œê³µí•˜ì—¬ ì˜¤ë²„í”Œë¡œì™€ ì–¸ë”í”Œë¡œ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. bf16ì€ fp16ê³¼ ë‹¬ë¦¬ ì†ì‹¤ ìŠ¤ì¼€ì¼ë§ ê¸°ë²•ì„ ì¶”ê°€í•˜ì§€ ì•Šê³ ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. bf16ì€ NVIDIAì˜ Ampere ì•„í‚¤í…ì²˜ ì´ìƒì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+\n+bf16 ìë£Œí˜•ìœ¼ë¡œ í˜¼í•© ì •ë°€ë„ í•™ìŠµì„ í™œì„±í™”í•˜ë ¤ë©´ [`TrainingArguments`]ì—ì„œ [`~TrainingArguments.bf16`] ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+args = TrainingArguments(\n+    per_device_train_batch_size=4,\n+    gradient_accumulation_steps=16,\n+    gradient_checkpointing=True,\n+    bf16=True,\n+)\n+```\n+\n+</hfoption>\n+<hfoption id=\"tf32\">\n+\n+[tf32](https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/)ëŠ” NVIDIA Ampere GPUì—ì„œ í•©ì„±ê³±ê³¼ í–‰ë ¬ê³± ì…ë ¥ì„ tf32ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“œì…ë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë“  ì €ì¥ê³¼ ì—°ì‚°ì€ fp32ë¡œ ìœ ì§€ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ tf32ëŠ” fp32ì™€ ë™ì¼í•œ ë²”ìœ„, fp16ê³¼ ë™ì¼í•œ ì •ë°€ë„, ê·¸ë¦¬ê³  bf16ë³´ë‹¤ ë” ë†’ì€ ì •ë°€ë„ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. tf32ë¥¼ fp16 ë˜ëŠ” bf16 í˜¼í•© ì •ë°€ë„ í•™ìŠµê³¼ ê²°í•©í•˜ë©´ ì²˜ë¦¬ëŸ‰ì„ 16ë°° í–¥ìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+tf32ëŠ” NVIDIA Ampere GPUì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”ë˜ì–´ ìˆì§€ë§Œ, fp32 í•™ìŠµ ë˜ëŠ” ì¶”ë¡  ì½”ë“œì— ì•„ë˜ ì½”ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+import torch\n+torch.backends.cuda.matmul.allow_tf32 = True\n+torch.backends.cudnn.allow_tf32 = True\n+```\n+\n+tf32 ëª¨ë“œì—ì„œ í˜¼í•© ì •ë°€ë„ í•™ìŠµì„ í™œì„±í™”í•˜ë ¤ë©´ [`TrainingArguments`]ì—ì„œ [tf32()](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.tf32) ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+args = TrainingArguments(\n+    per_device_train_batch_size=4,\n+    gradient_accumulation_steps=16,\n+    gradient_checkpointing=True,\n+    bf16=True.\n+    tf32=True,\n+)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+### ì˜µí‹°ë§ˆì´ì €[[optimizers]]\n+\n+TransformersëŠ” ê¸°ë³¸ì ìœ¼ë¡œ PyTorchì˜ [AdamW (adamw_torch)](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì´ ì˜µí‹°ë§ˆì´ì €ëŠ” ê³¼ê±° ê·¸ë ˆì´ë””ì–¸íŠ¸ì˜ ê°€ì¤‘ í‰ê· ì„ ì €ì¥í•˜ê¸° ë•Œë¬¸ì—, ê·¸ë ˆì´ë””ì–¸íŠ¸ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ìˆ˜ì— ë¹„ë¡€í•œ ì¶”ê°€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ëŠ” ë§¤ìš° í° ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ¬ë©´ ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [NVIDIA](https://github.com/NVIDIA/apex) ë˜ëŠ” [AMD](https://github.com/ROCm/apex)ì— [Apex](https://nvidia.github.io/apex/index.html)ê°€ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´, ëª¨ë“  AdamW ì˜µí‹°ë§ˆì´ì € ì¤‘ `adamw_apex_fused` ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ë¹ ë¥¸ í•™ìŠµ ì†ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„ íƒí•˜ê¸° ìœ„í•´ì„œëŠ” [`TrainingArguments`]ì—ì„œ [`~TrainingArguments.optim`] ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+args = TrainingArguments(\n+    per_device_train_batch_size=4,\n+    gradient_accumulation_steps=16,\n+    gradient_checkpointing=True,\n+    bf16=True,\n+    optim=\"adamw_bnb_8bit\"\n+)\n+```\n+í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì— ë§ê²Œ ì„ íƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì˜µí‹°ë§ˆì´ì €ê°€ ìˆìŠµë‹ˆë‹¤. (ì „ì²´ ì§€ì› ëª©ë¡ì€ [OptimizerNames](https://github.com/huggingface/transformers/blob/34f4080ff59b1668d919a1ba9f8bc4a3a2a3f478/src/transformers/training_args.py#L145)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”) ì˜ˆë¥¼ ë“¤ì–´ AdafactorëŠ” í–‰ë ¬ì˜ ê° ìš”ì†Œ ëŒ€ì‹  í–‰ ë˜ëŠ” ì—´ ë‹¨ìœ„ì˜ ê°€ì¤‘ í‰ê· ë§Œ ì €ì¥í•´ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ, ìˆ˜ë ´ ì†ë„ëŠ” ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜ ë‹¤ë¥¸ ì˜ˆë¡œ, bitandbytesì˜ [8-bit AdamW optimizer](https://huggingface.co/docs/bitsandbytes)ë¥¼ ì‚¬ìš©í•˜ë©´ ì˜µí‹°ë§ˆì´ì €ì˜ ìƒíƒœë¥¼ 8ë¹„íŠ¸ë¡œ ì–‘ìí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜µí‹°ë§ˆì´ì € ìƒíƒœëŠ” ë‚®ì€ ì •ë°€ë„ë¡œ ì €ì¥ë˜ì—ˆë‹¤ê°€ ì˜µí‹°ë§ˆì´ì € ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë˜ê¸° ì „ì— ì—­ ì–‘ìí™”ë©ë‹ˆë‹¤.\n+\n+íŠ¹í™”ëœ ì˜µí‹°ë§ˆì´ì €ì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ë‹¤ë©´ [optimizer](./optimizers) ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+### ë°ì´í„° ì‚¬ì „ ì ì¬[[data-preloading]]\n+\n+ë°ì´í„° ì‚¬ì „ ì ì¬(Data preloading)ëŠ” GPUê°€ ì§€ì†ì ìœ¼ë¡œ ì‘ì—…í•  ìˆ˜ ìˆë„ë¡ CPUì—ì„œ ë¯¸ë¦¬ ë°°ì¹˜ ë‹¨ìœ„ì˜ ë°ì´í„°ë¥¼ ì ì¬í•˜ê³  ì¤€ë¹„í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ GPU ìœ íœ´ ì‹œê°„ì„ ì¤„ì´ê³  í™œìš©ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. GPUê°€ í•­ìƒ ì‘ì—…ì„ ê³„ì†í•˜ë„ë¡ í•˜ë ¤ë©´ ë‹¤ìŒ ë°ì´í„° ì‚¬ì „ ì ì¬ë¥¼ ìœ„í•œ ë‘ ê°€ì§€ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+1. ë°ì´í„°ë¥¼ ì €ì¥í•  ê³ ì • ë©”ëª¨ë¦¬ë¥¼ CPUì— í• ë‹¹í•œ ë’¤, ì´ë¥¼ GPUë¡œ ì§ì ‘ ì „ì†¡í•©ë‹ˆë‹¤.\n+2. CPU ìŠ¤ë ˆë“œ ë° ì›Œì»¤ ìˆ˜ë¥¼ ëŠ˜ë ¤ ë°ì´í„°ë¥¼ ë” ë¹ ë¥´ê²Œ ì‚¬ì „ ì ì¬í•©ë‹ˆë‹¤.\n+\n+ê³ ì • ë©”ëª¨ë¦¬ë¥¼ í• ë‹¹í•˜ê³  ì›Œì»¤ ìˆ˜ë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ì„œëŠ” [`TrainingArguments`]ì—ì„œ [`~TrainingArguments.dataloader_pin_memory`]ì™€ [`~TrainingArguments.dataloader_num_workers`] ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+args = TrainingArguments(\n+    per_device_train_batch_size=4,\n+    gradient_accumulation_steps=16,\n+    gradient_checkpointing=True,\n+    bf16=True,\n+    optim=\"adamw_bnb_8bit\",\n+    dataloader_pin_memory=True,\n+    dataloader_num_workers=4,\n+)\n+```\n+\n+## PyTorch[[pytorch]]\n+\n+PyTorchëŠ” ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ì„ ì¤„ì´ê³  í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ì—¬ëŸ¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ëŠ¥ë“¤ì€ Transformersì—ì„œ ëª‡ ì¤„ì˜ ì½”ë“œë§Œ ì¶”ê°€í•˜ì—¬ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+### torch.empty_cache_steps[[torchemptycachesteps]]\n+\n+[torch.cuda.empty_cache](https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache) í•¨ìˆ˜ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ìºì‹œ ë©”ëª¨ë¦¬ë¥¼ í•´ì œí•˜ì—¬ ë©”ëª¨ë¦¬ ë¶€ì¡±(OOM) ì˜¤ë¥˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆì§€ë§Œ, í•™ìŠµ ì†ë„ê°€ ì•½ 10% ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+íŠ¹ì • í•™ìŠµ ë‹¨ê³„ ì´í›„ì— ì´ ê¸°ëŠ¥ì„ í™œì„±í™”í•˜ê³  ì‹¶ë‹¤ë©´, [`TrainingArguments`]ì—ì„œ [torch_empty_cache_steps()](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.torch_empty_cache_steps)ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+args = TrainingArguments(\n+    per_device_train_batch_size=4,\n+    gradient_accumulation_steps=16,\n+    gradient_checkpointing=True,\n+    bf16=True,\n+    optim=\"adamw_bnb_8bit\",\n+    dataloader_pin_memory=True,\n+    dataloader_num_workers=4,\n+    torch_empty_cache_steps=4,\n+)\n+```\n+\n+### torch.compile[[torchcompile]]\n+\n+[torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)ì€ PyTorch ì½”ë“œë¥¼ ìµœì í™”ëœ ì»¤ë„ë¡œ ì»´íŒŒì¼í•´ í•™ìŠµ ì†ë„ë¥¼ í¬ê²Œ ë†’ì—¬ì¤ë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì€ TorchDynamoë¥¼ ì‚¬ìš©í•´ í”„ë ˆì„ í‰ê°€ APIë¡œë¶€í„° PyTorch ê·¸ë˜í”„ë¥¼ ìº¡ì²˜í•˜ë©°, ì´ë ‡ê²Œ ìº¡ì²˜í•œ ê·¸ë˜í”„ëŠ” ë‹¤ì–‘í•œ ë°±ì—”ë“œì— ì¶”ê°€ë¡œ ìµœì í™”ëœ ì»¤ë„ë¡œ ì»´íŒŒì¼ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì´ë¥¼ í™œì„±í™”í•˜ë ¤ë©´ [`TrainingArguments`]ì—ì„œ [`~TrainingArguments.torch_compile`]ë¥¼ ì„¤ì •í•˜ì„¸ìš”. ë°±ì—”ë“œëŠ” [torch_compile_backend()](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.torch_compile_backend)ë¥¼ í†µí•´ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers import TrainingArguments\n+\n+args = TrainingArguments(\n+    per_device_train_batch_size=4,\n+    gradient_accumulation_steps=16,\n+    gradient_checkpointing=True,\n+    bf16=True,\n+    optim=\"adamw_bnb_8bit\",\n+    dataloader_pin_memory=True,\n+    dataloader_num_workers=4,\n+    torch_empty_cache_steps=4,\n+    torch_compile=True,\n+    torch_compile_backend=\"inductor\"\n+)\n+```\n+\n+ì•„ë˜ í‘œë¥¼ ì°¸ê³ í•˜ì—¬ í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì— ì í•©í•œ ë°±ì—”ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.\n+\n+| ë°±ì—”ë“œ         | ì„¤ëª…                                                                                                                                                                   | ëª©í‘œ         |\n+| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ |\n+| eager          | PyTorchë¥¼ ì‚¬ìš©í•´ ì¶”ì¶œëœ GraphModuleì„ ì‹¤í–‰í•©ë‹ˆë‹¤                                                                                                                       | ë””ë²„ê¹…       |\n+| aot_eager      | AOTAutogradë¡œ ì¶”ì¶œëœ ìˆœì „íŒŒ ë° ì—­ì „íŒŒ ê·¸ë˜í”„ë¥¼ Pytorch eager ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤                                                                                         | ë””ë²„ê¹…       |\n+| inductor       | Triton ì»¤ë„ì„ í™œìš©í•˜ëŠ” TorchInductorì™€ AOTAutograd, CUDA Graphsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤                                                                                           | í•™ìŠµ ë° ì¶”ë¡  |\n+| nvfuser        | TorchScriptì™€ í•¨ê»˜ nvFuserë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤                                                                                                                                | í•™ìŠµ ë° ì¶”ë¡  |\n+| aot_nvfuser    | AOTAutogradì™€ í•¨ê»˜ nvFuserë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤                                                                                                                                | í•™ìŠµ ë° ì¶”ë¡  |\n+| aot_cudagraphs | AOTAutogradì™€ í•¨ê»˜ CUDA Graphsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤                                                                                                                            | í•™ìŠµ ë° ì¶”ë¡  |\n+| ofi            | TorchScriptsì˜ [optimize_for_inference](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html#torch-jit-optimize-for-inference)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ | ì¶”ë¡          |\n+| fx2trt         | [Torch-TensorRT](https://pytorch.org/TensorRT/tutorials/getting_started_with_fx_path.html)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤                                                                | ì¶”ë¡          |\n+| onnxrt         | CPU ë° GPU ì¶”ë¡ ì„ ìœ„í•´ [ONNX-RT](https://onnxruntime.ai/)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤                                                                                                 | ì¶”ë¡          |\n+| ipex           | CPU ì¶”ë¡ ì„ ìœ„í•´ [IPEX](https://github.com/intel/intel-extension-for-pytorch)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤                                                                              | ì¶”ë¡          |\n+\n+### ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜[[scaled-dot-production-attention]]\n+\n+[torch.nn.functional.scaled_dot_product_attention](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA)ëŠ” ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ PyTorchì— ë‚´ì¥í•´ êµ¬í˜„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤. SDPAëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ê¸°ì¡´ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ë³´ë‹¤ ë” íš¨ìœ¨ì ì´ê³  ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì„¸ ê°€ì§€ ìœ í˜•ì˜ ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜ì„ ì§€ì›í•©ë‹ˆë‹¤.\n+\n+- [FlashAttention2](https://github.com/Dao-AILab/flash-attention)ëŠ” fp16 ë˜ëŠ” bf16 torch íƒ€ì… ëª¨ë¸ì—ì„œ ìë™ìœ¼ë¡œ í™œì„±í™”ë©ë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì ì ˆí•œ íƒ€ì…ìœ¼ë¡œ ìºìŠ¤íŒ…í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n+- [xFormers](https://github.com/facebookresearch/xformers) ë˜ëŠ” Memory-Efficient Attentionì€ fp32 torch íƒ€ì… ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤.\n+- C++ë¡œ êµ¬í˜„ëœ ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜ì…ë‹ˆë‹¤.\n+\n+SDPAëŠ” PyTorch 2.1.1 ë²„ì „ ì´ìƒì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”ë˜ì–´ ìˆì§€ë§Œ, [`~PreTrainedModel.from_pretrained`]ì—ì„œ `attn_implementation=\"sdpa\"`ë¥¼ ì„¤ì •í•´ ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers import AutoModelForCausalLM\n+\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", attn_implementation=\"sdpa\")\n+```"
        }
    ],
    "stats": {
        "total": 298,
        "additions": 296,
        "deletions": 2
    }
}