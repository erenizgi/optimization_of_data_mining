{
    "author": "kaixuanliu",
    "message": "add rotary kernel support to Qwen3 model (#41147)\n\n* add rotary kernel support to Qwen3 model\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* delete unnecessary import\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* adjust code\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* adjust code\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* put get rotary kernel to hub_kernels.py\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix wrong import\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* refine code and adjust related modular code\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix modular mismatch bug\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* update code, use lazy load kernels\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix check modular conversion issue\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix CI bug for qwen3-next\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix CI issue\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* delete unused code\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* rename to `apply_rotary_transformers`\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* adjust import `lazy_load_kernel` location\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* Update modular-generated modeling files with lazy_load_kernel import location\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix conflicts\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* add more check\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* use decorator to map kernels for functions\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* small fix\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* small adjustment\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* update code\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix LINT issue\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* update code to adapt to new `use_kernel_func_from_hub` API in kernels\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* do not consider check_modular first\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* update\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* add compatibility for old version `kernels`\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* add rotary fn kernel to all models\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* update modular part\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* Revert \"update modular part\"\n\nThis reverts commit b8b68c7caf466257b88ef910efa18d0812c33597.\n\n* update code\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n---------\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>",
    "sha": "6587d77ffcf647b848a71c32ce9cad3496e341b2",
    "files": [
        {
            "sha": "05f410c4437fec168ed413b762554341db4f9536",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -72,6 +72,7 @@\n         \"register_kernel_mapping\",\n         \"replace_kernel_forward_from_hub\",\n         \"use_kernel_forward_from_hub\",\n+        \"use_kernel_func_from_hub\",\n     ],\n     \"integration_utils\": [\n         \"INTEGRATION_TO_CALLBACK\",\n@@ -212,6 +213,7 @@\n         register_kernel_mapping,\n         replace_kernel_forward_from_hub,\n         use_kernel_forward_from_hub,\n+        use_kernel_func_from_hub,\n     )\n     from .integration_utils import (\n         INTEGRATION_TO_CALLBACK,"
        },
        {
            "sha": "8d3ae310687ebeb15e5a2a56978570722ad7e7ec",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 54,
            "deletions": 2,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -32,22 +32,52 @@\n         register_kernel_mapping,\n         replace_kernel_forward_from_hub,\n     )\n+    from kernels import (\n+        use_kernel_forward_from_hub as _kernels_use_kernel_forward_from_hub,\n+    )\n+\n+    # Try to import FuncRepository, fallback if not available\n+    try:\n+        from kernels import FuncRepository\n+    except ImportError:\n+        FuncRepository = None\n+\n+    # Try to import use_kernel_func_from_hub, fallback if not available\n+    try:\n+        from kernels import use_kernel_func_from_hub as _kernels_use_kernel_func_from_hub\n+\n+        _has_use_kernel_func_from_hub = True\n+    except ImportError:\n+        _has_use_kernel_func_from_hub = False\n \n     _TRANSFORMERS_USE_HUB_KERNELS = os.environ.get(\"USE_HUB_KERNELS\", \"YES\").upper()\n     _kernels_available = True\n     _kernels_enabled = _TRANSFORMERS_USE_HUB_KERNELS in ENV_VARS_TRUE_VALUES\n \n     def use_kernel_forward_from_hub(layer_name: str):\n         if _kernels_enabled:\n-            from kernels import use_kernel_forward_from_hub as _kernels_use_kernel_forward_from_hub\n-\n             return _kernels_use_kernel_forward_from_hub(layer_name)\n         else:\n             logger.warning_once(\n                 f\"kernels hub usage is disabled through the environment USE_HUB_KERNELS={_TRANSFORMERS_USE_HUB_KERNELS}\"\n             )\n             return lambda cls: cls\n \n+    def use_kernel_func_from_hub(func_name: str):\n+        if _kernels_enabled and _has_use_kernel_func_from_hub:\n+            return _kernels_use_kernel_func_from_hub(func_name)\n+        else:\n+            if not _has_use_kernel_func_from_hub:\n+                logger.warning_once(\n+                    \"use_kernel_func_from_hub is not available in the installed kernels version. \"\n+                    \"Please upgrade kernels to use this feature.\"\n+                )\n+            else:\n+                logger.warning_once(\n+                    f\"kernels hub usage is disabled through the environment USE_HUB_KERNELS={_TRANSFORMERS_USE_HUB_KERNELS}\"\n+                )\n+            return lambda func: func\n+\n     _KERNEL_MAPPING: dict[str, dict[Device | str, LayerRepository]] = {\n         \"MultiScaleDeformableAttention\": {\n             \"cuda\": LayerRepository(\n@@ -162,6 +192,16 @@ def use_kernel_forward_from_hub(layer_name: str):\n         },\n     }\n \n+    # Add function kernel mappings if FuncRepository is available\n+    if FuncRepository is not None:\n+        _KERNEL_MAPPING[\"rotary_pos_emb\"] = {\n+            \"xpu\": {\n+                Mode.INFERENCE: FuncRepository(\n+                    repo_id=\"kernels-community/rotary\", func_name=\"apply_rotary_transformers\"\n+                )\n+            }\n+        }\n+\n     def has_key(d, key):\n         return key in d or any(isinstance(v, dict) and has_key(v, key) for v in d.values())\n \n@@ -187,6 +227,12 @@ def decorator(cls):\n \n         return decorator\n \n+    def use_kernel_func_from_hub(*args, **kwargs):\n+        def decorator(func):\n+            return func\n+\n+        return decorator\n+\n     class LayerRepository:\n         def __init__(self, *args, **kwargs):\n             raise RuntimeError(\"LayerRepository requires `kernels` to be installed. Run `pip install kernels`.\")\n@@ -199,6 +245,11 @@ def replace_kernel_forward_from_hub(*args, **kwargs):\n     def register_kernel_mapping(*args, **kwargs):\n         raise RuntimeError(\"register_kernel_mapping requires `kernels` to be installed. Run `pip install kernels`.\")\n \n+    def register_kernel_mapping_transformers(*args, **kwargs):\n+        raise RuntimeError(\n+            \"register_kernel_mapping_transformers requires `kernels` to be installed. Run `pip install kernels`.\"\n+        )\n+\n \n _HUB_KERNEL_MAPPING: dict[str, dict[str, str]] = {\n     \"causal-conv1d\": {\"repo_id\": \"kernels-community/causal-conv1d\"},\n@@ -321,6 +372,7 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n __all__ = [\n     \"LayerRepository\",\n     \"use_kernel_forward_from_hub\",\n+    \"use_kernel_func_from_hub\",\n     \"register_kernel_mapping\",\n     \"register_kernel_mapping_transformers\",\n     \"replace_kernel_forward_from_hub\","
        },
        {
            "sha": "1780b6383cc45c82b4a9f1ae6c6fef98003c6623",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForTokenClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -147,6 +147,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -237,6 +238,7 @@ def __init__(self, config: ApertusConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = ApertusRMSNorm(self.head_dim, config.rms_norm_eps)\n         self.k_norm = ApertusRMSNorm(self.head_dim, config.rms_norm_eps)\n "
        },
        {
            "sha": "d7faffd25c6e125747904ece72b0a08716e0e8b0",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n@@ -154,6 +154,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -244,6 +245,7 @@ def __init__(self, config: ArceeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "ed38fd1c25252d73daa22e87b190cdaaf97fab3f",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -378,6 +378,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -468,6 +469,7 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "7f6cceeb8372787773a3b88e7662b03b7b9da894",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -370,6 +370,7 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "25568ca923655b560b1212d102e94b86c99fbbfc",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -27,7 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -85,6 +85,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -175,6 +176,7 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_sub_norm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward("
        },
        {
            "sha": "752b693fe7b87639e6b4e55de926cd843a3d53fa",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -247,6 +247,7 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.use_qk_norm = config.use_qk_norm\n         if self.use_qk_norm:\n             # When sharding the model using Tensor Parallelism, need to be careful to use n_local_heads"
        },
        {
            "sha": "5d30ab3b134364d6eff402c7f586f7cbfcfa354c",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -32,7 +32,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -206,6 +206,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -296,6 +297,7 @@ def __init__(self, config: CsmConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "53cc8c23a8c6d59fd22aa0b60470481b6005303b",
            "filename": "src/transformers/models/cwm/modeling_cwm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodeling_cwm.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -113,6 +113,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -195,6 +196,7 @@ def __init__(self, config: CwmConfig, layer_idx: int):\n         self.k_proj = torch.nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = torch.nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward("
        },
        {
            "sha": "92df09947bc4c03a1c7069130d11c0b35eec4672",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -112,6 +113,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "4c56277c69dd2c3f7b793338d35509c938261963",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -16,7 +16,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -253,6 +253,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "9e986d46c3165fb96996b9dddc3e0d06b0a38873",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -27,7 +27,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -200,6 +200,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "b23ee2ed948d69d3f848d94bbacbee26a864d39c",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -32,7 +32,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n from ...modeling_layers import (\n@@ -141,6 +141,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "a3a94f88df55c8fe13a1e107f9dbbe7dcb695a25",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -33,7 +33,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...integrations.flex_attention import compile_friendly_flex_attention\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n@@ -143,6 +143,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "9092a3533e43124735872093e118782c7206ce40",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -135,6 +135,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -226,6 +227,7 @@ def __init__(self, config: Dots1Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Dots1RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Dots1RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None"
        },
        {
            "sha": "96187fb93ac814c683395905b425995fe68cc5b2",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -33,7 +33,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -52,6 +52,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -142,6 +143,7 @@ def __init__(self, config: Emu3Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "c18f19206b21105fa76dd8060e891404d59a381f",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -221,6 +221,7 @@ def __init__(self, config: Ernie4_5Config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "75c36992b22db086e85b37b0878ba1f4f6556655",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -244,6 +244,7 @@ def __init__(self, config: Ernie4_5_MoeConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "2260204eb150bb165f151dc1e7be1804b1a2ce94",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -31,7 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -1051,6 +1051,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -1115,6 +1116,7 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "2287f9f8e88decec2898dccd9e1db17894c19403",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -31,7 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n@@ -140,6 +140,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "1cf19c500737329d560da2d4902d68c1b84c7bde",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -36,7 +36,7 @@\n from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -295,6 +295,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -385,6 +386,7 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.key_multiplier = config.key_multiplier\n \n     def forward("
        },
        {
            "sha": "993a3dae165296995c5afbea8fe97f9549867b0a",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -241,6 +241,7 @@ def __init__(self, config: FlexOlmoConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = FlexOlmoRMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = FlexOlmoRMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n "
        },
        {
            "sha": "ae5e25202a1df6a7ff29b110d5d4f0f3930766b7",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForSequenceClassification,\n@@ -152,6 +153,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -242,6 +244,7 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "1d090213dd613d024556929723dd0685a2bab690",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -153,6 +154,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -253,6 +255,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n "
        },
        {
            "sha": "650241f4b9c404954cf04476a3e5ede08c9aed50",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -31,6 +31,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n@@ -230,6 +231,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -330,6 +332,7 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n         self.is_sliding = self.layer_type == \"sliding_attention\""
        },
        {
            "sha": "e61a32fb4c965ecdd7873206ee75555fc43d9a26",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -1254,6 +1254,7 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n         self.is_sliding = self.layer_type == \"sliding_attention\"\n \n@@ -1475,6 +1476,7 @@ def __init__(self, config: Gemma3nConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n "
        },
        {
            "sha": "e03ac527f9f1af078ac3a568c456bec91f9f6b52",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -239,6 +239,7 @@ def __init__(self, config: GlmConfig, layer_idx: Optional[int] = None):\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "3620b06831fe0a476f80a1084a9e8dbeefbf7b5b",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -221,6 +221,7 @@ def __init__(self, config: Glm4Config, layer_idx: Optional[int] = None):\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "9c287a4b432cb81831d6a283c8b748f7df171904",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -204,6 +204,48 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    # Interleave them instead of usual shape\n+    cos = cos[..., : cos.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+    sin = sin[..., : sin.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+\n+    # Keep half or full tensor for later concatenation\n+    rotary_dim = cos.shape[-1]\n+    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n+    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n+\n+    # Apply rotary embeddings on the first half or full tensor\n+    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)\n+    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)\n+\n+    # Concatenate back to full shape\n+    q_embed = torch.cat([q_embed, q_pass], dim=-1)\n+    k_embed = torch.cat([k_embed, k_pass], dim=-1)\n+    return q_embed, k_embed\n+\n+\n def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding with Multimodal Sections to the query and key tensors (https://qwenlm.github.io/blog/qwen2-vl/).\n \n@@ -280,6 +322,7 @@ def __init__(self, config: Glm4vMoeTextConfig, layer_idx: Optional[int] = None):\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.rope_parameters = config.rope_parameters\n \n     def forward("
        },
        {
            "sha": "5e1173d823d017f1ae6351683141b683801919c6",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -325,6 +325,7 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n         self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n "
        },
        {
            "sha": "ae2d017d53bf43fff819b53da575cfc2f04a27c4",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -50,6 +50,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -140,6 +141,7 @@ def __init__(self, config: GraniteConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "d50147d5c686b3c87264474d1eb68a1be4e34835",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -272,6 +272,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -362,6 +363,7 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "9534b45c06243db69aeb624d501839ae15e285f5",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -31,7 +31,7 @@\n from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -59,6 +59,41 @@\n logger = logging.get_logger(__name__)\n \n \n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -122,6 +157,7 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "78bcdd03c43efedc0b360c032c25cf1a010a17d7",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -262,6 +262,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -352,6 +353,7 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "97a0f89b212b429f24e70a8d6292a8eea2e549a8",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -243,6 +243,7 @@ def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "28899e306f4c8abacaf7b11ae03ae9575c9158fa",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -87,6 +87,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -177,6 +178,7 @@ def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.query_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.key_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n "
        },
        {
            "sha": "dda4366f0d4d60874f8362cdbc0c97f10ca1a7cb",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -86,6 +86,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -176,6 +177,7 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.query_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.key_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n "
        },
        {
            "sha": "47673a3afab2fd8a3d5190a6774015cc62f7cc45",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -32,7 +32,7 @@\n from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -175,6 +175,41 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         return self.key_cache[layer_idx].shape[-2]\n \n \n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -229,6 +264,7 @@ def __init__(self, config: JambaConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "1ef99fc46d7fae89f33f3c8d8c212d4bb61cbb0a",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -366,6 +366,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "aa66a2e2d80c88ece3492e35b2e17f3a8628e6d6",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -26,7 +26,7 @@\n \n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -292,6 +292,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -371,6 +372,7 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.out_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.q_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n         self.k_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)"
        },
        {
            "sha": "55dcde8b07e99c03432240b5b383e7cefa074a66",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -363,6 +363,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -442,6 +443,7 @@ def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n         self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.out_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.q_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n         self.k_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)"
        },
        {
            "sha": "cd14ba847fa4107d47b6ebb7048dfcfe7774bd84",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -199,6 +199,7 @@ def __init__(self, config: LightGlueConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "eb938b6cbf7ef9cd8d545ee81696c28c55d5c4eb",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -26,7 +26,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n@@ -142,6 +142,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -248,6 +249,7 @@ def __init__(self, config: LlamaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "8b8f8c9adb3a10492694a29e8a679085494d7a2a",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -31,7 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -326,6 +326,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -407,6 +408,7 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "ccf6c9abcda5f1939ad627608c1da3caeea12632",
            "filename": "src/transformers/models/ministral/modeling_ministral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -13,7 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -54,6 +54,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -137,6 +138,7 @@ def __init__(self, config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward("
        },
        {
            "sha": "43dc5c18a90b751e55da64dee5fd5f9697ab9418",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -15,7 +15,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -55,6 +55,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -136,6 +137,7 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "95b236dadce6f5c7495baaef7818767b473f9082",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -37,7 +37,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -225,6 +225,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -306,6 +307,7 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "4869da818e01880399f9562319d704de787df58f",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -31,6 +31,7 @@\n \n from ... import initialization as init\n from ...activations import ACT2FN\n+from ...integrations import use_kernel_func_from_hub\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -331,6 +332,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "5059e623c9ddfcd530b71cf2f0398fa05771bdb7",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -31,6 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n@@ -183,6 +184,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "8d7dd0869ef3dff8b3b866a7cf0f6f5f72473a23",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -264,6 +264,7 @@ def __init__(\n             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n \n         # Pad head dimension to the next specified multiple.\n         if self.config.pad_head_dim_to_multiple_of is not None:"
        },
        {
            "sha": "1dcaa6247155e8c3f463e5121d6adefaba71d1ba",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -237,6 +237,7 @@ def __init__(self, config: OlmoConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "aa6296f344f1ed3c3029f00c2870d0571fd74ec6",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -230,6 +230,7 @@ def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n "
        },
        {
            "sha": "003e691e9c825289ac6f109e75475f1a07fb29c2",
            "filename": "src/transformers/models/olmo3/modeling_olmo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -161,6 +161,7 @@ def __init__(self, config: Olmo3Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Olmo3RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = Olmo3RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n         assert config.layer_types is not None"
        },
        {
            "sha": "d04cd421d4416828f69ee6b547f058c591d4ad75",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -27,7 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -148,6 +148,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -238,6 +239,7 @@ def __init__(self, config: OlmoeConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.k_norm = OlmoeRMSNorm(\n             (config.hidden_size // config.num_attention_heads) * config.num_key_value_heads, eps=config.rms_norm_eps"
        },
        {
            "sha": "d11d56d3a11c1443827b4f23d220c466ca16b20f",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -29,6 +29,7 @@\n \n from ... import initialization as init\n from ...activations import ACT2FN\n+from ...integrations import use_kernel_func_from_hub\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -182,6 +183,41 @@ def forward(self, hidden_states, attention_mask=None):\n         return hidden_states.transpose(1, 2)\n \n \n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -245,6 +281,7 @@ def __init__(self, config: ParakeetEncoderConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         # W_{k,R} projection\n         self.relative_k_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n         # global content bias"
        },
        {
            "sha": "b1f007f7ee91e0bbd09b5dd3641ba69740c2efa1",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -13,6 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForSequenceClassification,\n@@ -105,6 +106,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -185,6 +187,7 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n         self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.dense = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=True)\n         self.rotary_ndims = int(self.head_dim * config.rope_parameters[\"partial_rotary_factor\"])\n         self.qk_layernorm = config.qk_layernorm"
        },
        {
            "sha": "fc7c733f32713730b3bd5d6e47717a5c3dd3bf51",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -128,6 +128,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -218,6 +219,7 @@ def __init__(self, config: PhimoeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n     def forward(\n         self,"
        },
        {
            "sha": "060683cfb972de3fb254895662a0464d851519c2",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -13,7 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -119,6 +119,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -201,6 +202,7 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n     def forward("
        },
        {
            "sha": "389bf016243a024404178987918f9a5d63be38c7",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -35,7 +35,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n@@ -161,6 +161,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -243,6 +244,7 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.rotary_fn = apply_rotary_pos_emb\n         if self.config.layer_types[layer_idx] == \"sliding_attention\":\n             self.sliding_window = config.sliding_window\n "
        },
        {
            "sha": "9b6c41662929002108f8a1d6c6cc2ed87f17f9f2",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -155,6 +155,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -246,6 +247,7 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None"
        },
        {
            "sha": "caf7e26a39fe68e206d98312485d89faaffe2165",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -55,6 +55,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -145,6 +146,7 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = getattr(config, \"sliding_window\", None)"
        },
        {
            "sha": "96b5e19615ce47f2f1377e60856f60c66e780958",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -371,6 +371,7 @@ def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3NextRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3NextRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps"
        },
        {
            "sha": "636dab11c9d0b39eed8621ee40cb31b745a86dd0",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -35,7 +35,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1415,6 +1415,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -1467,6 +1468,7 @@ def __init__(self, config, layer_idx):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3OmniMoeThinkerTextRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps\n         )  # unlike olmo, only on the head dim!\n@@ -2348,6 +2350,7 @@ def __init__(self, config: Qwen3OmniMoeConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3OmniMoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3OmniMoeRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps\n@@ -3377,6 +3380,7 @@ def __init__(self, config: Qwen3OmniMoeCode2WavConfig, layer_idx):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = nn.Identity()\n         self.k_norm = nn.Identity()\n         self.sliding_window = config.sliding_window"
        },
        {
            "sha": "5293f097bd46406ffa2cd8015f9a60526f3deff9",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -385,6 +385,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -438,6 +439,7 @@ def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3VLTextRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3VLTextRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps"
        },
        {
            "sha": "28e2c85f156c7624dcefa5a5449c6b1b14fcfa47",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -31,7 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -198,6 +198,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -251,6 +252,7 @@ def __init__(self, config: Qwen3VLMoeTextConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.q_norm = Qwen3VLMoeTextRMSNorm(\n             self.head_dim, eps=config.rms_norm_eps\n         )  # unlike olmo, only on the head dim!"
        },
        {
            "sha": "877d5eaa4fc0406cae5531a69fece61d3c3c329d",
            "filename": "src/transformers/models/seed_oss/modeling_seed_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -27,7 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n@@ -90,6 +90,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "c59906f636922d125a33ae42e7191c9571a1cbb6",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -118,6 +118,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -208,6 +209,7 @@ def __init__(self, config: SmolLM3Config, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n \n         self.use_rope = config.no_rope_layers[layer_idx]\n         self.sliding_window = ("
        },
        {
            "sha": "c013e97f2169004f782c723f4f96025d6027340c",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -35,6 +35,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -74,6 +75,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -155,6 +157,7 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.residual_dropout = config.residual_dropout\n \n     def forward("
        },
        {
            "sha": "5c0410655ee45d91cf9d776e234e9b8e329d39ef",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -162,6 +163,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -263,6 +265,7 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n \n@@ -338,6 +341,7 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n \n         if config.cross_attention_hidden_size is None:"
        },
        {
            "sha": "077b5f4cdff9e6ece58bce8e53737751f03fddde",
            "filename": "src/transformers/models/vaultgemma/modeling_vaultgemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -84,6 +85,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -184,6 +186,7 @@ def __init__(self, config: VaultGemmaConfig, layer_idx: int):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n+        self.rotary_fn = apply_rotary_pos_emb\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if self.layer_type == \"sliding_attention\" else None\n "
        },
        {
            "sha": "9ade9d1dfc8d1cb2948e15d653f69ce9e55f4261",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6587d77ffcf647b848a71c32ce9cad3496e341b2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=6587d77ffcf647b848a71c32ce9cad3496e341b2",
            "patch": "@@ -32,6 +32,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -316,6 +317,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        }
    ],
    "stats": {
        "total": 412,
        "additions": 369,
        "deletions": 43
    }
}