{
    "author": "zucchini-nlp",
    "message": "[video processors] support frame sampling within processors (#38105)\n\n* apply updates smolVLM (still needs workaround for chat template)\n\n* add other models\n\n* dump qwen omni for now, come back later\n\n* port qwen omni from their impl\n\n* wait, all qwens sample videos in same way!\n\n* clean up\n\n* make smolvlm backwards compatible and fix padding\n\n* dix some tests\n\n* fox smolvlm tests\n\n* more clean up and test fixing\n\n* delete unused arg\n\n* fix\n\n* address comments\n\n* style\n\n* fix test",
    "sha": "27459025b8f77d53631f7961cc967fa659d43f7e",
    "files": [
        {
            "sha": "f120006d40dd4e64daa48136112c987763fc2999",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -35,7 +35,7 @@\n )\n from ...utils.import_utils import requires\n from ...video_processing_utils import BaseVideoProcessor\n-from ...video_utils import group_videos_by_shape, reorder_videos\n+from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n \n \n if is_vision_available():\n@@ -66,6 +66,7 @@ class InstructBlipVideoVideoProcessor(BaseVideoProcessor):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n+    do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n     valid_kwargs = InstructBlipVideoVideoProcessorInitKwargs\n     model_input_names = [\"pixel_values\"]\n \n@@ -75,6 +76,7 @@ def __init__(self, **kwargs: Unpack[InstructBlipVideoVideoProcessorInitKwargs]):\n     def _preprocess(\n         self,\n         videos: List[\"torch.Tensor\"],\n+        video_metadata: Union[List[VideoMetadata], List[dict]],\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -86,10 +88,18 @@ def _preprocess(\n         do_pad: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n+        do_sample_frames: bool,\n         image_mean: Optional[Union[float, List[float]]],\n         image_std: Optional[Union[float, List[float]]],\n-        return_tensors: Optional[Union[str, TensorType]],\n+        fps: Optional[int] = None,\n+        num_frames: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n     ) -> BatchFeature:\n+        if do_sample_frames:\n+            videos = [\n+                self.sample_frames(video, metadata, num_frames, fps) for video, metadata in zip(videos, video_metadata)\n+            ]\n+\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "bd47a231e5c65f8a5f0139428699cde6191b6550",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 61,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -21,7 +21,7 @@\n from ...image_utils import ImageInput, concatenate_list, make_flat_list_of_images\n from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...video_utils import VideoInput, VideoMetadata, load_video, make_batched_videos\n+from ...video_utils import VideoInput, make_batched_videos\n \n \n class InternVLImagesKwargs(ImagesKwargs, total=False):\n@@ -290,32 +290,6 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    def sample_indices_fn(\n-        self, metadata: VideoMetadata, num_frames: Optional[int] = None, initial_shift: Union[bool, float, int] = True\n-    ):\n-        \"\"\"\n-        The function to generate indices of frames to sample from a video.\n-\n-        Args:\n-            metadata (`VideoMetadata`):\n-                `VideoMetadata` object containing metadata about the video, such as \"total_num_frames\" or \"fps\".\n-            num_frames (`int`, *optional*):\n-                Number of frames to sample uniformly. If None, all frames are sampled.\n-            initial_shift (`bool`, `float` or `int`, defaults to `0`):\n-                The initial shift to apply when sampling frames. If `True`, the shift is set so that frames are sampled from the middle of the video.\n-\n-        Returns:\n-            `np.ndarray`: Array of frame indices to sample.\n-        \"\"\"\n-        num_frames = num_frames if num_frames is not None else metadata.total_num_frames\n-\n-        if initial_shift is True:\n-            initial_shift = metadata.total_num_frames / num_frames / 2\n-        indices = np.arange(initial_shift, metadata.total_num_frames, metadata.total_num_frames / num_frames).astype(\n-            int\n-        )\n-        return indices\n-\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n@@ -336,39 +310,5 @@ def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(tokenizer_input_names) + list(image_processor_input_names)\n \n-    # TODO: raushan, has to be public method under `VideoProcessorBase` when API is added\n-    def _load_video_for_model(\n-        self,\n-        video: Union[str, \"VideoInput\"],\n-        num_frames: Optional[int],\n-        backend: str = \"pyav\",\n-        initial_shift: bool = True,\n-        **kwargs,\n-    ) -> np.array:\n-        \"\"\"\n-        Loads `video` to a numpy array.\n-\n-        Args:\n-            video (`str` or `VideoInput`):\n-                The video to convert to the numpy array format. Can be a link to video or local path.\n-            num_frames (`int`, *optional*):\n-                Number of frames to sample uniformly. If not passed, the whole video is loaded.\n-            backend (`str`, *optional*, defaults to `\"pyav\"`):\n-                The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"pyav\".\n-            initial_shift (`bool`, *optional*, defaults to `True`):\n-                The initial shift to apply when sampling frames. If `True`, the shift is set so that frames are sampled from the middle of the video.\n-\n-        Returns:\n-            Tuple[`np.array`, Dict]: A tuple containing:\n-                - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n-                - Metadata dictionary.\n-        \"\"\"\n-\n-        def sample_indices_fn_func(metadata, **fn_kwargs):\n-            return self.sample_indices_fn(metadata, num_frames=num_frames, initial_shift=initial_shift, **fn_kwargs)\n-\n-        video, metadata = load_video(video, backend=backend, sample_indices_fn=sample_indices_fn_func)\n-        return video, metadata\n-\n \n __all__ = [\"InternVLProcessor\"]"
        },
        {
            "sha": "74f5981af95b18169e6a3c53a8c0a687731e4999",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 139,
            "deletions": 4,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -14,25 +14,43 @@\n # limitations under the License.\n \"\"\"Fast Video processor class for InternVL.\"\"\"\n \n+from typing import List, Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n     OPENAI_CLIP_STD,\n+    SizeDict,\n )\n from ...processing_utils import Unpack, VideosKwargs\n from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n     is_vision_available,\n )\n from ...utils.import_utils import requires\n-from ...video_processing_utils import (\n-    BaseVideoProcessor,\n-)\n+from ...video_processing_utils import BaseVideoProcessor\n+from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n+\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n \n+if is_torch_available():\n+    import torch\n \n if is_vision_available():\n     from ...image_utils import PILImageResampling\n \n \n-class InternVLVideoProcessorInitKwargs(VideosKwargs): ...\n+class InternVLVideoProcessorInitKwargs(VideosKwargs):\n+    initial_shift: Union[bool, float, int]\n \n \n @requires(backends=(\"torchvision\",))\n@@ -45,11 +63,128 @@ class InternVLVideoProcessor(BaseVideoProcessor):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n+    initial_shift = True\n+    do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n     valid_kwargs = InternVLVideoProcessorInitKwargs\n     model_input_names = [\"pixel_values_videos\"]\n \n     def __init__(self, **kwargs: Unpack[InternVLVideoProcessorInitKwargs]):\n         super().__init__(**kwargs)\n \n+    def sample_frames(\n+        self,\n+        video: \"torch.Tensor\",\n+        metadata: Optional[Union[VideoMetadata, dict]] = None,\n+        num_frames: Optional[int] = None,\n+        fps: Optional[int] = None,\n+        initial_shift: Optional[Union[bool, float, int]] = None,\n+    ):\n+        \"\"\"\n+        Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n+        If `fps` is passed along with metadata, `fps` frames per second are sampled uniformty. Arguments `num_frames`\n+        and `fps` are mutually exclusive.\n+\n+        Args:\n+            video (`torch.Tensor`):\n+                Video that need to be sampled.\n+            metadata (`VideoMetadata`, *optional*):\n+                Metadata of the video containing information about total duration, fps and total number of frames.\n+            num_frames (`int`, *optional*):\n+                Maximum number of frames to sample. Defaults to `self.num_frames`.\n+            fps (`int`, *optional*):\n+                Target frames to sample per second. Defaults to `self.fps`.\n+            initial_shift (`bool`, `float` or `int`, defaults to `self.initial_shift`):\n+                The initial shift to apply when sampling frames. If `True`, the shift is set so that frames are sampled from the middle of the video.\n+\n+        Returns:\n+            torch.Tensor:\n+                Sampled video frames.\n+        \"\"\"\n+        num_frames = num_frames if num_frames is not None else self.num_frames\n+        initial_shift = initial_shift if initial_shift is not None else self.initial_shift\n+        total_num_frames = video.shape[0]\n+\n+        # If num_frames is not given but fps is, calculate num_frames from fps\n+        if num_frames is None and fps is not None:\n+            if metadata is None:\n+                raise ValueError(\n+                    \"Asked to sample `fps` frames per second but no video metadata was provided which is required when sampling with `fps`. \"\n+                    \"Please pass in `VideoMetadata` object or use a fixed `num_frames` per input video\"\n+                )\n+            num_frames = int(total_num_frames / metadata[\"fps\"] * fps)\n+\n+        if initial_shift is True:\n+            initial_shift = total_num_frames / num_frames / 2\n+\n+        if num_frames > total_num_frames:\n+            raise ValueError(\n+                f\"Video can't be sampled. The `num_frames={num_frames}` exceeds `total_num_frames={total_num_frames}`. \"\n+            )\n+\n+        indices = torch.arange(initial_shift, total_num_frames, total_num_frames / num_frames).int()\n+        video = video[indices].contiguous()\n+        return video\n+\n+    def _preprocess(\n+        self,\n+        videos: List[\"torch.Tensor\"],\n+        video_metadata: Union[List[VideoMetadata], List[dict]],\n+        do_convert_rgb: bool,\n+        do_resize: bool,\n+        size: SizeDict,\n+        size_divisor: Optional[int],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        do_pad: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        do_sample_frames: Optional[bool] = None,\n+        fps: Optional[int] = None,\n+        num_frames: Optional[int] = None,\n+        initial_shift: Optional[Union[bool, float, int]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+    ) -> BatchFeature:\n+        if do_sample_frames:\n+            # Sample video frames\n+            videos = [\n+                self.sample_frames(video, metadata, fps=fps, num_frames=num_frames, initial_shift=initial_shift)\n+                for video, metadata in zip(videos, video_metadata)\n+            ]\n+\n+        # Group videos by size for batched resizing\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n+        resized_videos_grouped = {}\n+        for shape, stacked_videos in grouped_videos.items():\n+            if do_convert_rgb:\n+                stacked_videos = self.convert_to_rgb(stacked_videos)\n+            if do_resize:\n+                stacked_videos = self.resize(\n+                    stacked_videos, size=size, size_divisor=size_divisor, interpolation=interpolation\n+                )\n+            resized_videos_grouped[shape] = stacked_videos\n+        resized_videos = reorder_videos(resized_videos_grouped, grouped_videos_index)\n+\n+        # Group videos by size for further processing\n+        # Needed in case do_resize is False, or resize returns videos with different sizes\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(resized_videos)\n+        processed_videos_grouped = {}\n+        for shape, stacked_videos in grouped_videos.items():\n+            if do_center_crop:\n+                stacked_videos = self.center_crop(stacked_videos, crop_size)\n+            # Fused rescale and normalize\n+            stacked_videos = self.rescale_and_normalize(\n+                stacked_videos, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_videos_grouped[shape] = stacked_videos\n+\n+        processed_videos = reorder_videos(processed_videos_grouped, grouped_videos_index)\n+        processed_videos = torch.stack(processed_videos, dim=0) if return_tensors else processed_videos\n+\n+        return BatchFeature(data={\"pixel_values_videos\": processed_videos}, tensor_type=return_tensors)\n+\n \n __all__ = [\"InternVLVideoProcessor\"]"
        },
        {
            "sha": "95cd79da655112d554cece4857fd76fa8fed0bae",
            "filename": "src/transformers/models/llava_next_video/video_processing_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fvideo_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fvideo_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fvideo_processing_llava_next_video.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -46,6 +46,7 @@ class LlavaNextVideoVideoProcessor(BaseVideoProcessor):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n+    do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n     valid_kwargs = LlavaNextVideoFastVideoProcessorInitKwargs\n     model_input_names = [\"pixel_values_videos\"]\n "
        },
        {
            "sha": "3972f424a94fbb220b0f2e0f7c0f75c120cf661b",
            "filename": "src/transformers/models/llava_onevision/video_processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -47,6 +47,7 @@ class LlavaOnevisionVideoProcessor(BaseVideoProcessor):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n+    do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n     valid_kwargs = LlavaOnevisionFastVideoProcessorInitKwargs\n     model_input_names = [\"pixel_values_videos\"]\n "
        },
        {
            "sha": "e211b8d911fcea14218a3a47a092cdfb7d1a9805",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -154,7 +154,7 @@ def __call__(\n         seconds_per_chunk = output_kwargs[\"videos_kwargs\"].pop(\"seconds_per_chunk\")\n         position_id_per_seconds = output_kwargs[\"videos_kwargs\"].pop(\"position_id_per_seconds\")\n         use_audio_in_video = output_kwargs[\"videos_kwargs\"].pop(\"use_audio_in_video\")\n-        fps = output_kwargs[\"videos_kwargs\"].pop(\"fps\", 2.0)\n+        fps = output_kwargs[\"videos_kwargs\"].get(\"fps\", 2.0)\n \n         if audio is not None:\n             output_kwargs[\"audio_kwargs\"][\"padding\"] = \"max_length\"  # Support \"max_length\" padding only here"
        },
        {
            "sha": "e4685acc6443d691d968882a2711774ddba623ad",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -928,7 +928,6 @@ class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n             \"padding\": False,\n             \"return_mm_token_type_ids\": False,\n         },\n-        \"videos_kwargs\": {\"fps\": 2.0},\n     }\n \n \n@@ -1013,9 +1012,7 @@ def __call__(\n             image_grid_thw = image_inputs[\"image_grid_thw\"]\n \n         if videos is not None:\n-            # pop fps in advance for passing kwargs validation\n-            fps = output_kwargs[\"videos_kwargs\"].pop(\"fps\", 2.0)\n-\n+            fps = output_kwargs[\"videos_kwargs\"].get(\"fps\", 2.0)\n             videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n             video_grid_thw = videos_inputs[\"video_grid_thw\"]\n "
        },
        {
            "sha": "e145791eea9219ec4b990901bd33102baf84caba",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -54,7 +54,6 @@ class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n             \"padding\": False,\n             \"return_mm_token_type_ids\": False,\n         },\n-        \"videos_kwargs\": {\"fps\": 2.0},\n     }\n \n \n@@ -151,9 +150,7 @@ def __call__(\n             image_grid_thw = image_inputs[\"image_grid_thw\"]\n \n         if videos is not None:\n-            # pop fps in advance for passing kwargs validation\n-            fps = output_kwargs[\"videos_kwargs\"].pop(\"fps\", 2.0)\n-\n+            fps = output_kwargs[\"videos_kwargs\"].get(\"fps\", 2.0)\n             videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n             video_grid_thw = videos_inputs[\"video_grid_thw\"]\n "
        },
        {
            "sha": "49a4e9d2efca4be9c032e6342a0ad29d99883058",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 105,
            "deletions": 4,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -19,6 +19,7 @@\n # limitations under the License.\n \"\"\"video processor class for Qwen2-VL.\"\"\"\n \n+import math\n from typing import List, Optional, Union\n \n from ...image_processing_utils import (\n@@ -45,7 +46,7 @@\n     BASE_VIDEO_PROCESSOR_DOCSTRING,\n     BaseVideoProcessor,\n )\n-from ...video_utils import group_videos_by_shape, reorder_videos\n+from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n \n \n if is_vision_available():\n@@ -69,6 +70,8 @@ class Qwen2VLVideoProcessorInitKwargs(VideosKwargs):\n     patch_size: Optional[int]\n     temporal_patch_size: Optional[int]\n     merge_size: Optional[int]\n+    min_frames: Optional[int]\n+    max_frames: Optional[int]\n \n \n @add_start_docstrings(\n@@ -85,50 +88,148 @@ class Qwen2VLVideoProcessorInitKwargs(VideosKwargs):\n             The temporal patch size of the vision encoder.\n         merge_size (`int`, *optional*, defaults to 2):\n             The merge size of the vision encoder to llm encoder.\n+        min_frames (`int`, *optional*, defaults to 4):\n+            The minimum number of frames that can be sampled.\n+        max_frames (`int`, *optional*, defaults to 768):\n+            The maximum number of frames that can be sampled.\n     \"\"\",\n )\n @requires(backends=(\"torchvision\",))\n class Qwen2VLVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n-    size = {\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}\n+    size = {\"shortest_edge\": 128 * 28 * 28, \"longest_edge\": 28 * 28 * 768}\n     image_mean = OPENAI_CLIP_MEAN\n     image_std = OPENAI_CLIP_STD\n     do_resize = True\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n-    min_pixels = 56 * 56\n-    max_pixels = 28 * 28 * 1280\n+    min_pixels = 128 * 28 * 28\n+    max_pixels = 28 * 28 * 768\n     patch_size = 14\n     temporal_patch_size = 2\n     merge_size = 2\n+    min_frames = 4\n+    max_frames = 768\n+    do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n     valid_kwargs = Qwen2VLVideoProcessorInitKwargs\n     model_input_names = [\"pixel_values_videos\", \"video_grid_thw\"]\n \n     def __init__(self, **kwargs: Unpack[Qwen2VLVideoProcessorInitKwargs]):\n         super().__init__(**kwargs)\n         self.size = {\"shortest_edge\": self.min_pixels, \"longest_edge\": self.max_pixels}\n \n+    def sample_frames(\n+        self,\n+        video: \"torch.Tensor\",\n+        frame_factor: int,\n+        min_frames: int,\n+        max_frames: int,\n+        metadata: Optional[Union[VideoMetadata, dict]] = None,\n+        num_frames: Optional[int] = None,\n+        fps: Optional[int] = None,\n+    ):\n+        \"\"\"\n+        Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n+        If `fps` is passed along with metadata, `fps` frames per second are sampled uniformty. Arguments `num_frames`\n+        and `fps` are mutually exclusive.\n+\n+        Args:\n+            video (`torch.Tensor`):\n+                Video that need to be sampled.\n+            frame_factor (`int`):\n+                The temporal patch size of the vision encoder. Number of sampled frames will be rounded to be divisible by frame factor.\n+            min_frames (`int`):\n+                The minimum number of frames that can be sampled.\n+            max_frames (`int`):\n+                The maximum number of frames that can be sampled.\n+            metadata (`VideoMetadata`, *optional*):\n+                Metadata of the video containing information about total duration, fps and total number of frames.\n+            num_frames (`int`, *optional*):\n+                Maximum number of frames to sample. Defaults to `self.num_frames`.\n+            fps (`int`, *optional*):\n+                Target frames to sample per second. Defaults to `self.fps`.\n+\n+        Returns:\n+            torch.Tensor:\n+                Sampled video frames.\n+        \"\"\"\n+        if fps is not None and num_frames is not None:\n+            raise ValueError(\"`num_frames` and `fps` are mutually exclusive arguments, please use only one!\")\n+\n+        num_frames = num_frames if num_frames is not None else self.num_frames\n+        fps = fps if fps is not None else self.fps\n+        total_num_frames = video.shape[0]\n+\n+        # If num_frames is not given but fps is, calculate num_frames from fps\n+        if num_frames is not None:\n+            num_frames = round(num_frames / frame_factor) * frame_factor\n+        elif fps is not None:\n+            if metadata is None:\n+                raise ValueError(\n+                    \"Asked to sample `fps` frames per second but no video metadata was provided which is required when sampling with `fps`. \"\n+                    \"Please pass in `VideoMetadata` object or use a fixed `num_frames` per input video\"\n+                )\n+            max_frames = math.floor(min(max_frames, total_num_frames) / frame_factor) * frame_factor\n+            num_frames = total_num_frames / metadata[\"fps\"] * fps\n+            num_frames = min(min(max(num_frames, min_frames), max_frames), total_num_frames)\n+            num_frames = math.floor(num_frames / frame_factor) * frame_factor\n+\n+        if num_frames > total_num_frames:\n+            raise ValueError(\n+                f\"Video can't be sampled. The inferred `num_frames={num_frames}` exceeds `total_num_frames={total_num_frames}`. \"\n+                \"Decrease `num_frames` or `fps` for sampling.\"\n+            )\n+\n+        if num_frames is not None:\n+            indices = torch.arange(0, total_num_frames, total_num_frames / num_frames).int()\n+        else:\n+            indices = torch.arange(0, total_num_frames).int()\n+        video = video[indices].contiguous()\n+\n+        return video\n+\n     def _preprocess(\n         self,\n         videos: List[\"torch.Tensor\"],\n+        video_metadata: Union[List[VideoMetadata], List[dict]],\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n+        do_sample_frames: bool,\n         image_mean: Optional[Union[float, List[float]]],\n         image_std: Optional[Union[float, List[float]]],\n         min_pixels: Optional[int] = None,\n         max_pixels: Optional[int] = None,\n         patch_size: Optional[int] = None,\n         temporal_patch_size: Optional[int] = None,\n         merge_size: Optional[int] = None,\n+        fps: Optional[int] = None,\n+        num_frames: Optional[int] = None,\n+        min_frames: Optional[int] = None,\n+        max_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ):\n+        if do_sample_frames:\n+            # Sample video frames\n+            videos = [\n+                self.sample_frames(\n+                    video,\n+                    frame_factor=temporal_patch_size,\n+                    min_frames=min_frames,\n+                    max_frames=max_frames,\n+                    metadata=metadata,\n+                    num_frames=num_frames,\n+                    fps=fps,\n+                )\n+                for video, metadata in zip(videos, video_metadata)\n+            ]\n+\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "8613a2f88c67a87e435d83ef6bda8516265dd14e",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 107,
            "deletions": 157,
            "changes": 264,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -16,26 +16,29 @@\n Processor class for SmolVLM.\n \"\"\"\n \n-import copy\n from datetime import timedelta\n from typing import TYPE_CHECKING, Dict, List, Optional, Union\n \n-import numpy as np\n-\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, make_nested_list_of_images\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import AllKwargsForChatTemplate, ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, TextInput\n from ...utils import is_num2words_available, is_vision_available, logging\n-from ...video_utils import VideoInput, load_video, make_batched_videos\n+from ...video_utils import VideoInput\n \n \n if is_vision_available():\n     from .video_processing_smolvlm import (\n         DEFAULT_MEDIA_OUTTRO,\n         DEFAULT_VIDEO_INTRO,\n         FRAME_TIMESTAMP_MESSAGE,\n-        smolvlm_sample_indices_fn,\n+    )\n+\n+if is_vision_available():\n+    from .video_processing_smolvlm import (\n+        DEFAULT_MEDIA_OUTTRO,\n+        DEFAULT_VIDEO_INTRO,\n+        FRAME_TIMESTAMP_MESSAGE,\n     )\n \n if TYPE_CHECKING:\n@@ -50,6 +53,10 @@\n     num2words = None\n \n \n+# The correct chat template to be used for videos after #38105\n+DEFAULT_CHAT_TEMPLATE = \"<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% elif line['type'] == 'video' %}{{ '<video>' }}{% endif %}{% endfor %}<end_of_utterance>\\n{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\"\n+\n+\n def _prompt_split_image(\n     image_seq_len, image_rows, image_cols, fake_token_around_image, image_token, global_image_token\n ):\n@@ -140,9 +147,7 @@ class SmolVLMProcessor(ProcessorMixin):\n \n     attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n     image_processor_class = \"SmolVLMImageProcessor\"\n-    video_processor_class = (\n-        \"SmolVLMImageProcessor\"  # TODO: raushan should be VideoProcessor when LANCZOS resizing is settled\n-    )\n+    video_processor_class = \"SmolVLMVideoProcessor\"  # NOTE: uses different interpolation than slow processors\n     tokenizer_class = \"AutoTokenizer\"\n \n     def __init__(\n@@ -160,17 +165,7 @@ def __init__(\n         self.end_of_utterance_token = getattr(tokenizer, \"end_of_utterance_token\", \"<end_of_utterance>\")\n         self.global_image_token = getattr(tokenizer, \"global_image_token\", \"<global-img>\")\n         self.image_seq_len = image_seq_len\n-\n-        self.video_size = video_processor.video_sampling[\"video_size\"]\n-        self.image_size = image_processor.size\n-\n-        self.do_image_splitting = image_processor.do_image_splitting\n-        self.do_video_splitting = video_processor.video_sampling.get(\"do_image_splitting\", False)\n-\n-        self.default_max_frames = video_processor.video_sampling[\"max_frames\"]\n-        self.default_fps = video_processor.video_sampling[\"fps\"]\n-        # Matches one or more occurrences of <row_x_col_y> tags (where x and y are digits, optionally surrounded by newline characters\n-        # self._regex_to_remove_extra_special_tokens = re.compile(r\"(<row_\\d+_col_\\d+>\\n?)+\")\n+        self.video_token = getattr(tokenizer, \"video_token\", \"<video>\")\n \n         if not num2words:\n             raise ImportError(\n@@ -179,16 +174,12 @@ def __init__(\n \n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template, **kwargs)\n \n-    def process_vision(\n-        self, text, images, output_kwargs, do_image_splitting=False, image_processor_size=None, processor=None\n-    ):\n+    def process_vision(self, text, images, output_kwargs):\n         if text is not None:\n             n_images_in_text = [sample.count(self.image_token) for sample in text]\n \n         n_images_in_images = [len(sublist) for sublist in images]\n-        image_inputs = processor(\n-            images, do_image_splitting=do_image_splitting, size=image_processor_size, **output_kwargs\n-        )\n+        image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n \n         if text is None:\n             return None, image_inputs\n@@ -227,6 +218,50 @@ def process_vision(\n \n         return prompt_strings, image_inputs\n \n+    def process_video(self, text, videos, output_kwargs):\n+        if text is not None:\n+            n_videos_in_text = [sample.count(self.video_token) for sample in text]\n+\n+        n_videos_in_videos = [len(sublist) for sublist in videos]\n+        video_inputs = self.video_processor(videos, **output_kwargs[\"videos_kwargs\"])\n+\n+        num_frames = video_inputs[\"pixel_values\"].shape[1]\n+        batch_timestamps = iter(video_inputs.pop(\"timestamps\"))\n+        batch_durations = iter(video_inputs.pop(\"durations\"))\n+\n+        if text is None:\n+            return None, video_inputs\n+\n+        if n_videos_in_videos != n_videos_in_text:\n+            raise ValueError(\n+                f\"The number of videos in the text {n_videos_in_text} and videos {n_videos_in_videos} should be the same.\"\n+            )\n+\n+        prompt_strings = []\n+        for sample in text:\n+            while self.video_token in sample:\n+                timestamps = next(batch_timestamps)\n+                duration = next(batch_durations)\n+                duration_td = timedelta(seconds=int(duration))\n+                image_prompt_strings = DEFAULT_VIDEO_INTRO.format(\n+                    frame_count=num2words(num_frames), video_duration=str(duration_td)\n+                )\n+                for timestamp in timestamps:\n+                    image_prompt_string = _prompt_single_image(\n+                        self.image_seq_len,\n+                        image_token=self.image_token,\n+                        fake_token_around_image=self.fake_image_token,\n+                        global_image_token=self.global_image_token,\n+                    )\n+                    timestamp = f\"{timestamp[0]:02d}:{timestamp[1]:02d}\"\n+                    image_prompt_string = FRAME_TIMESTAMP_MESSAGE.format(timestamp=timestamp) + image_prompt_string\n+                    image_prompt_strings += image_prompt_string\n+\n+                image_prompt_strings += DEFAULT_MEDIA_OUTTRO\n+                sample = sample.replace(self.video_token, image_prompt_strings, 1)\n+            prompt_strings.append(sample)\n+        return prompt_strings, video_inputs\n+\n     def __call__(\n         self,\n         images: Union[ImageInput, List[ImageInput], List[List[ImageInput]]] = None,\n@@ -310,21 +345,14 @@ def __call__(\n             text, vision_inputs = self.process_vision(\n                 text,\n                 images,\n-                output_kwargs[\"images_kwargs\"],\n-                do_image_splitting=self.do_image_splitting,\n-                image_processor_size=self.image_size,\n-                processor=self.image_processor,\n+                output_kwargs,\n             )\n             inputs.update(vision_inputs)\n         elif videos is not None:\n-            videos = make_batched_videos(videos)\n-            text, vision_inputs = self.process_vision(\n+            text, vision_inputs = self.process_video(\n                 text,\n                 videos,\n-                output_kwargs[\"videos_kwargs\"],\n-                do_image_splitting=self.do_image_splitting,\n-                image_processor_size=self.video_size,\n-                processor=self.video_processor,\n+                output_kwargs,\n             )\n             inputs.update(vision_inputs)\n \n@@ -337,93 +365,6 @@ def __call__(\n \n         return BatchFeature(inputs, tensor_type=return_tensors)\n \n-    def _process_messages_for_chat_template(\n-        self,\n-        conversations: List[List[Dict[str, str]]],\n-        batch_images: List[ImageInput],\n-        batch_videos: List[VideoInput],\n-        batch_video_metadata: List[List[Dict[str, any]]],\n-        **chat_template_kwargs,\n-    ):\n-        \"\"\"\n-        Used within `apply_chat_template` when a model has special way to process conversation history. For example,\n-        video models might want to specify in the prompt the duration of video or which frame indices at which timestamps\n-        were sampled. This information cannot be accessed before the video is loaded.\n-        For most models it is a no-op, must be overridden by model processors which require special processing.\n-        Args:\n-            conversation (`List[Dict, str, str]`):\n-                The conversation to process. Always comes in batched format.\n-            batch_images (`List[List[ImageInput]]`):\n-                Batch of images that were loaded from url/path defined in the conversation. The images\n-                are ordered in the same way as in the conversation. Comes in nested list format, one list of `PIL` images\n-                per batch.\n-            batch_videos (`List[List[ImageInput]]`):\n-                Batch of videos that were loaded from url/path defined in the conversation. The videos\n-                are ordered in the same way as in the conversation. Comes in nested list format, one list of 4D video arrays\n-                per batch.\n-            batch_video_metadata (`List[List[Dict[[str, any]]]]`):\n-                Batch of metadata returned from loading videos. That includes video fps, duration and total number of framer in original video.\n-                Metadata are ordered in the same way as `batch_videos`. Comes in nested list format, one list of 4D video arrays\n-                per batch.\n-        \"\"\"\n-        # We don't want to modify in-place the messages passed by user\n-        # The user might want to add new turn on conv and continue generation\n-        conversations = copy.deepcopy(conversations)\n-        batch_num_frames, batch_timestamps = [], []\n-        for metadata_list, video_list in zip(batch_video_metadata, batch_videos):\n-            for metadata, video in zip(metadata_list, video_list):\n-                duration_sec = getattr(metadata, \"duration\")\n-                frames_idx = getattr(metadata, \"frames_indices\")\n-                fps = getattr(metadata, \"fps\")\n-\n-                timestamps = []\n-                for idx, frame_np in zip(frames_idx, video):\n-                    sec = idx / fps\n-                    mm = int(sec // 60)\n-                    ss = int(sec % 60)\n-                    timestamps.append(f\"{mm:02d}:{ss:02d}\")\n-                batch_timestamps.append(timestamps)\n-                batch_num_frames.append(len(video))\n-\n-        for conversation in conversations:\n-            # For each message, scan content for {\"type\": \"video\"}\n-            for msg in conversation:\n-                if \"content\" not in msg:\n-                    continue\n-\n-                new_content = []\n-                for block in msg[\"content\"]:\n-                    if block.get(\"type\") == \"video\":\n-                        curr_timestamps = batch_timestamps.pop(0)\n-                        curr_num_frames = batch_num_frames.pop(0)\n-\n-                        # Build the video intro texts\n-                        td = timedelta(seconds=int(duration_sec))\n-                        new_content.append(\n-                            {\n-                                \"type\": \"text\",\n-                                \"text\": DEFAULT_VIDEO_INTRO.format(\n-                                    frame_count=num2words(curr_num_frames), video_duration=str(td)\n-                                ),\n-                            }\n-                        )\n-\n-                        # 2) Insert per-frame lines: \"Frame from {timestamp}:\", then an \"image\" block\n-                        for i, ts in enumerate(curr_timestamps):\n-                            new_content.append({\"type\": \"text\", \"text\": FRAME_TIMESTAMP_MESSAGE.format(timestamp=ts)})\n-                            new_content.append({\"type\": \"image\"})\n-\n-                        # 3) Optionally add an outro (e.g. \"Now answer the question:\")\n-                        new_content.append({\"type\": \"text\", \"text\": DEFAULT_MEDIA_OUTTRO})\n-                        # Do NOT add the original block => we skip it (since we've replaced it)\n-                    else:\n-                        # keep original block\n-                        new_content.append(block)\n-\n-                # update the content\n-                msg[\"content\"] = new_content\n-        return conversations\n-\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to SmolVLMTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n@@ -446,45 +387,54 @@ def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(image_processor_input_names + tokenizer_input_names))\n \n-    # TODO: raushan, has to be public method under `VideoProcessorBase` when API is added\n-    def _load_video_for_model(\n+    def apply_chat_template(\n         self,\n-        video: Union[str, \"VideoInput\"],\n-        num_frames: Optional[int] = None,\n-        fps: Optional[int] = None,\n-        backend: str = \"opencv\",\n-        skip_secs: int = 0.0,\n-        **kwargs,\n-    ) -> np.array:\n+        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n+        chat_template: Optional[str] = None,\n+        **kwargs: Unpack[AllKwargsForChatTemplate],\n+    ) -> str:\n         \"\"\"\n-        Loads `video` to a numpy array.\n+        Similar to the `apply_chat_template` method on tokenizers, this method applies a Jinja template to input\n+        conversations to turn them into a single tokenizable string.\n+\n+        The input is expected to be in the following format, where each message content is a list consisting of text and\n+        optionally image or video inputs. One can also provide an image, video, URL or local path which will be used to form\n+        `pixel_values` when `return_dict=True`. If not provided, one will get only the formatted text, optionally tokenized text.\n+\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Please describe this image in detail.\"},\n+                ],\n+            },\n+        ]\n \n         Args:\n-            video (`str` or `VideoInput`):\n-                The video to convert to the numpy array format. Can be a link to video or local path.\n-            num_frames (`int`, *optional*):\n-                Number of frames to sample uniformly. If not passed, the whole video is loaded.\n-            fps (`int`, *optional*):\n-                Number of frames to sample per second. Should be passed only when `num_frames=None`.\n-                If not specified and `num_frames==None`, all frames are sampled.\n-            backend (`str`, *optional*, defaults to `\"opencv\"`):\n-                The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"opencv\".\n-\n-        Returns:\n-            Tuple[`np.array`, Dict]: A tuple containing:\n-                - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n-                - Metadata dictionary.\n+            conversation (`Union[List[Dict, [str, str]], List[List[Dict[str, str]]]]`):\n+                The conversation to format.\n+            chat_template (`Optional[str]`, *optional*):\n+                The Jinja template to use for formatting the conversation. If not provided, the tokenizer's\n+                chat template is used.\n         \"\"\"\n-        max_frames = self.default_max_frames if num_frames is None else num_frames\n-        target_fps = self.default_fps if fps is None else fps\n-\n-        def sample_indices_fn_func(metadata, **fn_kwargs):\n-            return smolvlm_sample_indices_fn(\n-                metadata, max_frames=max_frames, target_fps=target_fps, skip_secs=skip_secs, **fn_kwargs\n-            )\n-\n-        video, metadata = load_video(video, backend=backend, sample_indices_fn=sample_indices_fn_func)\n-        return video, metadata\n+        if isinstance(conversation, (list, tuple)) and (\n+            isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], \"content\")\n+        ):\n+            conversations = conversation\n+        else:\n+            conversations = [conversation]\n+\n+        has_video = any(\n+            (isinstance(content, dict) and content[\"type\"] == \"video\")\n+            for conversation in conversations\n+            for message in conversation\n+            for content in message[\"content\"]\n+        )\n+        if chat_template is None and has_video:\n+            # re-assign to the correct default template for BC, if user is not requesting their own template\n+            chat_template = DEFAULT_CHAT_TEMPLATE\n+        return super().apply_chat_template(conversation, chat_template, **kwargs)\n \n \n __all__ = [\"SmolVLMProcessor\"]"
        },
        {
            "sha": "dbfc94ba2f8d65cedda9bf31bdcc93e357e79ac1",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 133,
            "deletions": 71,
            "changes": 204,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -13,13 +13,13 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n from typing import List, Optional, Union\n \n import numpy as np\n \n from ...image_processing_utils import (\n     BatchFeature,\n+    get_size_dict,\n )\n from ...image_utils import (\n     IMAGENET_STANDARD_MEAN,\n@@ -38,7 +38,7 @@\n from ...video_processing_utils import (\n     BaseVideoProcessor,\n )\n-from ...video_utils import group_videos_by_shape, reorder_videos\n+from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n \n \n if is_vision_available():\n@@ -68,66 +68,6 @@\n MAX_IMAGE_SIZE = 4096  # 4k resolution as absolute maximum\n \n \n-def smolvlm_sample_indices_fn(metadata, max_frames, target_fps, skip_secs=0):\n-    \"\"\"\n-    Example sampling function which:\n-      - Uses `max_frames` (if provided) or calculates it from `fps` and metadata.\n-      - Applies a basic center-skip if fewer frames than available, otherwise\n-        optionally skips `skip_secs` from both the start and end.\n-      - Uniformly samples the desired number of frames between the start and end indices.\n-\n-    Args:\n-        max_frames (`int`):\n-            Maximum number of frames to sample.\n-        target_fps (`int`):\n-            Target frames to sample per second.\n-        metadata (`dict`):\n-            Contains video metadata such as \"n_frames\" and \"video_fps\".\n-        skip_secs (`float`, *optional*, defaults to 1.0):\n-            Number of seconds to skip from the start and end if the video is long enough.\n-\n-    Returns:\n-        numpy.ndarray:\n-            An array of unique frame indices to sample.\n-    \"\"\"\n-\n-    total_num_frames = getattr(metadata, \"total_num_frames\", 0)\n-    if total_num_frames <= 0:\n-        raise ValueError(f\"Invalid total_num_frames={total_num_frames} in metadata.\")\n-\n-    native_fps = getattr(metadata, \"fps\", 30.0)\n-    duration_seconds = getattr(metadata, \"duration\", 0)\n-\n-    if duration_seconds <= 0:\n-        raise ValueError(f\"Invalid duration_seconds={duration_seconds} in metadata.\")\n-\n-    # Step 1) Estimate how many frames we'd sample at `target_fps`, fallback if target_fps <= 0\n-    estimated_frames = int(round(target_fps * duration_seconds))\n-\n-    # Step 2) desired_frames\n-    desired_frames = min(estimated_frames, max_frames)\n-    if desired_frames < 1:\n-        desired_frames = 1\n-\n-    # Step 3) center skip logic\n-    start_idx = 0\n-    end_idx = total_num_frames - 1\n-\n-    if skip_secs > 0 and (duration_seconds - 2 * skip_secs) > (max_frames * target_fps):\n-        start_idx = int(skip_secs * native_fps)\n-        end_idx = int(total_num_frames - skip_secs * native_fps)\n-\n-    start_idx = max(0, start_idx)\n-    end_idx = min(end_idx, total_num_frames - 1)\n-    if start_idx >= end_idx:\n-        start_idx, end_idx = 0, total_num_frames - 1\n-\n-    indices = np.linspace(start_idx, end_idx, desired_frames, dtype=int)\n-    indices = np.unique(indices)\n-\n-    return indices\n-\n-\n def get_max_height_width(videos: list[\"torch.Tensor\"]) -> List[int]:\n     \"\"\"\n     Get the maximum height and width across all videos in a batch.\n@@ -180,25 +120,37 @@ def get_resize_output_image_size(\n     return height, width\n \n \n-class SmolVLMVideoProcessorInitKwargs(VideosKwargs): ...\n+class SmolVLMVideoProcessorInitKwargs(VideosKwargs):\n+    max_image_size: dict[str, int] = None\n \n \n @requires(backends=(\"torchvision\",))\n class SmolVLMVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.LANCZOS\n     size = {\"longest_edge\": 4 * 364}\n+    max_image_size = {\"longest_edge\": 364}\n     image_mean = IMAGENET_STANDARD_MEAN\n     image_std = IMAGENET_STANDARD_STD\n     do_resize = True\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n     do_pad = True\n+    do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n     valid_kwargs = SmolVLMVideoProcessorInitKwargs\n     model_input_names = [\"pixel_values\", \"pixel_attention_mask\"]\n \n     def __init__(self, **kwargs: Unpack[SmolVLMVideoProcessorInitKwargs]):\n         super().__init__(**kwargs)\n+        # For BC pop values from `config.video_sampling`. In official config `video_sampling` is guaranteed to be present\n+        # We check for `Noneness` only for certain tests such as `test_init_without_params`\n+        if \"size\" in kwargs and \"video_sampling\" in kwargs:\n+            kwargs[\"video_sampling\"][\"video_size\"] = kwargs[\"size\"]\n+\n+        if \"video_sampling\" in kwargs:\n+            self.num_frames = kwargs[\"video_sampling\"][\"max_frames\"]\n+            self.fps = kwargs[\"video_sampling\"][\"fps\"]\n+            self.size = get_size_dict(kwargs[\"video_sampling\"][\"video_size\"], default_to_square=self.default_to_square)\n \n     def resize(\n         self,\n@@ -240,12 +192,20 @@ def resize(\n             new_size = (size.height, size.width)\n         else:\n             raise ValueError(f\"Size must contain 'height' and 'width' keys, or 'longest_edge' key. Got {size}.\")\n-        return F.resize(video, new_size, interpolation=interpolation, antialias=antialias)\n+\n+        video = F.resize(video, new_size, interpolation=interpolation, antialias=antialias)\n+\n+        # Resize again to match image processor when `do_image_splitting=False`. Frames have to be squared to `max_image_size`\n+        # NOTE: videos are always processoed without image splitting\n+        max_size = self.max_image_size[\"longest_edge\"], self.max_image_size[\"longest_edge\"]\n+        video = F.resize(video, max_size, interpolation=interpolation, antialias=antialias)\n+        return video\n \n     def pad(\n         self,\n         video: \"torch.Tensor\",\n         padded_size: tuple[int, int],\n+        max_num_frames: int,\n         fill: int = 0,\n         return_pixel_mask: bool = True,\n     ):\n@@ -255,34 +215,108 @@ def pad(\n                 Video to pad.\n             padded_size (`Tuple[int, int]`):\n                 Height and width to pad.\n+            max_num_frames (`int`):\n+                The maximum number of frames to which video will be padded.\n             fill (`int`, *optional*):\n                 The value to use for the padding.\n             return_pixel_mask (`bool`, *optional*, defaults to `True`):\n                 Whether to return a pixel mask.\n         \"\"\"\n         original_size = video.size()[-2:]\n-        padding_bottom = padded_size[0] - original_size[0]\n-        padding_right = padded_size[1] - original_size[1]\n-        if padding_bottom < 0 or padding_right < 0:\n+        padding_height = padded_size[0] - original_size[0]\n+        padding_width = padded_size[1] - original_size[1]\n+        padding_frame = max_num_frames - video.shape[0]\n+        if padding_width < 0 or padding_height < 0:\n             raise ValueError(\n                 f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n                 f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n             )\n         if original_size != padded_size:\n-            padding = [0, 0, padding_right, padding_bottom]\n+            padding = [0, padding_width, 0, padding_height, 0, 0, 0, padding_frame]\n             video = F.pad(video, padding, fill=fill)\n \n         # Make a pixel mask for the video, where 1 indicates a valid pixel and 0 indicates padding.\n+        # Mask shape is (num_frames, height, width) so we omit the channel dim\n         pixel_mask = None\n         if return_pixel_mask:\n             pixel_mask = torch.zeros_like(video[..., 0, :, :], dtype=torch.int64)\n             pixel_mask[..., : original_size[0], : original_size[1]] = 1\n \n         return video, pixel_mask\n \n+    def sample_frames(\n+        self,\n+        video: \"torch.Tensor\",\n+        metadata: Union[VideoMetadata, dict],\n+        num_frames: Optional[int] = None,\n+        fps: Optional[int] = None,\n+        skip_secs: Optional[int] = 1,\n+    ):\n+        \"\"\"\n+        Video sampling function which:\n+            - Uses `num_frames` (if provided) or calculates it from `fps` and metadata.\n+            - Applies a basic center-skip if fewer frames than available, otherwise\n+                optionally skips `skip_secs` from both the start and end.\n+            - Uniformly samples the desired number of frames between the start and end indices.\n+\n+        Args:\n+            video (`torch.Tensor`):\n+                Video that need to be sampled.\n+            metadata (`VideoMetadata`):\n+                Metadata of the video containing information about total duration, fps and total number of frames.\n+            num_frames (`int`, *optional*):\n+                Maximum number of frames to sample. Defaults to `self.num_frames`.\n+            fps (`int`, *optional*):\n+                Target frames to sample per second. Defaults to `self.fps`.\n+            skip_secs (`float`, *optional*, defaults to `1`):\n+                Number of seconds to skip from the start and end if the video is long enough.\n+\n+        Returns:\n+            torch.Tensor:\n+                Sampled video frames.\n+        \"\"\"\n+        num_frames = num_frames if num_frames is not None else self.num_frames\n+        fps = fps if fps is not None else self.fps\n+\n+        total_num_frames = video.shape[0]\n+\n+        # Step 1) Estimate how many frames we'd sample at `target_fps`, fallback if target_fps <= 0\n+        estimated_frames = int(round(fps * metadata[\"duration\"]))\n+\n+        # Step 2) desired_frames\n+        desired_frames = min(estimated_frames, num_frames)\n+        if desired_frames < 1:\n+            desired_frames = 1\n+\n+        # Step 3) center skip logic\n+        start_idx = 0\n+        end_idx = total_num_frames - 1\n+\n+        if skip_secs > 0 and (metadata[\"duration\"] - 2 * skip_secs) > (num_frames * fps):\n+            start_idx = int(skip_secs * metadata[\"fps\"])\n+            end_idx = int(total_num_frames - skip_secs * metadata[\"fps\"])\n+\n+        start_idx = max(0, start_idx)\n+        end_idx = min(end_idx, total_num_frames - 1)\n+        if start_idx >= end_idx:\n+            start_idx, end_idx = 0, total_num_frames - 1\n+\n+        indices = np.linspace(start_idx, end_idx, desired_frames, dtype=int)\n+        indices = np.unique(indices)\n+        video = video[indices].contiguous()\n+\n+        timestamps = []\n+        for idx in indices:\n+            sec = idx / metadata[\"fps\"]\n+            mm = int(sec // 60)\n+            ss = int(sec % 60)\n+            timestamps.append([mm, ss])\n+        return video, timestamps, int(metadata[\"duration\"])\n+\n     def _preprocess(\n         self,\n         videos: List[\"torch.Tensor\"],\n+        video_metadata: Union[List[VideoMetadata], List[dict]],\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -291,13 +325,38 @@ def _preprocess(\n         rescale_factor: float,\n         do_normalize: bool,\n         do_pad: bool,\n+        do_sample_frames: bool,\n         image_mean: Optional[Union[float, List[float]]],\n         image_std: Optional[Union[float, List[float]]],\n+        fps: Optional[int] = None,\n+        num_frames: Optional[int] = None,\n+        skip_secs: Optional[int] = 0,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ):\n         # Group videos by size for batched resizing\n-        grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n+        if do_sample_frames:\n+            if video_metadata[0] is None:\n+                raise ValueError(\n+                    \"Frame sampling is enabled but no video metadata was found. SmolVLM requires metadata to correctly sample frames. \"\n+                    \"Please pass in `VideoMetadata` object per each input video or set `do_sample_frames=False`\"\n+                )\n+            processed_videos = []\n+            timestamps_list, durations_list = [], []\n+            for video, metadata in zip(videos, video_metadata):\n+                video, timestamps, duration = self.sample_frames(video, metadata, num_frames, fps, skip_secs)\n+                timestamps_list.append(timestamps)\n+                durations_list.append(duration)\n+                processed_videos.append(video)\n+        else:\n+            # Assume 24 fps by default and prepare timestamps for the whole video when all frames are sampled\n+            processed_videos = videos\n+            timestamps_list = [\n+                [(int((idx / 24) // 60), int((idx / 24) % 60)) for idx in range(len(video))] for video in videos\n+            ]\n+            durations_list = [len(video) // 24 for video in videos]\n+\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(processed_videos)\n         resized_videos_grouped = {}\n         for shape, stacked_videos in grouped_videos.items():\n             if do_convert_rgb:\n@@ -319,20 +378,23 @@ def _preprocess(\n \n         if do_pad:\n             pad_size = get_max_height_width(processed_videos)\n+            max_num_frames = max(len(video) for video in processed_videos)\n             grouped_videos, grouped_videos_index = group_videos_by_shape(processed_videos)\n             processed_padded_mask_grouped = {}\n             processed_videos_grouped = {}\n \n             for shape, stacked_videos in grouped_videos.items():\n-                stacked_videos, padded_masks = self.pad(stacked_videos, padded_size=pad_size)\n+                stacked_videos, padded_masks = self.pad(\n+                    stacked_videos, padded_size=pad_size, max_num_frames=max_num_frames\n+                )\n                 processed_videos_grouped[shape] = stacked_videos\n                 processed_padded_mask_grouped[shape] = padded_masks\n \n             processed_videos = reorder_videos(processed_videos_grouped, grouped_videos_index)\n             pixel_attention_mask = reorder_videos(processed_padded_mask_grouped, grouped_videos_index)\n \n         processed_videos = torch.stack(processed_videos, dim=0) if return_tensors else processed_videos\n-        data = {\"pixel_values\": processed_videos}\n+        data = {\"pixel_values\": processed_videos, \"timestamps\": timestamps_list, \"durations\": durations_list}\n \n         if do_pad:\n             data[\"pixel_attention_mask\"] = ("
        },
        {
            "sha": "a05ce9303fe6c72038cc66700ee8ca08127dab58",
            "filename": "src/transformers/models/video_llava/video_processing_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fvideo_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fvideo_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fvideo_processing_video_llava.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -46,6 +46,7 @@ class VideoLlavaVideoProcessor(BaseVideoProcessor):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n+    do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n     valid_kwargs = VideoLlavaFastVideoProcessorInitKwargs\n     model_input_names = [\"pixel_values_videos\"]\n "
        },
        {
            "sha": "02ea9b4210f7bb83c3eaa8b92c23871f2838859c",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 26,
            "deletions": 91,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -24,7 +24,7 @@\n import warnings\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Any, Dict, List, Optional, TypedDict, Union\n+from typing import Any, Dict, Optional, TypedDict, Union\n \n import numpy as np\n import typing_extensions\n@@ -33,9 +33,9 @@\n from .audio_utils import load_audio\n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature\n-from .image_utils import ChannelDimension, ImageInput, is_valid_image, is_vision_available, load_image\n+from .image_utils import ChannelDimension, is_valid_image, is_vision_available, load_image\n from .utils.chat_template_utils import render_jinja_template\n-from .video_utils import VideoInput, load_video\n+from .video_utils import VideoMetadata, load_video\n \n \n if is_vision_available():\n@@ -64,6 +64,7 @@\n     list_repo_templates,\n     logging,\n )\n+from .utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n@@ -235,6 +236,14 @@ class VideosKwargs(TypedDict, total=False):\n             Whether to pad the video to the `(max_height, max_width)` of the videos in the batch.\n         do_center_crop (`bool`, *optional*):\n             Whether to center crop the video.\n+        do_sample_frames (`bool`, *optional*):\n+            Whether to sample frames from the video before processing or to process the whole video.\n+        video_metadata (`VideoMetadata`, *optional*):\n+            Metadata of the video containing information about total duration, fps and total number of frames.\n+        num_frames (`int`, *optional*):\n+            Maximum number of frames to sample when `do_sample_frames=True`.\n+        fps (`int`, *optional*):\n+            Target frames to sample per second when `do_sample_frames=True`.\n         crop_size (`Dict[str, int]`, *optional*):\n             Desired output size when applying center-cropping.\n         data_format (`ChannelDimension` or `str`, *optional*):\n@@ -260,6 +269,10 @@ class VideosKwargs(TypedDict, total=False):\n     data_format: Optional[ChannelDimension]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n     device: Optional[str]\n+    do_sample_frames: Optional[bool]\n+    video_metadata: Optional[Union[VideoMetadata, dict]]\n+    fps: Optional[int]\n+    num_frames: Optional[int]\n \n \n class AudioKwargs(TypedDict, total=False):\n@@ -409,9 +422,6 @@ class ChatTemplateLoadKwargs(TypedDict, total=False):\n         The backend to use when loading the video which will be used only when there are videos in the conversation.\n         Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"pyav\" because it is the only backend\n         that supports all types of sources to load from.\n-    video_fps (`int`, *optional*):\n-        Number of frames to sample per second. Should be passed only when `num_frames=None`.\n-        If not specified and `num_frames==None`, all frames are sampled.\n     sample_indices_fn (`Callable`, *optional*):\n             A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n             by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n@@ -424,9 +434,7 @@ def sample_indices_fn(num_frames, fps, metadata, **kwargs):\n                 return np.linspace(start_idx, end_idx, num_frames, dtype=int)\n     \"\"\"\n \n-    num_frames: Optional[int] = None\n     video_load_backend: Optional[str] = \"pyav\"\n-    video_fps: Optional[int] = None\n     sampling_rate: Optional[int] = 16_000\n     load_audio_from_video: Optional[bool] = False\n \n@@ -1371,40 +1379,7 @@ def __call__(\n             )\n         return {arg_name: arg_value for arg_value, arg_name in zip(args, self.optional_call_args)}\n \n-    def _process_messages_for_chat_template(\n-        self,\n-        conversation: List[List[Dict[str, str]]],\n-        batch_images: List[ImageInput],\n-        batch_videos: List[VideoInput],\n-        batch_video_metadata: List[List[Dict[str, any]]],\n-        **mm_load_kwargs: Unpack[ChatTemplateLoadKwargs],\n-    ):\n-        \"\"\"\n-        Used within `apply_chat_template` when a model has a special way to process conversation history. For example,\n-        video models might want to specify in the prompt the duration of video or which frame indices at which timestamps\n-        were sampled. This information cannot be accessed before the video is loaded.\n-\n-        For most models it is a no-op, and must be overridden by model processors which require special processing.\n-\n-        Args:\n-            conversation (`List[Dict, str, str]`):\n-                The conversation to process. Always comes in batched format.\n-            batch_images (`List[List[ImageInput]]`):\n-                Batch of images that were loaded from url/path defined in the conversation. The images\n-                are ordered in the same way as in the conversation. Comes in nested list format, one list of `PIL` images\n-                per batch.\n-            batch_videos (`List[List[ImageInput]]`):\n-                Batch of videos that were loaded from url/path defined in the conversation. The videos\n-                are ordered in the samm way as in the conversation. Comes in nested list format, one list of 4D video arrays\n-                per batch.\n-            batch_video_metadata (`List[List[Dict[[str, any]]]]`):\n-                Batch of metadata returned from loading videos. That includes video fps, duration and total number of framer in original video.\n-                Metadata are ordered in the same way as `batch_videos`. Comes in nested list format, one list of 4D video arrays\n-                per batch.\n-\n-        \"\"\"\n-        return conversation\n-\n+    @deprecate_kwarg(\"video_fps\", version=\"4.58\", new_name=\"fps\")\n     def apply_chat_template(\n         self,\n         conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n@@ -1423,7 +1398,7 @@ def apply_chat_template(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\", \"image\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n                     {\"type\": \"text\", \"text\": \"Please describe this image in detail.\"},\n                 ],\n             },\n@@ -1436,7 +1411,6 @@ def apply_chat_template(\n                 The Jinja template to use for formatting the conversation. If not provided, the tokenizer's\n                 chat template is used.\n         \"\"\"\n-\n         if chat_template is None:\n             if isinstance(self.chat_template, dict) and \"default\" in self.chat_template:\n                 chat_template = self.chat_template[\"default\"]\n@@ -1545,16 +1519,12 @@ def apply_chat_template(\n                             metadata = None\n                             logger.warning(\n                                 \"When loading the video from list of images, we cannot infer metadata such as `fps` or `duration`. \"\n-                                \"If your model uses this metadata during processing, please load the whole video and let the model sample frames instead.\"\n+                                \"If your model requires metadata during processing, please load the whole video and let the processor sample frames instead.\"\n                             )\n                         else:\n-                            # TODO: raushan, should be `self.video_processor.load_video_for_model` when API is added\n-                            video, metadata = self._load_video_for_model(\n+                            video, metadata = load_video(\n                                 fname,\n-                                num_frames=mm_load_kwargs.get(\"num_frames\", None),\n-                                fps=mm_load_kwargs.get(\"video_fps\", None),\n                                 backend=mm_load_kwargs[\"video_load_backend\"],\n-                                **kwargs,\n                             )\n                         videos.append(video)\n                         video_metadata.append(metadata)\n@@ -1567,15 +1537,6 @@ def apply_chat_template(\n                     batch_videos.append(videos)\n                     batch_video_metadata.append(video_metadata)\n \n-            # Process conversation with video/image information if needed. Then convert into a prompt using Jinja template\n-            conversations = self._process_messages_for_chat_template(\n-                conversations,\n-                batch_images=batch_images,\n-                batch_videos=batch_videos,\n-                batch_video_metadata=batch_video_metadata,\n-                **processed_kwargs[\"mm_load_kwargs\"],\n-            )\n-\n         prompt, generation_indices = render_jinja_template(\n             conversations=conversations,\n             chat_template=chat_template,\n@@ -1597,11 +1558,17 @@ def apply_chat_template(\n             if self.tokenizer.bos_token is not None and single_prompt.startswith(self.tokenizer.bos_token):\n                 kwargs[\"add_special_tokens\"] = False\n \n+            # Always sample frames by default unless explicitly set to `False` by users. If users do not pass `num_frames`/`video_fps`\n+            # sampling should not done for BC.\n+            if \"do_sample_frames\" not in kwargs and (\"fps\" in kwargs or \"num_frames\" in kwargs):\n+                kwargs[\"do_sample_frames\"] = True\n+\n             out = self(\n                 text=prompt,\n                 images=batch_images if batch_images else None,\n                 videos=batch_videos if batch_videos else None,\n                 audio=batch_audios if batch_audios else None,\n+                video_metadata=batch_video_metadata,\n                 **kwargs,\n             )\n             if return_dict:\n@@ -1626,38 +1593,6 @@ def apply_chat_template(\n                 return out[\"input_ids\"]\n         return prompt\n \n-    # TODO: raushan, has to be public method under `VideoProcessorBase` when API is added\n-    # Keep private so we can simply remove when needed\n-    def _load_video_for_model(\n-        self,\n-        video: Union[str, \"VideoInput\"],\n-        num_frames: Optional[int] = None,\n-        fps: Optional[int] = None,\n-        backend: str = \"opencv\",\n-        **kwargs,\n-    ) -> np.array:\n-        \"\"\"\n-        Loads `video` to a numpy array.\n-\n-        Args:\n-            video (`str` or `VideoInput`):\n-                The video to convert to the numpy array format. Can be a link to video or local path.\n-            num_frames (`int`, *optional*):\n-                Number of frames to sample uniformly. If not passed, the whole video is loaded.\n-            fps (`int`, *optional*):\n-                Number of frames to sample per second. Should be passed only when `num_frames=None`.\n-                If not specified and `num_frames==None`, all frames are sampled.\n-            backend (`str`, *optional*, defaults to `\"opencv\"`):\n-                The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"opencv\".\n-\n-        Returns:\n-            Tuple[`np.array`, Dict]: A tuple containing:\n-                - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n-                - Metadata dictionary.\n-        \"\"\"\n-        video, metadata = load_video(video, num_frames, fps=fps, backend=backend)\n-        return video, metadata\n-\n     def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, **kwargs):\n         \"\"\"\n         Post-process the output of a vlm to decode the text."
        },
        {
            "sha": "c316b8ffb0276a1852e63559b3ea14be120dd0b9",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 93,
            "deletions": 3,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -51,6 +51,7 @@\n from .utils.import_utils import requires\n from .video_utils import (\n     VideoInput,\n+    VideoMetadata,\n     group_videos_by_shape,\n     load_video,\n     make_batched_videos,\n@@ -118,6 +119,14 @@\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_convert_rgb (`bool`, *optional*, defaults to `self.image_std`):\n             Whether to convert the video to RGB.\n+        video_metadata (`VideoMetadata`, *optional*):\n+            Metadata of the video containing information about total duration, fps and total number of frames.\n+        do_sample_frames (`int`, *optional*, defaults to `self.do_sample_frames`):\n+            Whether to sample frames from the video before processing or to process the whole video.\n+        num_frames (`int`, *optional*, defaults to `self.num_frames`):\n+            Maximum number of frames to sample when `do_sample_frames=True`.\n+        fps (`int`, *optional*, defaults to `self.fps`):\n+            Target frames to sample per second when `do_sample_frames=True`.\n         return_tensors (`str` or `TensorType`, *optional*):\n             Returns stacked tensors if set to `pt, otherwise returns a list of tensors.\n         data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n@@ -157,6 +166,10 @@ class BaseVideoProcessor(BaseImageProcessorFast):\n     rescale_factor = 1 / 255\n     do_normalize = None\n     do_convert_rgb = None\n+    do_sample_frames = None\n+    fps = None\n+    num_frames = None\n+    video_metadata = None\n     valid_kwargs = VideosKwargs\n     model_input_names = [\"pixel_values_videos\"]\n \n@@ -219,16 +232,79 @@ def convert_to_rgb(\n         video = (1 - alpha[..., None, :, :]) * 255 + alpha[..., None, :, :] * video[..., :3, :, :]\n         return video\n \n+    def sample_frames(\n+        self,\n+        video: \"torch.Tensor\",\n+        metadata: Optional[Union[VideoMetadata, dict]] = None,\n+        num_frames: Optional[int] = None,\n+        fps: Optional[int] = None,\n+    ):\n+        \"\"\"\n+        Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n+        If `fps` is passed along with metadata, `fps` frames per second are sampled uniformty. Arguments `num_frames`\n+        and `fps` are mutually exclusive.\n+\n+        Args:\n+            video (`torch.Tensor`):\n+                Video that need to be sampled.\n+            metadata (`VideoMetadata`, *optional*):\n+                Metadata of the video containing information about total duration, fps and total number of frames.\n+            num_frames (`int`, *optional*):\n+                Maximum number of frames to sample. Defaults to `self.num_frames`.\n+            fps (`int`, *optional*):\n+                Target frames to sample per second. Defaults to `self.fps`.\n+\n+        Returns:\n+            torch.Tensor:\n+                Sampled video frames.\n+        \"\"\"\n+        if fps is not None and num_frames is not None:\n+            raise ValueError(\n+                \"`num_frames`, `fps`, and `sample_indices_fn` are mutually exclusive arguments, please use only one!\"\n+            )\n+\n+        num_frames = num_frames if num_frames is not None else self.num_frames\n+        fps = fps if fps is not None else self.fps\n+        total_num_frames = video.shape[0]\n+\n+        # If num_frames is not given but fps is, calculate num_frames from fps\n+        if num_frames is None and fps is not None:\n+            if metadata is None:\n+                raise ValueError(\n+                    \"Asked to sample `fps` frames per second but no video metadata was provided which is required when sampling with `fps`. \"\n+                    \"Please pass in `VideoMetadata` object or use a fixed `num_frames` per input video\"\n+                )\n+            num_frames = int(total_num_frames / metadata[\"fps\"] * fps)\n+\n+        if num_frames > total_num_frames:\n+            raise ValueError(\n+                f\"Video can't be sampled. The `num_frames={num_frames}` exceeds `total_num_frames={total_num_frames}`. \"\n+            )\n+\n+        if num_frames is not None:\n+            indices = torch.arange(0, total_num_frames, total_num_frames / num_frames).int()\n+        else:\n+            indices = torch.arange(0, total_num_frames).int()\n+\n+        video = video[indices].contiguous()\n+        return video\n+\n     def _prepare_input_videos(\n         self,\n         videos: VideoInput,\n+        video_metadata: VideoMetadata = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         device: Optional[\"torch.device\"] = None,\n     ) -> List[\"torch.Tensor\"]:\n         \"\"\"\n         Prepare the input videos for processing.\n         \"\"\"\n         videos = make_batched_videos(videos)\n+        if video_metadata is not None:\n+            batch_metadata = [metadata for batch_list in video_metadata for metadata in batch_list]\n+        else:\n+            batch_metadata = [None] * len(videos)\n+\n         processed_videos = []\n         for video in videos:\n             # `make_batched_videos` always returns a 4D array per video\n@@ -242,7 +318,7 @@ def _prepare_input_videos(\n                 video = video.to(device)\n \n             processed_videos.append(video)\n-        return processed_videos\n+        return processed_videos, batch_metadata\n \n     @add_start_docstrings(BASE_VIDEO_PROCESSOR_DOCSTRING)\n     def preprocess(\n@@ -261,7 +337,10 @@ def preprocess(\n \n         input_data_format = kwargs.pop(\"input_data_format\")\n         device = kwargs.pop(\"device\")\n-        videos = self._prepare_input_videos(videos=videos, input_data_format=input_data_format, device=device)\n+        video_metadata = kwargs.pop(\"video_metadata\")\n+        videos, video_metadata = self._prepare_input_videos(\n+            videos=videos, video_metadata=video_metadata, input_data_format=input_data_format, device=device\n+        )\n \n         kwargs = self._further_process_kwargs(**kwargs)\n         self._validate_preprocess_kwargs(**kwargs)\n@@ -276,11 +355,12 @@ def preprocess(\n         kwargs.pop(\"default_to_square\")\n         kwargs.pop(\"data_format\")\n \n-        return self._preprocess(videos=videos, **kwargs)\n+        return self._preprocess(videos=videos, video_metadata=video_metadata, **kwargs)\n \n     def _preprocess(\n         self,\n         videos: List[\"torch.Tensor\"],\n+        video_metadata: Union[List[VideoMetadata], List[dict]],\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n@@ -294,8 +374,18 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, List[float]]],\n         image_std: Optional[Union[float, List[float]]],\n+        do_sample_frames: Optional[bool] = None,\n+        fps: Optional[int] = None,\n+        num_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n     ) -> BatchFeature:\n+        if do_sample_frames:\n+            # Sample video frames\n+            videos = [\n+                self.sample_frames(video, metadata=metadata, num_frames=num_frames, fps=fps)\n+                for video, metadata in zip(videos, video_metadata)\n+            ]\n+\n         # Group videos by size for batched resizing\n         grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n         resized_videos_grouped = {}"
        },
        {
            "sha": "7e23e377d8b9148f53719407316e63a9971d5aa9",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -74,6 +74,9 @@ class VideoMetadata:\n     duration: float\n     video_backend: str\n \n+    def __getitem__(self, item):\n+        return getattr(self, item)\n+\n \n def is_valid_video_frame(frame):\n     return isinstance(frame, PIL.Image.Image) or (\n@@ -163,7 +166,7 @@ def make_batched_videos(videos) -> List[Union[\"np.ndarray\", \"torch.Tensor\"]]:\n         videos = [np.array(videos)[None, ...]]\n     # nested batch so we need to unflatten\n     elif isinstance(videos[0], (list, tuple)) and is_valid_video(videos[0][0]):\n-        return [video for sublist in videos for video in sublist]\n+        videos = [video for sublist in videos for video in sublist]\n     return convert_pil_frames_to_video(videos)\n \n "
        },
        {
            "sha": "a7cb6293e83a12344706b63c4a8a6ab0f3346a99",
            "filename": "tests/models/instructblipvideo/test_video_processing_instructblipvideo.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Finstructblipvideo%2Ftest_video_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Finstructblipvideo%2Ftest_video_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_video_processing_instructblipvideo.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "previous_filename": "tests/models/instructblipvideo/test_video_processing_instrictblipvideo.py"
        },
        {
            "sha": "614c5b4866645a5005e5091481205816a75bb96c",
            "filename": "tests/models/internvl/test_processor_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 74,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -17,7 +17,6 @@\n import tempfile\n import unittest\n \n-from huggingface_hub import hf_hub_download\n from parameterized import parameterized\n \n from transformers import AutoProcessor, AutoTokenizer, InternVLProcessor\n@@ -180,77 +179,6 @@ def test_process_interleaved_images_videos(self):\n             )\n             images_patches_index += inputs[\"pixel_values\"].shape[0]\n \n-    # Override video chat_template tests as InternVLProcessor returns flattened video features\n-    @require_av\n-    @require_torch\n-    def test_apply_chat_template_video_special_processing(self):\n-        \"\"\"\n-        Tests that models can use their own preprocessing to preprocess conversations.\n-        \"\"\"\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        signature = inspect.signature(processor.__call__)\n-        if \"videos\" not in {*signature.parameters.keys()} or (\n-            signature.parameters.get(\"videos\") is not None\n-            and signature.parameters[\"videos\"].annotation == inspect._empty\n-        ):\n-            self.skipTest(\"Processor doesn't accept videos at input\")\n-\n-        video_file_path = hf_hub_download(\n-            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n-        )\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"video\", \"path\": video_file_path},\n-                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        def _process_messages_for_chat_template(\n-            conversation,\n-            batch_images,\n-            batch_videos,\n-            batch_video_metadata,\n-            **chat_template_kwargs,\n-        ):\n-            # Let us just always return a dummy prompt\n-            new_msg = [\n-                [\n-                    {\n-                        \"role\": \"user\",\n-                        \"content\": [\n-                            {\"type\": \"video\"},  # no need to use path, video is loaded already by this moment\n-                            {\"type\": \"text\", \"text\": \"Dummy prompt for preprocess testing\"},\n-                        ],\n-                    },\n-                ]\n-            ]\n-            return new_msg\n-\n-        processor._process_messages_for_chat_template = _process_messages_for_chat_template\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            return_tensors=\"pt\",\n-            num_frames=8,\n-        )\n-        self.assertTrue(self.videos_input_name in out_dict_with_video)\n-\n-        # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n-        formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n-        self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n-        # Difference with common tests, InternVLProcessor returns flattened video features, and uses 8 frames by default\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 8)\n-\n     @require_torch\n     @require_av\n     def test_apply_chat_template_video_frame_sampling(self):\n@@ -393,13 +321,13 @@ def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n             tokenize=True,\n             return_dict=True,\n             return_tensors=\"pt\",\n-            num_frames=4,  # by default no more than 4 frames, otherwise too slow\n+            num_frames=2,  # by default no more than 2 frames, otherwise too slow\n         )\n         self.assertTrue(self.videos_input_name in out_dict)\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n \n-        video_len = 4 if batch_size == 1 else 3  # InternVL patches out and removes frames after processing\n+        video_len = 2 if batch_size == 1 else 3  # InternVL patches out and removes frames after processing\n         self.assertEqual(len(out_dict[self.videos_input_name]), video_len)\n         for k in out_dict:\n             self.assertIsInstance(out_dict[k], torch.Tensor)"
        },
        {
            "sha": "493d08c0d4ff52b32c2e41568211adf78b3a838c",
            "filename": "tests/models/qwen2_5_omni/test_processor_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 68,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -407,14 +407,14 @@ def _test_apply_chat_template(\n             tokenize=True,\n             return_dict=True,\n             return_tensors=return_tensors,\n-            num_frames=4,  # by default no more than 4 frames, otherwise too slow\n+            num_frames=2,  # by default no more than 2 frames, otherwise too slow\n         )\n         input_name = getattr(self, input_name)\n         self.assertTrue(input_name in out_dict)\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n \n-        video_len = 5760 if batch_size == 1 else 5808  # qwen pixels don't scale with bs same way as other models\n+        video_len = 2880 if batch_size == 1 else 5808  # qwen pixels don't scale with bs same way as other models\n         mm_len = batch_size * 1564 if modality == \"image\" else video_len\n         self.assertEqual(len(out_dict[input_name]), mm_len)\n \n@@ -525,72 +525,6 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 2904)\n \n-    @require_av\n-    def test_apply_chat_template_video_special_processing(self):\n-        \"\"\"\n-        Tests that models can use their own preprocessing to preprocess conversations.\n-        \"\"\"\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        signature = inspect.signature(processor.__call__)\n-        if \"videos\" not in {*signature.parameters.keys()} or (\n-            signature.parameters.get(\"videos\") is not None\n-            and signature.parameters[\"videos\"].annotation == inspect._empty\n-        ):\n-            self.skipTest(\"Processor doesn't accept videos at input\")\n-\n-        video_file_path = hf_hub_download(\n-            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n-        )\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"video\", \"path\": video_file_path},\n-                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        def _process_messages_for_chat_template(\n-            conversation,\n-            batch_images,\n-            batch_videos,\n-            batch_video_metadata,\n-            **chat_template_kwargs,\n-        ):\n-            # Let us just always return a dummy prompt\n-            new_msg = [\n-                [\n-                    {\n-                        \"role\": \"user\",\n-                        \"content\": [\n-                            {\"type\": \"video\"},  # no need to use path, video is loaded already by this moment\n-                            {\"type\": \"text\", \"text\": \"Dummy prompt for preprocess testing\"},\n-                        ],\n-                    },\n-                ]\n-            ]\n-            return new_msg\n-\n-        processor._process_messages_for_chat_template = _process_messages_for_chat_template\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-        )\n-        self.assertTrue(self.videos_input_name in out_dict_with_video)\n-\n-        # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n-        formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n-        self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 145912)\n-\n     @require_librosa\n     @require_av\n     @unittest.skip("
        },
        {
            "sha": "930b96e555c2a595d92fe3357066e924d063320a",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 70,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -19,7 +19,6 @@\n \n import numpy as np\n import pytest\n-from huggingface_hub import hf_hub_download\n \n from transformers import AutoProcessor, Qwen2Tokenizer\n from transformers.testing_utils import require_av, require_torch, require_vision\n@@ -219,14 +218,14 @@ def _test_apply_chat_template(\n             tokenize=True,\n             return_dict=True,\n             return_tensors=return_tensors,\n-            num_frames=4,  # by default no more than 4 frames, otherwise too slow\n+            num_frames=2,  # by default no more than 2 frames, otherwise too slow\n         )\n         input_name = getattr(self, input_name)\n         self.assertTrue(input_name in out_dict)\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n \n-        video_len = 360 if batch_size == 1 else 320  # qwen pixels don't scale with bs same way as other models\n+        video_len = 180 if batch_size == 1 else 320  # qwen pixels don't scale with bs same way as other models\n         mm_len = batch_size * 192 if modality == \"image\" else video_len\n         self.assertEqual(len(out_dict[input_name]), mm_len)\n \n@@ -346,70 +345,3 @@ def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         self.assertEqual(inputs[self.images_input_name].shape[0], 612)\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.images_input_name].shape[0], 100)\n-\n-    @require_av\n-    def test_apply_chat_template_video_special_processing(self):\n-        \"\"\"\n-        Tests that models can use their own preprocessing to preprocess conversations.\n-        \"\"\"\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        signature = inspect.signature(processor.__call__)\n-        if \"videos\" not in {*signature.parameters.keys()} or (\n-            signature.parameters.get(\"videos\") is not None\n-            and signature.parameters[\"videos\"].annotation == inspect._empty\n-        ):\n-            self.skipTest(\"Processor doesn't accept videos at input\")\n-\n-        video_file_path = hf_hub_download(\n-            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n-        )\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"video\", \"path\": video_file_path},\n-                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        def _process_messages_for_chat_template(\n-            conversation,\n-            batch_images,\n-            batch_videos,\n-            batch_video_metadata,\n-            **chat_template_kwargs,\n-        ):\n-            # Let us just always return a dummy prompt\n-            new_msg = [\n-                [\n-                    {\n-                        \"role\": \"user\",\n-                        \"content\": [\n-                            {\"type\": \"video\"},  # no need to use path, video is loaded already by this moment\n-                            {\"type\": \"text\", \"text\": \"Dummy prompt for preprocess testing\"},\n-                        ],\n-                    },\n-                ]\n-            ]\n-            return new_msg\n-\n-        processor._process_messages_for_chat_template = _process_messages_for_chat_template\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertTrue(self.videos_input_name in out_dict_with_video)\n-\n-        # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n-        formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n-        self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 21960)"
        },
        {
            "sha": "b8181ff5b32200962ed34953e5021db83074f770",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 70,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -19,7 +19,6 @@\n \n import numpy as np\n import pytest\n-from huggingface_hub import hf_hub_download\n \n from transformers import AutoProcessor, Qwen2Tokenizer\n from transformers.testing_utils import require_av, require_torch, require_vision\n@@ -220,14 +219,14 @@ def _test_apply_chat_template(\n             tokenize=True,\n             return_dict=True,\n             return_tensors=return_tensors,\n-            num_frames=4,  # by default no more than 4 frames, otherwise too slow\n+            num_frames=2,  # by default no more than 2 frames, otherwise too slow\n         )\n         input_name = getattr(self, input_name)\n         self.assertTrue(input_name in out_dict)\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n \n-        video_len = 360 if batch_size == 1 else 320  # qwen pixels don't scale with bs same way as other models\n+        video_len = 180 if batch_size == 1 else 320  # qwen pixels don't scale with bs same way as other models\n         mm_len = batch_size * 192 if modality == \"image\" else video_len\n         self.assertEqual(len(out_dict[input_name]), mm_len)\n \n@@ -337,73 +336,6 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 160)\n \n-    @require_av\n-    def test_apply_chat_template_video_special_processing(self):\n-        \"\"\"\n-        Tests that models can use their own preprocessing to preprocess conversations.\n-        \"\"\"\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        signature = inspect.signature(processor.__call__)\n-        if \"videos\" not in {*signature.parameters.keys()} or (\n-            signature.parameters.get(\"videos\") is not None\n-            and signature.parameters[\"videos\"].annotation == inspect._empty\n-        ):\n-            self.skipTest(\"Processor doesn't accept videos at input\")\n-\n-        video_file_path = hf_hub_download(\n-            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n-        )\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"video\", \"path\": video_file_path},\n-                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        def _process_messages_for_chat_template(\n-            conversation,\n-            batch_images,\n-            batch_videos,\n-            batch_video_metadata,\n-            **chat_template_kwargs,\n-        ):\n-            # Let us just always return a dummy prompt\n-            new_msg = [\n-                [\n-                    {\n-                        \"role\": \"user\",\n-                        \"content\": [\n-                            {\"type\": \"video\"},  # no need to use path, video is loaded already by this moment\n-                            {\"type\": \"text\", \"text\": \"Dummy prompt for preprocess testing\"},\n-                        ],\n-                    },\n-                ]\n-            ]\n-            return new_msg\n-\n-        processor._process_messages_for_chat_template = _process_messages_for_chat_template\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertTrue(self.videos_input_name in out_dict_with_video)\n-\n-        # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n-        formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n-        self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 21960)\n-\n     def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         processor = self.get_processor()\n         self.skip_processor_without_typed_kwargs(processor)"
        },
        {
            "sha": "097c35edebe5891817ca28454d1a17dc9da2f0ab",
            "filename": "tests/models/qwen2_vl/test_video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 70,
            "deletions": 2,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -99,8 +99,9 @@ def prepare_video_processor_dict(self):\n         }\n \n     @require_vision\n-    def expected_output_video_shape(self, videos):\n-        grid_t = self.num_frames // self.temporal_patch_size\n+    def expected_output_video_shape(self, videos, num_frames=None):\n+        num_frames = num_frames if num_frames is not None else self.num_frames\n+        grid_t = num_frames // self.temporal_patch_size\n         hidden_dim = self.num_channels * self.temporal_patch_size * self.patch_size * self.patch_size\n         seq_len = 0\n         for video in videos:\n@@ -289,3 +290,70 @@ def test_call_numpy_4_channels(self):\n             )[self.input_name]\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n             self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_call_sample_frames(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+\n+            prev_num_frames = self.video_processor_tester.num_frames\n+            self.video_processor_tester.num_frames = 8\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False,\n+                return_tensors=\"torch\",\n+            )\n+\n+            # Force set sampling to False. No sampling is expected even when `num_frames` exists\n+            video_processing.do_sample_frames = False\n+\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            expected_output_video_shape_batched = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertListEqual(list(encoded_videos.shape), expected_output_video_shape)\n+            self.assertListEqual(list(encoded_videos_batched.shape), expected_output_video_shape_batched)\n+\n+            # Set sampling to True. Video frames should be sampled with `num_frames` in the output\n+            video_processing.do_sample_frames = True\n+\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=4)[self.input_name]\n+            encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=4)[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(\n+                [video_inputs[0]], num_frames=4\n+            )\n+            expected_output_video_shape_batched = self.video_processor_tester.expected_output_video_shape(\n+                video_inputs, num_frames=4\n+            )\n+            self.assertListEqual(list(encoded_videos.shape), expected_output_video_shape)\n+            self.assertListEqual(list(encoded_videos_batched.shape), expected_output_video_shape_batched)\n+\n+            # Sample with `fps` requires metadata to infer number of frames from total duration\n+            with self.assertRaises(ValueError):\n+                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=3)[self.input_name]\n+                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", fps=3)[self.input_name]\n+\n+            metadata = [[{\"duration\": 2.0, \"total_num_frames\": 8, \"fps\": 4}]]\n+            batched_metadata = metadata * len(video_inputs)\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=3, video_metadata=metadata)[\n+                self.input_name\n+            ]\n+            encoded_videos_batched = video_processing(\n+                video_inputs, return_tensors=\"pt\", fps=3, video_metadata=batched_metadata\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(\n+                [video_inputs[0]], num_frames=6\n+            )\n+            expected_output_video_shape_batched = self.video_processor_tester.expected_output_video_shape(\n+                video_inputs, num_frames=6\n+            )\n+            self.assertListEqual(list(encoded_videos.shape), expected_output_video_shape)\n+            self.assertListEqual(list(encoded_videos_batched.shape), expected_output_video_shape_batched)\n+\n+            # We should raise error when asked to sample more frames than there are in input video\n+            with self.assertRaises(ValueError):\n+                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=10)[self.input_name]\n+                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=10)[\n+                    self.input_name\n+                ]\n+\n+            # Assign back the actual num frames in tester\n+            self.video_processor_tester.num_frames = prev_num_frames"
        },
        {
            "sha": "9be7dce11c29b7ce12a4d60f8f90382767592485",
            "filename": "tests/models/smolvlm/test_processor_smolvlm.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -16,6 +16,7 @@\n import tempfile\n import unittest\n from io import BytesIO\n+from typing import Optional\n \n import numpy as np\n import requests\n@@ -63,7 +64,7 @@ def setUpClass(cls):\n         )\n         cls.bos_token = processor.tokenizer.bos_token\n         cls.image_token = processor.image_token\n-        cls.video_token = processor.image_token * 8  # SmolVLM uses image token and repeats it `num_frames` times\n+        cls.video_token = processor.video_token\n         cls.fake_image_token = processor.fake_image_token\n         cls.global_img_token = processor.global_image_token\n \n@@ -93,6 +94,13 @@ def prepare_processor_dict():\n             \"chat_template\": \"<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\\n{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\",\n         }\n \n+    def prepare_video_inputs(self, batch_size: Optional[int] = None):\n+        \"\"\"This function prepares a list of numpy videos.\"\"\"\n+        video_input = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)] * 8\n+        if batch_size is None:\n+            return [[video_input]]\n+        return [[video_input]] * batch_size\n+\n     def get_split_image_expected_tokens(self, processor, image_rows, image_cols):\n         text_split_images = []\n         for n_h in range(image_rows):\n@@ -347,7 +355,6 @@ def test_apply_chat_template(self):\n                     {\"type\": \"text\", \"text\": \"What do these images show?\"},\n                     {\"type\": \"image\"},\n                     {\"type\": \"image\"},\n-                    \"What do these images show?\",\n                 ],\n             },\n             {\n@@ -373,11 +380,8 @@ def test_apply_chat_template(self):\n         )\n         self.assertEqual(rendered, expected_rendered)\n \n-    @unittest.skip(reason=\"SmolVLM replaced `type=video` with `type=image` in chat templates\")\n-    def test_apply_chat_template_video_special_processing(self):\n-        pass\n-\n     @require_av\n+    @require_torch\n     def test_apply_chat_template_video_frame_sampling(self):\n         # overridden because SmolVLM has special preprocessing for videos\n         processor = self.get_processor()\n@@ -406,7 +410,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             tokenize=True,\n             return_dict=True,\n             num_frames=num_frames,\n-            return_tensors=\"np\",\n+            return_tensors=\"pt\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n@@ -421,7 +425,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             tokenize=True,\n             return_dict=True,\n             video_fps=video_fps,\n-            return_tensors=\"np\",\n+            return_tensors=\"pt\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n@@ -482,11 +486,11 @@ def test_unstructured_kwargs_batched_video(self):\n             do_rescale=True,\n             rescale_factor=-1,\n             padding=\"max_length\",\n-            max_length=76,\n+            max_length=172,\n         )\n \n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 172)\n \n     @require_torch\n     @require_vision"
        },
        {
            "sha": "fa229af99ce9c0318bdc585125b7c4ccfcc8c1be",
            "filename": "tests/models/smolvlm/test_video_processing_smolvlm.py",
            "status": "modified",
            "additions": 64,
            "deletions": 15,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fsmolvlm%2Ftest_video_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Fmodels%2Fsmolvlm%2Ftest_video_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_video_processing_smolvlm.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -15,22 +15,16 @@\n \n import unittest\n \n-import numpy as np\n-\n from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n \n \n-if is_torch_available():\n-    import torch\n-\n if is_vision_available():\n     if is_torchvision_available():\n         from transformers import SmolVLMVideoProcessor\n-        from transformers.models.smolvlm.video_processing_smolvlm import get_resize_output_image_size\n \n \n class SmolVLMVideoProcessingTester:\n@@ -58,6 +52,7 @@ def __init__(\n         self.max_resolution = max_resolution\n         self.do_resize = do_resize\n         self.size = size\n+        self.max_image_size = size\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean\n         self.image_std = image_std\n@@ -71,17 +66,16 @@ def prepare_video_processor_dict(self):\n             \"image_mean\": self.image_mean,\n             \"image_std\": self.image_std,\n             \"do_convert_rgb\": self.do_convert_rgb,\n+            \"max_image_size\": self.max_image_size,\n         }\n \n     def expected_output_video_shape(self, videos):\n-        max_height, max_width = 0, 0\n-        if not isinstance(videos[0], torch.Tensor):\n-            videos = [torch.tensor(np.array(video)).permute(0, -1, -3, -2) for video in videos]\n-        for video in videos:\n-            height, width = get_resize_output_image_size(video, self.size[\"longest_edge\"])\n-            max_height = max(height, max_height)\n-            max_width = max(width, max_width)\n-        return [self.num_frames, self.num_channels, max_height, max_width]\n+        return [\n+            self.num_frames,\n+            self.num_channels,\n+            self.max_image_size[\"longest_edge\"],\n+            self.max_image_size[\"longest_edge\"],\n+        ]\n \n     def prepare_video_inputs(self, equal_resolution=False, return_tensors=\"pil\"):\n         videos = prepare_video_inputs(\n@@ -116,3 +110,58 @@ def test_video_processor_from_dict_with_kwargs(self):\n \n         video_processor = self.fast_video_processing_class.from_dict(self.video_processor_dict, size=42)\n         self.assertEqual(video_processor.size, {\"height\": 42, \"width\": 42})\n+\n+    # overwrite, SmolVLM requires to have metadata no matter how we sample\n+    def test_call_sample_frames(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+\n+            prev_num_frames = self.video_processor_tester.num_frames\n+            self.video_processor_tester.num_frames = 8\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False,\n+                return_tensors=\"torch\",\n+            )\n+\n+            # Force set sampling to False. No sampling is expected even when `num_frames` exists\n+            video_processing.do_sample_frames = False\n+\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            self.assertEqual(encoded_videos.shape[1], 8)\n+            self.assertEqual(encoded_videos_batched.shape[1], 8)\n+\n+            # Set sampling to True. Video frames should be sampled with `num_frames` in the output\n+            video_processing.do_sample_frames = True\n+            metadata = [[{\"duration\": 2.0, \"total_num_frames\": 8, \"fps\": 4}]]\n+            batched_metadata = metadata * len(video_inputs)\n+\n+            # Sample with `fps` requires metadata to infer number of frames from total duration\n+            with self.assertRaises(ValueError):\n+                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=6, fps=3)[\n+                    self.input_name\n+                ]\n+                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=6, fps=3)[\n+                    self.input_name\n+                ]\n+\n+            encoded_videos = video_processing(\n+                video_inputs[0], return_tensors=\"pt\", num_frames=6, fps=3, video_metadata=metadata\n+            )[self.input_name]\n+            encoded_videos_batched = video_processing(\n+                video_inputs, return_tensors=\"pt\", num_frames=6, fps=3, video_metadata=batched_metadata\n+            )[self.input_name]\n+            self.assertEqual(encoded_videos.shape[1], 6)\n+            self.assertEqual(encoded_videos_batched.shape[1], 6)\n+\n+            # We should raise error when asked to sample more frames than there are in input video\n+            with self.assertRaises(ValueError):\n+                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=10, num_frames=20)[\n+                    self.input_name\n+                ]\n+                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", fps=10, num_frames=20)[\n+                    self.input_name\n+                ]\n+\n+            # Assign back the actual num frames in tester\n+            self.video_processor_tester.num_frames = prev_num_frames"
        },
        {
            "sha": "ebede32f3e4d1da52860eb088f7a1c7c0e6967a4",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 30,
            "deletions": 84,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -507,15 +507,15 @@ def test_tokenizer_defaults_preserved_by_kwargs_video(self):\n         if \"video_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n-        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=167, padding=\"max_length\")\n         processor_kwargs = self.prepare_processor_dict()\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n         inputs = processor(text=input_str, videos=video_input, return_tensors=\"pt\")\n-        self.assertEqual(inputs[self.text_input_name].shape[-1], 117)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 167)\n \n     def test_video_processor_defaults_preserved_by_video_kwargs(self):\n         \"\"\"\n@@ -529,7 +529,7 @@ def test_video_processor_defaults_preserved_by_video_kwargs(self):\n         processor_components[\"video_processor\"] = self.get_component(\n             \"video_processor\", do_rescale=True, rescale_factor=-1\n         )\n-        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=167, padding=\"max_length\")\n         processor_kwargs = self.prepare_processor_dict()\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n@@ -553,9 +553,9 @@ def test_kwargs_overrides_default_tokenizer_kwargs_video(self):\n         input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n         inputs = processor(\n-            text=input_str, videos=video_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n+            text=input_str, videos=video_input, return_tensors=\"pt\", max_length=162, padding=\"max_length\"\n         )\n-        self.assertEqual(inputs[self.text_input_name].shape[-1], 112)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 162)\n \n     def test_kwargs_overrides_default_video_processor_kwargs(self):\n         if \"video_processor\" not in self.processor_class.attributes:\n@@ -564,7 +564,7 @@ def test_kwargs_overrides_default_video_processor_kwargs(self):\n         processor_components[\"video_processor\"] = self.get_component(\n             \"video_processor\", do_rescale=True, rescale_factor=1\n         )\n-        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=167, padding=\"max_length\")\n         processor_kwargs = self.prepare_processor_dict()\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n@@ -593,11 +593,11 @@ def test_unstructured_kwargs_video(self):\n             do_rescale=True,\n             rescale_factor=-1,\n             padding=\"max_length\",\n-            max_length=76,\n+            max_length=176,\n         )\n \n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n-        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 176)\n \n     def test_unstructured_kwargs_batched_video(self):\n         if \"video_processor\" not in self.processor_class.attributes:\n@@ -616,13 +616,13 @@ def test_unstructured_kwargs_batched_video(self):\n             do_rescale=True,\n             rescale_factor=-1,\n             padding=\"longest\",\n-            max_length=76,\n+            max_length=176,\n         )\n \n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n         self.assertTrue(\n             len(inputs[self.text_input_name][0]) == len(inputs[self.text_input_name][1])\n-            and len(inputs[self.text_input_name][1]) < 76\n+            and len(inputs[self.text_input_name][1]) < 176\n         )\n \n     def test_doubly_passed_kwargs_video(self):\n@@ -659,14 +659,14 @@ def test_structured_kwargs_nested_video(self):\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n             \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 176},\n         }\n \n         inputs = processor(text=input_str, videos=video_input, **all_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n-        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 176)\n \n     def test_structured_kwargs_nested_from_dict_video(self):\n         if \"video_processor\" not in self.processor_class.attributes:\n@@ -682,12 +682,12 @@ def test_structured_kwargs_nested_from_dict_video(self):\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n             \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 176},\n         }\n \n         inputs = processor(text=input_str, videos=video_input, **all_kwargs)\n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n-        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 176)\n \n     # TODO: the same test, but for audio + text processors that have strong overlap in kwargs\n     # TODO (molbap) use the same structure of attribute kwargs for other tests to avoid duplication\n@@ -884,7 +884,7 @@ def _test_apply_chat_template(\n             tokenize=True,\n             return_dict=True,\n             return_tensors=return_tensors,\n-            num_frames=4,  # by default no more than 4 frames, otherwise too slow\n+            num_frames=2,  # by default no more than 2 frames, otherwise too slow\n         )\n         input_name = getattr(self, input_name)\n         self.assertTrue(input_name in out_dict)\n@@ -983,6 +983,21 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), video_fps * 10)\n \n+        # Whan `do_sample_frames=False` no sampling is done and whole video is loaded, even if number of frames is passed\n+        video_fps = 1\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            do_sample_frames=False,\n+            video_fps=video_fps,\n+            return_tensors=\"pt\",\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 300)\n+\n         # Load with `video_fps` and `num_frames` args, should raise an error\n         with self.assertRaises(ValueError):\n             out_dict_with_video = processor.apply_chat_template(\n@@ -1024,75 +1039,6 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 2)\n \n-    @require_av\n-    @require_torch\n-    def test_apply_chat_template_video_special_processing(self):\n-        \"\"\"\n-        Tests that models can use their own preprocessing to preprocess conversations.\n-        \"\"\"\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        signature = inspect.signature(processor.__call__)\n-        if \"videos\" not in {*signature.parameters.keys()} or (\n-            signature.parameters.get(\"videos\") is not None\n-            and signature.parameters[\"videos\"].annotation == inspect._empty\n-        ):\n-            self.skipTest(\"Processor doesn't accept videos at input\")\n-\n-        video_file_path = hf_hub_download(\n-            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n-        )\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"video\", \"path\": video_file_path},\n-                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        def _process_messages_for_chat_template(\n-            conversation,\n-            batch_images,\n-            batch_videos,\n-            batch_video_metadata,\n-            **chat_template_kwargs,\n-        ):\n-            # Let us just always return a dummy prompt\n-            new_msg = [\n-                [\n-                    {\n-                        \"role\": \"user\",\n-                        \"content\": [\n-                            {\"type\": \"video\"},  # no need to use path, video is loaded already by this moment\n-                            {\"type\": \"text\", \"text\": \"Dummy prompt for preprocess testing\"},\n-                        ],\n-                    },\n-                ]\n-            ]\n-            return new_msg\n-\n-        processor._process_messages_for_chat_template = _process_messages_for_chat_template\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertTrue(self.videos_input_name in out_dict_with_video)\n-\n-        # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n-        formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n-        self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 243)\n-\n     @require_librosa\n     @require_av\n     def test_chat_template_audio_from_video(self):"
        },
        {
            "sha": "1bbdb4fdcc878307fd00bbe70d33fe1cadc842e6",
            "filename": "tests/test_video_processing_common.py",
            "status": "modified",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Ftest_video_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/27459025b8f77d53631f7961cc967fa659d43f7e/tests%2Ftest_video_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_video_processing_common.py?ref=27459025b8f77d53631f7961cc967fa659d43f7e",
            "patch": "@@ -293,6 +293,59 @@ def test_call_pytorch(self):\n                 (self.video_processor_tester.batch_size, *expected_output_video_shape),\n             )\n \n+    def test_call_sample_frames(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+\n+            prev_num_frames = self.video_processor_tester.num_frames\n+            self.video_processor_tester.num_frames = 8\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False,\n+                return_tensors=\"torch\",\n+            )\n+\n+            # Force set sampling to False. No sampling is expected even when `num_frames` exists\n+            video_processing.do_sample_frames = False\n+\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            self.assertEqual(encoded_videos.shape[1], 8)\n+            self.assertEqual(encoded_videos_batched.shape[1], 8)\n+\n+            # Set sampling to True. Video frames should be sampled with `num_frames` in the output\n+            video_processing.do_sample_frames = True\n+\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            self.assertEqual(encoded_videos.shape[1], 3)\n+            self.assertEqual(encoded_videos_batched.shape[1], 3)\n+\n+            # Sample with `fps` requires metadata to infer number of frames from total duration\n+            with self.assertRaises(ValueError):\n+                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=3)[self.input_name]\n+                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", fps=3)[self.input_name]\n+\n+            metadata = [[{\"duration\": 2.0, \"total_num_frames\": 8, \"fps\": 4}]]\n+            batched_metadata = metadata * len(video_inputs)\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=3, video_metadata=metadata)[\n+                self.input_name\n+            ]\n+            encoded_videos_batched = video_processing(\n+                video_inputs, return_tensors=\"pt\", fps=3, video_metadata=batched_metadata\n+            )[self.input_name]\n+            self.assertEqual(encoded_videos.shape[1], 6)\n+            self.assertEqual(encoded_videos_batched.shape[1], 6)\n+\n+            # We should raise error when asked to sample more frames than there are in input video\n+            with self.assertRaises(ValueError):\n+                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=10)[self.input_name]\n+                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=10)[\n+                    self.input_name\n+                ]\n+\n+            # Assign back the actual num frames in tester\n+            self.video_processor_tester.num_frames = prev_num_frames\n+\n     def test_nested_input(self):\n         \"\"\"Tests that the processor can work with nested list where each video is a list of arrays\"\"\"\n         for video_processing_class in self.video_processor_list:"
        }
    ],
    "stats": {
        "total": 1661,
        "additions": 865,
        "deletions": 796
    }
}