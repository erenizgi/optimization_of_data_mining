{
    "author": "zucchini-nlp",
    "message": "Validate processing kwargs with @strict from huggingface_hub  (#40793)\n\n* initial design draft\n\n* delete\n\n* fix a few tests\n\n* fix\n\n* fix the rest of tests\n\n* common-kwargs\n\n* why the runner complains about typing with \"|\"?\n\n* revert\n\n* forgot to delete\n\n* update\n\n* fix last issues\n\n* add more detalis in docs\n\n* pin the latest hub release\n\n* fix tests for new models\n\n* also fast image processor\n\n* fix copies\n\n* image processing ast validated\n\n* fix more tests\n\n* typo.and fix copies\n\n* bump\n\n* style\n\n* fix some tests\n\n* fix copies\n\n* pin rc4 and mark all TypedDict as non-total\n\n* delete typed dict adaptor\n\n* address comments\n\n* delete optionals",
    "sha": "89a4115a6b3697a3a2018d8d4f8c439e4196691e",
    "files": [
        {
            "sha": "7cfd5bf2d299875e346255395584ee5523e1effc",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -114,7 +114,7 @@\n     \"GitPython<3.1.19\",\n     \"hf-doc-builder>=0.3.0\",\n     \"hf_xet\",\n-    \"huggingface-hub==1.0.0.rc2\",\n+    \"huggingface-hub==1.0.0.rc4\",\n     \"importlib_metadata\",\n     \"ipadic>=1.0.0,<2.0\",\n     \"jinja2>=3.1.0\","
        },
        {
            "sha": "1caefce16c3e183a7463fb411e6d51fca2edb9d3",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -23,7 +23,7 @@\n     \"GitPython\": \"GitPython<3.1.19\",\n     \"hf-doc-builder\": \"hf-doc-builder>=0.3.0\",\n     \"hf_xet\": \"hf_xet\",\n-    \"huggingface-hub\": \"huggingface-hub==1.0.0.rc2\",\n+    \"huggingface-hub\": \"huggingface-hub==1.0.0.rc4\",\n     \"importlib_metadata\": \"importlib_metadata\",\n     \"ipadic\": \"ipadic>=1.0.0,<2.0\",\n     \"jinja2\": \"jinja2>=3.1.0\","
        },
        {
            "sha": "8d4c79afbb1de0dccaa62da822034192b2149c79",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -18,6 +18,7 @@\n from typing import Any, Optional, Union\n \n import numpy as np\n+from huggingface_hub.dataclasses import validate_typed_dict\n \n from .image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from .image_transforms import (\n@@ -710,6 +711,10 @@ def _validate_preprocess_kwargs(\n     def preprocess(self, images: ImageInput, *args, **kwargs: Unpack[ImagesKwargs]) -> BatchFeature:\n         # args are not validated, but their order in the `preprocess` and `_preprocess` signatures must be the same\n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_kwargs_names)\n+\n+        # Perform type validation on received kwargs\n+        validate_typed_dict(self.valid_kwargs, kwargs)\n+\n         # Set default kwargs from self. This ensures that if a kwarg is not provided\n         # by the user, it gets its default value from the instance, or is set to None.\n         for kwarg_name in self._valid_kwargs_names:"
        },
        {
            "sha": "46e35911c1f1f14de50539228e1a1ed5fbcdfd16",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -38,7 +38,7 @@\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import PreTrainedModel\n-from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils import PreTokenizedInput, TextInput\n from ...utils import TensorType, TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoTokenizer\n@@ -904,7 +904,15 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         return num_patches\n \n \n+class AriaImagesKwargs(ImagesKwargs, total=False):\n+    split_image: bool\n+    max_image_size: int\n+    min_image_size: int\n+\n+\n class AriaProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: AriaImagesKwargs\n+\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "d0841c96aee26a3bf45000f08b07f7fd5c5930f8",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -24,13 +24,21 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils import PreTokenizedInput, TextInput\n from ...utils import TensorType\n from ..auto import AutoTokenizer\n \n \n+class AriaImagesKwargs(ImagesKwargs, total=False):\n+    split_image: bool\n+    max_image_size: int\n+    min_image_size: int\n+\n+\n class AriaProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: AriaImagesKwargs\n+\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,"
        },
        {
            "sha": "884619f12b13d9797c3c454e501062e351e62820",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -55,15 +55,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class BeitImageProcessorKwargs(ImagesKwargs):\n+class BeitImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n         Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n         is used for background, and background itself is not included in all classes of a dataset (e.g.\n         ADE20k). The background label will be replaced by 255.\n     \"\"\"\n \n-    do_reduce_labels: Optional[bool]\n+    do_reduce_labels: bool\n \n \n @requires(backends=(\"vision\",))"
        },
        {
            "sha": "73bfc740766648a58b3fe31dd47cb5acdc81ae29",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -123,8 +123,8 @@ def get_resize_output_image_size(\n     return new_height, new_width\n \n \n-class BridgeTowerImageProcessorKwargs(ImagesKwargs):\n-    size_divisor: Optional[int]\n+class BridgeTowerImageProcessorKwargs(ImagesKwargs, total=False):\n+    size_divisor: int\n \n \n class BridgeTowerImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "afdd683e231299eb5c19e02a061f431003a8cb37",
            "filename": "src/transformers/models/cohere2_vision/image_processing_cohere2_vision_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -33,7 +33,7 @@\n from ...utils import TensorType, auto_docstring\n \n \n-class Cohere2VisionFastImageProcessorKwargs(ImagesKwargs):\n+class Cohere2VisionFastImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     crop_to_patches (`bool`, *optional*, defaults to `False`):\n         Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n@@ -46,9 +46,9 @@ class Cohere2VisionFastImageProcessorKwargs(ImagesKwargs):\n         set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n     \"\"\"\n \n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n+    crop_to_patches: bool\n+    min_patches: int\n+    max_patches: int\n \n \n @lru_cache(maxsize=10)"
        },
        {
            "sha": "b801c24575ca65ffd8ee4f7646dbae37f95d7731",
            "filename": "src/transformers/models/cohere2_vision/modular_cohere2_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -303,7 +303,7 @@ def get_optimal_tiled_canvas(\n     return best_grid\n \n \n-class Cohere2VisionFastImageProcessorKwargs(ImagesKwargs):\n+class Cohere2VisionFastImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     crop_to_patches (`bool`, *optional*, defaults to `False`):\n         Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n@@ -316,9 +316,9 @@ class Cohere2VisionFastImageProcessorKwargs(ImagesKwargs):\n         set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n     \"\"\"\n \n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n+    crop_to_patches: bool\n+    min_patches: int\n+    max_patches: int\n \n \n @auto_docstring"
        },
        {
            "sha": "3f639e0c1ae35953b19bf5eb7a7e54c0ea2ad425",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -729,7 +729,7 @@ def compute_segments(\n     return segmentation, segments\n \n \n-class ConditionalDetrImageProcessorKwargs(ImagesKwargs):\n+class ConditionalDetrImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n         Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n@@ -745,9 +745,9 @@ class ConditionalDetrImageProcessorKwargs(ImagesKwargs):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n+    format: Union[str, AnnotationFormat]\n+    do_convert_annotations: bool\n+    return_segmentation_masks: bool\n     annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n     masks_path: Optional[Union[str, pathlib.Path]]\n "
        },
        {
            "sha": "c4e279346f3c0657cbdf09c26d0c0341e5eb3835",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -50,14 +50,14 @@\n logger = logging.get_logger(__name__)\n \n \n-class ConvNextImageProcessorKwargs(ImagesKwargs):\n+class ConvNextImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     crop_pct (`float`, *optional*):\n         Percentage of the image to crop. Only has an effect if size < 384. Can be\n         overridden by `crop_pct` in the`preprocess` method.\n     \"\"\"\n \n-    crop_pct: Optional[float]\n+    crop_pct: float\n \n \n @requires(backends=(\"vision\",))"
        },
        {
            "sha": "763182de403958aeebb1c0767c002dc314f32029",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -49,7 +49,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class DeepseekVLImageProcessorKwargs(ImagesKwargs):\n+class DeepseekVLImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     min_size (`int`, *optional*, defaults to 14):\n         The minimum allowed size for the resized image. Ensures that neither the height nor width"
        },
        {
            "sha": "c91aab91fca571e68d8fe04e3937bb62dd8e3a52",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -50,7 +50,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class DeepseekVLHybridImageProcessorKwargs(ImagesKwargs):\n+class DeepseekVLHybridImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     min_size (`int`, *optional*, defaults to 14):\n         The minimum allowed size for the resized image. Ensures that neither the height nor width\n@@ -71,9 +71,9 @@ class DeepseekVLHybridImageProcessorKwargs(ImagesKwargs):\n \n     min_size: int\n     high_res_size: dict\n-    high_res_resample: \"PILImageResampling\"\n-    high_res_image_mean: list[float]\n-    high_res_image_std: list[float]\n+    high_res_resample: Union[\"PILImageResampling\", int]\n+    high_res_image_mean: Union[float, list[float], tuple[float, ...]]\n+    high_res_image_std: Union[float, list[float], tuple[float, ...]]\n \n \n class DeepseekVLHybridImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "43af7d43dfb3dae40ec7d6b02193edb68c0836b5",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -429,7 +429,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n \n-class DeepseekVLHybridImageProcessorKwargs(ImagesKwargs):\n+class DeepseekVLHybridImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     min_size (`int`, *optional*, defaults to 14):\n         The minimum allowed size for the resized image. Ensures that neither the height nor width\n@@ -450,9 +450,9 @@ class DeepseekVLHybridImageProcessorKwargs(ImagesKwargs):\n \n     min_size: int\n     high_res_size: dict\n-    high_res_resample: \"PILImageResampling\"\n-    high_res_image_mean: list[float]\n-    high_res_image_std: list[float]\n+    high_res_resample: Union[\"PILImageResampling\", int]\n+    high_res_image_mean: Union[float, list[float], tuple[float, ...]]\n+    high_res_image_std: Union[float, list[float], tuple[float, ...]]\n \n \n class DeepseekVLHybridImageProcessor(DeepseekVLImageProcessor):"
        },
        {
            "sha": "83587f45c295059db56751de023808043c22ffea",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -82,7 +82,7 @@\n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n \n-class DeformableDetrImageProcessorKwargs(ImagesKwargs):\n+class DeformableDetrImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n         Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n@@ -98,9 +98,9 @@ class DeformableDetrImageProcessorKwargs(ImagesKwargs):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n+    format: Union[str, AnnotationFormat]\n+    do_convert_annotations: bool\n+    return_segmentation_masks: bool\n     annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n     masks_path: Optional[Union[str, pathlib.Path]]\n "
        },
        {
            "sha": "2f149b662ec21460ff2546f8a9fba9cc4ec8aea9",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -84,7 +84,7 @@\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n-class DetrImageProcessorKwargs(ImagesKwargs):\n+class DetrImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n         Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n@@ -100,9 +100,9 @@ class DetrImageProcessorKwargs(ImagesKwargs):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n+    format: Union[str, AnnotationFormat]\n+    do_convert_annotations: bool\n+    return_segmentation_masks: bool\n     annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n     masks_path: Optional[Union[str, pathlib.Path]]\n "
        },
        {
            "sha": "6518b5444639a4b2ac4aef5555ed74110aae7e5a",
            "filename": "src/transformers/models/dia/processing_dia.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -55,7 +55,9 @@ class DiaProcessorKwargs(ProcessingKwargs, total=False):\n             \"generation\": True,\n             \"sampling_rate\": 44100,\n         },\n-        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+        \"common_kwargs\": {\n+            \"return_tensors\": \"pt\",\n+        },\n     }\n \n "
        },
        {
            "sha": "0f74ac62ec92c5032f3d2f2ca41c916495fa9654",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -52,16 +52,16 @@\n     import PIL\n \n \n-class DonutImageProcessorKwargs(ImagesKwargs):\n+class DonutImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     do_thumbnail (`bool`, *optional*, defaults to `self.do_thumbnail`):\n         Whether to resize the image using thumbnail method.\n     do_align_long_axis (`bool`, *optional*, defaults to `self.do_align_long_axis`):\n         Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n     \"\"\"\n \n-    do_thumbnail: Optional[bool]\n-    do_align_long_axis: Optional[bool]\n+    do_thumbnail: bool\n+    do_align_long_axis: bool\n \n \n @requires(backends=(\"vision\",))"
        },
        {
            "sha": "6246b1f3f7c04cf543f9bde44703ee9f448af425",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -64,7 +64,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class DPTImageProcessorKwargs(ImagesKwargs):\n+class DPTImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     ensure_multiple_of (`int`, *optional*, defaults to 1):\n         If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n@@ -78,10 +78,10 @@ class DPTImageProcessorKwargs(ImagesKwargs):\n         ADE20k). The background label will be replaced by 255.\n     \"\"\"\n \n-    ensure_multiple_of: Optional[int]\n-    size_divisor: Optional[int]\n-    keep_aspect_ratio: Optional[bool]\n-    do_reduce_labels: Optional[bool]\n+    ensure_multiple_of: int\n+    size_divisor: int\n+    keep_aspect_ratio: bool\n+    do_reduce_labels: bool\n \n \n def get_resize_output_image_size("
        },
        {
            "sha": "acf9105fe77aad58434b6346cb429e8271c0e83e",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -50,13 +50,13 @@\n logger = logging.get_logger(__name__)\n \n \n-class EfficientLoFTRImageProcessorKwargs(ImagesKwargs):\n+class EfficientLoFTRImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     do_grayscale (`bool`, *optional*, defaults to `True`):\n         Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n     \"\"\"\n \n-    do_grayscale: Optional[bool] = True\n+    do_grayscale: bool\n \n \n # Copied from transformers.models.superpoint.image_processing_superpoint.is_grayscale"
        },
        {
            "sha": "2a5b5c93749ba6faa6fc65ba43a9dcffa5b23c95",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -44,7 +44,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class EfficientNetImageProcessorKwargs(ImagesKwargs):\n+class EfficientNetImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     rescale_offset (`bool`, *optional*, defaults to `self.rescale_offset`):\n         Whether to rescale the image between [-max_range/2, scale_range/2] instead of [0, scale_range]."
        },
        {
            "sha": "0c550937581f95f25f7950ea8e5fdf6d20a7a1ef",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -47,9 +47,9 @@\n logger = logging.get_logger(__name__)\n \n \n-class Emu3ImageProcessorKwargs(ImagesKwargs):\n-    ratio: Optional[str]\n-    image_area: Optional[int]\n+class Emu3ImageProcessorKwargs(ImagesKwargs, total=False):\n+    ratio: str\n+    image_area: int\n \n \n def smart_resize("
        },
        {
            "sha": "3459911cde1f320897d897a5689aa4d854fb8158",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -55,7 +55,7 @@\n     import torch.nn.functional as F\n \n \n-class EomtImageProcessorKwargs(ImagesKwargs):\n+class EomtImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     do_split_image (`bool`, *optional*, defaults to `False`):\n         Whether to split the input images into overlapping patches for semantic segmentation. If set to `True`, the\n@@ -67,7 +67,7 @@ class EomtImageProcessorKwargs(ImagesKwargs):\n     \"\"\"\n \n     do_split_image: bool\n-    ignore_index: Optional[int] = None\n+    ignore_index: Optional[int]\n \n \n # Adapted from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks"
        },
        {
            "sha": "b62717ae2cd6d5b9f3fcb32047120381dd6c23db",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -57,7 +57,7 @@\n LOGIT_LAPLACE_EPS: float = 0.1\n \n \n-class FlavaImageProcessorKwargs(ImagesKwargs):\n+class FlavaImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     return_image_mask (`bool`, *optional*, defaults to `False`):\n         Whether to return the image mask. Can be overridden by the `return_image_mask` parameter in `preprocess`.\n@@ -118,26 +118,26 @@ class FlavaImageProcessorKwargs(ImagesKwargs):\n     \"\"\"\n \n     # Mask related params\n-    return_image_mask: Optional[bool]\n-    input_size_patches: Optional[int]\n-    total_mask_patches: Optional[int]\n-    mask_group_min_patches: Optional[int]\n-    mask_group_max_patches: Optional[int]\n-    mask_group_min_aspect_ratio: Optional[float]\n-    mask_group_max_aspect_ratio: Optional[float]\n+    return_image_mask: bool\n+    input_size_patches: int\n+    total_mask_patches: int\n+    mask_group_min_patches: int\n+    mask_group_max_patches: int\n+    mask_group_min_aspect_ratio: float\n+    mask_group_max_aspect_ratio: float\n     # Codebook related params\n-    return_codebook_pixels: Optional[bool]\n-    codebook_do_resize: Optional[bool]\n-    codebook_size: Optional[bool]\n-    codebook_resample: Optional[int]\n-    codebook_do_center_crop: Optional[bool]\n-    codebook_crop_size: Optional[int]\n-    codebook_do_rescale: Optional[bool]\n-    codebook_rescale_factor: Optional[Union[int, float]]\n-    codebook_do_map_pixels: Optional[bool]\n-    codebook_do_normalize: Optional[bool]\n-    codebook_image_mean: Optional[Union[float, Iterable[float]]]\n-    codebook_image_std: Optional[Union[float, Iterable[float]]]\n+    return_codebook_pixels: bool\n+    codebook_do_resize: bool\n+    codebook_size: bool\n+    codebook_resample: int\n+    codebook_do_center_crop: bool\n+    codebook_crop_size: int\n+    codebook_do_rescale: bool\n+    codebook_rescale_factor: Union[int, float]\n+    codebook_do_map_pixels: bool\n+    codebook_do_normalize: bool\n+    codebook_image_mean: Union[float, Iterable[float]]\n+    codebook_image_std: Union[float, Iterable[float]]\n \n \n # Inspired from https://github.com/microsoft/unilm/blob/master/beit/masking_generator.py"
        },
        {
            "sha": "d4bd7a00000e69559ebec6c7e57a736997635748",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -51,7 +51,7 @@\n     import PIL\n \n \n-class Gemma3ImageProcessorKwargs(ImagesKwargs):\n+class Gemma3ImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     do_pan_and_scan (`bool`, *optional*):\n         Whether to apply `pan_and_scan` to images.\n@@ -63,10 +63,10 @@ class Gemma3ImageProcessorKwargs(ImagesKwargs):\n         Minimum aspect ratio to activate pan and scan.\n     \"\"\"\n \n-    do_pan_and_scan: Optional[bool]\n-    pan_and_scan_min_crop_size: Optional[int]\n-    pan_and_scan_max_num_crops: Optional[int]\n-    pan_and_scan_min_ratio_to_activate: Optional[float]\n+    do_pan_and_scan: bool\n+    pan_and_scan_min_crop_size: int\n+    pan_and_scan_max_num_crops: int\n+    pan_and_scan_min_ratio_to_activate: float\n \n \n class Gemma3ImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "9a4348010750e4f173346e124f41a34ce3668c00",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -47,7 +47,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Glm4vImageProcessorKwargs(ImagesKwargs):\n+class Glm4vImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     patch_size (`int`, *optional*, defaults to 14):\n         The spatial patch size of the vision encoder.\n@@ -57,9 +57,9 @@ class Glm4vImageProcessorKwargs(ImagesKwargs):\n         The merge size of the vision encoder to llm encoder.\n     \"\"\"\n \n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n+    patch_size: int\n+    temporal_patch_size: int\n+    merge_size: int\n \n \n def smart_resize("
        },
        {
            "sha": "f27adfc7e25e81ef49d4de5449c568fff8f759dd",
            "filename": "src/transformers/models/glm4v/video_processing_glm4v.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -36,12 +36,12 @@\n from .image_processing_glm4v import smart_resize\n \n \n-class Glm4vVideoProcessorInitKwargs(VideosKwargs):\n-    max_image_size: Optional[dict[str, int]]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-    max_duration: Optional[int]\n+class Glm4vVideoProcessorInitKwargs(VideosKwargs, total=False):\n+    max_image_size: dict[str, int]\n+    patch_size: int\n+    temporal_patch_size: int\n+    merge_size: int\n+    max_duration: int\n \n \n @add_start_docstrings("
        },
        {
            "sha": "3fd5f7d512c133986b147f04cce9f41ee728c548",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -49,7 +49,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class GotOcr2ImageProcessorKwargs(ImagesKwargs):\n+class GotOcr2ImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     crop_to_patches (`bool`, *optional*, defaults to `False`):\n         Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n@@ -62,9 +62,9 @@ class GotOcr2ImageProcessorKwargs(ImagesKwargs):\n         set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n     \"\"\"\n \n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n+    crop_to_patches: bool\n+    min_patches: int\n+    max_patches: int\n \n \n # Similar to image_processing_mllama.get_all_supported_aspect_ratios"
        },
        {
            "sha": "1843b7f288300a9785754fa19b26ba6db766d348",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -36,13 +36,13 @@ class GotOcr2TextKwargs(TextKwargs, total=False):\n \n \n class GotOcr2ImagesKwargs(ImagesKwargs, total=False):\n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n+    crop_to_patches: bool\n+    min_patches: int\n+    max_patches: int\n     box: Optional[Union[list, tuple[float, float], tuple[float, float, float, float]]]\n     color: Optional[str]\n-    num_image_tokens: Optional[int]\n-    multi_page: Optional[bool]\n+    num_image_tokens: int\n+    multi_page: bool\n \n \n class GotOcr2ProcessorKwargs(ProcessingKwargs, total=False):"
        },
        {
            "sha": "eb21ea3b376e6c463816e53791e9fe2ac3d935ed",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -93,7 +93,7 @@ class AnnotationFormat(ExplicitEnum):\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n-class GroundingDinoImageProcessorKwargs(ImagesKwargs):\n+class GroundingDinoImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n         Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n@@ -109,9 +109,9 @@ class GroundingDinoImageProcessorKwargs(ImagesKwargs):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n+    format: Union[str, AnnotationFormat]\n+    do_convert_annotations: bool\n+    return_segmentation_masks: bool\n     annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n     masks_path: Optional[Union[str, pathlib.Path]]\n "
        },
        {
            "sha": "870c741b826d4a70371c42a553325605de532594",
            "filename": "src/transformers/models/idefics/image_processing_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -36,7 +36,7 @@\n IDEFICS_STANDARD_STD = [0.26862954, 0.26130258, 0.27577711]\n \n \n-class IdeficsImageProcessorKwargs(ImagesKwargs):\n+class IdeficsImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     transform (`Callable`, *optional*):\n         A custom transform function that accepts a single image can be passed for training. For example,\n@@ -47,7 +47,7 @@ class IdeficsImageProcessorKwargs(ImagesKwargs):\n     \"\"\"\n \n     transform: Optional[Callable]\n-    image_size: Optional[dict[str, int]]\n+    image_size: dict[str, int]\n \n \n def convert_to_rgb(image):"
        },
        {
            "sha": "e068ac42f4034a81c2ea526f8deba226969b7dec",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -47,13 +47,13 @@\n     from PIL import Image\n \n \n-class Idefics2ImageProcessorKwargs(ImagesKwargs):\n+class Idefics2ImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     do_image_splitting (`bool`, *optional*, defaults to `False`):\n         Whether to split the image into a sequence 4 equal sub-images concatenated with the original image.\n     \"\"\"\n \n-    do_image_splitting: Optional[bool]\n+    do_image_splitting: bool\n \n \n def get_resize_output_image_size(image, size, input_data_format) -> tuple[int, int]:"
        },
        {
            "sha": "65e17ef4b776936652aa55a79083d35082f0c6f2",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -48,7 +48,7 @@\n     from PIL import Image\n \n \n-class Idefics3ImageProcessorKwargs(ImagesKwargs):\n+class Idefics3ImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     do_image_splitting (`bool`, *optional*, defaults to `True`):\n         Whether to split the image into sub-images concatenated with the original image. They are split into patches\n@@ -59,9 +59,9 @@ class Idefics3ImageProcessorKwargs(ImagesKwargs):\n         Whether to return the row and column information of the images.\n     \"\"\"\n \n-    do_image_splitting: Optional[bool]\n-    max_image_size: Optional[dict[str, int]]\n-    return_row_col_info: Optional[bool]\n+    do_image_splitting: bool\n+    max_image_size: dict[str, int]\n+    return_row_col_info: bool\n \n \n def _resize_output_size_rescale_to_max_len("
        },
        {
            "sha": "ab7057f7d40711322162bb08b10fd00c5ae7ab9c",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -45,7 +45,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class ImageGPTImageProcessorKwargs(ImagesKwargs):\n+class ImageGPTImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     clusters (`np.ndarray` or `list[list[int]]` or `torch.Tensor`, *optional*):\n         The color clusters to use, of shape `(n_clusters, 3)` when color quantizing. Can be overridden by `clusters`\n@@ -56,7 +56,7 @@ class ImageGPTImageProcessorKwargs(ImagesKwargs):\n     \"\"\"\n \n     clusters: Optional[Union[np.ndarray, list[list[int]], \"torch.Tensor\"]]\n-    do_color_quantize: Optional[bool]\n+    do_color_quantize: bool\n \n \n def squared_euclidean_distance(a, b):"
        },
        {
            "sha": "f2c49925ef190966a35fb5eaf33d4d10002c9356",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -24,15 +24,11 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling, SizeDict\n-from ...processing_utils import Unpack, VideosKwargs\n from ...utils import TensorType\n from ...video_processing_utils import BaseVideoProcessor\n from ...video_utils import group_videos_by_shape, reorder_videos\n \n \n-class InstructBlipVideoVideoProcessorInitKwargs(VideosKwargs): ...\n-\n-\n class InstructBlipVideoVideoProcessor(BaseVideoProcessor):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN\n@@ -44,12 +40,8 @@ class InstructBlipVideoVideoProcessor(BaseVideoProcessor):\n     do_normalize = True\n     do_convert_rgb = True\n     do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n-    valid_kwargs = InstructBlipVideoVideoProcessorInitKwargs\n     model_input_names = [\"pixel_values\"]\n \n-    def __init__(self, **kwargs: Unpack[InstructBlipVideoVideoProcessorInitKwargs]):\n-        super().__init__(**kwargs)\n-\n     def _preprocess(\n         self,\n         videos: list[\"torch.Tensor\"],"
        },
        {
            "sha": "a544bb08815a92d1bf28150d4eb3fc8c85cc99f7",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -27,7 +27,7 @@\n from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n \n \n-class InternVLVideoProcessorInitKwargs(VideosKwargs):\n+class InternVLVideoProcessorInitKwargs(VideosKwargs, total=False):\n     initial_shift: Union[bool, float, int]\n \n "
        },
        {
            "sha": "c47461174516bd7d95d64e3d0e32686533aec992",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -51,7 +51,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class JanusImageProcessorKwargs(ImagesKwargs):\n+class JanusImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     min_size (`int`, *optional*, defaults to 14):\n         The minimum allowed size for the resized image. Ensures that neither the height nor width"
        },
        {
            "sha": "6a1742b443625f29c822dc412585753326f36170",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -1289,7 +1289,7 @@ def generate(\n             return generated_tokens\n \n \n-class JanusImageProcessorKwargs(ImagesKwargs):\n+class JanusImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     min_size (`int`, *optional*, defaults to 14):\n         The minimum allowed size for the resized image. Ensures that neither the height nor width"
        },
        {
            "sha": "f9fb98df6ac25590f610153a85e5cd2369d0615a",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -33,15 +33,17 @@\n     list[list[tuple[float, float, float]]],\n ]\n \n+NestedList = list[Union[Optional[int], \"NestedList\"]]\n+\n \n class Kosmos2ImagesKwargs(ImagesKwargs, total=False):\n-    bboxes: Optional[list[float]]\n-    num_image_tokens: Optional[int]\n+    bboxes: Optional[NestedList]  # NOTE: hub validators can't accept `Sequence`\n+    num_image_tokens: int\n     first_image_token_id: Optional[int]\n \n \n class Kosmos2TextKwargs(TextKwargs, total=False):\n-    add_eos_token: Optional[bool]\n+    add_eos_token: bool\n \n \n class Kosmos2ProcessorKwargs(ProcessingKwargs, total=False):"
        },
        {
            "sha": "fed17e08e1a71b04045492c0ab6b83cff3f05fa4",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -46,7 +46,7 @@\n DEFAULT_FONT_PATH = \"ybelkada/fonts\"\n \n \n-class Kosmos2_5ImageProcessorKwargs(ImagesKwargs):\n+class Kosmos2_5ImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     patch_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n         The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16.\n@@ -55,8 +55,8 @@ class Kosmos2_5ImageProcessorKwargs(ImagesKwargs):\n         [KOSMOS 2.5 paper](https://huggingface.co/papers/2309.11419).\n     \"\"\"\n \n-    patch_size: Optional[dict[str, int]]\n-    max_patches: Optional[int]\n+    patch_size: dict[str, int]\n+    max_patches: int\n \n \n # Copied from transformers.models.pix2struct.image_processing_pix2struct.torch_extract_patches"
        },
        {
            "sha": "6f53698f30b2870b29cd7df69b49847a07284aa1",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -52,7 +52,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class LayoutLMv2ImageProcessorKwargs(ImagesKwargs):\n+class LayoutLMv2ImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     apply_ocr (`bool`, *optional*, defaults to `True`):\n         Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n@@ -66,7 +66,7 @@ class LayoutLMv2ImageProcessorKwargs(ImagesKwargs):\n         `preprocess` method.\n     \"\"\"\n \n-    apply_ocr: Optional[bool]\n+    apply_ocr: bool\n     ocr_lang: Optional[str]\n     tesseract_config: Optional[str]\n "
        },
        {
            "sha": "44d4b33e11d9bc6550b950e525dc02d268322e1e",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -56,7 +56,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class LayoutLMv3ImageProcessorKwargs(ImagesKwargs):\n+class LayoutLMv3ImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     apply_ocr (`bool`, *optional*, defaults to `True`):\n         Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n@@ -70,7 +70,7 @@ class LayoutLMv3ImageProcessorKwargs(ImagesKwargs):\n         `preprocess` method.\n     \"\"\"\n \n-    apply_ocr: Optional[bool]\n+    apply_ocr: bool\n     ocr_lang: Optional[str]\n     tesseract_config: Optional[str]\n "
        },
        {
            "sha": "85d8fcd11b92d77e61f796119031acfeb4bff1fa",
            "filename": "src/transformers/models/lfm2_vl/image_processing_lfm2_vl_fast.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n import math\n from functools import lru_cache\n-from typing import Optional, Union\n+from typing import Union\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -169,24 +169,24 @@ def pad_along_first_dim(\n     return images, pixel_mask\n \n \n-class Lfm2VlImageProcessorKwargs(ImagesKwargs):\n+class Lfm2VlImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     downsample_factor (`int`, *optional*, defaults to `2`):\n         The downsampling factor for images used when resizing the image.\n     \"\"\"\n \n-    downsample_factor: Optional[int]\n-    do_image_splitting: Optional[bool]\n-    min_tiles: Optional[int]\n-    max_tiles: Optional[int]\n-    use_thumbnail: Optional[bool]\n-    min_image_tokens: Optional[int]\n-    max_image_tokens: Optional[int]\n-    encoder_patch_size: Optional[int]\n-    tile_size: Optional[int]\n-    max_pixels_tolerance: Optional[float]\n-    do_pad: Optional[bool]\n-    return_row_col_info: Optional[bool]\n+    downsample_factor: int\n+    do_image_splitting: bool\n+    min_tiles: int\n+    max_tiles: int\n+    use_thumbnail: bool\n+    min_image_tokens: int\n+    max_image_tokens: int\n+    encoder_patch_size: int\n+    tile_size: int\n+    max_pixels_tolerance: float\n+    do_pad: bool\n+    return_row_col_info: bool\n \n \n @auto_docstring"
        },
        {
            "sha": "311dfdc3b1235c74bd9d33cb49331d643aae1520",
            "filename": "src/transformers/models/lfm2_vl/processing_lfm2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 21,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -18,9 +18,9 @@\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, make_nested_list_of_images\n from ...processing_utils import (\n-    ImagesKwargs,\n     ProcessingKwargs,\n     ProcessorMixin,\n+    TextKwargs,\n     Unpack,\n )\n from ...tokenization_utils_base import BatchEncoding, TextInput\n@@ -30,25 +30,12 @@\n logger = logging.get_logger(__name__)\n \n \n-class Lfm2VlImagesKwargs(ImagesKwargs, total=False):\n-    downsample_factor: Optional[int]\n-    do_image_splitting: Optional[bool]\n-    min_tiles: Optional[int]\n-    max_tiles: Optional[int]\n-    use_thumbnail: Optional[bool]\n-    min_image_tokens: Optional[int]\n-    max_image_tokens: Optional[int]\n-    encoder_patch_size: Optional[int]\n-    tile_size: Optional[int]\n-    max_pixels_tolerance: Optional[float]\n-    patch_size: Optional[int]\n-    do_pad: Optional[bool]\n-    return_row_col_info: Optional[bool]\n+class Lfm2VlTextKwargs(TextKwargs, total=False):\n+    use_image_special_tokens: Optional[bool]\n \n \n class Lfm2VlProcessorKwargs(ProcessingKwargs, total=False):\n-    images_kwargs: Lfm2VlImagesKwargs\n-\n+    text_kwargs: Lfm2VlTextKwargs\n     _defaults = {\n         \"images_kwargs\": {\n             \"return_row_col_info\": True,\n@@ -75,8 +62,6 @@ class Lfm2VlProcessor(ProcessorMixin):\n             An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n         chat_template (`str`, *optional*):\n             A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n-        use_image_special_tokens (`bool`, *optional*, defaults to `True`):\n-            Whether to use image special tokens or not when processing.\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n@@ -88,12 +73,10 @@ def __init__(\n         image_processor,\n         tokenizer,\n         chat_template: Optional[str] = None,\n-        use_image_special_tokens: Optional[bool] = True,\n         **kwargs,\n     ):\n         self.image_token = tokenizer.image_token\n         self.image_token_id = tokenizer.image_token_id\n-        self.use_image_special_tokens = use_image_special_tokens\n         self.image_start_token = tokenizer.image_start_token\n         self.image_end_token = tokenizer.image_end_token\n         self.image_thumbnail_token = tokenizer.image_thumbnail"
        },
        {
            "sha": "ccbb60585b0b7f1474a623a8cc5e36de51547e55",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -308,7 +308,7 @@ def get_best_fit(\n     return optimal_canvas\n \n \n-class Llama4ImageProcessorKwargs(ImagesKwargs):\n+class Llama4ImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     max_patches (`int`, *optional*, defaults to 16):\n         The maximum number of patches to be extracted from the image.\n@@ -320,8 +320,8 @@ class Llama4ImageProcessorKwargs(ImagesKwargs):\n         but never upsample, unless the image is smaller than the patch size.\n     \"\"\"\n \n-    max_patches: Optional[int]\n-    resize_to_max_canvas: Optional[bool]\n+    max_patches: int\n+    resize_to_max_canvas: bool\n \n \n @auto_docstring"
        },
        {
            "sha": "c4bc1ed07287f2a4e895d7b415a9467a690ecd49",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -59,15 +59,15 @@\n     from PIL import Image\n \n \n-class LlavaNextImageProcessorKwargs(ImagesKwargs):\n+class LlavaNextImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     image_grid_pinpoints (`list[list[int]]`, *optional*):\n         A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n         based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n         method.\n     \"\"\"\n \n-    image_grid_pinpoints: Optional[list[list[int]]]\n+    image_grid_pinpoints: list[list[int]]\n \n \n def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.ndarray]:"
        },
        {
            "sha": "4b0f399e49592e263f8381e743cf8f8b440fd7f4",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -58,15 +58,15 @@\n     from PIL import Image\n \n \n-class LlavaOnevisionImageProcessorKwargs(ImagesKwargs):\n+class LlavaOnevisionImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     image_grid_pinpoints (`list[list[int]]`, *optional*):\n         A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n         based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n         method.\n     \"\"\"\n \n-    image_grid_pinpoints: Optional[list[list[int]]]\n+    image_grid_pinpoints: list[list[int]]\n \n \n # Copied from transformers.models.llava_next.image_processing_llava_next.divide_to_patches"
        },
        {
            "sha": "b80b2b76b1a7c64cf27d2ad2116f4d1fb392e906",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -76,8 +76,7 @@ def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionImagePro\n             batch_num_images = [1] * len(images)\n         else:\n             batch_num_images = [1]\n-        kwargs[\"batch_num_images\"] = batch_num_images\n-        return super().preprocess(images, **kwargs)\n+        return super().preprocess(images, batch_num_images, **kwargs)\n \n     def _resize_for_patching(\n         self,\n@@ -202,6 +201,7 @@ def _pad_for_batching(\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n+        batch_num_images: list[int],\n         do_resize: bool,\n         size: SizeDict,\n         image_grid_pinpoints: list[list[int]],\n@@ -214,7 +214,6 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n-        batch_num_images: list[int],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,"
        },
        {
            "sha": "88d1c10ab122ab03acf7f0b2aa4f336ba908ef1d",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -35,7 +35,7 @@\n \n from ...cache_utils import Cache\n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import group_images_by_shape, reorder_images\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n     OPENAI_CLIP_STD,\n@@ -128,12 +128,12 @@ def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionImagePro\n             batch_num_images = [1] * len(images)\n         else:\n             batch_num_images = [1]\n-        kwargs[\"batch_num_images\"] = batch_num_images\n-        return super().preprocess(images, **kwargs)\n+        return BaseImageProcessorFast.preprocess(images, batch_num_images, **kwargs)\n \n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n+        batch_num_images: list[int],\n         do_resize: bool,\n         size: SizeDict,\n         image_grid_pinpoints: list[list[int]],\n@@ -146,7 +146,6 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n-        batch_num_images: list[int],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,"
        },
        {
            "sha": "79b449eae416ddbec7dbc67a6e80f451f042c3ff",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -61,7 +61,7 @@\n     from torch import nn\n \n \n-class Mask2FormerImageProcessorKwargs(ImagesKwargs):\n+class Mask2FormerImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     ignore_index (`int`, *optional*):\n         Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n@@ -74,9 +74,9 @@ class Mask2FormerImageProcessorKwargs(ImagesKwargs):\n         The number of labels in the segmentation map.\n     \"\"\"\n \n-    size_divisor: Optional[int]\n+    size_divisor: int\n     ignore_index: Optional[int]\n-    do_reduce_labels: Optional[bool]\n+    do_reduce_labels: bool\n     num_labels: Optional[int]\n \n "
        },
        {
            "sha": "7d83809ced6638a3f67a077c01fac4adecc4b565",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -67,7 +67,7 @@\n     from torch import nn\n \n \n-class MaskFormerImageProcessorKwargs(ImagesKwargs):\n+class MaskFormerImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     ignore_index (`int`, *optional*):\n         Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n@@ -80,9 +80,9 @@ class MaskFormerImageProcessorKwargs(ImagesKwargs):\n         The number of labels in the segmentation map.\n     \"\"\"\n \n-    size_divisor: Optional[int]\n+    size_divisor: int\n     ignore_index: Optional[int]\n-    do_reduce_labels: Optional[bool]\n+    do_reduce_labels: bool\n     num_labels: Optional[int]\n \n "
        },
        {
            "sha": "1a1d76774868705465762e8b0d1ffd0d5185c3b2",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -50,13 +50,13 @@\n logger = logging.get_logger(__name__)\n \n \n-class MllamaImageProcessorKwargs(ImagesKwargs):\n+class MllamaImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     max_image_tiles (`int`, *optional*):\n         The maximum number of tiles allowed.\n     \"\"\"\n \n-    max_image_tiles: Optional[int]\n+    max_image_tiles: int\n \n \n @lru_cache(maxsize=10)"
        },
        {
            "sha": "53bf4cc210a066152fc040dae9fe0a047eb3a57c",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -258,9 +258,7 @@ def __call__(\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n         )\n-\n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        images_kwargs = output_kwargs[\"images_kwargs\"]\n \n         data = {}\n         if text is not None:\n@@ -306,7 +304,7 @@ def __call__(\n                     )\n \n         if images is not None:\n-            image_features = self.image_processor(images, **images_kwargs)\n+            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n             num_tiles = image_features.pop(\"num_tiles\")\n             data.update(image_features)\n "
        },
        {
            "sha": "876d9c6be444daead0a73294ad70cf748b498a4b",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -51,15 +51,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class MobileNetV2ImageProcessorKwargs(ImagesKwargs):\n+class MobileNetV2ImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n         Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n         is used for background, and background itself is not included in all classes of a dataset (e.g.\n         ADE20k). The background label will be replaced by 255.\n     \"\"\"\n \n-    do_reduce_labels: Optional[bool]\n+    do_reduce_labels: bool\n \n \n @requires(backends=(\"vision\",))"
        },
        {
            "sha": "0a9b6bc644231403a93ead15b947f7e9cebdd8e5",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -53,7 +53,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class MobileVitImageProcessorKwargs(ImagesKwargs):\n+class MobileVitImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     do_flip_channel_order (`bool`, *optional*, defaults to `self.do_flip_channel_order`):\n         Whether to flip the color channels from RGB to BGR or vice versa.\n@@ -63,8 +63,8 @@ class MobileVitImageProcessorKwargs(ImagesKwargs):\n         ADE20k). The background label will be replaced by 255.\n     \"\"\"\n \n-    do_flip_channel_order: Optional[bool]\n-    do_reduce_labels: Optional[bool]\n+    do_flip_channel_order: bool\n+    do_reduce_labels: bool\n \n \n @requires(backends=(\"vision\",))"
        },
        {
            "sha": "a9178ab43e07bbe00ff06e1ef75cd9ada764bdb1",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -52,7 +52,7 @@\n     import PIL\n \n \n-class NougatImageProcessorKwargs(ImagesKwargs):\n+class NougatImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     do_crop_margin (`bool`, *optional*, defaults to `True`):\n         Whether to crop the image margins.\n@@ -62,9 +62,9 @@ class NougatImageProcessorKwargs(ImagesKwargs):\n         Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n     \"\"\"\n \n-    do_crop_margin: Optional[bool]\n-    do_thumbnail: Optional[bool]\n-    do_align_long_axis: Optional[bool]\n+    do_crop_margin: bool\n+    do_thumbnail: bool\n+    do_align_long_axis: bool\n \n \n class NougatImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "00d4989fdf28e29d0f684af5e3278c8f4de58b8d",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -64,7 +64,7 @@\n     from torch import nn\n \n \n-class OneFormerImageProcessorKwargs(ImagesKwargs):\n+class OneFormerImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     repo_path (`str`, *optional*, defaults to `shi-labs/oneformer_demo`):\n         Path to a local directory or Hugging Face Hub repository containing model metadata.\n@@ -85,7 +85,7 @@ class OneFormerImageProcessorKwargs(ImagesKwargs):\n     num_text: Optional[int]\n     num_labels: Optional[int]\n     ignore_index: Optional[int]\n-    do_reduce_labels: Optional[bool]\n+    do_reduce_labels: bool\n \n \n # Copied from transformers.models.detr.image_processing_detr.max_across_indices"
        },
        {
            "sha": "4598e9f3f521cbea59ee7b7a663b9fa0382174d2",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -44,7 +44,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Ovis2ImageProcessorKwargs(ImagesKwargs):\n+class Ovis2ImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     crop_to_patches (`bool`, *optional*, defaults to `False`):\n         Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n@@ -61,10 +61,10 @@ class Ovis2ImageProcessorKwargs(ImagesKwargs):\n         `preprocess` method.\n     \"\"\"\n \n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n-    use_covering_area_grid: Optional[bool]\n+    crop_to_patches: bool\n+    min_patches: int\n+    max_patches: int\n+    use_covering_area_grid: bool\n \n \n # Similar to image_processing_mllama.get_all_supported_aspect_ratios"
        },
        {
            "sha": "03ff515e63af1aff4442fcc28837cbcfeb8ced98",
            "filename": "src/transformers/models/perception_lm/image_processing_perception_lm_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -42,7 +42,7 @@\n )\n \n \n-class PerceptionLMImageProcessorKwargs(ImagesKwargs):\n+class PerceptionLMImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     vision_input_type (`str`, *optional*, defaults to `\"thumb+tile\"`):\n         Vision processing strategy. `\"thumb+tile\"` uses both thumbnails and multiple tiles for\n@@ -54,8 +54,8 @@ class PerceptionLMImageProcessorKwargs(ImagesKwargs):\n     \"\"\"\n \n     vision_input_type: Optional[str]\n-    tile_size: Optional[int]\n-    max_num_tiles: Optional[int]\n+    tile_size: int\n+    max_num_tiles: int\n \n \n @auto_docstring\n@@ -68,7 +68,7 @@ class PerceptionLMImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n-    vision_input_type = \"thumb+tail\"\n+    vision_input_type = \"thumb+tile\"\n     tile_size = 448\n     max_num_tiles = 36\n     size = {\"width\": 448, \"height\": 448}  # for backward compatibility in tests"
        },
        {
            "sha": "98f160a1fd5e429b3e041ea63d8688769c53a66d",
            "filename": "src/transformers/models/phi4_multimodal/image_processing_phi4_multimodal_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -35,16 +35,16 @@\n logger = logging.get_logger(__name__)\n \n \n-class Phi4MultimodalImageProcessorKwargs(ImagesKwargs):\n+class Phi4MultimodalImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     patch_size (`int`, *optional*):\n         The size of the patch.\n     dynamic_hd (`int`, *optional*):\n         The maximum number of crops per image.\n     \"\"\"\n \n-    patch_size: Optional[int]\n-    dynamic_hd: Optional[int]\n+    patch_size: int\n+    dynamic_hd: int\n \n \n @auto_docstring"
        },
        {
            "sha": "3ec36ebda4402ac2e2337bc0d24795b7db3677d3",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -49,15 +49,15 @@\n DEFAULT_FONT_PATH = \"ybelkada/fonts\"\n \n \n-class Pix2StructImageProcessorKwargs(ImagesKwargs):\n+class Pix2StructImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     max_patches (`int`, *optional*):\n         Maximum number of patches to extract.\n     header_text (`Union[list[str], str]`, *optional*):\n         Text to render as a header. Only has an effect if `image_processor.is_vqa` is `True`.\n     \"\"\"\n \n-    max_patches: Optional[int]\n+    max_patches: int\n     header_text: Optional[Union[list[str], str]]\n \n "
        },
        {
            "sha": "3cbfaeb41922900e09f19b36d15d69af8779f57e",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -50,13 +50,13 @@\n     import PIL\n \n \n-class PixtralImageProcessorKwargs(ImagesKwargs):\n+class PixtralImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n-    patch_size (`dict[str, int]` *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n+    patch_size (`Union[dict[str, int], int]` *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n         Size of the patches in the model, used to calculate the output image size. Can be overridden by `patch_size` in the `preprocess` method.\n     \"\"\"\n \n-    patch_size: Optional[dict[str, int]]\n+    patch_size: Union[dict[str, int], int]\n \n \n # Adapted from function in image_transforms.py to ensure any transparent pixels are converted to white."
        },
        {
            "sha": "8d466739638d77e9c02c7bfec8ebe3df65b608a5",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -48,13 +48,13 @@\n logger = logging.get_logger(__name__)\n \n \n-class PoolFormerImageProcessorKwargs(ImagesKwargs):\n+class PoolFormerImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     crop_pct (`float`, *optional*, defaults to `self.crop_pct`):\n         Percentage of the image to crop. Only has an effect if `do_resize` is set to `True`.\n     \"\"\"\n \n-    crop_pct: Optional[float]\n+    crop_pct: float\n \n \n class PoolFormerImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "b62ba7994f0ab262328e9cbf8f66408a25cb4ea4",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -54,7 +54,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class PromptDepthAnythingImageProcessorKwargs(ImagesKwargs):\n+class PromptDepthAnythingImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     keep_aspect_ratio (`bool`, *optional*):\n         If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved.\n@@ -64,10 +64,10 @@ class PromptDepthAnythingImageProcessorKwargs(ImagesKwargs):\n         Scale factor to convert the prompt depth to meters.\n     \"\"\"\n \n-    keep_aspect_ratio: Optional[bool]\n-    ensure_multiple_of: Optional[int]\n-    size_divisor: Optional[int]\n-    prompt_scale_to_meter: Optional[float]\n+    keep_aspect_ratio: bool\n+    ensure_multiple_of: int\n+    size_divisor: int\n+    prompt_scale_to_meter: float\n \n \n def _constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):"
        },
        {
            "sha": "ea60155999e69d36c0665bf1eb0ee80b951f9199",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -32,17 +32,17 @@\n \n # Redefine kwargs for videos because Qwen-Omni uses some kwargs for processing omni\n # and does not use them in video processor class\n-class Qwen2_5_OmniVideosKwargs(VideosKwargs):\n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-    min_frames: Optional[int]\n-    max_frames: Optional[int]\n-    use_audio_in_video: Optional[bool]\n-    seconds_per_chunk: Optional[float]\n-    position_id_per_seconds: Optional[int]\n+class Qwen2_5_OmniVideosKwargs(VideosKwargs, total=False):\n+    min_pixels: int\n+    max_pixels: int\n+    patch_size: int\n+    temporal_patch_size: int\n+    merge_size: int\n+    min_frames: int\n+    max_frames: int\n+    use_audio_in_video: bool\n+    seconds_per_chunk: float\n+    position_id_per_seconds: Union[int, float]\n \n \n class Qwen2_5OmniProcessorKwargs(ProcessingKwargs, total=False):"
        },
        {
            "sha": "e5a1e0a7551e44fa82b5ff7ad6971c26f742a79a",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -52,7 +52,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Qwen2VLImageProcessorKwargs(ImagesKwargs):\n+class Qwen2VLImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     min_pixels (`int`, *optional*, defaults to `56 * 56`):\n         The min pixels of the image to resize the image.\n@@ -66,11 +66,11 @@ class Qwen2VLImageProcessorKwargs(ImagesKwargs):\n         The merge size of the vision encoder to llm encoder.\n     \"\"\"\n \n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n+    min_pixels: int\n+    max_pixels: int\n+    patch_size: int\n+    temporal_patch_size: int\n+    merge_size: int\n \n \n def smart_resize("
        },
        {
            "sha": "11b5ff80dadef7e0e4a4ad177a2f64cfd47c95fe",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -41,14 +41,14 @@\n from .image_processing_qwen2_vl import smart_resize\n \n \n-class Qwen2VLVideoProcessorInitKwargs(VideosKwargs):\n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-    min_frames: Optional[int]\n-    max_frames: Optional[int]\n+class Qwen2VLVideoProcessorInitKwargs(VideosKwargs, total=False):\n+    min_pixels: int\n+    max_pixels: int\n+    patch_size: int\n+    temporal_patch_size: int\n+    merge_size: int\n+    min_frames: int\n+    max_frames: int\n \n \n @add_start_docstrings("
        },
        {
            "sha": "df5629931fa39aee10e1a8c219ab20f9bb5ae637",
            "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -20,7 +20,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import re\n-from typing import Optional\n+from typing import Union\n \n import numpy as np\n \n@@ -34,17 +34,17 @@\n \n # Redefine kwargs for videos because Qwen-Omni uses some kwargs for processing omni\n # and does not use them in video processor class\n-class Qwen3OmniMoeVideosKwargs(VideosKwargs):\n-    min_pixels: Optional[int]\n-    max_pixels: Optional[int]\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-    min_frames: Optional[int]\n-    max_frames: Optional[int]\n-    use_audio_in_video: Optional[bool]\n-    seconds_per_chunk: Optional[float]\n-    position_id_per_seconds: Optional[int]\n+class Qwen3OmniMoeVideosKwargs(VideosKwargs, total=False):\n+    min_pixels: int\n+    max_pixels: int\n+    patch_size: int\n+    temporal_patch_size: int\n+    merge_size: int\n+    min_frames: int\n+    max_frames: int\n+    use_audio_in_video: bool\n+    seconds_per_chunk: float\n+    position_id_per_seconds: Union[int, float]\n \n \n class Qwen3OmniMoeProcessorKwargs(ProcessingKwargs, total=False):"
        },
        {
            "sha": "e74f55b642dd7b727beaf0537163aeb133fe9e11",
            "filename": "src/transformers/models/qwen3_vl/video_processing_qwen3_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -64,12 +64,12 @@ def smart_resize(\n     return h_bar, w_bar\n \n \n-class Qwen3VLVideoProcessorInitKwargs(VideosKwargs):\n-    patch_size: Optional[int]\n-    temporal_patch_size: Optional[int]\n-    merge_size: Optional[int]\n-    min_frames: Optional[int]\n-    max_frames: Optional[int]\n+class Qwen3VLVideoProcessorInitKwargs(VideosKwargs, total=False):\n+    patch_size: int\n+    temporal_patch_size: int\n+    merge_size: int\n+    min_frames: int\n+    max_frames: int\n \n \n @add_start_docstrings("
        },
        {
            "sha": "b366ca62fabfb701fbac900cae598d70a7877a82",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -68,7 +68,7 @@\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION,)\n \n \n-class RTDetrImageProcessorKwargs(ImagesKwargs):\n+class RTDetrImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n         Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n@@ -84,9 +84,9 @@ class RTDetrImageProcessorKwargs(ImagesKwargs):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n+    format: Union[str, AnnotationFormat]\n+    do_convert_annotations: bool\n+    return_segmentation_masks: bool\n     annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n     masks_path: Optional[Union[str, pathlib.Path]]\n "
        },
        {
            "sha": "eb2615b3e963279c55cb822845a86fcbad3248df",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -58,7 +58,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class SamImageProcessorKwargs(ImagesKwargs):\n+class SamImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     mask_size (`dict[str, int]`, *optional*):\n         The size `{\"longest_edge\": int}` to resize the segmentation maps to.\n@@ -67,8 +67,8 @@ class SamImageProcessorKwargs(ImagesKwargs):\n         map size provided for preprocessing.\n     \"\"\"\n \n-    mask_size: Optional[dict[str, int]]\n-    mask_pad_size: Optional[dict[str, int]]\n+    mask_size: dict[str, int]\n+    mask_pad_size: dict[str, int]\n \n \n class SamImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "d6cdd2ab265306c3d8bd526855fb1a775384d5cf",
            "filename": "src/transformers/models/sam/processing_sam.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -31,14 +31,14 @@\n     import torch\n \n \n-class SamImagesKwargs(ImagesKwargs):\n+class SamImagesKwargs(ImagesKwargs, total=False):\n     segmentation_maps: Optional[ImageInput]\n     input_points: Optional[list[list[float]]]\n     input_labels: Optional[list[list[int]]]\n     input_boxes: Optional[list[list[list[float]]]]\n-    point_pad_value: Optional[int]\n-    mask_size: Optional[dict[str, int]]\n-    mask_pad_size: Optional[dict[str, int]]\n+    point_pad_value: int\n+    mask_size: dict[str, int]\n+    mask_pad_size: dict[str, int]\n \n \n class SamProcessorKwargs(ProcessingKwargs, total=False):"
        },
        {
            "sha": "014354d8c64261758c61daa9da1dad4b8daafdb8",
            "filename": "src/transformers/models/sam2/image_processing_sam2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -43,13 +43,13 @@\n from ...utils import TensorType, auto_docstring\n \n \n-class Sam2FastImageProcessorKwargs(ImagesKwargs):\n+class Sam2FastImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     mask_size (`dict[str, int]`, *optional*):\n         The size `{\"height\": int, \"width\": int}` to resize the segmentation maps to.\n     \"\"\"\n \n-    mask_size: Optional[dict[str, int]]\n+    mask_size: dict[str, int]\n \n \n def _compute_stability_score(masks: \"torch.Tensor\", mask_threshold: float, stability_score_offset: int):"
        },
        {
            "sha": "d451fc946e6d69fc46bc50342db61c47253902b2",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -70,13 +70,13 @@\n logger = logging.get_logger(__name__)\n \n \n-class Sam2FastImageProcessorKwargs(ImagesKwargs):\n+class Sam2FastImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     mask_size (`dict[str, int]`, *optional*):\n         The size `{\"height\": int, \"width\": int}` to resize the segmentation maps to.\n     \"\"\"\n \n-    mask_size: Optional[dict[str, int]]\n+    mask_size: dict[str, int]\n \n \n @auto_docstring"
        },
        {
            "sha": "d0b11ab06146ece25397bd46c64d915b0de9f66d",
            "filename": "src/transformers/models/sam_hq/processing_samhq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -31,14 +31,14 @@\n     import torch\n \n \n-class SamHQImagesKwargs(ImagesKwargs):\n+class SamHQImagesKwargs(ImagesKwargs, total=False):\n     segmentation_maps: Optional[ImageInput]\n     input_points: Optional[list[list[float]]]\n     input_labels: Optional[list[list[int]]]\n     input_boxes: Optional[list[list[list[float]]]]\n     point_pad_value: Optional[int]\n-    mask_size: Optional[dict[str, int]]\n-    mask_pad_size: Optional[dict[str, int]]\n+    mask_size: dict[str, int]\n+    mask_pad_size: dict[str, int]\n \n \n class SamHQProcessorKwargs(ProcessingKwargs, total=False):"
        },
        {
            "sha": "ede9d589294b645a8e9d39cc01952d7666c06616",
            "filename": "src/transformers/models/segformer/image_processing_segformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -55,15 +55,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class SegformerImageProcessorKwargs(ImagesKwargs):\n+class SegformerImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n         Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n         is used for background, and background itself is not included in all classes of a dataset (e.g.\n         ADE20k). The background label will be replaced by 255.\n     \"\"\"\n \n-    do_reduce_labels: Optional[bool]\n+    do_reduce_labels: bool\n \n \n @requires(backends=(\"vision\",))"
        },
        {
            "sha": "85063fc9078a4b5f89650d4e1b3b877497786b21",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -48,7 +48,7 @@\n     from PIL import Image\n \n \n-class Siglip2ImageProcessorKwargs(ImagesKwargs):\n+class Siglip2ImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     patch_size (`int`, *optional*, defaults to 16):\n         The size (resolution) of each patch the image will be split to.\n@@ -57,8 +57,8 @@ class Siglip2ImageProcessorKwargs(ImagesKwargs):\n         and then padded in \"patch\" dimension to match this number exactly.\n     \"\"\"\n \n-    patch_size: Optional[int]\n-    max_num_patches: Optional[int]\n+    patch_size: int\n+    max_num_patches: int\n \n \n @lru_cache(maxsize=256)"
        },
        {
            "sha": "a946cc0c191b413918d7b084296643898a23cc44",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -53,7 +53,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class SmolVLMImageProcessorKwargs(ImagesKwargs):\n+class SmolVLMImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     do_image_splitting (`bool`, *optional*, defaults to `True`):\n         Whether to split the image into sub-images concatenated with the original image. They are split into patches\n@@ -64,9 +64,9 @@ class SmolVLMImageProcessorKwargs(ImagesKwargs):\n         Whether to return the row and column information of the images.\n     \"\"\"\n \n-    do_image_splitting: Optional[bool]\n-    max_image_size: Optional[dict[str, int]]\n-    return_row_col_info: Optional[bool]\n+    do_image_splitting: bool\n+    max_image_size: dict[str, int]\n+    return_row_col_info: bool\n \n \n MAX_IMAGE_SIZE = 4096  # 4k resolution as absolute maximum"
        },
        {
            "sha": "09751486f0ae5a23c813c5c017301561881ec4ef",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -90,8 +90,8 @@ def get_resize_output_image_size(\n     return height, width\n \n \n-class SmolVLMVideoProcessorInitKwargs(VideosKwargs):\n-    max_image_size: Optional[dict[str, int]]\n+class SmolVLMVideoProcessorInitKwargs(VideosKwargs, total=False):\n+    max_image_size: dict[str, int]\n \n \n class SmolVLMVideoProcessor(BaseVideoProcessor):"
        },
        {
            "sha": "57b1a9dc6cb14a28f22938c86586584249a33d81",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -46,13 +46,13 @@\n logger = logging.get_logger(__name__)\n \n \n-class SuperPointImageProcessorKwargs(ImagesKwargs):\n+class SuperPointImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     do_grayscale (`bool`, *optional*, defaults to `True`):\n         Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n     \"\"\"\n \n-    do_grayscale: Optional[bool] = True\n+    do_grayscale: bool\n \n \n def is_grayscale("
        },
        {
            "sha": "0ba052e92e05b32711160822a9158e0c7f39b220",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -38,8 +38,8 @@\n logger = logging.get_logger(__name__)\n \n \n-class Swin2SRImageProcessorKwargs(ImagesKwargs):\n-    size_divisor: Optional[int]\n+class Swin2SRImageProcessorKwargs(ImagesKwargs, total=False):\n+    size_divisor: int\n \n \n class Swin2SRImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "bd7aa6f5086ef43f393e1adb5d8f342e2306c072",
            "filename": "src/transformers/models/textnet/image_processing_textnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -49,8 +49,8 @@\n     import PIL\n \n \n-class TextNetImageProcessorKwargs(ImagesKwargs):\n-    size_divisor: Optional[int]\n+class TextNetImageProcessorKwargs(ImagesKwargs, total=False):\n+    size_divisor: int\n \n \n class TextNetImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "d1ae5c374b4b54fd8facf113e7be472e3aa8e3b0",
            "filename": "src/transformers/models/tvp/image_processing_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -50,7 +50,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class TvpImageProcessorKwargs(ImagesKwargs):\n+class TvpImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     do_flip_channel_order (`bool`, *optional*):\n         Whether to flip the channel order of the image from RGB to BGR.\n@@ -60,7 +60,7 @@ class TvpImageProcessorKwargs(ImagesKwargs):\n         Padding mode to use  `'constant'`, `'edge'`, `'reflect'`, or `'symmetric'`.\n     \"\"\"\n \n-    do_flip_channel_order: Optional[bool]\n+    do_flip_channel_order: bool\n     constant_values: Optional[Union[float, list[float]]]\n     pad_mode: Optional[str]\n "
        },
        {
            "sha": "c44fa3d504eac2d6e676edf24708de9593323b63",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -31,7 +31,7 @@\n \n class UdopTextKwargs(TextKwargs, total=False):\n     word_labels: Optional[Union[list[int], list[list[int]]]]\n-    boxes: Union[list[list[int]], list[list[list[int]]]]\n+    boxes: Optional[Union[list[list[int]], list[list[list[int]]]]]\n \n \n class UdopProcessorKwargs(ProcessingKwargs, total=False):"
        },
        {
            "sha": "5c1b2acf6e4b6c8c99f3b5d1f8e42bcb8f9d7803",
            "filename": "src/transformers/models/vilt/image_processing_vilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -47,8 +47,8 @@\n logger = logging.get_logger(__name__)\n \n \n-class ViltImageProcessorKwargs(ImagesKwargs):\n-    size_divisor: Optional[int]\n+class ViltImageProcessorKwargs(ImagesKwargs, total=False):\n+    size_divisor: int\n \n \n def max_across_indices(values: Iterable[Any]) -> list[Any]:"
        },
        {
            "sha": "ea54ba6034353bbe38f9093c8cfc2892e3aa687c",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -41,8 +41,8 @@\n logger = logging.get_logger(__name__)\n \n \n-class VitMatteImageProcessorKwargs(ImagesKwargs):\n-    size_divisor: Optional[int]\n+class VitMatteImageProcessorKwargs(ImagesKwargs, total=False):\n+    size_divisor: int\n \n \n class VitMatteImageProcessor(BaseImageProcessor):"
        },
        {
            "sha": "b594c296707bec730061aaea31bc09a845b21c7b",
            "filename": "src/transformers/models/yolos/image_processing_yolos.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -81,7 +81,7 @@\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n-class YolosImageProcessorKwargs(ImagesKwargs):\n+class YolosImageProcessorKwargs(ImagesKwargs, total=False):\n     r\"\"\"\n     format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n         Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n@@ -97,9 +97,9 @@ class YolosImageProcessorKwargs(ImagesKwargs):\n         Path to the directory containing the segmentation masks.\n     \"\"\"\n \n-    format: Optional[Union[str, AnnotationFormat]]\n-    do_convert_annotations: Optional[bool]\n-    return_segmentation_masks: Optional[bool]\n+    format: Union[str, AnnotationFormat]\n+    do_convert_annotations: bool\n+    return_segmentation_masks: bool\n     annotations: Optional[Union[AnnotationType, list[AnnotationType]]]\n     masks_path: Optional[Union[str, pathlib.Path]]\n "
        },
        {
            "sha": "d94a2ee088ebdd925960c790588a3df1923c64cf",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -62,7 +62,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class ZoeDepthImageProcessorKwargs(ImagesKwargs):\n+class ZoeDepthImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     keep_aspect_ratio (`bool`, *optional*, defaults to `True`):\n         If `True`, the image is resized by choosing the smaller of the height and width scaling factors and using it\n@@ -77,8 +77,8 @@ class ZoeDepthImageProcessorKwargs(ImagesKwargs):\n         Can be overridden by `ensure_multiple_of` in `preprocess`.\n     \"\"\"\n \n-    keep_aspect_ratio: Optional[bool]\n-    ensure_multiple_of: Optional[int]\n+    keep_aspect_ratio: bool\n+    ensure_multiple_of: int\n \n \n def get_resize_output_image_size("
        },
        {
            "sha": "55844c8d9ccef98e97df2dfa4a8e99614d883d9a",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 84,
            "deletions": 45,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -25,24 +25,35 @@\n import warnings\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Any, Optional, TypedDict, TypeVar, Union\n+from typing import Annotated, Any, Literal, Optional, TypedDict, TypeVar, Union\n \n import numpy as np\n import typing_extensions\n+from huggingface_hub.dataclasses import validate_typed_dict\n from huggingface_hub.errors import EntryNotFoundError\n \n from .audio_utils import AudioInput, load_audio\n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature\n from .image_utils import ChannelDimension, ImageInput, is_vision_available\n from .utils.chat_template_utils import render_jinja_template\n-from .video_utils import VideoInput, VideoMetadata\n+from .utils.type_validators import (\n+    device_validator,\n+    image_size_validator,\n+    padding_validator,\n+    positive_any_number,\n+    positive_int,\n+    resampling_validator,\n+    tensor_type_validator,\n+    truncation_validator,\n+    video_metadata_validator,\n+)\n+from .video_utils import VideoInput, VideoMetadataType\n \n \n if is_vision_available():\n     from .image_utils import PILImageResampling\n \n-\n from .tokenization_utils_base import (\n     PaddingStrategy,\n     PreTokenizedInput,\n@@ -72,8 +83,6 @@\n \n \n if is_torch_available():\n-    import torch\n-\n     from .modeling_utils import PreTrainedAudioTokenizerBase\n \n \n@@ -137,28 +146,32 @@ class TextKwargs(TypedDict, total=False):\n             The side on which padding will be applied.\n         return_mm_token_type_ids (`bool`, *optional*):\n             Whether to return multimodal token type ids indicating mm placeholder token positions.\n+        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+            If set, will return tensors of a particular framework. Acceptable values are:\n+            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+            - `'np'`: Return NumPy `np.ndarray` objects.\n     \"\"\"\n \n     text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n-    text_target: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]\n+    text_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n     text_pair_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n     add_special_tokens: Optional[bool]\n-    padding: Union[bool, str, PaddingStrategy]\n-    truncation: Union[bool, str, TruncationStrategy]\n-    max_length: Optional[int]\n-    stride: Optional[int]\n+    padding: Annotated[Optional[Union[bool, str, PaddingStrategy]], padding_validator()]\n+    truncation: Annotated[Optional[Union[bool, str, TruncationStrategy]], truncation_validator()]\n+    max_length: Annotated[Optional[int], positive_int()]\n+    stride: Annotated[Optional[int], positive_int()]\n     is_split_into_words: Optional[bool]\n-    pad_to_multiple_of: Optional[int]\n+    pad_to_multiple_of: Annotated[Optional[int], positive_int()]\n     return_token_type_ids: Optional[bool]\n     return_attention_mask: Optional[bool]\n     return_overflowing_tokens: Optional[bool]\n     return_special_tokens_mask: Optional[bool]\n     return_offsets_mapping: Optional[bool]\n     return_length: Optional[bool]\n     verbose: Optional[bool]\n-    padding_side: Optional[str]\n+    padding_side: Optional[Literal[\"left\", \"right\"]]\n     return_mm_token_type_ids: Optional[bool]\n-    return_tensors: Optional[Union[str, TensorType]]\n+    return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n \n \n class ImagesKwargs(TypedDict, total=False):\n@@ -175,6 +188,8 @@ class methods and docstrings.\n             Resize the shorter side of the input to `size[\"shortest_edge\"]`.\n         crop_size (`dict[str, int]`, *optional*):\n             Desired output size when applying center-cropping.\n+        do_convert_rgb (`bool`):\n+            Whether to convert the video to RGB format.\n         resample (`PILImageResampling`, *optional*):\n             Resampling filter to use if resizing the image.\n         do_rescale (`bool`, *optional*):\n@@ -183,9 +198,9 @@ class methods and docstrings.\n             Scale factor to use if rescaling the image.\n         do_normalize (`bool`, *optional*):\n             Whether to normalize the image.\n-        image_mean (`float` or `list[float]`, *optional*):\n+        image_mean (`float` or `list[float] or tuple[float, float, float]`, *optional*):\n             Mean to use if normalizing the image.\n-        image_std (`float` or `list[float]`, *optional*):\n+        image_std (`float` or `list[float] or tuple[float, float, float]`, *optional*):\n             Standard deviation to use if normalizing the image.\n         do_pad (`bool`, *optional*):\n             Whether to pad the images in the batch.\n@@ -199,28 +214,32 @@ class methods and docstrings.\n             The channel dimension format for the input image.\n         device (`Union[str, torch.Tensor]`, *optional*):\n             The device to use for processing (e.g. \"cpu\", \"cuda\"), only relevant for fast image processing.\n+        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+            If set, will return tensors of a particular framework. Acceptable values are:\n+            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+            - `'np'`: Return NumPy `np.ndarray` objects.\n         disable_grouping (`bool`, *optional*):\n             Whether to group images by shapes when processing or not, only relevant for fast image processing.\n     \"\"\"\n \n     do_convert_rgb: Optional[bool]\n     do_resize: Optional[bool]\n-    size: Optional[dict[str, int]]\n-    crop_size: Optional[dict[str, int]]\n-    resample: Optional[Union[\"PILImageResampling\", int]]\n+    size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n+    crop_size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n+    resample: Annotated[Optional[Union[\"PILImageResampling\", int]], resampling_validator()]\n     do_rescale: Optional[bool]\n     rescale_factor: Optional[float]\n     do_normalize: Optional[bool]\n-    image_mean: Optional[Union[float, list[float]]]\n-    image_std: Optional[Union[float, list[float]]]\n+    image_mean: Optional[Union[float, list[float], tuple[float, ...]]]\n+    image_std: Optional[Union[float, list[float], tuple[float, ...]]]\n     do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n+    pad_size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n     do_center_crop: Optional[bool]\n-    data_format: Optional[ChannelDimension]\n+    data_format: Optional[Union[str, ChannelDimension]]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n-    device: Optional[Union[str, \"torch.device\"]]\n+    device: Annotated[Optional[str], device_validator()]\n+    return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n     disable_grouping: Optional[bool]\n-    return_tensors: Optional[Union[str, TensorType]]\n \n \n class VideosKwargs(TypedDict, total=False):\n@@ -244,9 +263,9 @@ class VideosKwargs(TypedDict, total=False):\n             Scale factor to use if rescaling the video.\n         do_normalize (`bool`, *optional*):\n             Whether to normalize the video.\n-        image_mean (`float` or `list[float]`, *optional*):\n+        image_mean (`float` or `list[float] or tuple[float, float, float]`, *optional*):\n             Mean to use if normalizing the video.\n-        image_std (`float` or `list[float]`, *optional*):\n+        image_std (`float` or `list[float] or tuple[float, float, float]`, *optional*):\n             Standard deviation to use if normalizing the video.\n         do_center_crop (`bool`, *optional*):\n             Whether to center crop the video.\n@@ -268,32 +287,36 @@ class VideosKwargs(TypedDict, total=False):\n             The channel dimension format for the input video.\n         device (`Union[str, torch.Tensor]`, *optional*):\n             The device to use for processing (e.g. \"cpu\", \"cuda\"), only relevant for fast image processing.\n-        return_metadata (`ChannelDimension` or `str`, *optional*):\n+        return_metadata (`bool`, *optional*):\n             Whether to return video metadata or not.\n+        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+            If set, will return tensors of a particular framework. Acceptable values are:\n+            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+            - `'np'`: Return NumPy `np.ndarray` objects.\n     \"\"\"\n \n     do_convert_rgb: Optional[bool]\n     do_resize: Optional[bool]\n-    size: Optional[dict[str, int]]\n+    size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n     default_to_square: Optional[bool]\n-    resample: Optional[\"PILImageResampling\"]\n+    resample: Annotated[Optional[Union[\"PILImageResampling\", int]], resampling_validator()]\n     do_rescale: Optional[bool]\n     rescale_factor: Optional[float]\n     do_normalize: Optional[bool]\n-    image_mean: Optional[Union[float, list[float]]]\n-    image_std: Optional[Union[float, list[float]]]\n+    image_mean: Optional[Union[float, list[float], tuple[float, ...]]]\n+    image_std: Optional[Union[float, list[float], tuple[float, ...]]]\n     do_center_crop: Optional[bool]\n     do_pad: Optional[bool]\n-    crop_size: Optional[dict[str, int]]\n-    data_format: Optional[ChannelDimension]\n+    crop_size: Annotated[Optional[Union[int, list[int], tuple[int, ...], dict[str, int]]], image_size_validator()]\n+    data_format: Optional[Union[str, ChannelDimension]]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n-    device: Optional[Union[str, \"torch.device\"]]\n+    device: Annotated[Optional[str], device_validator()]\n     do_sample_frames: Optional[bool]\n-    video_metadata: Optional[Union[VideoMetadata, dict]]\n-    fps: Optional[Union[int, float]]\n-    num_frames: Optional[int]\n+    video_metadata: Annotated[Optional[VideoMetadataType], video_metadata_validator()]\n+    fps: Annotated[Optional[Union[int, float]], positive_any_number()]\n+    num_frames: Annotated[Optional[int], positive_int()]\n     return_metadata: Optional[bool]\n-    return_tensors: Optional[Union[str, TensorType]]\n+    return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n \n \n class AudioKwargs(TypedDict, total=False):\n@@ -324,16 +347,20 @@ class AudioKwargs(TypedDict, total=False):\n             If set, will pad the sequence to a multiple of the provided value.\n         return_attention_mask (`bool`, *optional*):\n             Whether or not [`~ASTFeatureExtractor.__call__`] should return `attention_mask`.\n+        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+            If set, will return tensors of a particular framework. Acceptable values are:\n+            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+            - `'np'`: Return NumPy `np.ndarray` objects.\n     \"\"\"\n \n-    sampling_rate: Optional[int]\n-    raw_speech: Optional[Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]]]\n-    padding: Optional[Union[bool, str, PaddingStrategy]]\n-    max_length: Optional[int]\n-    truncation: Optional[bool]\n-    pad_to_multiple_of: Optional[int]\n+    sampling_rate: Annotated[Optional[int], positive_int()]\n+    raw_speech: Optional[Union[\"np.ndarray\", list[float], list[\"np.ndarray\"], list[list[float]]]]\n+    padding: Annotated[Optional[Union[bool, str, PaddingStrategy]], padding_validator()]\n+    max_length: Annotated[Optional[int], positive_int()]\n+    truncation: Annotated[Optional[Union[bool, str, TruncationStrategy]], truncation_validator()]\n+    pad_to_multiple_of: Annotated[Optional[int], positive_int()]\n     return_attention_mask: Optional[bool]\n-    return_tensors: Optional[Union[str, TensorType]]\n+    return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n \n \n class ProcessingKwargs(TypedDict, total=False):\n@@ -1361,6 +1388,18 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n                         f\"Keyword argument `{key}` is not a valid argument for this processor and will be ignored.\"\n                     )\n \n+        for key, typed_dict_obj in ModelProcessorKwargs.__annotations__.items():\n+            if key in map_preprocessor_kwargs:\n+                preprocessor = getattr(self, map_preprocessor_kwargs[key], None)\n+                if preprocessor is None or getattr(preprocessor, \"valid_kwargs\", None) is None:\n+                    continue\n+                preprocessor_typed_dict_obj = getattr(preprocessor, \"valid_kwargs\")\n+                typed_dict_obj = TypedDict(\n+                    \"merged_typed_dict\",\n+                    {**preprocessor_typed_dict_obj.__annotations__, **typed_dict_obj.__annotations__},\n+                    total=False,\n+                )\n+            validate_typed_dict(typed_dict_obj, output_kwargs[key])\n         return output_kwargs\n \n     @classmethod"
        },
        {
            "sha": "6e6ccdc4c8e927e80a7db03dfcaec357628aa036",
            "filename": "src/transformers/utils/type_validators.py",
            "status": "added",
            "additions": 115,
            "deletions": 0,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Futils%2Ftype_validators.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Futils%2Ftype_validators.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ftype_validators.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -0,0 +1,115 @@\n+from collections.abc import Sequence\n+from typing import Optional, Union\n+\n+from ..tokenization_utils_base import PaddingStrategy, TruncationStrategy\n+from ..video_utils import VideoMetadataType\n+from .generic import TensorType\n+from .import_utils import is_vision_available\n+\n+\n+if is_vision_available():\n+    from ..image_utils import PILImageResampling\n+\n+\n+def positive_any_number(value: Optional[Union[int, float]] = None):\n+    if value is not None and (not isinstance(value, (int, float)) or not value >= 0):\n+        raise ValueError(f\"Value must be a positive integer or floating number, got {value}\")\n+\n+\n+def positive_int(value: Optional[int] = None):\n+    if value is not None and (not isinstance(value, int) or not value >= 0):\n+        raise ValueError(f\"Value must be a positive integer, got {value}\")\n+\n+\n+def padding_validator(value: Optional[Union[bool, str, PaddingStrategy]] = None):\n+    possible_names = [\"longest\", \"max_length\", \"do_not_pad\"]\n+    if value is None:\n+        pass\n+    elif not isinstance(value, (bool, str, PaddingStrategy)):\n+        raise ValueError(\"Value for padding must be either a boolean, a string or a `PaddingStrategy`\")\n+    elif isinstance(value, str) and value not in possible_names:\n+        raise ValueError(f\"If padding is a string, the value must be one of {possible_names}\")\n+\n+\n+def truncation_validator(value: Optional[Union[bool, str, TruncationStrategy]] = None):\n+    possible_names = [\"only_first\", \"only_second\", \"longest_first\", \"do_not_truncate\"]\n+    if value is None:\n+        pass\n+    elif not isinstance(value, (bool, str, TruncationStrategy)):\n+        raise ValueError(\"Value for truncation must be either a boolean, a string or a `TruncationStrategy`\")\n+    elif isinstance(value, str) and value not in possible_names:\n+        raise ValueError(f\"If truncation is a string, value must be one of {possible_names}\")\n+\n+\n+def image_size_validator(value: Optional[Union[int, Sequence[int], dict[str, int]]] = None):\n+    possible_keys = [\"height\", \"width\", \"longest_edge\", \"shortest_edge\", \"max_height\", \"max_width\"]\n+    if value is None:\n+        pass\n+    elif isinstance(value, dict) and any(k not in possible_keys for k in value.keys()):\n+        raise ValueError(f\"Value for size must be a dict with keys {possible_keys} but got size={value}\")\n+\n+\n+def device_validator(value: Optional[Union[str, int]] = None):\n+    possible_names = [\"cpu\", \"cuda\", \"xla\", \"xpu\", \"mps\", \"meta\"]\n+    if value is None:\n+        pass\n+    elif isinstance(value, int) and value < 0:\n+        raise ValueError(\n+            f\"If device is an integer, the value must be a strictly positive integer but got device={value}\"\n+        )\n+    elif isinstance(value, str) and value.split(\":\")[0] not in possible_names:\n+        raise ValueError(f\"If device is an string, the value must be one of {possible_names} but got device={value}\")\n+    elif not isinstance(value, (int, str)):\n+        raise ValueError(\n+            f\"Device must be either an integer device ID or a string (e.g., 'cpu', 'cuda:0'), but got device={value}\"\n+        )\n+\n+\n+def resampling_validator(value: Optional[Union[int, \"PILImageResampling\"]] = None):\n+    if value is None:\n+        pass\n+    elif isinstance(value, int) and value not in list(range(6)):\n+        raise ValueError(\n+            f\"The resampling should be one of {list(range(6))} when provided as integer, but got resampling={value}\"\n+        )\n+    elif is_vision_available() and not isinstance(value, (PILImageResampling, int)):\n+        raise ValueError(f\"The resampling should an integer or `PIL.Image.Resampling`, but got resampling={value}\")\n+\n+\n+def video_metadata_validator(value: Optional[VideoMetadataType] = None):\n+    if value is None:\n+        return\n+\n+    valid_keys = [\"total_num_frames\", \"fps\", \"width\", \"height\", \"duration\", \"video_backend\", \"frames_indices\"]\n+\n+    def check_dict_keys(d: dict) -> bool:\n+        return all(key in valid_keys for key in d.keys())\n+\n+    if isinstance(value, Sequence) and isinstance(value[0], Sequence) and isinstance(value[0][0], dict):\n+        for sublist in value:\n+            for item in sublist:\n+                if not check_dict_keys(item):\n+                    raise ValueError(\n+                        f\"Invalid keys found in video metadata. Valid keys: {valid_keys} got: {list(item.keys())}\"\n+                    )\n+\n+    elif isinstance(value, Sequence) and isinstance(value[0], dict):\n+        for item in value:\n+            if not check_dict_keys(item):\n+                raise ValueError(\n+                    f\"Invalid keys found in video metadata. Valid keys: {valid_keys} got: {list(item.keys())}\"\n+                )\n+\n+    elif isinstance(value, dict):\n+        if not check_dict_keys(value):\n+            raise ValueError(\n+                f\"Invalid keys found in video metadata. Valid keys: {valid_keys}, got: {list(value.keys())}\"\n+            )\n+\n+\n+def tensor_type_validator(value: Optional[Union[str, TensorType]] = None):\n+    possible_names = [\"pt\", \"np\", \"mlx\"]\n+    if value is None:\n+        pass\n+    elif not isinstance(value, str) or value not in possible_names:\n+        raise ValueError(f\"The tensor type should be one of {possible_names} but got tensor_type={value}\")"
        },
        {
            "sha": "4283c163c574508724526775d19a0cd9d08d26eb",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -21,6 +21,7 @@\n from typing import Any, Callable, Optional, Union\n \n import numpy as np\n+from huggingface_hub.dataclasses import validate_typed_dict\n \n from .dynamic_module_utils import custom_object_save\n from .image_processing_utils import (\n@@ -358,6 +359,10 @@ def preprocess(\n             captured_kwargs=kwargs.keys(),\n             valid_processor_keys=list(self.valid_kwargs.__annotations__.keys()) + [\"return_tensors\"],\n         )\n+\n+        # Perform type validation on received kwargs\n+        validate_typed_dict(self.valid_kwargs, kwargs)\n+\n         # Set default kwargs from self. This ensures that if a kwarg is not provided\n         # by the user, it gets its default value from the instance, or is set to None.\n         for kwarg_name in self.valid_kwargs.__annotations__:"
        },
        {
            "sha": "1faecf9791c4dca54c074fd1cecd73e36199684a",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -112,6 +112,11 @@ def update(self, dictionary):\n                 setattr(self, key, value)\n \n \n+VideoMetadataType = Union[\n+    VideoMetadata, dict, list[Union[dict, VideoMetadata]], list[list[Union[dict, VideoMetadata]]]\n+]\n+\n+\n def is_valid_video_frame(frame):\n     return isinstance(frame, PIL.Image.Image) or (\n         (is_numpy_array(frame) or is_torch_tensor(frame)) and frame.ndim == 3\n@@ -217,7 +222,7 @@ def make_batched_videos(videos) -> list[Union[np.ndarray, \"torch.Tensor\", \"URL\",\n     return flat_videos_list\n \n \n-def make_batched_metadata(videos: VideoInput, video_metadata: Union[VideoMetadata, dict]):\n+def make_batched_metadata(videos: VideoInput, video_metadata: VideoMetadataType) -> list[VideoMetadata]:\n     if video_metadata is None:\n         # Create default metadata and fill attributes we can infer from given video\n         video_metadata = ["
        },
        {
            "sha": "81a16ba39c147cfec3c43ccba8cf75112125cfca",
            "filename": "tests/models/cohere2_vision/test_image_processing_cohere2_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fcohere2_vision%2Ftest_image_processing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fcohere2_vision%2Ftest_image_processing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_image_processing_cohere2_vision.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -176,8 +176,8 @@ def test_call_numpy_4_channels(self):\n                 image_inputs[0],\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             ).pixel_values\n             self.assertEqual(tuple(encoded_images.shape), (10, 4, 30, 30))\n \n@@ -186,7 +186,7 @@ def test_call_numpy_4_channels(self):\n                 image_inputs,\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             ).pixel_values\n             self.assertEqual(tuple(encoded_images.shape), (70, 4, 30, 30))"
        },
        {
            "sha": "119af1432ce1663efcd0baa26dd2017503d7b6a0",
            "filename": "tests/models/colpali/test_processing_colpali.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -133,15 +133,15 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n \n     def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         \"\"\"\n-        We use do_rescale=True, rescale_factor=-1 to ensure that image_processor kwargs are preserved in the processor.\n+        We use do_rescale=True, rescale_factor=-1.0 to ensure that image_processor kwargs are preserved in the processor.\n         We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n         Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n         \"\"\"\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\n-            \"image_processor\", do_rescale=True, rescale_factor=-1\n+            \"image_processor\", do_rescale=True, rescale_factor=-1.0\n         )\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n \n@@ -179,7 +179,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n \n         image_input = self.prepare_image_inputs()\n \n-        inputs = processor(images=image_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n+        inputs = processor(images=image_input, do_rescale=True, rescale_factor=-1.0, return_tensors=\"pt\")\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_unstructured_kwargs(self):\n@@ -194,7 +194,7 @@ def test_unstructured_kwargs(self):\n             text=input_str,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"max_length\",\n             max_length=76,\n         )\n@@ -213,7 +213,7 @@ def test_unstructured_kwargs_batched(self):\n             images=image_input,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"longest\",\n             max_length=76,\n         )\n@@ -231,7 +231,7 @@ def test_doubly_passed_kwargs(self):\n         with self.assertRaises(ValueError):\n             _ = processor(\n                 images=image_input,\n-                images_kwargs={\"do_rescale\": True, \"rescale_factor\": -1},\n+                images_kwargs={\"do_rescale\": True, \"rescale_factor\": -1.0},\n                 do_rescale=True,\n                 return_tensors=\"pt\",\n             )\n@@ -248,7 +248,7 @@ def test_structured_kwargs_nested(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1.0},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n         }\n \n@@ -268,7 +268,7 @@ def test_structured_kwargs_nested_from_dict(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1.0},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n         }\n "
        },
        {
            "sha": "236456dd7f8829057751562cecee06f4e65d333e",
            "filename": "tests/models/colqwen2/test_processing_colqwen2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -132,15 +132,15 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n \n     def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         \"\"\"\n-        We use do_rescale=True, rescale_factor=-1 to ensure that image_processor kwargs are preserved in the processor.\n+        We use do_rescale=True, rescale_factor=-1.0 to ensure that image_processor kwargs are preserved in the processor.\n         We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n         Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n         \"\"\"\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\n-            \"image_processor\", do_rescale=True, rescale_factor=-1\n+            \"image_processor\", do_rescale=True, rescale_factor=-1.0\n         )\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n \n@@ -178,7 +178,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n \n         image_input = self.prepare_image_inputs()\n \n-        inputs = processor(images=image_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n+        inputs = processor(images=image_input, do_rescale=True, rescale_factor=-1.0, return_tensors=\"pt\")\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_unstructured_kwargs(self):\n@@ -193,7 +193,7 @@ def test_unstructured_kwargs(self):\n             text=input_str,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"max_length\",\n             max_length=76,\n         )\n@@ -212,7 +212,7 @@ def test_unstructured_kwargs_batched(self):\n             images=image_input,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"longest\",\n             max_length=76,\n         )\n@@ -230,7 +230,7 @@ def test_doubly_passed_kwargs(self):\n         with self.assertRaises(ValueError):\n             _ = processor(\n                 images=image_input,\n-                images_kwargs={\"do_rescale\": True, \"rescale_factor\": -1},\n+                images_kwargs={\"do_rescale\": True, \"rescale_factor\": -1.0},\n                 do_rescale=True,\n                 return_tensors=\"pt\",\n             )\n@@ -247,7 +247,7 @@ def test_structured_kwargs_nested(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1.0},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n         }\n \n@@ -267,7 +267,7 @@ def test_structured_kwargs_nested_from_dict(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1.0},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n         }\n "
        },
        {
            "sha": "1226fe473db9d57363448e0794fbb68bcff44994",
            "filename": "tests/models/glm4v/test_image_processing_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fglm4v%2Ftest_image_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fglm4v%2Ftest_image_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_image_processing_glm4v.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -236,8 +236,8 @@ def test_call_numpy_4_channels(self):\n                 image_inputs[0],\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             ).pixel_values\n             expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n@@ -247,8 +247,8 @@ def test_call_numpy_4_channels(self):\n                 image_inputs,\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             ).pixel_values\n             expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)"
        },
        {
            "sha": "8443c728f2f200e55ce16dd526998bc435d314d8",
            "filename": "tests/models/glm4v/test_video_processing_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -250,8 +250,8 @@ def test_call_numpy_4_channels(self):\n                 video_inputs[0],\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             )[self.input_name]\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n             self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n@@ -261,8 +261,8 @@ def test_call_numpy_4_channels(self):\n                 video_inputs,\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             )[self.input_name]\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n             self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)"
        },
        {
            "sha": "47efd5c2be6dd340429a590069ee4a3fad1b4695",
            "filename": "tests/models/janus/test_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -444,7 +444,7 @@ def test_chat_template_accepts_processing_kwargs(self):\n             tokenize=True,\n             return_dict=True,\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             return_tensors=\"np\",\n         )\n         self.assertLessEqual(out_dict[self.images_input_name][0][0].mean(), 0)"
        },
        {
            "sha": "d1f7669bdddddac9b39b55fb6861d4f3a71b59d3",
            "filename": "tests/models/lfm2_vl/test_processing_lfm2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Flfm2_vl%2Ftest_processing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Flfm2_vl%2Ftest_processing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_processing_lfm2_vl.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -100,7 +100,7 @@ def prepare_processor_dict():\n             \"{{'<|im_start|>assistant\\n' }}\"\n             \"{% endif %}\"\n         )\n-        return {\"chat_template\": chat_template, \"use_image_special_tokens\": True}\n+        return {\"chat_template\": chat_template}\n \n     # Override as Lfm2VL needs images/video to be an explicitly nested batch\n     def prepare_image_inputs(self, batch_size=None):"
        },
        {
            "sha": "50a6b7db0f4e7255ed26e8da798defcb554ee8e4",
            "filename": "tests/models/mllama/test_processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -386,7 +386,7 @@ def test_unstructured_kwargs_batched(self):\n             images=image_input,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"longest\",\n             max_length=76,\n         )"
        },
        {
            "sha": "68a71a6dfb8c918a1c7dadd80c00c06657c6293b",
            "filename": "tests/models/nougat/test_image_processing_nougat.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -282,8 +282,8 @@ def test_call_numpy_4_channels(self):\n                     image_inputs[0],\n                     return_tensors=\"pt\",\n                     input_data_format=\"channels_last\",\n-                    image_mean=0,\n-                    image_std=1,\n+                    image_mean=(0.0, 0.0, 0.0, 0.0),\n+                    image_std=(1.0, 1.0, 1.0, 1.0),\n                 ).pixel_values\n                 expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(\n                     [image_inputs[0]]\n@@ -295,8 +295,8 @@ def test_call_numpy_4_channels(self):\n                     image_inputs,\n                     return_tensors=\"pt\",\n                     input_data_format=\"channels_last\",\n-                    image_mean=0,\n-                    image_std=1,\n+                    image_mean=(0.0, 0.0, 0.0, 0.0),\n+                    image_std=(1.0, 1.0, 1.0, 1.0),\n                 ).pixel_values\n                 expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n                 self.assertEqual("
        },
        {
            "sha": "1343d069d819bba59f1c2e20b3f9b8a049f71a51",
            "filename": "tests/models/oneformer/test_image_processing_oneformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -224,7 +224,6 @@ def comm_get_image_processor_inputs(\n             annotations,\n             return_tensors=\"pt\",\n             instance_id_to_semantic_id=instance_id_to_semantic_id,\n-            pad_and_return_pixel_mask=True,\n         )\n \n         return inputs"
        },
        {
            "sha": "b80adebbd9ab2066be1a8b8f007903cff2936890",
            "filename": "tests/models/qwen2_vl/test_video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -265,8 +265,8 @@ def test_call_numpy_4_channels(self):\n                 video_inputs[0],\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             )[self.input_name]\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n             self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n@@ -276,8 +276,8 @@ def test_call_numpy_4_channels(self):\n                 video_inputs,\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             )[self.input_name]\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n             self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)"
        },
        {
            "sha": "60f4023938bb466e9161fc44c43ed032edb4eec2",
            "filename": "tests/models/qwen3_vl/test_video_processing_qwen3_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fqwen3_vl%2Ftest_video_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fqwen3_vl%2Ftest_video_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2Ftest_video_processing_qwen3_vl.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -249,8 +249,8 @@ def test_call_numpy_4_channels(self):\n                 video_inputs[0],\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             )[self.input_name]\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n             self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n@@ -260,8 +260,8 @@ def test_call_numpy_4_channels(self):\n                 video_inputs,\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n             )[self.input_name]\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n             self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)"
        },
        {
            "sha": "40aaaf7a6ca20c29a66b3abf15ef32ab9c78b223",
            "filename": "tests/models/smolvlm/test_processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -482,7 +482,7 @@ def test_unstructured_kwargs_batched_video(self):\n             videos=video_input,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"max_length\",\n             max_length=172,\n         )"
        },
        {
            "sha": "6d454daf9e4b88d79b56ef1019549c127210f476",
            "filename": "tests/models/tvp/test_image_processing_tvp.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -274,7 +274,11 @@ def test_call_numpy_4_channels(self):\n             # Test not batched input\n             expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n             encoded_videos = image_processing(\n-                test_inputs[0], return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n+                test_inputs[0],\n+                return_tensors=\"pt\",\n+                image_mean=(0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0),\n+                input_data_format=\"channels_first\",\n             ).pixel_values\n             self.assertListEqual(\n                 list(encoded_videos.shape),\n@@ -292,7 +296,11 @@ def test_call_numpy_4_channels(self):\n                 video_inputs, batched=True\n             )\n             encoded_videos = image_processing(\n-                test_inputs, return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n+                test_inputs,\n+                return_tensors=\"pt\",\n+                image_mean=(0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0),\n+                input_data_format=\"channels_first\",\n             ).pixel_values\n             self.assertListEqual(\n                 list(encoded_videos.shape),"
        },
        {
            "sha": "f8576a7bc8afa0bd5dd0d97c881676db66c1ff97",
            "filename": "tests/models/videomae/test_image_processing_videomae.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fvideomae%2Ftest_image_processing_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fvideomae%2Ftest_image_processing_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_image_processing_videomae.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -177,14 +177,22 @@ def test_call_numpy_4_channels(self):\n \n         # Test not batched input\n         encoded_videos = image_processing(\n-            video_inputs[0], return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n+            video_inputs[0],\n+            return_tensors=\"pt\",\n+            image_mean=(0.0, 0.0, 0.0, 0.0),\n+            image_std=(1.0, 1.0, 1.0, 1.0),\n+            input_data_format=\"channels_first\",\n         ).pixel_values\n         expected_output_video_shape = self.image_processor_tester.expected_output_image_shape([encoded_videos[0]])\n         self.assertEqual(tuple(encoded_videos.shape), (1, *expected_output_video_shape))\n \n         # Test batched\n         encoded_videos = image_processing(\n-            video_inputs, return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n+            video_inputs,\n+            return_tensors=\"pt\",\n+            image_mean=(0.0, 0.0, 0.0, 0.0),\n+            image_std=(1.0, 1.0, 1.0, 1.0),\n+            input_data_format=\"channels_first\",\n         ).pixel_values\n         expected_output_video_shape = self.image_processor_tester.expected_output_image_shape(encoded_videos)\n         self.assertEqual("
        },
        {
            "sha": "b100fb3c30b6d2e62d00ae7fe8a572fb425fa235",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -220,8 +220,8 @@ def test_call_numpy_4_channels(self):\n                 images=image,\n                 trimaps=trimap,\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n                 return_tensors=\"pt\",\n             ).pixel_values\n \n@@ -255,18 +255,24 @@ def test_image_processor_preprocess_arguments(self):\n         # vitmatte require additional trimap input for image_processor\n         # that is why we override original common test\n \n-        for image_processing_class in self.image_processor_list:\n+        for i, image_processing_class in enumerate(self.image_processor_list):\n             image_processor = image_processing_class(**self.image_processor_dict)\n             image = self.image_processor_tester.prepare_image_inputs()[0]\n             trimap = np.random.randint(0, 3, size=image.size[::-1])\n \n-            with warnings.catch_warnings(record=True) as raised_warnings:\n-                warnings.simplefilter(\"always\")\n-                image_processor(image, trimaps=trimap, extra_argument=True)\n-\n-            messages = \" \".join([str(w.message) for w in raised_warnings])\n-            self.assertGreaterEqual(len(raised_warnings), 1)\n-            self.assertIn(\"extra_argument\", messages)\n+            # Type validation will fail for fast processors only (for now)\n+            if image_processing_class.__name__.endswith(\"Fast\"):\n+                with self.assertRaises(TypeError):\n+                    image_processor(image, trimaps=trimap, extra_argument=True)\n+            else:\n+                # Else we just consume extra kwargs and raise a warning\n+                with warnings.catch_warnings(record=True) as raised_warnings:\n+                    warnings.simplefilter(\"always\")\n+                    image_processor(image, trimaps=trimap, extra_argument=True)\n+\n+                messages = \" \".join([str(w.message) for w in raised_warnings])\n+                self.assertGreaterEqual(len(raised_warnings), 1)\n+                self.assertIn(\"extra_argument\", messages)\n \n     @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n     def test_fast_is_faster_than_slow(self):"
        },
        {
            "sha": "c0ede8e22de0df2c3febebea91e2bc98352ccb64",
            "filename": "tests/models/vitpose/test_image_processing_vitpose.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -205,8 +205,8 @@ def test_call_numpy_4_channels(self):\n             boxes=boxes,\n             return_tensors=\"pt\",\n             input_data_format=\"channels_last\",\n-            image_mean=0,\n-            image_std=1,\n+            image_mean=(0.0, 0.0, 0.0, 0.0),\n+            image_std=(1.0, 1.0, 1.0, 1.0),\n         ).pixel_values\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n         self.assertEqual(tuple(encoded_images.shape), (len(boxes[0]), *expected_output_image_shape))\n@@ -218,8 +218,8 @@ def test_call_numpy_4_channels(self):\n             boxes=boxes,\n             return_tensors=\"pt\",\n             input_data_format=\"channels_last\",\n-            image_mean=0,\n-            image_std=1,\n+            image_mean=(0.0, 0.0, 0.0, 0.0),\n+            image_std=(1.0, 1.0, 1.0, 1.0),\n         ).pixel_values\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n         self.assertEqual("
        },
        {
            "sha": "323dbd3cc55fb0900efcae6be7b1a84334c69e2f",
            "filename": "tests/models/vivit/test_image_processing_vivit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fvivit%2Ftest_image_processing_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Fmodels%2Fvivit%2Ftest_image_processing_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvivit%2Ftest_image_processing_vivit.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -191,14 +191,22 @@ def test_call_numpy_4_channels(self):\n \n         # Test not batched input\n         encoded_videos = image_processing(\n-            video_inputs[0], return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n+            video_inputs[0],\n+            return_tensors=\"pt\",\n+            image_mean=(0.0, 0.0, 0.0, 0.0),\n+            image_std=(1.0, 1.0, 1.0, 1.0),\n+            input_data_format=\"channels_first\",\n         ).pixel_values\n         expected_output_video_shape = self.image_processor_tester.expected_output_image_shape([encoded_videos[0]])\n         self.assertEqual(tuple(encoded_videos.shape), (1, *expected_output_video_shape))\n \n         # Test batched\n         encoded_videos = image_processing(\n-            video_inputs, return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n+            video_inputs,\n+            return_tensors=\"pt\",\n+            image_mean=(0.0, 0.0, 0.0, 0.0),\n+            image_std=(1.0, 1.0, 1.0, 1.0),\n+            input_data_format=\"channels_first\",\n         ).pixel_values\n         expected_output_video_shape = self.image_processor_tester.expected_output_image_shape(encoded_videos)\n         self.assertEqual("
        },
        {
            "sha": "15e334c73a140eaaf691ee1d29807885c8b61992",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -519,8 +519,8 @@ def test_call_numpy_4_channels(self):\n                 image_inputs[0],\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=[0.0, 0.0, 0.0, 0.0],\n+                image_std=[1.0, 1.0, 1.0, 1.0],\n             ).pixel_values\n             expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n             self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n@@ -530,8 +530,8 @@ def test_call_numpy_4_channels(self):\n                 image_inputs,\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=[0.0, 0.0, 0.0, 0.0],\n+                image_std=[1.0, 1.0, 1.0, 1.0],\n             ).pixel_values\n             expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n             self.assertEqual("
        },
        {
            "sha": "295ee03a769eae82ce32f0071d7c6f2ce26932cc",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -383,15 +383,15 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n \n     def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         \"\"\"\n-        We use do_rescale=True, rescale_factor=-1 to ensure that image_processor kwargs are preserved in the processor.\n+        We use do_rescale=True, rescale_factor=-1.0 to ensure that image_processor kwargs are preserved in the processor.\n         We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n         Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n         \"\"\"\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"image_processor\"] = self.get_component(\n-            \"image_processor\", do_rescale=True, rescale_factor=-1\n+            \"image_processor\", do_rescale=True, rescale_factor=-1.0\n         )\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n         processor_kwargs = self.prepare_processor_dict()\n@@ -437,7 +437,9 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n         input_str = self.prepare_text_inputs(modalities=\"image\")\n         image_input = self.prepare_image_inputs()\n \n-        inputs = processor(text=input_str, images=image_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n+        inputs = processor(\n+            text=input_str, images=image_input, do_rescale=True, rescale_factor=-1.0, return_tensors=\"pt\"\n+        )\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_unstructured_kwargs(self):\n@@ -455,7 +457,7 @@ def test_unstructured_kwargs(self):\n             images=image_input,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"max_length\",\n             max_length=76,\n         )\n@@ -478,7 +480,7 @@ def test_unstructured_kwargs_batched(self):\n             images=image_input,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"longest\",\n             max_length=76,\n         )\n@@ -503,7 +505,7 @@ def test_doubly_passed_kwargs(self):\n             _ = processor(\n                 text=input_str,\n                 images=image_input,\n-                images_kwargs={\"do_rescale\": True, \"rescale_factor\": -1},\n+                images_kwargs={\"do_rescale\": True, \"rescale_factor\": -1.0},\n                 do_rescale=True,\n                 return_tensors=\"pt\",\n             )\n@@ -534,7 +536,7 @@ def test_structured_kwargs_nested(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1.0},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n         }\n \n@@ -557,7 +559,7 @@ def test_structured_kwargs_nested_from_dict(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1.0},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n         }\n \n@@ -683,15 +685,15 @@ def test_tokenizer_defaults_preserved_by_kwargs_video(self):\n \n     def test_video_processor_defaults_preserved_by_video_kwargs(self):\n         \"\"\"\n-        We use do_rescale=True, rescale_factor=-1 to ensure that image_processor kwargs are preserved in the processor.\n+        We use do_rescale=True, rescale_factor=-1.0 to ensure that image_processor kwargs are preserved in the processor.\n         We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n         Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n         \"\"\"\n         if \"video_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n         processor_components = self.prepare_components()\n         processor_components[\"video_processor\"] = self.get_component(\n-            \"video_processor\", do_rescale=True, rescale_factor=-1\n+            \"video_processor\", do_rescale=True, rescale_factor=-1.0\n         )\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=167, padding=\"max_length\")\n         processor_kwargs = self.prepare_processor_dict()\n@@ -747,7 +749,7 @@ def test_kwargs_overrides_default_video_processor_kwargs(self):\n             videos=video_input,\n             do_sample_frames=False,\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             return_tensors=\"pt\",\n         )\n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n@@ -768,7 +770,7 @@ def test_unstructured_kwargs_video(self):\n             do_sample_frames=False,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"max_length\",\n             max_length=176,\n         )\n@@ -792,7 +794,7 @@ def test_unstructured_kwargs_batched_video(self):\n             do_sample_frames=False,\n             return_tensors=\"pt\",\n             do_rescale=True,\n-            rescale_factor=-1,\n+            rescale_factor=-1.0,\n             padding=\"longest\",\n             max_length=176,\n         )\n@@ -818,7 +820,7 @@ def test_doubly_passed_kwargs_video(self):\n                 text=input_str,\n                 videos=video_input,\n                 do_sample_frames=False,\n-                videos_kwargs={\"do_rescale\": True, \"rescale_factor\": -1},\n+                videos_kwargs={\"do_rescale\": True, \"rescale_factor\": -1.0},\n                 do_rescale=True,\n                 return_tensors=\"pt\",\n             )\n@@ -837,7 +839,7 @@ def test_structured_kwargs_nested_video(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1, \"do_sample_frames\": False},\n+            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1.0, \"do_sample_frames\": False},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 176},\n         }\n \n@@ -860,7 +862,7 @@ def test_structured_kwargs_nested_from_dict_video(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1, \"do_sample_frames\": False},\n+            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1.0, \"do_sample_frames\": False},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 176},\n         }\n "
        },
        {
            "sha": "67a31cf8d20e3ee4eb49e555cff4be1b1a502925",
            "filename": "tests/test_video_processing_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Ftest_video_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89a4115a6b3697a3a2018d8d4f8c439e4196691e/tests%2Ftest_video_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_video_processing_common.py?ref=89a4115a6b3697a3a2018d8d4f8c439e4196691e",
            "patch": "@@ -398,8 +398,8 @@ def test_call_numpy_4_channels(self):\n                 video_inputs[0],\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=0.0,\n+                image_std=1.0,\n             )[self.input_name]\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n             if video_processor.do_convert_rgb:\n@@ -412,8 +412,8 @@ def test_call_numpy_4_channels(self):\n                 video_inputs,\n                 return_tensors=\"pt\",\n                 input_data_format=\"channels_last\",\n-                image_mean=0,\n-                image_std=1,\n+                image_mean=0.0,\n+                image_std=1.0,\n             )[self.input_name]\n             expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n             if video_processor.do_convert_rgb:"
        }
    ],
    "stats": {
        "total": 1105,
        "additions": 648,
        "deletions": 457
    }
}