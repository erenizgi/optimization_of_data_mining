{
    "author": "PhilipMay",
    "message": "Fix typing in `load_balancing_loss_func` function of `modeling_mixtral.py`. (#33641)\n\n* fix return type\r\n\r\n* update to union\r\n\r\n* fix gate_logits typing\r\n\r\n* fix num_experts type\r\n\r\n* fix typing\r\n\r\n* run fix-copies\r\n\r\n* add doc for top_k\r\n\r\n* run fix-copies\r\n\r\n* empty commit to trigger CI",
    "sha": "2e24ee4dfa39cc0bc264b89edbccc373c8337086",
    "files": [
        {
            "sha": "693ffc004ff4046fc32655ddb15fc1cce88c8f81",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=2e24ee4dfa39cc0bc264b89edbccc373c8337086",
            "patch": "@@ -105,8 +105,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n # Copied from transformers.models.jetmoe.modeling_jetmoe.load_balancing_loss_func\n def load_balancing_loss_func(\n-    gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2, attention_mask: Optional[torch.Tensor] = None\n-) -> float:\n+    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -115,14 +118,17 @@ def load_balancing_loss_func(\n     experts is too unbalanced.\n \n     Args:\n-        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):\n+        gate_logits:\n             Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n             shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n         attention_mask (`torch.Tensor`, *optional*):\n             The attention_mask used in forward function\n             shape [batch_size X sequence_length] if not None.\n-        num_experts (`int`, *optional*):\n-            Number of experts\n \n     Returns:\n         The auxiliary loss."
        },
        {
            "sha": "07f84b362eee7ae245c9b7662376e4041cbed67f",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=2e24ee4dfa39cc0bc264b89edbccc373c8337086",
            "patch": "@@ -83,11 +83,11 @@\n \n # Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func with gate->router\n def load_balancing_loss_func(\n-    router_logits: torch.Tensor,\n-    num_experts: torch.Tensor = None,\n+    router_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n     top_k=2,\n     attention_mask: Optional[torch.Tensor] = None,\n-) -> float:\n+) -> Union[torch.Tensor, int]:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -96,14 +96,17 @@ def load_balancing_loss_func(\n     experts is too unbalanced.\n \n     Args:\n-        router_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):\n+        router_logits:\n             Logits from the `router`, should be a tuple of model.config.num_hidden_layers tensors of\n             shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n         attention_mask (`torch.Tensor`, *optional*):\n             The attention_mask used in forward function\n             shape [batch_size X sequence_length] if not None.\n-        num_experts (`int`, *optional*):\n-            Number of experts\n \n     Returns:\n         The auxiliary loss."
        },
        {
            "sha": "36ec5a88435040728abbd9b7cb6bc68d09e6566f",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=2e24ee4dfa39cc0bc264b89edbccc373c8337086",
            "patch": "@@ -110,8 +110,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n # Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func\n def load_balancing_loss_func(\n-    gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2, attention_mask: Optional[torch.Tensor] = None\n-) -> float:\n+    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -120,14 +123,17 @@ def load_balancing_loss_func(\n     experts is too unbalanced.\n \n     Args:\n-        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):\n+        gate_logits:\n             Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n             shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n         attention_mask (`torch.Tensor`, *optional*):\n             The attention_mask used in forward function\n             shape [batch_size X sequence_length] if not None.\n-        num_experts (`int`, *optional*):\n-            Number of experts\n \n     Returns:\n         The auxiliary loss."
        },
        {
            "sha": "03ed1d821a03b922d2f94bd1162b2c912673eae4",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=2e24ee4dfa39cc0bc264b89edbccc373c8337086",
            "patch": "@@ -124,8 +124,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n \n def load_balancing_loss_func(\n-    gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2, attention_mask: Optional[torch.Tensor] = None\n-) -> float:\n+    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -134,14 +137,17 @@ def load_balancing_loss_func(\n     experts is too unbalanced.\n \n     Args:\n-        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):\n+        gate_logits:\n             Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n             shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n         attention_mask (`torch.Tensor`, *optional*):\n             The attention_mask used in forward function\n             shape [batch_size X sequence_length] if not None.\n-        num_experts (`int`, *optional*):\n-            Number of experts\n \n     Returns:\n         The auxiliary loss."
        },
        {
            "sha": "993893c34c28f0c705a95619d0ad787bef30ab20",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=2e24ee4dfa39cc0bc264b89edbccc373c8337086",
            "patch": "@@ -107,8 +107,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n # Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func\n def load_balancing_loss_func(\n-    gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2, attention_mask: Optional[torch.Tensor] = None\n-) -> float:\n+    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -117,14 +120,17 @@ def load_balancing_loss_func(\n     experts is too unbalanced.\n \n     Args:\n-        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):\n+        gate_logits:\n             Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n             shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n         attention_mask (`torch.Tensor`, *optional*):\n             The attention_mask used in forward function\n             shape [batch_size X sequence_length] if not None.\n-        num_experts (`int`, *optional*):\n-            Number of experts\n \n     Returns:\n         The auxiliary loss."
        },
        {
            "sha": "08025e53fd134ca44e00bce357ce522b8bff0cbb",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=2e24ee4dfa39cc0bc264b89edbccc373c8337086",
            "patch": "@@ -117,8 +117,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n # Copied from transformers.models.mixtral.modeling_mixtral.load_balancing_loss_func\n def load_balancing_loss_func(\n-    gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2, attention_mask: Optional[torch.Tensor] = None\n-) -> float:\n+    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -127,14 +130,17 @@ def load_balancing_loss_func(\n     experts is too unbalanced.\n \n     Args:\n-        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):\n+        gate_logits:\n             Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n             shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n         attention_mask (`torch.Tensor`, *optional*):\n             The attention_mask used in forward function\n             shape [batch_size X sequence_length] if not None.\n-        num_experts (`int`, *optional*):\n-            Number of experts\n \n     Returns:\n         The auxiliary loss."
        }
    ],
    "stats": {
        "total": 95,
        "additions": 64,
        "deletions": 31
    }
}