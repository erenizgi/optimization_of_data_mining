{
    "author": "zrohyun",
    "message": "[video processors] Support float fps for precise frame sampling (#39134)\n\n* [video processors] Support float fps for precise frame sampling\n\nEnable fractional fps values (e.g., 1.5, 29.97) in video processors\nfor more precise frame sampling control.\n\n- Change fps type from int to float across all video processors\n- Maintain backward compatibility with integer values\n\nExtends: #38105\n\n* [video processors] Refine fps typing to Union[int, float]\n\nChange fps type from Optional[float] to Optional[Union[int, float]]\nfor more explicit type information about supporting both integer\nand floating-point frame rates.\n\n- Update type hints and docstrings across 8 files\n- Maintain backward compatibility\n- Clarify support for both int and float values\n\nExtends: #38105\n\n* Revert \"[video processors] Support float fps for precise frame sampling\"\n\nThis reverts commit 7360d6e661b413ca0239e5ef61f9b1abbeab8e65.",
    "sha": "b0a8e0b8d7eba2af7346331dd1d48c50892867b2",
    "files": [
        {
            "sha": "55f3a7494c40c907e848822d8aa65d84208e7f13",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=b0a8e0b8d7eba2af7346331dd1d48c50892867b2",
            "patch": "@@ -91,7 +91,7 @@ def _preprocess(\n         do_sample_frames: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n-        fps: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n         num_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         device: Optional[\"torch.Tensor\"] = None,"
        },
        {
            "sha": "7817eddddbb116d60ac76dd5f22563c4d5f9746d",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=b0a8e0b8d7eba2af7346331dd1d48c50892867b2",
            "patch": "@@ -76,7 +76,7 @@ def sample_frames(\n         video: \"torch.Tensor\",\n         metadata: Optional[Union[VideoMetadata, dict]] = None,\n         num_frames: Optional[int] = None,\n-        fps: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n         initial_shift: Optional[Union[bool, float, int]] = None,\n     ):\n         \"\"\"\n@@ -91,7 +91,7 @@ def sample_frames(\n                 Metadata of the video containing information about total duration, fps and total number of frames.\n             num_frames (`int`, *optional*):\n                 Maximum number of frames to sample. Defaults to `self.num_frames`.\n-            fps (`int`, *optional*):\n+            fps (`int` or `float`, *optional*):\n                 Target frames to sample per second. Defaults to `self.fps`.\n             initial_shift (`bool`, `float` or `int`, defaults to `self.initial_shift`):\n                 The initial shift to apply when sampling frames. If `True`, the shift is set so that frames are sampled from the middle of the video.\n@@ -143,7 +143,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_sample_frames: Optional[bool] = None,\n-        fps: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n         num_frames: Optional[int] = None,\n         initial_shift: Optional[Union[bool, float, int]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "31bb8287848b65be8f732fc95782864f14be4e74",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=b0a8e0b8d7eba2af7346331dd1d48c50892867b2",
            "patch": "@@ -31,7 +31,7 @@\n \n \n class Qwen2_5_OmniVideosKwargs(VideosKwargs):\n-    fps: Optional[list[int]] = None\n+    fps: Optional[list[Union[int, float]]] = None\n     use_audio_in_video: Optional[bool] = None\n     seconds_per_chunk: Optional[float] = None\n     position_id_per_seconds: Optional[int] = None"
        },
        {
            "sha": "2f34b9df7bf22c5a872c92ed8def74246e0aab81",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=b0a8e0b8d7eba2af7346331dd1d48c50892867b2",
            "patch": "@@ -127,7 +127,7 @@ def sample_frames(\n         max_frames: int,\n         metadata: Optional[Union[VideoMetadata, dict]] = None,\n         num_frames: Optional[int] = None,\n-        fps: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n     ):\n         \"\"\"\n         Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n@@ -147,7 +147,7 @@ def sample_frames(\n                 Metadata of the video containing information about total duration, fps and total number of frames.\n             num_frames (`int`, *optional*):\n                 Maximum number of frames to sample. Defaults to `self.num_frames`.\n-            fps (`int`, *optional*):\n+            fps (`int` or `float`, *optional*):\n                 Target frames to sample per second. Defaults to `self.fps`.\n \n         Returns:\n@@ -208,7 +208,7 @@ def _preprocess(\n         patch_size: Optional[int] = None,\n         temporal_patch_size: Optional[int] = None,\n         merge_size: Optional[int] = None,\n-        fps: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n         num_frames: Optional[int] = None,\n         min_frames: Optional[int] = None,\n         max_frames: Optional[int] = None,"
        },
        {
            "sha": "b66facefa41bbda1eac17a4f2db7fc81a2444787",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=b0a8e0b8d7eba2af7346331dd1d48c50892867b2",
            "patch": "@@ -249,7 +249,7 @@ def sample_frames(\n         video: \"torch.Tensor\",\n         metadata: Union[VideoMetadata, dict],\n         num_frames: Optional[int] = None,\n-        fps: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n         skip_secs: Optional[int] = 1,\n     ):\n         \"\"\"\n@@ -266,7 +266,7 @@ def sample_frames(\n                 Metadata of the video containing information about total duration, fps and total number of frames.\n             num_frames (`int`, *optional*):\n                 Maximum number of frames to sample. Defaults to `self.num_frames`.\n-            fps (`int`, *optional*):\n+            fps (`int` or `float`, *optional*):\n                 Target frames to sample per second. Defaults to `self.fps`.\n             skip_secs (`float`, *optional*, defaults to `1`):\n                 Number of seconds to skip from the start and end if the video is long enough.\n@@ -328,7 +328,7 @@ def _preprocess(\n         do_sample_frames: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n-        fps: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n         num_frames: Optional[int] = None,\n         skip_secs: Optional[int] = 0,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "838231a420cc11894e1ccbbd72ba25ecf7432fbb",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=b0a8e0b8d7eba2af7346331dd1d48c50892867b2",
            "patch": "@@ -251,7 +251,7 @@ class VideosKwargs(TypedDict, total=False):\n             Metadata of the video containing information about total duration, fps and total number of frames.\n         num_frames (`int`, *optional*):\n             Maximum number of frames to sample when `do_sample_frames=True`.\n-        fps (`int`, *optional*):\n+        fps (`int` or `float`, *optional*):\n             Target frames to sample per second when `do_sample_frames=True`.\n         crop_size (`dict[str, int]`, *optional*):\n             Desired output size when applying center-cropping.\n@@ -280,7 +280,7 @@ class VideosKwargs(TypedDict, total=False):\n     device: Optional[str]\n     do_sample_frames: Optional[bool]\n     video_metadata: Optional[Union[VideoMetadata, dict]]\n-    fps: Optional[int]\n+    fps: Optional[Union[int, float]]\n     num_frames: Optional[int]\n \n "
        },
        {
            "sha": "6caa2bfb65d7070f57c3cc31cfd86f99d4bb387e",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=b0a8e0b8d7eba2af7346331dd1d48c50892867b2",
            "patch": "@@ -125,7 +125,7 @@\n             Whether to sample frames from the video before processing or to process the whole video.\n         num_frames (`int`, *optional*, defaults to `self.num_frames`):\n             Maximum number of frames to sample when `do_sample_frames=True`.\n-        fps (`int`, *optional*, defaults to `self.fps`):\n+        fps (`int` or `float`, *optional*, defaults to `self.fps`):\n             Target frames to sample per second when `do_sample_frames=True`.\n         return_tensors (`str` or `TensorType`, *optional*):\n             Returns stacked tensors if set to `pt, otherwise returns a list of tensors.\n@@ -237,7 +237,7 @@ def sample_frames(\n         video: \"torch.Tensor\",\n         metadata: Optional[Union[VideoMetadata, dict]] = None,\n         num_frames: Optional[int] = None,\n-        fps: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n     ):\n         \"\"\"\n         Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n@@ -251,7 +251,7 @@ def sample_frames(\n                 Metadata of the video containing information about total duration, fps and total number of frames.\n             num_frames (`int`, *optional*):\n                 Maximum number of frames to sample. Defaults to `self.num_frames`.\n-            fps (`int`, *optional*):\n+            fps (`int` or `float`, *optional*):\n                 Target frames to sample per second. Defaults to `self.fps`.\n \n         Returns:\n@@ -369,7 +369,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_sample_frames: Optional[bool] = None,\n-        fps: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n         num_frames: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         device: Optional[\"torch.Tensor\"] = None,"
        },
        {
            "sha": "8282a25d080cd04125a4f254ec8722a22c5ff34a",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0a8e0b8d7eba2af7346331dd1d48c50892867b2/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=b0a8e0b8d7eba2af7346331dd1d48c50892867b2",
            "patch": "@@ -227,7 +227,7 @@ def default_sample_indices_fn(metadata: VideoMetadata, num_frames=None, fps=None\n             `VideoMetadata` object containing metadata about the video, such as \"total_num_frames\" or \"fps\".\n         num_frames (`int`, *optional*):\n             Number of frames to sample uniformly.\n-        fps (`int`, *optional*):\n+        fps (`int` or `float`, *optional*):\n             Desired frames per second. Takes priority over num_frames if both are provided.\n \n     Returns:\n@@ -514,7 +514,7 @@ def sample_indices_fn(metadata, **kwargs):\n def load_video(\n     video: Union[str, \"VideoInput\"],\n     num_frames: Optional[int] = None,\n-    fps: Optional[int] = None,\n+    fps: Optional[Union[int, float]] = None,\n     backend: str = \"pyav\",\n     sample_indices_fn: Optional[Callable] = None,\n     **kwargs,\n@@ -527,7 +527,7 @@ def load_video(\n             The video to convert to the numpy array format. Can be a link to video or local path.\n         num_frames (`int`, *optional*):\n             Number of frames to sample uniformly. If not passed, the whole video is loaded.\n-        fps (`int`, *optional*):\n+        fps (`int` or `float`, *optional*):\n             Number of frames to sample per second. Should be passed only when `num_frames=None`.\n             If not specified and `num_frames==None`, all frames are sampled.\n         backend (`str`, *optional*, defaults to `\"pyav\"`):"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 20,
        "deletions": 20
    }
}