{
    "author": "cyyever",
    "message": "Fix typos (#40175)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "c167faa0813ef261623d6cfcaee21ddaff607ec5",
    "files": [
        {
            "sha": "5fc4ed061ce12422c1d1acb0cd6389934c7a59f5",
            "filename": "docs/source/en/auto_docstring.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fauto_docstring.md?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -290,7 +290,7 @@ The `@auto_docstring` decorator automatically generates docstrings by:\n \n 7. Adding return values to the docstring. For methods like `forward`, the decorator automatically generates the `Returns` field in the docstring based on the method's return type annotation.\n \n-    For example, if a method returns a [`~transformers.utils.ModelOutput`] subclass, `@auto_docstring` extracts the field descriptions from the class' docstring to create a comprehensive return value description. You can also manually specifiy a custom `Returns` field in a functions docstring.\n+    For example, if a method returns a [`~transformers.utils.ModelOutput`] subclass, `@auto_docstring` extracts the field descriptions from the class' docstring to create a comprehensive return value description. You can also manually specify a custom `Returns` field in a functions docstring.\n \n 8. Unrolling kwargs typed with the unpack operator. For specific methods (defined in `UNROLL_KWARGS_METHODS`) or classes (defined in `UNROLL_KWARGS_CLASSES`), the decorator processes `**kwargs` parameters that are typed with `Unpack[KwargsTypedDict]`. It extracts the documentations from the `TypedDict` and adds each parameter to the function's docstring.\n "
        },
        {
            "sha": "fac353eb298fa6c5eb2ac6f9447db69ce849adf9",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -161,7 +161,7 @@ It can be confusing to manage multiple templates though, so we recommend using a\n \n It is important to set a chat template format that matches the template format a model was pretrained on, otherwise performance may suffer. Even if you’re training the model further, performance is best if the chat tokens are kept constant.\n \n-But if you’re training a model from scratch or finetuning a model for chat, you have more options to select a template. For example, [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md) is a popular format that is flexbile enough to handle many use cases. It even includes support for [generation prompts](#add_generation_prompt), but it doesn’t add beginning-of-string (`BOS`) or end-of-string (`EOS`) tokens. If your model expects `BOS` and `EOS` tokens, set `add_special_tokens=True` and make sure to add them to your template.\n+But if you’re training a model from scratch or finetuning a model for chat, you have more options to select a template. For example, [ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md) is a popular format that is flexible enough to handle many use cases. It even includes support for [generation prompts](#add_generation_prompt), but it doesn’t add beginning-of-string (`BOS`) or end-of-string (`EOS`) tokens. If your model expects `BOS` and `EOS` tokens, set `add_special_tokens=True` and make sure to add them to your template.\n \n ```py\n {%- for message in messages %}"
        },
        {
            "sha": "159e06918e15634262f22b7ffd53ad411afa54d7",
            "filename": "docs/source/en/model_doc/qwen2_5_omni.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -308,7 +308,7 @@ text_ids = model.generate(**inputs, return_audio=False)\n ```\n \n #### Change voice type of output audio\n-Qwen2.5-Omni supports the ability to change the voice of the output audio. Users can use the `spk` parameter of `generate` function to specify the voice type. The `\"Qwen/Qwen2.5-Omni-7B\"` checkpoint support two voice types: `Chelsie` and `Ethan`, while `Chelsie` is a female voice and `Ethan` is a male voice. By defalut, if `spk` is not specified, the default voice type is `Chelsie`.\n+Qwen2.5-Omni supports the ability to change the voice of the output audio. Users can use the `spk` parameter of `generate` function to specify the voice type. The `\"Qwen/Qwen2.5-Omni-7B\"` checkpoint support two voice types: `Chelsie` and `Ethan`, while `Chelsie` is a female voice and `Ethan` is a male voice. By default, if `spk` is not specified, the default voice type is `Chelsie`.\n \n ```python\n text_ids, audio = model.generate(**inputs, spk=\"Chelsie\")"
        },
        {
            "sha": "c88a7ab0dcf35aba9b27a4a9b82cf8daefe5196b",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -101,7 +101,7 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Update the key and value caches in-place, and return the necesary kes and value states.\n+        Update the key and value caches in-place, and return the necessary kes and value states.\n \n         Args:\n             key_states (`torch.Tensor`): The new key states to cache.\n@@ -184,7 +184,7 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Update the key and value caches in-place, and return the necesary kes and value states.\n+        Update the key and value caches in-place, and return the necessary kes and value states.\n \n         Args:\n             key_states (`torch.Tensor`): The new key states to cache.\n@@ -306,7 +306,7 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Update the key and value caches in-place, and return the necesary kes and value states.\n+        Update the key and value caches in-place, and return the necessary kes and value states.\n \n         Args:\n             key_states (`torch.Tensor`): The new key states to cache.\n@@ -382,7 +382,7 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Update the key and value caches in-place, and return the necesary kes and value states.\n+        Update the key and value caches in-place, and return the necessary kes and value states.\n \n         Args:\n             key_states (`torch.Tensor`): The new key states to cache.\n@@ -461,7 +461,7 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Update the key and value caches in-place, and return the necesary kes and value states.\n+        Update the key and value caches in-place, and return the necessary kes and value states.\n \n         Args:\n             key_states (`torch.Tensor`): The new key states to cache.\n@@ -570,7 +570,7 @@ def update(\n         cache_kwargs: Optional[dict[str, Any]] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n-        Update the key and value caches in-place, and return the necesary kes and value states.\n+        Update the key and value caches in-place, and return the necessary kes and value states.\n \n         Args:\n             key_states (`torch.Tensor`): The new key states to cache.\n@@ -783,7 +783,7 @@ def prefetch(self, layer_idx: int, only_non_sliding: bool = True):\n             # Try to find next non-sliding, starting at `layer_idx`\n             try:\n                 layer_idx = layer_idx + self.is_sliding[layer_idx:].index(False)\n-            # In this case, we need to circle back to the begining\n+            # In this case, we need to circle back to the beginning\n             except ValueError:\n                 layer_idx = self.is_sliding.index(False)\n         else:"
        },
        {
            "sha": "055688f27be84c6e7b20fe88bfff2b0d8b6a0b99",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -370,7 +370,7 @@ def output_attentions(self):\n \n     @output_attentions.setter\n     def output_attentions(self, value: bool):\n-        # If we set `output_attentions` explictily before the attn implementation, dispatch eager\n+        # If we set `output_attentions` explicitly before the attn implementation, dispatch eager\n         if value and self._attn_implementation is None:\n             self._attn_implementation = \"eager\"\n         if value and self._attn_implementation != \"eager\":"
        },
        {
            "sha": "9998ad60fd5b2721b230a57f1b8f17b5751d3ab1",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -675,7 +675,7 @@ def _preprocess_image_like_inputs(\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs.\n-        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        To be overridden by subclasses when image-like inputs other than images should be processed.\n         It can be used for segmentation maps, depth maps, etc.\n         \"\"\"\n         # Prepare input images"
        },
        {
            "sha": "e5a4a1aa516fe80286496ca79aecb2577b7bbfc8",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -912,7 +912,7 @@ def group_images_by_shape(\n             - A dictionary with shape as key and list of images with that shape as value\n             - A dictionary mapping original indices to (shape, index) tuples\n     \"\"\"\n-    # If disable grouping is not explicitely provided, we favor disabling it if the images are on CPU, and enabling it otherwise.\n+    # If disable grouping is not explicitly provided, we favor disabling it if the images are on CPU, and enabling it otherwise.\n     if disable_grouping is None:\n         device = images[0][0].device if is_nested else images[0].device\n         disable_grouping = device == \"cpu\"\n@@ -949,7 +949,7 @@ def reorder_images(\n         grouped_images_index (dict[Union[int, tuple[int, int]], tuple[tuple[int, int], int]]):\n             Dictionary mapping original indices to (shape, index) tuples.\n         is_nested (bool, *optional*, defaults to False):\n-            Whether the images are nested. Cannot be infered from the input, as some processing functions outputs nested images.\n+            Whether the images are nested. Cannot be inferred from the input, as some processing functions outputs nested images.\n             even with non nested images,e.g functions splitting images into patches. We thus can't deduce is_nested from the input.\n \n "
        },
        {
            "sha": "0c9a28b6c533e378d9a5d10f01fb51177d2b5c24",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -89,7 +89,7 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n \n def chunked_overlay(chunk_size: int) -> Callable:\n     \"\"\"\n-    This is an overlay depicting a chuned attention pattern. Add it on top of a causal mask for a proper chunked\n+    This is an overlay depicting a chunked attention pattern. Add it on top of a causal mask for a proper chunked\n     attention mask.\n     \"\"\"\n \n@@ -186,7 +186,7 @@ def prepare_padding_mask(\n     \"\"\"\n     local_padding_mask = attention_mask\n     if attention_mask is not None:\n-        # Pad it if necesary\n+        # Pad it if necessary\n         if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n             local_padding_mask = torch.nn.functional.pad(attention_mask, (0, padding_length))\n         # For flex, we should not slice them, only use an offset\n@@ -515,7 +515,7 @@ def flash_attention_mask(\n     **kwargs,\n ):\n     \"\"\"\n-    Create the attention mask necesary to use FA2. Since FA2 is un-padded by definition, here we simply return\n+    Create the attention mask necessary to use FA2. Since FA2 is un-padded by definition, here we simply return\n     `None` if the mask is fully causal, or we return the 2D mask which will then be used to extract the seq_lens.\n     We just slice it in case of sliding window.\n \n@@ -704,7 +704,7 @@ def _preprocess_mask_arguments(\n     if attention_mask is not None and attention_mask.ndim == 2:\n         attention_mask = attention_mask.to(device=cache_position.device, dtype=torch.bool)\n \n-    # If using a cache, it can give all informations about mask sizes based on seen tokens\n+    # If using a cache, it can give all information about mask sizes based on seen tokens\n     if past_key_values is not None:\n         kv_length, kv_offset = past_key_values.get_mask_sizes(cache_position, layer_idx)\n     # Otherwise, the sizes are simply the input sizes"
        },
        {
            "sha": "9f763c83c66d27609f95b5b73c07b38445b36be7",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -408,7 +408,7 @@ def model_addition_debugger_context(\n \n     It tracks all forward calls within a model forward and logs a slice of each input and output on a nested JSON file.\n     If `use_repr=True` (the default), the JSON file will record a `repr()`-ized version of the tensors as a list of\n-    strings. If `use_repr=False`, the full tensors will be stored in spearate SafeTensors files and the JSON file will\n+    strings. If `use_repr=False`, the full tensors will be stored in separate SafeTensors files and the JSON file will\n     provide a relative path to that file.\n \n     To note, this context manager enforces `torch.no_grad()`."
        },
        {
            "sha": "e4e37d7ffab8b43486891c3be11dcd3202fcb63e",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -506,7 +506,7 @@ def _process_flash_attention_kwargs(\n     \"\"\"\n     Returns a set of kwargs that are passed down to the according flash attention function based on\n     requested features and whether it is supported - depends on the version and kernel implementation\n-    which is dynamically configued at `lazy_import_flash_attention`. The (un)supported features can be\n+    which is dynamically configured at `lazy_import_flash_attention`. The (un)supported features can be\n     inspected in `supports_mapping`, see `_lazy_define_process_function` for more details.\n \n     Args:\n@@ -629,7 +629,7 @@ def _flash_attention_forward(\n     # Case 2. Some models pass directly pre-computed `cu_seqlens` so we don't need to infer it from position ids. It is safe to\n     # use `flash_varlen_fn` knowing we already have all necessary the kwargs.\n     #\n-    # NOTE: it is user's responsibility to take care of flattenning `position_ids` if that's needed by the model.\n+    # NOTE: it is user's responsibility to take care of flattening `position_ids` if that's needed by the model.\n     # See #39121 for more information.\n     is_fa_with_position_ids = _is_packed_sequence(position_ids, batch_size=query_states.size(0))\n     is_fa_with_varlen_kwargs = all("
        },
        {
            "sha": "0c2a5f8d10efe15fc1abb63db528ade26c0340fd",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -2523,7 +2523,7 @@ def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n                 ' or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`'\n             )\n \n-        # With the early check, the parameters are not yet initalized correctly\n+        # With the early check, the parameters are not yet initialized correctly\n         if not is_init_check:\n             if getattr(self, \"use_bettertransformer\", False):\n                 raise ValueError(\n@@ -2611,7 +2611,7 @@ def _flash_attn_3_can_dispatch(self, is_init_check: bool = False) -> bool:\n                 f\"Model has attention_dropout={self.config.attention_dropout}, which is not supported by Flash Attention 3.\"\n             )\n \n-        # With the early check, the parameters are not yet initalized correctly\n+        # With the early check, the parameters are not yet initialized correctly\n         if not is_init_check:\n             param_devices = list({param.device for param in self.parameters()})\n             if len(param_devices) == 1 and param_devices[0].type == \"cpu\":\n@@ -2948,13 +2948,13 @@ def disable_input_require_grads(self):\n     def _init_weights(self, module):\n         \"\"\"\n         Initialize the weights. This is quite general on purpose, in the spirit of what we usually do. For more complex\n-        initialization scheme, it should be overriden by the derived `PreTrainedModel` class. In case a model adds an explicit\n-        `nn.Parameter`, this method should also be overriden in order to initialize it correctly.\n+        initialization scheme, it should be overridden by the derived `PreTrainedModel` class. In case a model adds an explicit\n+        `nn.Parameter`, this method should also be overridden in order to initialize it correctly.\n         \"\"\"\n         if hasattr(self.config, \"initializer_range\"):\n             std = self.config.initializer_range\n         else:\n-            # 0.02 is the standard default value accross the library\n+            # 0.02 is the standard default value across the library\n             std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n \n         if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):"
        },
        {
            "sha": "32a351ea2355933de10fe622e9896ca6ba6291df",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -138,21 +138,21 @@ class EfficientLoFTRImageProcessor(BaseImageProcessor):\n \n     Args:\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overriden\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden\n             by `do_resize` in the `preprocess` method.\n         size (`Dict[str, int]` *optional*, defaults to `{\"height\": 480, \"width\": 640}`):\n             Resolution of the output image after `resize` is applied. Only has an effect if `do_resize` is set to\n-            `True`. Can be overriden by `size` in the `preprocess` method.\n+            `True`. Can be overridden by `size` in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n-            Resampling filter to use if resizing the image. Can be overriden by `resample` in the `preprocess` method.\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overriden by `do_rescale` in\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n             the `preprocess` method.\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n             method.\n         do_grayscale (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to grayscale. Can be overriden by `do_grayscale` in the `preprocess` method.\n+            Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]"
        },
        {
            "sha": "85d5c19c38243c2b5cb23c7f64a0f83542bb366b",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -312,7 +312,7 @@ def _preprocess_image_like_inputs(\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs.\n-        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        To be overridden by subclasses when image-like inputs other than images should be processed.\n         It can be used for segmentation maps, depth maps, etc.\n         \"\"\"\n         # Prepare input images"
        },
        {
            "sha": "d2e0424265eb4a7541674b2bcb9fa7758df9ae42",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -313,7 +313,7 @@ def _preprocess_image_like_inputs(\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs.\n-        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        To be overridden by subclasses when image-like inputs other than images should be processed.\n         It can be used for segmentation maps, depth maps, etc.\n         \"\"\"\n         # Prepare input images"
        },
        {
            "sha": "a61745e87e58609d92afc36d12100f23ec108047",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -399,7 +399,7 @@ def _preprocess_image_like_inputs(\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs.\n-        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        To be overridden by subclasses when image-like inputs other than images should be processed.\n         It can be used for segmentation maps, depth maps, etc.\n         \"\"\"\n         # Prepare input images"
        },
        {
            "sha": "33efe1929c061264b93abce2e73074eda63270b5",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -161,7 +161,7 @@ def _preprocess_image_like_inputs(\n     ) -> BatchFeature:\n         \"\"\"\n         Preprocess image-like inputs.\n-        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        To be overridden by subclasses when image-like inputs other than images should be processed.\n         It can be used for segmentation maps, depth maps, etc.\n         \"\"\"\n         # Prepare input images"
        },
        {
            "sha": "93776773c85db33ccae79cc5bd57ceef860efb71",
            "filename": "src/transformers/pipelines/zero_shot_audio_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -49,7 +49,7 @@ class ZeroShotAudioClassificationPipeline(Pipeline):\n     >>> dataset = load_dataset(\"ashraq/esc50\")\n     >>> audio = next(iter(dataset[\"train\"][\"audio\"]))[\"array\"]\n     >>> classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/clap-htsat-unfused\")\n-    >>> classifier(audio, candidate_labels=[\"Sound of a dog\", \"Sound of vaccum cleaner\"])\n+    >>> classifier(audio, candidate_labels=[\"Sound of a dog\", \"Sound of vacuum cleaner\"])\n     [{'score': 0.9996, 'label': 'Sound of a dog'}, {'score': 0.0004, 'label': 'Sound of vaccum cleaner'}]\n     ```\n "
        },
        {
            "sha": "bf232adbe1badc33971cf43eac6c53e9972d16cc",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -223,7 +223,7 @@ class VideosKwargs(TypedDict, total=False):\n \n     Attributes:\n         do_convert_rgb (`bool`):\n-            Whether to convert the video to RGB fromat.\n+            Whether to convert the video to RGB format.\n         do_resize (`bool`):\n             Whether to resize the video.\n         size (`dict[str, int]`, *optional*):"
        },
        {
            "sha": "08627d62c123859aab53accfbb3070377ef0fd12",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -104,7 +104,7 @@ def add(self, word: str):\n \n     def split(self, text: str) -> list[str]:\n         \"\"\"\n-        Will look for the words added to the trie within `text`. Output is the original string splitted along the\n+        Will look for the words added to the trie within `text`. Output is the original string split along the\n         boundaries of the words found.\n \n         This trie will match the longest possible word first !"
        },
        {
            "sha": "48077d6ffe051e0173d7ead540f68238014bb80c",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -927,7 +927,7 @@ def _align_special_tokens(self):\n         # token.\n         tokenizer_has_new_eos = tokenizer.eos_token_id != self.model.config.eos_token_id\n         if model_has_generation_config:\n-            # `generation_config.eos_token_id` is None: direct comparision\n+            # `generation_config.eos_token_id` is None: direct comparison\n             if self.model.generation_config.eos_token_id is None:\n                 tokenizer_has_new_eos |= tokenizer.eos_token_id != self.model.generation_config.eos_token_id\n             else:\n@@ -3896,7 +3896,7 @@ def training_step(\n \n         kwargs = {}\n \n-        # For LOMO optimizers you need to explicitly use the learnign rate\n+        # For LOMO optimizers you need to explicitly use the learning rate\n         if self.args.optim in [OptimizerNames.LOMO, OptimizerNames.ADALOMO]:\n             kwargs[\"learning_rate\"] = self._get_learning_rate()\n \n@@ -3947,7 +3947,7 @@ def compute_loss(\n             The loss of the model along with its output if return_outputs was set to True\n \n         Subclass and override for custom behavior. If you are not using `num_items_in_batch` when computing your loss,\n-        make sure to overwrite `self.model_accepts_loss_kwargs` to `False`. Otherwise, the loss calculationg might be slightly inacurate when performing gradient accumulation.\n+        make sure to overwrite `self.model_accepts_loss_kwargs` to `False`. Otherwise, the loss calculating might be slightly inaccurate when performing gradient accumulation.\n         \"\"\"\n         if (self.label_smoother is not None or self.compute_loss_func is not None) and \"labels\" in inputs:\n             labels = inputs.pop(\"labels\")"
        },
        {
            "sha": "b73dd6979b47a32ce1491ea8cd54a11b9a1aaa09",
            "filename": "src/transformers/utils/backbone_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Futils%2Fbackbone_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Futils%2Fbackbone_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fbackbone_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -323,7 +323,7 @@ def load_backbone(config):\n     if backbone_config is not None and backbone_checkpoint is not None and use_pretrained_backbone is not None:\n         raise ValueError(\"Cannot specify both config.backbone_config and config.backbone\")\n \n-    # If any of thhe following are set, then the config passed in is from a model which contains a backbone.\n+    # If any of the following are set, then the config passed in is from a model which contains a backbone.\n     if backbone_config is None and use_timm_backbone is None and backbone_checkpoint is None:\n         return AutoBackbone.from_config(config=config, **backbone_kwargs)\n "
        },
        {
            "sha": "44b0ef061e7260442c2de0dd612e8933e8ee36c7",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -862,7 +862,7 @@ class TransformersKwargs(TypedDict, total=False):\n             Number of items in the batch. It is recommended to pass it when\n             you are doing gradient accumulation.\n         output_hidden_states (`Optional[bool]`, *optional*):\n-            Most of the models support outputing all hidden states computed during the forward pass.\n+            Most of the models support outputting all hidden states computed during the forward pass.\n         output_attentions (`Optional[bool]`, *optional*):\n             Turn this on to return the intermediary attention scores.\n         output_router_logits (`Optional[bool]`, *optional*):"
        },
        {
            "sha": "2325b15a7f12b7487d44964a87668e61942a203c",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -2769,7 +2769,7 @@ def propagate_frozenset(unordered_import_structure):\n \n                     else:\n                         # If k is not a frozenset, it means that the dictionary is not \"level\": some keys (top-level)\n-                        # are frozensets, whereas some are not -> frozenset keys are at an unkown depth-level of the\n+                        # are frozensets, whereas some are not -> frozenset keys are at an unknown depth-level of the\n                         # dictionary.\n                         #\n                         # We recursively propagate the frozenset for this specific dictionary so that the frozensets"
        },
        {
            "sha": "9dd07d236dc96cae1cb15954f25387460d89d7cb",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -1631,7 +1631,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n         modules_to_not_convert (`list`, *optional*, default to `None`):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have\n             some modules left in their original precision.\n-        inlcude_embedding (`bool`, default to `False`):\n+        include_input_output_embeddings (`bool`, default to `False`):\n             Whether to include embedding in quantization or not, input embedding will be removed from\n             the module_not_to_convert list as well if this flag is set.\n         untie_embedding_weights (`bool`, default to `False`):"
        },
        {
            "sha": "51df926ed658a6cc7972abf60cc818ee78f9e7d7",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c167faa0813ef261623d6cfcaee21ddaff607ec5/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=c167faa0813ef261623d6cfcaee21ddaff607ec5",
            "patch": "@@ -652,7 +652,7 @@ def get_video_processor_dict(\n             resolved_video_processor_file = download_url(pretrained_model_name_or_path)\n         else:\n             try:\n-                # Try to load with a new config name first and if not successfull try with\n+                # Try to load with a new config name first and if not successful try with\n                 # the old file name. In case we can load with old name only, raise a deprecation warning\n                 # Deprecated until v5.0\n                 video_processor_file = VIDEO_PROCESSOR_NAME"
        }
    ],
    "stats": {
        "total": 94,
        "additions": 47,
        "deletions": 47
    }
}