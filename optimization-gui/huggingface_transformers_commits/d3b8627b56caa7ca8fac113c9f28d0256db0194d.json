{
    "author": "zucchini-nlp",
    "message": "[VLMs] split out \"get placeholder mask\" to helper (#39777)\n\n* batch upidate all models\n\n* update\n\n* forgot about llava onevision\n\n* update\n\n* fix tests\n\n* delete file\n\n* typo\n\n* fix emu3 once and forever\n\n* update cohere2 vision as well",
    "sha": "d3b8627b56caa7ca8fac113c9f28d0256db0194d",
    "files": [
        {
            "sha": "deda70ec558df0e42b13a5559bbe096151fab106",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 27,
            "deletions": 17,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -978,6 +978,30 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1007,29 +1031,15 @@ def forward(\n \n         # 2. Merge text and images\n         if pixel_values is not None and inputs_embeds.shape[1] != 1:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values,\n                 pixel_mask=pixel_mask,\n                 vision_feature_layer=self.config.vision_feature_layer,\n             )\n-            n_images, n_features_per_image = image_features.shape[0], image_features.shape[1]\n-            n_image_features = n_images * n_features_per_image\n-            if n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self._get_image_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "d126c601a52aded00ed8aecc5e669cd231f58aef",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 17,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1431,29 +1431,15 @@ def forward(\n \n         # 2. Merge text and images\n         if pixel_values is not None and inputs_embeds.shape[1] != 1:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values,\n                 pixel_mask=pixel_mask,\n                 vision_feature_layer=self.config.vision_feature_layer,\n             )\n-            n_images, n_features_per_image = image_features.shape[0], image_features.shape[1]\n-            n_image_features = n_images * n_features_per_image\n-            if n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self._get_image_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "e899c1ebef03819927e2f54dc7811a74d1fc7b82",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 28,
            "deletions": 18,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -32,7 +32,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_aya_vision import AyaVisionConfig\n@@ -242,6 +242,30 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -279,24 +303,10 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "4d18b5806c1a6f51a5c30b669a0703d1c3892af6",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -32,7 +32,7 @@\n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torchdynamo_compiling, logging\n+from ...utils import auto_docstring, logging\n from ...utils.generic import check_model_inputs\n from .configuration_aya_vision import AyaVisionConfig\n \n@@ -200,24 +200,10 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "169d35081f9b994461bba3f63e70f8773b0d1c6d",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 32,
            "deletions": 18,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1455,6 +1455,21 @@ def get_qformer_features(\n \n         return query_outputs\n \n+    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        return special_image_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1545,16 +1560,8 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        if input_ids is None:\n-            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-            )\n-            special_image_mask = special_image_mask.all(-1)\n-        else:\n-            special_image_mask = input_ids == self.config.image_token_id\n-\n-        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(language_model_inputs.device)\n         language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        special_image_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n         inputs_embeds = inputs_embeds.to(language_model_inputs.device).masked_scatter(\n             special_image_mask, language_model_inputs\n         )\n@@ -1938,6 +1945,21 @@ def get_image_features(\n             return language_model_inputs, vision_outputs, query_outputs\n         return language_model_inputs\n \n+    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        return special_image_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -2042,16 +2064,8 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        if input_ids is None:\n-            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-            )\n-            special_image_mask = special_image_mask.all(-1)\n-        else:\n-            special_image_mask = input_ids == self.config.image_token_id\n-\n-        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(language_model_inputs.device)\n         language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        special_image_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n         inputs_embeds = inputs_embeds.to(language_model_inputs.device).masked_scatter(\n             special_image_mask, language_model_inputs\n         )"
        },
        {
            "sha": "6a3f8db7494aefb93fcb4f790386a0a3717062ad",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 27,
            "deletions": 17,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -35,7 +35,6 @@\n     TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_chameleon import ChameleonConfig, ChameleonVQVAEConfig\n@@ -888,6 +887,30 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         vision_embeddings = self.get_input_embeddings()(image_tokens)\n         return vision_embeddings\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.vocabulary_mapping.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -924,23 +947,10 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if pixel_values is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.vocabulary_mapping.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n-\n-            n_image_tokens_in_text = special_image_mask.sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n             image_embeds = self.get_image_features(pixel_values)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_embeds.numel():\n-                n_image_features = image_embeds.shape[0] * image_embeds.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens_in_text}, features {n_image_features}\"\n-                )\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_embeds)\n \n         # torch.jit.trace() doesn't support cache objects in the output"
        },
        {
            "sha": "36a07c34107ad2f0d55a97bcf40b82ea92a6cd10",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 27,
            "deletions": 10,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -197,6 +197,30 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -225,16 +249,9 @@ def forward(\n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values, image_num_patches=image_num_patches)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "c141c02c7c5c0bc2453ca2fadfd565c444f7e837",
            "filename": "src/transformers/models/cohere2_vision/modular_cohere2_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -145,16 +145,9 @@ def forward(\n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values, image_num_patches=image_num_patches)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "c113d41e756e6d848f44264b78b47c3767f9a6da",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 27,
            "deletions": 9,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -177,6 +177,30 @@ def get_image_features(self, pixel_values):\n         image_embeds = self.aligner(image_embeds.last_hidden_state)\n         return image_embeds\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -200,18 +224,12 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            if input_ids is None:\n-                image_attention_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_attention_mask = image_attention_mask.all(-1)\n-            else:\n-                image_attention_mask = input_ids == self.config.image_token_id\n-\n-            image_attention_mask = image_attention_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_embeds = self.get_image_features(pixel_values)\n             image_features = image_embeds.reshape(-1, inputs_embeds.shape[-1])\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_attention_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n \n         lm_output = self.language_model("
        },
        {
            "sha": "23ce4c061a320bbef2866435934e6b4bae83b5c8",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -285,6 +285,30 @@ def get_image_features(self, pixel_values, high_res_pixel_values):\n         images_embeds = self.aligner(vision_encodings, high_res_vision_encodings)\n         return images_embeds\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n     def forward("
        },
        {
            "sha": "b5ee232439f18362fe1978e2f481c95ac9abf7f5",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 27,
            "deletions": 10,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1383,6 +1383,30 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n         image = self.vqmodel.decode(image_tokens)\n         return image\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.vocabulary_mapping.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1415,16 +1439,9 @@ def forward(\n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_sizes)\n             image_embeds = torch.cat(image_embeds, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.vocabulary_mapping.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_embeds)\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)"
        },
        {
            "sha": "e8e66fc8534b8a8a121140941bcd6c210af71d80",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 27,
            "deletions": 10,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -965,6 +965,30 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n         image = self.vqmodel.decode(image_tokens)\n         return image\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.vocabulary_mapping.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -997,16 +1021,9 @@ def forward(\n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_sizes)\n             image_embeds = torch.cat(image_embeds, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.vocabulary_mapping.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_embeds)\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)"
        },
        {
            "sha": "409333a8c600a03f16ac1f6c97a702ba8379d46b",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 28,
            "deletions": 12,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -147,6 +147,30 @@ def get_image_features(self, pixel_values: torch.FloatTensor, **kwargs):\n         ]\n         return patch_embeddings\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -200,18 +224,10 @@ def forward(\n \n         if image_patches is not None:\n             patch_embeddings = self.get_image_features(image_patches)\n-            patch_embeddings = torch.cat(patch_embeddings, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            patch_embeddings = patch_embeddings.to(inputs_embeds.device, inputs_embeds.dtype)\n+            patch_embeddings = torch.cat(patch_embeddings, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_tokens(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=patch_embeddings\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, patch_embeddings)\n \n         outputs = self.language_model("
        },
        {
            "sha": "92fa45a66396e0bd7c54a4fed1932f08d5148fba",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 28,
            "deletions": 26,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -38,14 +38,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    ModelOutput,\n-    TransformersKwargs,\n-    auto_docstring,\n-    can_return_tuple,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n@@ -805,6 +798,30 @@ def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         image_features = self.multi_modal_projector(vision_outputs)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -880,25 +897,10 @@ def forward(\n         # Merge text and images\n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of images does not match number of special image tokens in the input text. \"\n-                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n-                    \"tokens from image embeddings.\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         # It may already have been prepared by e.g. `generate`"
        },
        {
            "sha": "0ca7505c56b6970396dd84e737931a671d941553",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 19,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -31,7 +31,7 @@\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -805,25 +805,10 @@ def forward(\n         # Merge text and images\n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of images does not match number of special image tokens in the input text. \"\n-                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n-                    \"tokens from image embeddings.\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         # It may already have been prepared by e.g. `generate`"
        },
        {
            "sha": "3430c45fb0859ce548ce7a02e00f7749fd90faf8",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 49,
            "deletions": 40,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -39,14 +39,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    ModelOutput,\n-    TransformersKwargs,\n-    auto_docstring,\n-    can_return_tuple,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..auto import AutoModel\n from .configuration_gemma3n import Gemma3nAudioConfig, Gemma3nConfig, Gemma3nTextConfig, Gemma3nVisionConfig\n \n@@ -1968,6 +1961,48 @@ def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         vision_outputs *= self.config.vision_config.hidden_size**0.5\n         return self.embed_vision(inputs_embeds=vision_outputs)\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor,\n+        audio_features: torch.FloatTensor,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_audio_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            ).all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_audio_mask = input_ids == self.config.audio_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0] * image_features.shape[1]}\"\n+            )\n+\n+        n_audio_tokens = special_audio_mask.sum()\n+        special_audio_mask = special_audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if audio_features is not None and inputs_embeds[special_audio_mask].numel() != audio_features.numel():\n+            raise ValueError(\n+                f\"Audio features and image tokens do not match: tokens: {n_audio_tokens}, features {audio_features.shape[0] * audio_features.shape[1]}\"\n+            )\n+\n+        return special_image_mask, special_audio_mask\n+\n     @can_return_tuple\n     def forward(\n         self,\n@@ -2054,23 +2089,10 @@ def forward(\n         # Merge text and images\n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-            else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of images does not match number of special image tokens in the input text. \"\n-                    f\"Got {image_tokens_in_text} image tokens in the text and \"\n-                    f\"{image_features.shape[0] * image_features.shape[1]} tokens from image embeddings.\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         # Merge text and audio\n@@ -2091,23 +2113,10 @@ def forward(\n             extra_padding_features = audio_padding_embs.expand(audio_batch_size, extra_padding_tokens, audio_embed_dim)\n \n             audio_features = torch.cat((audio_features, extra_padding_features), dim=1)\n-\n-            if input_ids is None:\n-                special_audio_mask = inputs_embeds == self.embed_audio(\n-                    input_ids=torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-            else:\n-                special_audio_mask = (input_ids == self.config.audio_token_id).unsqueeze(-1)\n-                special_audio_mask = special_audio_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_audio_mask].numel() != audio_features.numel():\n-                audio_tokens_in_text = (special_audio_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of audio input features does not match number of special audio tokens in the input text. \"\n-                    f\"Got {audio_tokens_in_text} audio tokens in the text and \"\n-                    f\"{audio_features.shape[0] * audio_features.shape[1]} tokens from audio embeddings.\"\n-                )\n             audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, special_audio_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, audio_features=audio_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_audio_mask, audio_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "fd402535538b81af39c84d707a300d829399dda3",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 49,
            "deletions": 33,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -31,7 +31,7 @@\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..auto import AutoModel\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n@@ -2259,6 +2259,48 @@ def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         vision_outputs *= self.config.vision_config.hidden_size**0.5\n         return self.embed_vision(inputs_embeds=vision_outputs)\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor,\n+        audio_features: torch.FloatTensor,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_audio_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            ).all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_audio_mask = input_ids == self.config.audio_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0] * image_features.shape[1]}\"\n+            )\n+\n+        n_audio_tokens = special_audio_mask.sum()\n+        special_audio_mask = special_audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if audio_features is not None and inputs_embeds[special_audio_mask].numel() != audio_features.numel():\n+            raise ValueError(\n+                f\"Audio features and image tokens do not match: tokens: {n_audio_tokens}, features {audio_features.shape[0] * audio_features.shape[1]}\"\n+            )\n+\n+        return special_image_mask, special_audio_mask\n+\n     @can_return_tuple\n     def forward(\n         self,\n@@ -2345,23 +2387,10 @@ def forward(\n         # Merge text and images\n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-            else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of images does not match number of special image tokens in the input text. \"\n-                    f\"Got {image_tokens_in_text} image tokens in the text and \"\n-                    f\"{image_features.shape[0] * image_features.shape[1]} tokens from image embeddings.\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         # Merge text and audio\n@@ -2382,23 +2411,10 @@ def forward(\n             extra_padding_features = audio_padding_embs.expand(audio_batch_size, extra_padding_tokens, audio_embed_dim)\n \n             audio_features = torch.cat((audio_features, extra_padding_features), dim=1)\n-\n-            if input_ids is None:\n-                special_audio_mask = inputs_embeds == self.embed_audio(\n-                    input_ids=torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-            else:\n-                special_audio_mask = (input_ids == self.config.audio_token_id).unsqueeze(-1)\n-                special_audio_mask = special_audio_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_audio_mask].numel() != audio_features.numel():\n-                audio_tokens_in_text = (special_audio_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of audio input features does not match number of special audio tokens in the input text. \"\n-                    f\"Got {audio_tokens_in_text} audio tokens in the text and \"\n-                    f\"{audio_features.shape[0] * audio_features.shape[1]} tokens from audio embeddings.\"\n-                )\n             audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, special_audio_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, audio_features=audio_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_audio_mask, audio_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "1eb28d32014129a1bcff0ee9a92de20151e1d886",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 44,
            "deletions": 38,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1182,6 +1182,46 @@ def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Op\n         image_embeds = torch.split(image_embeds, split_sizes)\n         return image_embeds\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -1224,48 +1264,14 @@ def forward(\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-            image_embeds = torch.cat(image_embeds, dim=0)\n-\n-            if input_ids is None:\n-                image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_mask = image_mask.all(-1)\n-            else:\n-                image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = image_mask.sum()\n-            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            n_image_features = image_embeds.shape[0]\n-            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-\n-            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(input_ids, inputs_embeds, image_features=image_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n         if pixel_values_videos is not None:\n             video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-            video_embeds = torch.cat(video_embeds, dim=0)\n-\n-            if input_ids is None:\n-                video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                video_mask = video_mask.all(-1)\n-            else:\n-                video_mask = input_ids == self.config.image_token_id\n-\n-            n_video_tokens = video_mask.sum()\n-            n_video_features = video_embeds.shape[0]\n-            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-\n-            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask = self.get_placeholder_mask(input_ids, inputs_embeds, video_features=video_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:"
        },
        {
            "sha": "df32beb5da973b0803a4d47a5b8a78d11c20e309",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 38,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1228,48 +1228,14 @@ def forward(\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-            image_embeds = torch.cat(image_embeds, dim=0)\n-\n-            if input_ids is None:\n-                image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_mask = image_mask.all(-1)\n-            else:\n-                image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = image_mask.sum()\n-            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            n_image_features = image_embeds.shape[0]\n-            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-\n-            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(input_ids, inputs_embeds, image_features=image_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n         if pixel_values_videos is not None:\n             video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-            video_embeds = torch.cat(video_embeds, dim=0)\n-\n-            if input_ids is None:\n-                video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                video_mask = video_mask.all(-1)\n-            else:\n-                video_mask = input_ids == self.config.image_token_id\n-\n-            n_video_tokens = video_mask.sum()\n-            n_video_features = video_embeds.shape[0]\n-            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-\n-            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask = self.get_placeholder_mask(input_ids, inputs_embeds, video_features=video_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:"
        },
        {
            "sha": "d9f057b7370931921296f2763b966adcdd915a38",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 16,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -573,6 +573,30 @@ def get_image_features(\n         image_outputs = self.vision_tower(pixel_values).last_hidden_state\n         return self.multi_modal_projector(image_outputs)\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -603,24 +627,11 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n             image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n-            n_image_features = image_features.shape[0] * image_features.shape[1]\n-            if n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "f1ec914bf4cdef53e530e476601123017d0f3238",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 16,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -349,24 +349,11 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n             image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n-            n_image_features = image_features.shape[0] * image_features.shape[1]\n-            if n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "49943140fee3baa45602bfd557a892da653679df",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 35,
            "deletions": 27,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1195,6 +1195,21 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n+    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1278,21 +1293,15 @@ def forward(\n         )\n         query_output = query_outputs[0][:, : query_tokens.size(1), :]\n \n-        # step 3: use the language model, conditioned on the query outputs and the prompt\n-        language_model_inputs = self.language_projection(query_output)\n         if inputs_embeds is None:\n             inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n-            special_image_mask = input_ids == self.config.image_token_id\n             if attention_mask is None:\n                 attention_mask = torch.ones_like(input_ids)\n-        else:\n-            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-            )\n-            special_image_mask = special_image_mask.all(-1)\n \n-        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        # step 3: use the language model, conditioned on the query outputs and the prompt\n+        language_model_inputs = self.language_projection(query_output)\n         language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        special_image_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n@@ -1461,6 +1470,21 @@ def get_image_features(\n             return language_model_inputs, vision_outputs, query_outputs\n         return language_model_inputs\n \n+    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1567,16 +1591,8 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        if input_ids is None:\n-            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-            )\n-            special_image_mask = special_image_mask.all(-1)\n-        else:\n-            special_image_mask = input_ids == self.config.image_token_id\n-\n-        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        special_image_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n@@ -1678,16 +1694,8 @@ def generate(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        if input_ids is None:\n-            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-            )\n-            special_image_mask = special_image_mask.all(-1)\n-        else:\n-            special_image_mask = input_ids == self.config.image_token_id\n-\n-        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        special_image_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}"
        },
        {
            "sha": "adf9a9ced6984ac4569d53bddc028120ea1b642d",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 32,
            "deletions": 18,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1191,6 +1191,21 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n+    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1433,6 +1448,21 @@ def get_image_features(\n         \"\"\"\n         pass\n \n+    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.video_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1534,16 +1564,8 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        if input_ids is None:\n-            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-            )\n-            special_image_mask = special_image_mask.all(-1)\n-        else:\n-            special_image_mask = input_ids == self.config.video_token_id\n-\n-        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        special_image_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n@@ -1645,16 +1667,8 @@ def generate(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        if input_ids is None:\n-            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-            )\n-            special_image_mask = special_image_mask.all(-1)\n-        else:\n-            special_image_mask = input_ids == self.config.video_token_id\n-\n-        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        special_image_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}"
        },
        {
            "sha": "9588061e7e428767c4b1fd49dd28ea643693053e",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -372,6 +372,21 @@ def get_image_features(\n     ):\n         pass\n \n+    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.video_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        return special_image_mask\n+\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -471,16 +486,8 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        if input_ids is None:\n-            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-            )\n-            special_image_mask = special_image_mask.all(-1)\n-        else:\n-            special_image_mask = input_ids == self.config.video_token_id\n-\n-        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        special_image_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n@@ -582,16 +589,8 @@ def generate(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        if input_ids is None:\n-            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-            )\n-            special_image_mask = special_image_mask.all(-1)\n-        else:\n-            special_image_mask = input_ids == self.config.video_token_id\n-\n-        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        special_image_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n         inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}"
        },
        {
            "sha": "ab0b30d8397efb7518949c0f5f9b8aa79c213a2c",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 28,
            "deletions": 25,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -36,14 +36,7 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    ModelOutput,\n-    TransformersKwargs,\n-    auto_docstring,\n-    can_return_tuple,\n-    is_torchdynamo_compiling,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, torch_int\n from ..auto import AutoModel\n from .configuration_internvl import InternVLConfig, InternVLVisionConfig\n \n@@ -638,6 +631,30 @@ def get_image_features(\n         vision_features = self.multi_modal_projector(vision_features)\n         return vision_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -683,24 +700,10 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "28ce0b06d926114a5883cc70a09cc26108c0fb64",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -29,7 +29,7 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging, torch_int\n+from ...utils import auto_docstring, can_return_tuple, logging, torch_int\n from ..clip.modeling_clip import CLIPMLP\n from ..janus.modeling_janus import JanusVisionAttention\n from ..llama.modeling_llama import LlamaRMSNorm\n@@ -616,24 +616,10 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "794514867a3cb62a0b9a8c4d3ae4adb1d767456e",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 27,
            "deletions": 9,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1046,6 +1046,30 @@ def get_image_features(self, pixel_values):\n         image_embeds = self.aligner(image_embeds.last_hidden_state)\n         return image_embeds\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1069,18 +1093,12 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            if input_ids is None:\n-                image_attention_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_attention_mask = image_attention_mask.all(-1)\n-            else:\n-                image_attention_mask = input_ids == self.config.image_token_id\n-\n-            image_attention_mask = image_attention_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_embeds = self.get_image_features(pixel_values)\n             image_features = image_embeds.reshape(-1, inputs_embeds.shape[-1])\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_attention_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n \n         lm_output = self.language_model("
        },
        {
            "sha": "ad4ffd196ec58d9aa77cfb5d9aa7634d74eaed43",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 27,
            "deletions": 9,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -906,6 +906,30 @@ def get_image_features(self, pixel_values):\n         image_embeds = self.aligner(image_embeds.last_hidden_state)\n         return image_embeds\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -929,18 +953,12 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            if input_ids is None:\n-                image_attention_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_attention_mask = image_attention_mask.all(-1)\n-            else:\n-                image_attention_mask = input_ids == self.config.image_token_id\n-\n-            image_attention_mask = image_attention_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_embeds = self.get_image_features(pixel_values)\n             image_features = image_embeds.reshape(-1, inputs_embeds.shape[-1])\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_attention_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n \n         lm_output = self.language_model("
        },
        {
            "sha": "9d70d8eca03623a42de20c575f7ff0ff8c6ed095",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 29,
            "deletions": 19,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1201,6 +1201,29 @@ def get_image_features(\n         hidden_state = image_outputs.last_hidden_state\n         return hidden_state\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+        return special_image_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1286,25 +1309,12 @@ def forward(\n             )\n \n             vision_flat = image_features.view(-1, image_features.size(-1))\n-            projected_vision_flat = self.multi_modal_projector(vision_flat)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if n_image_tokens != projected_vision_flat.size(0):\n-                raise ValueError(\n-                    f\"Mismatch: final_mask wants {n_image_tokens} embeddings, \"\n-                    f\"but multi_modal_projector returned {projected_vision_flat.size(0)}\"\n-                )\n-            projected_vision_flat = projected_vision_flat.to(inputs_embeds.device, inputs_embeds.dtype)\n+            projected_vision_flat = self.multi_modal_projector(vision_flat).to(\n+                inputs_embeds.device, inputs_embeds.dtype\n+            )\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=projected_vision_flat\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, projected_vision_flat)\n \n         outputs = self.language_model("
        },
        {
            "sha": "b2fec92999fa9646626a88219f21a0ea6709af8a",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 29,
            "deletions": 20,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..auto import AutoModel\n from .configuration_llava import LlavaConfig\n \n@@ -218,6 +218,30 @@ def get_image_features(\n             image_features = list(image_features)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -265,25 +289,10 @@ def forward(\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n                 image_sizes=image_sizes,\n             )\n-            image_features = torch.cat(image_features, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_features = torch.cat(image_features, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "241acaa3983358c8dfc7095627c8c20ca73e312a",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 28,
            "deletions": 20,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -30,7 +30,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..auto import AutoModel\n from .configuration_llava_next import LlavaNextConfig\n \n@@ -426,6 +426,29 @@ def get_image_features(\n         )\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -479,25 +502,10 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-            image_features = torch.cat(image_features, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_features = torch.cat(image_features, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "721da01a9b540c062d8012b2cb031979be2c9ff6",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 52,
            "deletions": 40,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -35,7 +35,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..auto import AutoModel\n from .configuration_llava_next_video import LlavaNextVideoConfig\n \n@@ -476,6 +476,46 @@ def get_image_features(\n         )\n         return image_features\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -523,35 +563,20 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-        if pixel_values is not None and pixel_values.size(0) > 0:\n+        if pixel_values is not None:\n             image_features = self.get_image_features(\n                 pixel_values,\n                 image_sizes,\n                 vision_feature_layer=self.vision_feature_layer,\n                 vision_feature_select_strategy=self.vision_feature_select_strategy,\n             )\n-            image_features = torch.cat(image_features, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_features = torch.cat(image_features, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n+        if pixel_values_videos is not None:\n             video_features = self.get_video_features(\n                 pixel_values_videos,\n                 vision_feature_layer=self.vision_feature_layer,\n@@ -561,25 +586,12 @@ def forward(\n             video_feature_lens = [feature.size(0) for feature in video_features]\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.video_token_id\n-\n-            n_video_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_features = video_features.shape[0]\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n             video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+\n+            _, special_video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_features\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "15f3ad9141aa15b2085eefc2d6e47edd525a0988",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 52,
            "deletions": 40,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -33,7 +33,7 @@\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n-from ...utils import is_torchdynamo_compiling, logging\n+from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n@@ -397,6 +397,46 @@ def get_video_features(\n         video_features = torch.split(video_features, frames, dim=0)\n         return video_features\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -436,35 +476,20 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-        if pixel_values is not None and pixel_values.size(0) > 0:\n+        if pixel_values is not None:\n             image_features = self.get_image_features(\n                 pixel_values,\n                 image_sizes,\n                 vision_feature_layer=self.vision_feature_layer,\n                 vision_feature_select_strategy=self.vision_feature_select_strategy,\n             )\n-            image_features = torch.cat(image_features, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_features = torch.cat(image_features, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n+        if pixel_values_videos is not None:\n             video_features = self.get_video_features(\n                 pixel_values_videos,\n                 vision_feature_layer=self.vision_feature_layer,\n@@ -474,25 +499,12 @@ def forward(\n             video_feature_lens = [feature.size(0) for feature in video_features]\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.video_token_id\n-\n-            n_video_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_features = video_features.shape[0]\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n             video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+\n+            _, special_video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_features\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "d63cde2b578997d3ea8eff8af40453e311512fd4",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 47,
            "deletions": 37,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -39,7 +39,6 @@\n     TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from ..auto import AutoModel\n@@ -491,6 +490,46 @@ def get_image_features(\n         )\n         return image_features\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -557,24 +596,10 @@ def forward(\n                 batch_num_images=batch_num_images,\n             )\n             image_features = torch.cat(image_features, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         # Video are simply embedded and further pooled to decrease seq len\n@@ -588,25 +613,10 @@ def forward(\n                 self.image_newline[None, None, :].repeat(video_features.shape[0], 1, 1).to(video_features.device)\n             )\n             video_features = torch.cat((video_features, image_newline), dim=1)\n-            video_features = video_features.flatten(0, 1)\n-\n-            if input_ids is None:\n-                special_video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_video_mask = special_video_mask.all(-1)\n-            else:\n-                special_video_mask = input_ids == self.config.video_token_id\n-\n-            n_video_tokens = (special_video_mask).sum()\n-            special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_video_mask].numel() != video_features.numel():\n-                n_video_features = video_features.shape[0]\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            video_features = video_features.flatten(0, 1).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, special_video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "14a8f39915f0baeb527ac2fc6f1ad832902c5b75",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 37,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -50,7 +50,6 @@\n     TensorType,\n     auto_docstring,\n     can_return_tuple,\n-    is_torchdynamo_compiling,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n     logging,\n@@ -541,24 +540,10 @@ def forward(\n                 batch_num_images=batch_num_images,\n             )\n             image_features = torch.cat(image_features, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         # Video are simply embedded and further pooled to decrease seq len\n@@ -572,25 +557,10 @@ def forward(\n                 self.image_newline[None, None, :].repeat(video_features.shape[0], 1, 1).to(video_features.device)\n             )\n             video_features = torch.cat((video_features, image_newline), dim=1)\n-            video_features = video_features.flatten(0, 1)\n-\n-            if input_ids is None:\n-                special_video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_video_mask = special_video_mask.all(-1)\n-            else:\n-                special_video_mask = input_ids == self.config.video_token_id\n-\n-            n_video_tokens = (special_video_mask).sum()\n-            special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_video_mask].numel() != video_features.numel():\n-                n_video_features = video_features.shape[0]\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            video_features = video_features.flatten(0, 1).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, special_video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "2ac42c4e10de87f50fb54346ce5ad6fd8b79da4a",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 29,
            "deletions": 20,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -33,7 +33,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ..auto import AutoModel\n from .configuration_mistral3 import Mistral3Config\n \n@@ -262,6 +262,30 @@ def get_image_features(\n         image_features = torch.split(image_features.squeeze(0), split_sizes)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -302,25 +326,10 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 image_sizes=image_sizes,\n             )\n-            image_features = torch.cat(image_features, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_features = torch.cat(image_features, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "454904c16b365b42a6afaae7434f5c07fa95f7f0",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 20,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -22,7 +22,7 @@\n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n-from ...utils import is_torchdynamo_compiling, logging\n+from ...utils import logging\n from ..llava.modeling_llava import (\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n@@ -200,25 +200,10 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 image_sizes=image_sizes,\n             )\n-            image_features = torch.cat(image_features, dim=0)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_features = torch.cat(image_features, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "29a14e555a788b48d1efe9cc61eeacd0bc0e46cf",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 27,
            "deletions": 19,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -32,7 +32,6 @@\n     TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n-    is_torchdynamo_compiling,\n     logging,\n )\n from ..auto import AutoModel\n@@ -251,6 +250,30 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -332,25 +355,10 @@ def forward(\n         # Merge text and images\n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of images does not match number of special image tokens in the input text. \"\n-                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n-                    \"tokens from image embeddings.\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         causal_mask = self._update_causal_mask("
        },
        {
            "sha": "4210cd73e545e5793544241dbfda2357df124c3d",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 48,
            "deletions": 20,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -207,6 +207,46 @@ def get_image_features(\n         image_features = self.multi_modal_projector(image_outputs)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.size()[:-1].numel()}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.size()[:-1].numel()}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -241,24 +281,20 @@ def forward(\n \n         image_features = None\n         if pixel_values is not None:\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values.to(inputs_embeds),\n+            image_features = self.get_image_features(pixel_values=pixel_values)\n+            image_features = image_features.to(inputs_embeds.device, dtype=inputs_embeds.dtype)\n+            special_image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n             )\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            self.check_mask_feature_size_match(special_image_mask, image_features)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            image_features = image_features.to(inputs_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         video_features = None\n         if pixel_values_videos is not None:\n-            video_features = self.get_image_features(\n-                pixel_values=pixel_values_videos.to(inputs_embeds),\n+            video_features = self.get_image_features(pixel_values=pixel_values_videos)\n+            video_features = video_features.to(inputs_embeds.device, dtype=inputs_embeds.dtype)\n+            _, special_video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_features\n             )\n-            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            self.check_mask_feature_size_match(special_video_mask, video_features)\n-            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            video_features = video_features.to(inputs_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n \n         outputs = self.language_model(\n@@ -283,14 +319,6 @@ def forward(\n             video_hidden_states=(video_features if pixel_values_videos is not None else None),\n         )\n \n-    def check_mask_feature_size_match(self, media_mask, media_features):\n-        media_token_count = media_mask.sum()\n-        media_feature_size = media_features.size()[:-1].numel()\n-        if media_token_count != media_feature_size:\n-            raise ValueError(\n-                f\"The number of tokens in the media mask ({media_token_count}) does not match the number of features in the media features ({media_feature_size}. Features shape: {media_features.shape})\"\n-            )\n-\n \n @auto_docstring\n class PerceptionLMForConditionalGeneration(PerceptionLMPreTrainedModel, GenerationMixin):"
        },
        {
            "sha": "bb1a03a04017b4e7f81bc3474d2e22036260f4e4",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "modified",
            "additions": 45,
            "deletions": 17,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -168,14 +168,46 @@ def get_image_features(\n         image_features = self.multi_modal_projector(image_outputs)\n         return image_features\n \n-    def check_mask_feature_size_match(self, media_mask, media_features):\n-        media_token_count = media_mask.sum()\n-        media_feature_size = media_features.size()[:-1].numel()\n-        if media_token_count != media_feature_size:\n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n             raise ValueError(\n-                f\"The number of tokens in the media mask ({media_token_count}) does not match the number of features in the media features ({media_feature_size}. Features shape: {media_features.shape})\"\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.size()[:-1].numel()}\"\n             )\n \n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.size()[:-1].numel()}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -210,24 +242,20 @@ def forward(\n \n         image_features = None\n         if pixel_values is not None:\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values.to(inputs_embeds),\n+            image_features = self.get_image_features(pixel_values=pixel_values)\n+            image_features = image_features.to(inputs_embeds.device, dtype=inputs_embeds.dtype)\n+            special_image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n             )\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            self.check_mask_feature_size_match(special_image_mask, image_features)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            image_features = image_features.to(inputs_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         video_features = None\n         if pixel_values_videos is not None:\n-            video_features = self.get_image_features(\n-                pixel_values=pixel_values_videos.to(inputs_embeds),\n+            video_features = self.get_image_features(pixel_values=pixel_values_videos)\n+            video_features = video_features.to(inputs_embeds.device, dtype=inputs_embeds.dtype)\n+            _, special_video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_features\n             )\n-            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            self.check_mask_feature_size_match(special_video_mask, video_features)\n-            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            video_features = video_features.to(inputs_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "68e3b1982f611d88e224141c289c68b4b82ba8db",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 55,
            "deletions": 27,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1771,6 +1771,54 @@ def get_audio_features(\n \n         return audio_features\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+            special_audio_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            ).all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+            special_audio_mask = input_ids == self.config.audio_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        special_audio_mask = special_audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        return special_image_mask, special_video_mask, special_audio_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1870,44 +1918,24 @@ def forward(\n                 feature_attention_mask=feature_attention_mask,\n                 audio_feature_lengths=audio_feature_lengths,\n             )\n-            if input_ids is None:\n-                audio_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                audio_mask = audio_mask.all(-1)\n-            else:\n-                audio_mask = input_ids == self.config.audio_token_id\n-\n-            audio_mask = audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, _, audio_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(audio_mask, audio_features)\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-            if input_ids is None:\n-                image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_mask = image_mask.all(-1)\n-            else:\n-                image_mask = input_ids == self.config.image_token_id\n-\n-            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n         if pixel_values_videos is not None:\n             video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-            if input_ids is None:\n-                video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                video_mask = video_mask.all(-1)\n-            else:\n-                video_mask = input_ids == self.config.video_token_id\n-\n-            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if feature_attention_mask is not None:"
        },
        {
            "sha": "01995f56d64006dd28366a6dc5c1c9382b98e840",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 55,
            "deletions": 27,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -2216,6 +2216,54 @@ def get_audio_features(\n \n         return audio_features\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+            special_audio_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            ).all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+            special_audio_mask = input_ids == self.config.audio_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        special_audio_mask = special_audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        return special_image_mask, special_video_mask, special_audio_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -2315,44 +2363,24 @@ def forward(\n                 feature_attention_mask=feature_attention_mask,\n                 audio_feature_lengths=audio_feature_lengths,\n             )\n-            if input_ids is None:\n-                audio_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                audio_mask = audio_mask.all(-1)\n-            else:\n-                audio_mask = input_ids == self.config.audio_token_id\n-\n-            audio_mask = audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, _, audio_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds)\n             inputs_embeds = inputs_embeds.masked_scatter(audio_mask, audio_features)\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-            if input_ids is None:\n-                image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_mask = image_mask.all(-1)\n-            else:\n-                image_mask = input_ids == self.config.image_token_id\n-\n-            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n         if pixel_values_videos is not None:\n             video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-            if input_ids is None:\n-                video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                video_mask = video_mask.all(-1)\n-            else:\n-                video_mask = input_ids == self.config.video_token_id\n-\n-            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if feature_attention_mask is not None:"
        },
        {
            "sha": "0c65b91376954d2e82ef576e787c4ccc7d51ed67",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 48,
            "deletions": 37,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1190,6 +1190,46 @@ def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Op\n         image_embeds = torch.split(image_embeds, split_sizes)\n         return image_embeds\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1233,47 +1273,18 @@ def forward(\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-            image_embeds = torch.cat(image_embeds, dim=0)\n-\n-            if input_ids is None:\n-                image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_mask = image_mask.all(-1)\n-            else:\n-                image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (image_mask).sum()\n-            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            n_image_features = image_embeds.shape[0]\n-            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n         if pixel_values_videos is not None:\n             video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-            video_embeds = torch.cat(video_embeds, dim=0)\n-\n-            if input_ids is None:\n-                video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                video_mask = video_mask.all(-1)\n-            else:\n-                video_mask = input_ids == self.config.video_token_id\n-\n-            n_video_tokens = (video_mask).sum()\n-            n_video_features = video_embeds.shape[0]\n-            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-\n-            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:"
        },
        {
            "sha": "b4e521ff935ef6a3d301c5e5e2d15065db0c636a",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 37,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -575,47 +575,18 @@ def forward(\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-            image_embeds = torch.cat(image_embeds, dim=0)\n-\n-            if input_ids is None:\n-                image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_mask = image_mask.all(-1)\n-            else:\n-                image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (image_mask).sum()\n-            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            n_image_features = image_embeds.shape[0]\n-            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n         if pixel_values_videos is not None:\n             video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-            video_embeds = torch.cat(video_embeds, dim=0)\n-\n-            if input_ids is None:\n-                video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                video_mask = video_mask.all(-1)\n-            else:\n-                video_mask = input_ids == self.config.video_token_id\n-\n-            n_video_tokens = (video_mask).sum()\n-            n_video_features = video_embeds.shape[0]\n-            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-\n-            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:"
        },
        {
            "sha": "c49c10714a09f5553718127cf0a0eeb6958c45e0",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 48,
            "deletions": 38,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -1127,6 +1127,46 @@ def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Op\n         image_embeds = torch.split(image_embeds, split_sizes)\n         return image_embeds\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1167,48 +1207,18 @@ def forward(\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-            image_embeds = torch.cat(image_embeds, dim=0)\n-\n-            if input_ids is None:\n-                image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                image_mask = image_mask.all(-1)\n-            else:\n-                image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = image_mask.sum()\n-            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            n_image_features = image_embeds.shape[0]\n-            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-\n-            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n         if pixel_values_videos is not None:\n             video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-            video_embeds = torch.cat(video_embeds, dim=0)\n-\n-            if input_ids is None:\n-                video_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                n_video_tokens = (video_mask).sum(dim=1).sum(dim=0)[0]\n-            else:\n-                video_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                video_mask = video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_video_tokens = (input_ids == self.config.image_token_id).sum()\n-\n-            n_video_features = video_embeds.shape[0]\n-            if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-\n-            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:"
        },
        {
            "sha": "98786832c49404fb411bc617e22fb4f0417d9315",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 48,
            "deletions": 35,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..auto import AutoModel\n from .configuration_video_llava import VideoLlavaConfig\n \n@@ -282,6 +282,46 @@ def get_video_features(\n \n         return video_features, num_frames\n \n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: torch.FloatTensor = None,\n+        video_features: torch.FloatTensor = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0] * image_features.shape[1]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0] * video_features.shape[1]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -334,48 +374,21 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         if pixel_values_videos is not None:\n             video_features, num_frames = self.get_video_features(\n                 pixel_values_videos=pixel_values_videos, vision_feature_layer=vision_feature_layer\n             )\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.video_token_id\n-\n-            n_video_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_features = video_features.shape[0] * video_features.shape[1]\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n             video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+            _, special_video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_features\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "f18e25c0bc8581713071d780a32d70d76cfa8449",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 28,
            "deletions": 18,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -30,7 +30,7 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils import auto_docstring, can_return_tuple\n from ..auto import AutoModel\n from .configuration_vipllava import VipLlavaConfig\n \n@@ -186,6 +186,30 @@ def get_image_features(\n         image_features = self.multi_modal_projector(image_features)\n         return image_features\n \n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -227,24 +251,10 @@ def forward(\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n             )\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "720065979379d5fb48d611091071cec59c78148e",
            "filename": "src/transformers/models/vipllava/modular_vipllava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -28,7 +28,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...utils import auto_docstring, is_torchdynamo_compiling, logging\n+from ...utils import auto_docstring, logging\n from .configuration_vipllava import VipLlavaConfig\n \n \n@@ -144,24 +144,10 @@ def forward(\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n             )\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = (special_image_mask).sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model("
        },
        {
            "sha": "b891612055d1e3c9fe271b4f8b42267784c68cf3",
            "filename": "tests/models/deepseek_vl/test_modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -89,9 +89,9 @@ def __init__(\n         self.hidden_size = text_config[\"hidden_size\"]\n         self.num_attention_heads = text_config[\"num_attention_heads\"]\n         self.image_size = vision_config[\"image_size\"]\n-        self.num_image_tokens = vision_config[\"image_size\"] // vision_config[\"patch_size\"]\n+        self.num_image_tokens = 16\n         self.pad_token_id = text_config[\"pad_token_id\"]\n-        self.image_token_id = self.vocab_size - 1\n+        self.image_token_id = 0\n \n     def get_config(self):\n         return DeepseekVLConfig(\n@@ -115,6 +115,7 @@ def prepare_config_and_inputs(self):\n             ]\n         )\n         # fill image_tokens\n+        input_ids[input_ids == self.num_image_tokens] = config.text_config.pad_token_id\n         input_ids[:, : self.num_image_tokens] = self.image_token_id\n \n         return config, input_ids, attention_mask, pixel_values"
        },
        {
            "sha": "8975cfe4a0b4d3bf9aca3587bbc03371709fafac",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -198,12 +198,12 @@ def __init__(\n         bos_token_id=1,\n         eos_token_id=2,\n         image_token_id=3,\n-        image_size=30,\n+        image_size=15,\n         codebook_size=20,\n         temporal_downsample_factor=1,\n         base_channels=32,\n-        vq_channel_multiplier=[1, 1],\n-        image_seq_length=100,\n+        vq_channel_multiplier=[1, 2, 1],\n+        image_seq_length=12,\n         vq_img_token_start_id=3,\n     ):\n         self.parent = parent\n@@ -288,6 +288,7 @@ def get_config(self):\n             \"base_channels\": self.base_channels,\n             \"channel_multiplier\": self.vq_channel_multiplier,\n             \"hidden_size\": self.base_channels,\n+            \"attn_resolutions\": [],\n         }\n         return Emu3Config(text_config=text_config, vq_config=vq_config, vocabulary_map=vocab_map)\n \n@@ -358,6 +359,10 @@ def test_initialization(self):\n     def test_generate_with_static_cache(self):\n         pass\n \n+    # @unittest.skip(\"Emu3 can't be smaller than currently if we want to downsample images\")\n+    # def test_model_is_small(self):\n+    #     pass\n+\n \n @require_torch\n class Emu3IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "2fa257bc5c42862c28579f6dbaa7e284d7c60e02",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -89,7 +89,7 @@ def __init__(\n             \"use_labels\": True,\n             \"image_size\": 20,\n             \"patch_size\": 5,\n-            \"num_image_tokens\": 4,\n+            \"num_image_tokens\": 16,\n             \"num_channels\": 3,\n             \"is_training\": True,\n             \"hidden_size\": 32,"
        },
        {
            "sha": "56fe8dcbb2936543412857e96bd9b0696a513a2a",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3b8627b56caa7ca8fac113c9f28d0256db0194d/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3b8627b56caa7ca8fac113c9f28d0256db0194d/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=d3b8627b56caa7ca8fac113c9f28d0256db0194d",
            "patch": "@@ -61,6 +61,7 @@ def __init__(\n         parent,\n         ignore_index=-100,\n         image_token_index=1,\n+        video_token_index=2,\n         projector_hidden_act=\"gelu\",\n         seq_length=7,\n         vision_feature_select_strategy=\"full\",\n@@ -108,6 +109,7 @@ def __init__(\n         self.parent = parent\n         self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n+        self.video_token_index = video_token_index\n         self.projector_hidden_act = projector_hidden_act\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.vision_feature_layer = vision_feature_layer\n@@ -134,6 +136,7 @@ def get_config(self):\n             vision_config=self.vision_config,\n             ignore_index=self.ignore_index,\n             image_token_index=self.image_token_index,\n+            video_token_index=self.video_token_index,\n             projector_hidden_act=self.projector_hidden_act,\n             vision_feature_select_strategy=self.vision_feature_select_strategy,\n             vision_feature_layer=self.vision_feature_layer,"
        }
    ],
    "stats": {
        "total": 2437,
        "additions": 1369,
        "deletions": 1068
    }
}