{
    "author": "cjfghk5697",
    "message": "ğŸŒ [i18n-KO] Translated `blip.md` to Korean (#33515)\n\n* docs: ko:  model_doc/blip\r\n\r\n* feat: nmt darft\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\r\n\r\n* Update docs/source/ko/model_doc/blip.md\r\n\r\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Jiwook Han <33192762+mreraser@users.noreply.github.com>\r\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>",
    "sha": "109b1e7591e703e0634fdcadd61216440b421f4c",
    "files": [
        {
            "sha": "61b5dba5ce39899ff3478fe24172ba01434c1aba",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/109b1e7591e703e0634fdcadd61216440b421f4c/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/109b1e7591e703e0634fdcadd61216440b421f4c/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=109b1e7591e703e0634fdcadd61216440b421f4c",
            "patch": "@@ -673,8 +673,8 @@\n         title: (ë²ˆì—­ì¤‘) ALIGN\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) AltCLIP\n-      - local: in_translation\n-        title: (ë²ˆì—­ì¤‘) BLIP\n+      - local: model_doc/blip\n+        title: BLIP\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) BLIP-2\n       - local: in_translation"
        },
        {
            "sha": "27e085315b891130598a49e4de909ccc28ecc8d5",
            "filename": "docs/source/ko/model_doc/blip.md",
            "status": "added",
            "additions": 136,
            "deletions": 0,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/109b1e7591e703e0634fdcadd61216440b421f4c/docs%2Fsource%2Fko%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/109b1e7591e703e0634fdcadd61216440b421f4c/docs%2Fsource%2Fko%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fblip.md?ref=109b1e7591e703e0634fdcadd61216440b421f4c",
            "patch": "@@ -0,0 +1,136 @@\n+<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# BLIP[[blip]]\n+\n+## ê°œìš”[[overview]]\n+\n+BLIP ëª¨ë¸ì€ Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoiì˜ [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) ë…¼ë¬¸ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+BLIPì€ ì—¬ëŸ¬ ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì…ë‹ˆë‹¤:\n+\n+- ì‹œê° ì§ˆë¬¸ ì‘ë‹µ (Visual Question Answering, VQA)\n+- ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­)\n+- ì´ë¯¸ì§€ ìº¡ì…”ë‹\n+\n+ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ë¹„ì „-ì–¸ì–´ ì‚¬ì „ í•™ìŠµ(Vision-Language Pre-training, VLP)ì€ ë‹¤ì–‘í•œ ë¹„ì „-ì–¸ì–´ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ë“¤ì€ ì´í•´ ê¸°ë°˜ ì‘ì—…ì´ë‚˜ ìƒì„± ê¸°ë°˜ ì‘ì—… ì¤‘ í•˜ë‚˜ì—ì„œë§Œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. ë˜í•œ ì„±ëŠ¥ í–¥ìƒì€ ì£¼ë¡œ ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ë…¸ì´ì¦ˆê°€ ë§ì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ ë°ì´í„°ì…‹ì˜ ê·œëª¨ë¥¼ í‚¤ìš°ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì¡ŒëŠ”ë°, ì´ëŠ” ìµœì ì˜ ì§€ë„ í•™ìŠµ ë°©ì‹ì´ë¼ê³  ë³´ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” BLIPì´ë¼ëŠ” ìƒˆë¡œìš´ VLP í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë¹„ì „-ì–¸ì–´ ì´í•´ ë° ìƒì„± ì‘ì—… ëª¨ë‘ì— ìœ ì—°í•˜ê²Œ ì ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. BLIPëŠ” ìº¡ì…”ë„ˆê°€ í•©ì„± ìº¡ì…˜ì„ ìƒì„±í•˜ê³  í•„í„°ê°€ ë…¸ì´ì¦ˆ ìº¡ì…˜ì„ ì œê±°í•˜ëŠ” ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ ë°©ë²•ì„ í†µí•´ ì›¹ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ìƒ‰(Recall@1ì—ì„œ +2.7%), ì´ë¯¸ì§€ ìº¡ì…”ë‹(CIDErì—ì„œ +2.8%), ê·¸ë¦¬ê³  VQA(VQA ì ìˆ˜ì—ì„œ +1.6%)ì™€ ê°™ì€ ë‹¤ì–‘í•œ ë¹„ì „-ì–¸ì–´ ì‘ì—…ì—ì„œ ìµœì‹  ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë˜í•œ BLIPì€ ì œë¡œìƒ· ë°©ì‹ìœ¼ë¡œ ë¹„ë””ì˜¤-ì–¸ì–´ ì‘ì—…ì— ì§ì ‘ ì „ì´ë  ë•Œë„ ê°•ë ¥í•œ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì˜ ì½”ë“œ, ëª¨ë¸, ë°ì´í„°ì…‹ì€ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.*\n+\n+![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif)\n+\n+ì´ ëª¨ë¸ì€ [ybelkada](https://huggingface.co/ybelkada)ê°€ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.\n+ì›ë³¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/salesforce/BLIP)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ìë£Œ[[resources]]\n+\n+- [Jupyter notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb): ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ì—ì„œ BLIPë¥¼ ì´ë¯¸ì§€ ìº¡ì…”ë‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•\n+\n+## BlipConfig[[transformers.BlipConfig]]\n+\n+[[autodoc]] BlipConfig\n+    - from_text_vision_configs\n+\n+## BlipTextConfig[[transformers.BlipTextConfig]]\n+\n+[[autodoc]] BlipTextConfig\n+\n+## BlipVisionConfig[[transformers.BlipVisionConfig]]\n+\n+[[autodoc]] BlipVisionConfig\n+\n+## BlipProcessor[[transformers.BlipProcessor]]\n+\n+[[autodoc]] BlipProcessor\n+\n+## BlipImageProcessor[[transformers.BlipImageProcessor]]\n+\n+[[autodoc]] BlipImageProcessor\n+    - preprocess\n+\n+<frameworkcontent>\n+<pt>\n+\n+## BlipModel[[transformers.BlipModel]]\n+\n+`BlipModel`ì€ í–¥í›„ ë²„ì „ì—ì„œ ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•Šì„ ì˜ˆì •ì…ë‹ˆë‹¤. ëª©ì ì— ë”°ë¼ `BlipForConditionalGeneration`, `BlipForImageTextRetrieval` ë˜ëŠ” `BlipForQuestionAnswering`ì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n+\n+[[autodoc]] BlipModel\n+    - forward\n+    - get_text_features\n+    - get_image_features\n+\n+## BlipTextModel[[transformers.BlipTextModel]]\n+\n+[[autodoc]] BlipTextModel\n+    - forward\n+\n+## BlipVisionModel[[transformers.BlipVisionModel]]\n+\n+[[autodoc]] BlipVisionModel\n+    - forward\n+\n+## BlipForConditionalGeneration[[transformers.BlipForConditionalGeneration]]\n+\n+[[autodoc]] BlipForConditionalGeneration\n+    - forward\n+\n+## BlipForImageTextRetrieval[[transformers.BlipForImageTextRetrieval]]\n+\n+[[autodoc]] BlipForImageTextRetrieval\n+    - forward\n+\n+## BlipForQuestionAnswering[[transformers.BlipForQuestionAnswering]]\n+\n+[[autodoc]] BlipForQuestionAnswering\n+    - forward\n+\n+</pt>\n+<tf>\n+\n+## TFBlipModel[[transformers.TFBlipModel]]\n+\n+[[autodoc]] TFBlipModel\n+    - call\n+    - get_text_features\n+    - get_image_features\n+\n+## TFBlipTextModel[[transformers.TFBlipTextModel]]\n+\n+[[autodoc]] TFBlipTextModel\n+    - call\n+\n+## TFBlipVisionModel[[transformers.TFBlipVisionModel]]\n+\n+[[autodoc]] TFBlipVisionModel\n+    - call\n+\n+## TFBlipForConditionalGeneration[[transformers.TFBlipForConditionalGeneration]]\n+\n+[[autodoc]] TFBlipForConditionalGeneration\n+    - call\n+\n+## TFBlipForImageTextRetrieval[[transformers.TFBlipForImageTextRetrieval]]\n+\n+[[autodoc]] TFBlipForImageTextRetrieval\n+    - call\n+\n+## TFBlipForQuestionAnswering[[transformers.TFBlipForQuestionAnswering]]\n+\n+[[autodoc]] TFBlipForQuestionAnswering\n+    - call\n+</tf>\n+</frameworkcontent>"
        }
    ],
    "stats": {
        "total": 140,
        "additions": 138,
        "deletions": 2
    }
}