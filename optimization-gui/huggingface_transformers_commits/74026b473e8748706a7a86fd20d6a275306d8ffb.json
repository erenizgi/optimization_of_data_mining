{
    "author": "sywangyi",
    "message": "idefics2 enable_input_require_grads not aligned with disable_input_reâ€¦ (#33194)\n\n* idefics2 enable_input_require_grads not aligned with disable_input_require_grads\r\nmake peft+idefics2 checkpoints disable fail\r\n\r\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\r\n\r\n* split test case\r\n\r\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\r\n\r\n* fix ci failure\r\n\r\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\r\n\r\n* refine test\r\n\r\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\r\n\r\n---------\r\n\r\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>",
    "sha": "74026b473e8748706a7a86fd20d6a275306d8ffb",
    "files": [
        {
            "sha": "08ada424ea77b4b2b92f373d5fcb9d87c097cf0d",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/74026b473e8748706a7a86fd20d6a275306d8ffb/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/74026b473e8748706a7a86fd20d6a275306d8ffb/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=74026b473e8748706a7a86fd20d6a275306d8ffb",
            "patch": "@@ -1256,6 +1256,10 @@ def make_inputs_require_grads(module, input, output):\n             make_inputs_require_grads\n         )\n \n+    def disable_input_require_grads(self):\n+        self._text_require_grads_hook.remove()\n+        self._vision_require_grads_hook.remove()\n+\n     def get_input_embeddings(self):\n         return self.text_model.get_input_embeddings()\n \n@@ -1466,6 +1470,10 @@ def make_inputs_require_grads(module, input, output):\n             make_inputs_require_grads\n         )\n \n+    def disable_input_require_grads(self):\n+        self._text_require_grads_hook.remove()\n+        self._vision_require_grads_hook.remove()\n+\n     def get_input_embeddings(self):\n         return self.model.text_model.get_input_embeddings()\n "
        },
        {
            "sha": "e13cf8dd56c3efa50ad20a48f1d90a4d0a6e521b",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/74026b473e8748706a7a86fd20d6a275306d8ffb/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/74026b473e8748706a7a86fd20d6a275306d8ffb/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=74026b473e8748706a7a86fd20d6a275306d8ffb",
            "patch": "@@ -239,6 +239,12 @@ def test_torchscript_output_hidden_state(self):\n     def test_torchscript_simple(self):\n         pass\n \n+    @unittest.skip(\n+        reason=\"Model returns None for input_embeds, check: https://github.com/huggingface/transformers/issues/33527\"\n+    )\n+    def test_peft_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n \n @require_torch\n class SpeechT5ForSpeechToTextTester:\n@@ -1743,6 +1749,12 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n+    @unittest.skip(\n+        reason=\"Model returns None for input_embeds, check: https://github.com/huggingface/transformers/issues/33527\"\n+    )\n+    def test_peft_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n     # overwrite from test_modeling_common\n     def _mock_init_weights(self, module):\n         if hasattr(module, \"weight\") and module.weight is not None:"
        },
        {
            "sha": "c7af0b1c9f5b604c4c1d3ad238a2d4e024413497",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/74026b473e8748706a7a86fd20d6a275306d8ffb/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/74026b473e8748706a7a86fd20d6a275306d8ffb/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=74026b473e8748706a7a86fd20d6a275306d8ffb",
            "patch": "@@ -403,6 +403,44 @@ def test_gradient_checkpointing_enable_disable(self):\n                         m.gradient_checkpointing, f\"Module {n} does not have gradient_checkpointing set to False\"\n                     )\n \n+    def test_peft_gradient_checkpointing_enable_disable(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class.supports_gradient_checkpointing:\n+                continue\n+\n+            # at init model should have gradient checkpointing disabled\n+            model = model_class(config)\n+            self.assertFalse(model.is_gradient_checkpointing)\n+\n+            # check enable works\n+            model._hf_peft_config_loaded = True\n+            try:\n+                model.gradient_checkpointing_enable()\n+            except NotImplementedError:\n+                continue\n+\n+            self.assertTrue(model.is_gradient_checkpointing)\n+\n+            # Loop over all modules and check that relevant modules have gradient_checkpointing set to True\n+            for n, m in model.named_modules():\n+                if hasattr(m, \"gradient_checkpointing\"):\n+                    self.assertTrue(\n+                        m.gradient_checkpointing, f\"Module {n} does not have gradient_checkpointing set to True\"\n+                    )\n+\n+            # check disable works\n+            model.gradient_checkpointing_disable()\n+            self.assertFalse(model.is_gradient_checkpointing)\n+\n+            # Loop over all modules and check that relevant modules have gradient_checkpointing set to False\n+            for n, m in model.named_modules():\n+                if hasattr(m, \"gradient_checkpointing\"):\n+                    self.assertFalse(\n+                        m.gradient_checkpointing, f\"Module {n} does not have gradient_checkpointing set to False\"\n+                    )\n+\n     @is_flaky(description=\"low likelihood of failure, reason not yet discovered\")\n     def test_save_load_fast_init_from_base(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        }
    ],
    "stats": {
        "total": 58,
        "additions": 58,
        "deletions": 0
    }
}