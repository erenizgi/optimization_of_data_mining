{
    "author": "ydshieh",
    "message": "fix `llama` tests (#39161)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "8e87adc45f20ba88360afbc29ab3f7a0063bf720",
    "files": [
        {
            "sha": "fcd060a37b8318abb371541603ccfa9dfa62f710",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 11,
            "deletions": 19,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e87adc45f20ba88360afbc29ab3f7a0063bf720/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e87adc45f20ba88360afbc29ab3f7a0063bf720/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=8e87adc45f20ba88360afbc29ab3f7a0063bf720",
            "patch": "@@ -25,6 +25,7 @@\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n+    run_test_using_subprocess,\n     slow,\n     torch_device,\n )\n@@ -96,36 +97,28 @@ class LlamaModelTest(CausalLMModelTest, unittest.TestCase):\n \n \n @require_torch_accelerator\n+@require_read_token\n class LlamaIntegrationTest(unittest.TestCase):\n+    def setup(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def tearDown(self):\n         # TODO (joao): automatic compilation, i.e. compilation when `cache_implementation=\"static\"` is used, leaves\n         # some memory allocated in the cache, which means some object is not being released properly. This causes some\n         # unoptimal memory usage, e.g. after certain tests a 7B model in FP16 no longer fits in a 24GB GPU.\n         # Investigate the root cause.\n-        cleanup(torch_device, gc_collect=False)\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n-    @require_read_token\n     def test_llama_3_1_hard(self):\n         \"\"\"\n         An integration test for llama 3.1. It tests against a long output to ensure the subtle numerical differences\n         from llama 3.1.'s RoPE can be detected\n         \"\"\"\n-        # diff on `EXPECTED_TEXT`:\n-        # 2024-08-26: updating from torch 2.3.1 to 2.4.0 slightly changes the results.\n-        expected_base_text = (\n-            \"Tell me about the french revolution. The french revolution was a period of radical political and social \"\n-            \"upheaval in France that lasted from 1789 until 1799. It was a time of great change and upheaval, marked \"\n-            \"by the overthrow of the monarchy, the rise of the middle class, and the eventual establishment of the \"\n-            \"First French Republic.\\nThe revolution began in 1789 with the Estates-General, a representative \"\n-            \"assembly that had not met since 1614. The Third Estate, which represented the common people, \"\n-            \"demanded greater representation and eventually broke away to form the National Assembly. This marked \"\n-            \"the beginning of the end of the absolute monarchy and the rise of the middle class.\\n\"\n-        )\n         expected_texts = Expectations(\n             {\n-                (\"rocm\", (9, 5)): expected_base_text.replace(\"political and social\", \"social and political\"),\n-                (\"cuda\", None): expected_base_text,\n+                (\"rocm\", (9, 5)): 'Tell me about the french revolution. The french revolution was a period of radical social and political upheaval in France that lasted from 1789 until 1799. It was a time of great change and upheaval, marked by the overthrow of the monarchy, the rise of the middle class, and the eventual establishment of the First French Republic.\\nThe revolution began in 1789 with the Estates-General, a representative assembly that had not met since 1614. The Third Estate, which represented the common people, demanded greater representation and eventually broke away to form the National Assembly. This marked the beginning of the end of the absolute monarchy and the rise of the middle class.\\n',\n+                (\"cuda\", None): 'Tell me about the french revolution. The french revolution was a period of radical political and social upheaval in France that lasted from 1789 until 1799. It was a time of great change and upheaval, marked by the overthrow of the monarchy, the rise of the middle class, and the eventual establishment of the First French Republic.\\nThe revolution began in 1789 with the Estates-General, a representative assembly that had not met since 1614. The Third Estate, which represented the common people, demanded greater representation and eventually broke away to form the National Assembly. The National Assembly adopted the Declaration of the Rights of Man and of the Citizen, which enshr',\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT = expected_texts.get_expectation()\n@@ -142,7 +135,6 @@ def test_llama_3_1_hard(self):\n         self.assertEqual(generated_text, EXPECTED_TEXT)\n \n     @slow\n-    @require_read_token\n     def test_model_7b_logits_bf16(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n \n@@ -191,7 +183,6 @@ def test_model_7b_logits_bf16(self):\n         )\n \n     @slow\n-    @require_read_token\n     def test_model_7b_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n \n@@ -240,6 +231,9 @@ def test_model_7b_logits(self):\n             )\n         )\n \n+    # TODO: check why we have the following strange situation.\n+    # without running in subprocess, this test causes subsequent tests failing with `RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0!`\n+    @run_test_using_subprocess\n     @slow\n     def test_model_7b_dola_generation(self):\n         # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n@@ -265,7 +259,6 @@ def test_model_7b_dola_generation(self):\n \n     @slow\n     @require_torch_accelerator\n-    @require_read_token\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2\n         # work as intended. See https://github.com/pytorch/pytorch/issues/121943\n@@ -306,7 +299,6 @@ def test_compile_static_cache(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_text)\n \n     @slow\n-    @require_read_token\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")"
        },
        {
            "sha": "927aa54fa08401674be44cd9a4e55ac914c001a9",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e87adc45f20ba88360afbc29ab3f7a0063bf720/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e87adc45f20ba88360afbc29ab3f7a0063bf720/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=8e87adc45f20ba88360afbc29ab3f7a0063bf720",
            "patch": "@@ -407,6 +407,8 @@ def test_fast_special_tokens(self):\n         self.tokenizer.add_eos_token = False\n         self.rust_tokenizer.add_eos_token = False\n \n+    # See internal discussion: https://huggingface.slack.com/archives/C01NE71C4F7/p1750680376085749?thread_ts=1750676268.233309&cid=C01NE71C4F7\n+    @unittest.skip(\"failing, won't fix\")\n     @slow\n     def test_conversion(self):\n         # This is excruciatingly slow since it has to recreate the entire merge"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 13,
        "deletions": 19
    }
}