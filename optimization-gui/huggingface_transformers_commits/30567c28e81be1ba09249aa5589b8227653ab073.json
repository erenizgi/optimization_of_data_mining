{
    "author": "Yozer",
    "message": "[timm_wrapper] add support for gradient checkpointing (#39287)\n\n* feat: add support for gradient checkpointing in TimmWrapperModel and TimmWrapperForImageClassification\n\n* ruff fix\n\n* refactor + add test for not supported model\n\n* ruff\n\n* Update src/transformers/models/timm_wrapper/modeling_timm_wrapper.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/models/timm_wrapper/modeling_timm_wrapper.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/models/timm_wrapper/modeling_timm_wrapper.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/models/timm_wrapper/modeling_timm_wrapper.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "30567c28e81be1ba09249aa5589b8227653ab073",
    "files": [
        {
            "sha": "d6d844af479497bbe39950552ccbffb7031089ec",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/30567c28e81be1ba09249aa5589b8227653ab073/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/30567c28e81be1ba09249aa5589b8227653ab073/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=30567c28e81be1ba09249aa5589b8227653ab073",
            "patch": "@@ -70,6 +70,10 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\", \"timm\"])\n         super().__init__(*args, **kwargs)\n \n+    def post_init(self):\n+        self.supports_gradient_checkpointing = self._timm_model_supports_gradient_checkpointing()\n+        super().post_init()\n+\n     @staticmethod\n     def _fix_state_dict_key_on_load(key) -> tuple[str, bool]:\n         \"\"\"\n@@ -107,6 +111,24 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n+    def _timm_model_supports_gradient_checkpointing(self):\n+        \"\"\"\n+        Check if the timm model supports gradient checkpointing by checking if the `set_grad_checkpointing` method is available.\n+        Some timm models will have the method but will raise an AssertionError when called so in this case we return False.\n+        \"\"\"\n+        if not hasattr(self.timm_model, \"set_grad_checkpointing\"):\n+            return False\n+\n+        try:\n+            self.timm_model.set_grad_checkpointing(enable=True)\n+            self.timm_model.set_grad_checkpointing(enable=False)\n+            return True\n+        except Exception:\n+            return False\n+\n+    def _set_gradient_checkpointing(self, enable: bool = True, *args, **kwargs):\n+        self.timm_model.set_grad_checkpointing(enable)\n+\n \n class TimmWrapperModel(TimmWrapperPreTrainedModel):\n     \"\"\""
        },
        {
            "sha": "b7653f4e770989137fa44d08f8135a8e8d35e8cf",
            "filename": "tests/models/timm_wrapper/test_modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/30567c28e81be1ba09249aa5589b8227653ab073/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/30567c28e81be1ba09249aa5589b8227653ab073/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py?ref=30567c28e81be1ba09249aa5589b8227653ab073",
            "patch": "@@ -170,6 +170,16 @@ def test_mismatched_shapes_have_properly_initialized_weights(self):\n     def test_model_is_small(self):\n         pass\n \n+    def test_gradient_checkpointing(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        model = TimmWrapperModel._from_config(config)\n+        self.assertTrue(model.supports_gradient_checkpointing)\n+\n+    def test_gradient_checkpointing_on_non_supported_model(self):\n+        config = TimmWrapperConfig.from_pretrained(\"timm/hrnet_w18.ms_aug_in1k\")\n+        model = TimmWrapperModel._from_config(config)\n+        self.assertFalse(model.supports_gradient_checkpointing)\n+\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n "
        }
    ],
    "stats": {
        "total": 32,
        "additions": 32,
        "deletions": 0
    }
}