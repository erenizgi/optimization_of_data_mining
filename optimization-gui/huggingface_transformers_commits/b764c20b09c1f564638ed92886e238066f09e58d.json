{
    "author": "zucchini-nlp",
    "message": "Fix: loading DBRX back from saved path (#35728)\n\n* fix dtype as dict for some models + add test\r\n\r\n* add comment in tests",
    "sha": "b764c20b09c1f564638ed92886e238066f09e58d",
    "files": [
        {
            "sha": "9c99a6d770d18361f2b1830af7d576b43afec21b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b764c20b09c1f564638ed92886e238066f09e58d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b764c20b09c1f564638ed92886e238066f09e58d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=b764c20b09c1f564638ed92886e238066f09e58d",
            "patch": "@@ -4037,7 +4037,9 @@ def from_pretrained(\n                             sub_config = getattr(config, sub_config_key)\n                             sub_config.torch_dtype = torch_dtype\n                 elif isinstance(torch_dtype, torch.dtype):\n-                    pass\n+                    for sub_config_key in config.sub_configs.keys():\n+                        sub_config = getattr(config, sub_config_key)\n+                        sub_config.torch_dtype = torch_dtype\n                 elif isinstance(torch_dtype, dict):\n                     for key, curr_dtype in torch_dtype.items():\n                         if hasattr(config, key):"
        },
        {
            "sha": "72df1fe335bab446fa1e88186d2ff14ae70f3351",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b764c20b09c1f564638ed92886e238066f09e58d/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b764c20b09c1f564638ed92886e238066f09e58d/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=b764c20b09c1f564638ed92886e238066f09e58d",
            "patch": "@@ -57,7 +57,7 @@ def __init__(\n         self.kv_n_heads = kv_n_heads\n         self.rope_theta = rope_theta\n \n-        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\"]:\n+        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\", \"torch_dtype\"]:\n             if k in kwargs:\n                 kwargs.pop(k)\n         if len(kwargs) != 0:\n@@ -109,7 +109,7 @@ def __init__(\n         self.moe_loss_weight = moe_loss_weight\n         self.moe_normalize_expert_weights = moe_normalize_expert_weights\n \n-        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\"]:\n+        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\", \"torch_dtype\"]:\n             if k in kwargs:\n                 kwargs.pop(k)\n         if len(kwargs) != 0:"
        },
        {
            "sha": "1d52e82814495e05e756fee756d418d7fad7c9c5",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b764c20b09c1f564638ed92886e238066f09e58d/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b764c20b09c1f564638ed92886e238066f09e58d/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=b764c20b09c1f564638ed92886e238066f09e58d",
            "patch": "@@ -331,6 +331,12 @@ def check_save_load(out1, out2):\n                 with torch.no_grad():\n                     second = model(**self._prepare_for_class(inputs_dict, model_class))[0]\n \n+                # Save and load second time because `from_pretrained` adds a bunch of new config fields\n+                # so we need to make sure those fields can be loaded back after saving\n+                # Simply init as `model(config)` doesn't add those fields\n+                model.save_pretrained(tmpdirname)\n+                model = model_class.from_pretrained(tmpdirname)\n+\n             if isinstance(first, tuple) and isinstance(second, tuple):\n                 for tensor1, tensor2 in zip(first, second):\n                     check_save_load(tensor1, tensor2)"
        },
        {
            "sha": "dd52927a250b968b8fb85e22214baf7ff00aae3c",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b764c20b09c1f564638ed92886e238066f09e58d/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b764c20b09c1f564638ed92886e238066f09e58d/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=b764c20b09c1f564638ed92886e238066f09e58d",
            "patch": "@@ -466,13 +466,14 @@ def test_model_from_config_torch_dtype_str(self):\n     def test_model_from_config_torch_dtype_composite(self):\n         \"\"\"\n         Test that from_pretrained works with torch_dtype being as a dict per each sub-config in composite config\n+        Tiny-Llava has saved auto dtype as `torch.float32` for all modules.\n         \"\"\"\n         # should be able to set torch_dtype as a simple string and the model loads it correctly\n         model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, torch_dtype=\"float32\")\n         self.assertEqual(model.language_model.dtype, torch.float32)\n         self.assertEqual(model.vision_tower.dtype, torch.float32)\n \n-        model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, torch_dtype=\"float16\")\n+        model = LlavaForConditionalGeneration.from_pretrained(TINY_LLAVA, torch_dtype=torch.float16)\n         self.assertEqual(model.language_model.dtype, torch.float16)\n         self.assertEqual(model.vision_tower.dtype, torch.float16)\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 13,
        "deletions": 4
    }
}