{
    "author": "Player256",
    "message": "[WIP] Add OneformerFastImageProcessor (#38343)\n\n* [WIP] OneformerFastImageProcessor\n\n* update init\n\n* Fully working oneformer image processor fast\n\n* change Nearest to Neares exact interpolation where needed\n\n* fix doc\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "c6d0500d15b9eedc33e9131a6bec6db56282b875",
    "files": [
        {
            "sha": "7beb97deb3472f0e035523a2e56f20dcc4f35de1",
            "filename": "docs/source/en/model_doc/oneformer.md",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -38,7 +38,7 @@ This model was contributed by [Jitesh Jain](https://huggingface.co/praeclarumjj3\n \n ## Usage tips\n \n--  OneFormer requires two inputs during inference: *image* and *task token*. \n+-  OneFormer requires two inputs during inference: *image* and *task token*.\n - During training, OneFormer only uses panoptic annotations.\n - If you want to train the model in a distributed environment across multiple nodes, then one should update the\n   `get_num_masks` function inside in the `OneFormerLoss` class of `modeling_oneformer.py`. When training on multiple nodes, this should be\n@@ -69,7 +69,14 @@ The resource should ideally demonstrate something new instead of duplicating an\n \n [[autodoc]] OneFormerImageProcessor\n     - preprocess\n-    - encode_inputs\n+    - post_process_semantic_segmentation\n+    - post_process_instance_segmentation\n+    - post_process_panoptic_segmentation\n+\n+## OneFormerImageProcessorFast\n+\n+[[autodoc]] OneFormerImageProcessorFast\n+    - preprocess\n     - post_process_semantic_segmentation\n     - post_process_instance_segmentation\n     - post_process_panoptic_segmentation\n@@ -87,4 +94,3 @@ The resource should ideally demonstrate something new instead of duplicating an\n \n [[autodoc]] OneFormerForUniversalSegmentation\n     - forward\n-    \n\\ No newline at end of file"
        },
        {
            "sha": "b3f4658fdd612632531ec00847826ee43d2d2732",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -130,7 +130,7 @@\n             (\"mobilevitv2\", (\"MobileViTImageProcessor\", \"MobileViTImageProcessorFast\")),\n             (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"nougat\", (\"NougatImageProcessor\", \"NougatImageProcessorFast\")),\n-            (\"oneformer\", (\"OneFormerImageProcessor\",)),\n+            (\"oneformer\", (\"OneFormerImageProcessor\", \"OneFormerImageProcessorFast\")),\n             (\"owlv2\", (\"Owlv2ImageProcessor\",)),\n             (\"owlvit\", (\"OwlViTImageProcessor\", \"OwlViTImageProcessorFast\")),\n             (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),"
        },
        {
            "sha": "c63598e0d6b8f982be3d39c53c85b0f60f1afffe",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -454,10 +454,10 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "172bdf80d41c4fb2c1739bd0caa5808d3ccb4ea5",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -445,10 +445,10 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "8a3235a41d091823eaa30a8db385a7f183801645",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -466,10 +466,10 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "6505ddd896036a29c7f648e9285f6e54e6684fbc",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -476,10 +476,10 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "df25ebac9d0b7a0f68f69c2dd012a8a0449ed14c",
            "filename": "src/transformers/models/oneformer/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Foneformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Foneformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2F__init__.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_oneformer import *\n     from .image_processing_oneformer import *\n+    from .image_processing_oneformer_fast import *\n     from .modeling_oneformer import *\n     from .processing_oneformer import *\n else:"
        },
        {
            "sha": "bf12429e8ddc5538f38f70074633acff07d3a23c",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer_fast.py",
            "status": "added",
            "additions": 1002,
            "deletions": 0,
            "changes": 1002,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -0,0 +1,1002 @@\n+# coding=utf-8\n+# Copyright 2025 SHI Labs and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for OneFormer.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+    get_max_height_width,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+from .image_processing_oneformer import load_metadata, prepare_metadata\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+def make_pixel_mask(image: \"torch.Tensor\", output_size: tuple[int, int]) -> \"torch.Tensor\":\n+    \"\"\"\n+    Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+\n+    Args:\n+        image (`torch.Tensor`):\n+            Image to make the pixel mask for.\n+        output_size (`Tuple[int, int]`):\n+            Output size of the mask.\n+    \"\"\"\n+\n+    input_height, input_width = image.shape[-2], image.shape[-1]\n+    mask = torch.zeros(output_size, dtype=torch.int64)\n+    mask[:input_height, :input_width] = 1\n+    return mask\n+\n+\n+def binary_mask_to_rle(mask):\n+    \"\"\"\n+    Converts given binary mask of shape `(height, width)` to the run-length encoding (RLE) format.\n+\n+    Args:\n+        mask (`torch.Tensor` or `numpy.array`):\n+            A binary mask tensor of shape `(height, width)` where 0 denotes background and 1 denotes the target\n+            segment_id or class_id.\n+    Returns:\n+        `List`: Run-length encoded list of the binary mask. Refer to COCO API for more information about the RLE\n+        format.\n+    \"\"\"\n+    pixels = mask.flatten()\n+    pixels = torch.concat([[0], pixels, [0]])\n+    runs = torch.where(pixels[1:] != pixels[:-1])[0] + 1\n+    runs[1::2] -= runs[::2]\n+    return list(runs)\n+\n+\n+def convert_segmentation_to_rle(segmentation):\n+    \"\"\"\n+    Converts given segmentation map of shape `(height, width)` to the run-length encoding (RLE) format.\n+\n+    Args:\n+        segmentation (`torch.Tensor` or `numpy.array`):\n+            A segmentation map of shape `(height, width)` where each value denotes a segment or class id.\n+    Returns:\n+        `List[List]`: A list of lists, where each list is the run-length encoding of a segment / class id.\n+    \"\"\"\n+    segment_ids = torch.unique(segmentation)\n+\n+    run_length_encodings = []\n+    for idx in segment_ids:\n+        mask = torch.where(segmentation == idx, 1, 0)\n+        rle = binary_mask_to_rle(mask)\n+        run_length_encodings.append(rle)\n+\n+    return run_length_encodings\n+\n+\n+def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):\n+    \"\"\"\n+    Binarize the given masks using `object_mask_threshold`, it returns the associated values of `masks`, `scores` and\n+    `labels`.\n+\n+    Args:\n+        masks (`torch.Tensor`):\n+            A tensor of shape `(num_queries, height, width)`.\n+        scores (`torch.Tensor`):\n+            A tensor of shape `(num_queries)`.\n+        labels (`torch.Tensor`):\n+            A tensor of shape `(num_queries)`.\n+        object_mask_threshold (`float`):\n+            A number between 0 and 1 used to binarize the masks.\n+    Raises:\n+        `ValueError`: Raised when the first dimension doesn't match in all input tensors.\n+    Returns:\n+        `Tuple[`torch.Tensor`, `torch.Tensor`, `torch.Tensor`]`: The `masks`, `scores` and `labels` without the region\n+        < `object_mask_threshold`.\n+    \"\"\"\n+    if not (masks.shape[0] == scores.shape[0] == labels.shape[0]):\n+        raise ValueError(\"mask, scores and labels must have the same shape!\")\n+\n+    to_keep = labels.ne(num_labels) & (scores > object_mask_threshold)\n+\n+    return masks[to_keep], scores[to_keep], labels[to_keep]\n+\n+\n+def check_segment_validity(mask_labels, mask_probs, k, mask_threshold=0.5, overlap_mask_area_threshold=0.8):\n+    # Get the mask associated with the k class\n+    mask_k = mask_labels == k\n+    mask_k_area = mask_k.sum()\n+\n+    # Compute the area of all the stuff in query k\n+    original_area = (mask_probs[k] >= mask_threshold).sum()\n+    mask_exists = mask_k_area > 0 and original_area > 0\n+\n+    # Eliminate disconnected tiny segments\n+    if mask_exists:\n+        area_ratio = mask_k_area / original_area\n+        if not area_ratio.item() > overlap_mask_area_threshold:\n+            mask_exists = False\n+\n+    return mask_exists, mask_k\n+\n+\n+def compute_segments(\n+    mask_probs,\n+    pred_scores,\n+    pred_labels,\n+    mask_threshold: float = 0.5,\n+    overlap_mask_area_threshold: float = 0.8,\n+    label_ids_to_fuse: Optional[set[int]] = None,\n+    target_size: Optional[tuple[int, int]] = None,\n+):\n+    height = mask_probs.shape[1] if target_size is None else target_size[0]\n+    width = mask_probs.shape[2] if target_size is None else target_size[1]\n+\n+    segmentation = torch.zeros((height, width), dtype=torch.int32, device=mask_probs.device)\n+    segments: list[dict] = []\n+\n+    if target_size is not None:\n+        mask_probs = F.resize(\n+            mask_probs.unsqueeze(0),\n+            size=target_size,\n+            interpolation=F.InterpolationMode.BILINEAR,\n+        )[0]\n+\n+    current_segment_id = 0\n+\n+    mask_probs *= pred_scores.view(-1, 1, 1)\n+    mask_labels = mask_probs.argmax(0)  # [height, width]\n+\n+    stuff_memory_list: dict[str, int] = {}\n+    for k in range(pred_labels.shape[0]):\n+        pred_class = pred_labels[k].item()\n+        should_fuse = pred_class in label_ids_to_fuse\n+\n+        mask_exists, mask_k = check_segment_validity(\n+            mask_labels, mask_probs, k, mask_threshold, overlap_mask_area_threshold\n+        )\n+\n+        if mask_exists:\n+            if pred_class in stuff_memory_list:\n+                current_segment_id = stuff_memory_list[pred_class]\n+            else:\n+                current_segment_id += 1\n+\n+            segmentation[mask_k] = current_segment_id\n+            segment_score = round(pred_scores[k].item(), 6)\n+            segments.append(\n+                {\n+                    \"id\": current_segment_id,\n+                    \"label_id\": pred_class,\n+                    \"was_fused\": should_fuse,\n+                    \"score\": segment_score,\n+                }\n+            )\n+            if should_fuse:\n+                stuff_memory_list[pred_class] = current_segment_id\n+\n+    return segmentation, segments\n+\n+\n+def convert_segmentation_map_to_binary_masks_fast(\n+    segmentation_map: \"torch.Tensor\",\n+    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+    ignore_index: Optional[int] = None,\n+    do_reduce_labels: bool = False,\n+):\n+    if do_reduce_labels and ignore_index is None:\n+        raise ValueError(\"If `do_reduce_labels` is True, `ignore_index` must be provided.\")\n+\n+    if do_reduce_labels:\n+        segmentation_map = torch.where(segmentation_map == 0, ignore_index, segmentation_map - 1)\n+\n+    all_labels = torch.unique(segmentation_map)\n+\n+    if ignore_index is not None:\n+        all_labels = all_labels[all_labels != ignore_index]\n+\n+    binary_masks = [(segmentation_map == i) for i in all_labels]\n+\n+    if binary_masks:\n+        binary_masks = torch.stack(binary_masks, dim=0)\n+    else:\n+        binary_masks = torch.zeros((0, *segmentation_map.shape), device=segmentation_map.device)\n+\n+    # Convert instance ids to class ids\n+    if instance_id_to_semantic_id is not None:\n+        labels = torch.zeros(all_labels.shape[0], device=segmentation_map.device)\n+\n+        for i, label in enumerate(all_labels):\n+            class_id = instance_id_to_semantic_id[(label.item() + 1 if do_reduce_labels else label.item())]\n+            labels[i] = class_id - 1 if do_reduce_labels else class_id\n+    else:\n+        labels = all_labels\n+\n+    return (\n+        binary_masks.float(),\n+        labels.long(),\n+    )\n+\n+\n+def get_oneformer_resize_output_image_size(\n+    image: \"torch.Tensor\",\n+    size: Union[int, tuple[int, int], list[int], tuple[int]],\n+    max_size: Optional[int] = None,\n+    default_to_square: bool = True,\n+) -> tuple:\n+    \"\"\"\n+    Computes the output size given the desired size.\n+\n+    Args:\n+        image (`torch.Tensor`):\n+            The input image.\n+        size (`int` or `Tuple[int, int]` or `List[int]` or `Tuple[int]`):\n+            The size of the output image.\n+        max_size (`int`, *optional*):\n+            The maximum size of the output image.\n+        default_to_square (`bool`, *optional*, defaults to `True`):\n+            Whether to default to square if no size is provided.\n+    Returns:\n+        `Tuple[int, int]`: The output size.\n+    \"\"\"\n+    if isinstance(size, (tuple, list)):\n+        if len(size) == 2:\n+            return tuple(size)\n+        elif len(size) == 1:\n+            # Perform same logic as if size was an int\n+            size = size[0]\n+        else:\n+            raise ValueError(\"size must have 1 or 2 elements if it is a list or tuple\")\n+\n+    if default_to_square:\n+        return (size, size)\n+\n+    height, width = image.shape[-2], image.shape[-1]\n+    short, long = (width, height) if width <= height else (height, width)\n+    requested_new_short = size\n+\n+    new_short, new_long = requested_new_short, int(requested_new_short * long / short)\n+\n+    if max_size is not None:\n+        if max_size <= requested_new_short:\n+            raise ValueError(\n+                f\"max_size = {max_size} must be strictly greater than the requested \"\n+                f\"size for the smaller edge size = {size}\"\n+            )\n+        if new_long > max_size:\n+            new_short, new_long = int(max_size * new_short / new_long), max_size\n+\n+    return (new_long, new_short) if width <= height else (new_short, new_long)\n+\n+\n+class OneFormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    repo_path (`str`, *optional*, defaults to `shi-labs/oneformer_demo`):\n+        Path to a local directory or Hugging Face Hub repository containing model metadata.\n+    class_info_file (`str`, *optional*):\n+        Path to the JSON file within the repository that contains class metadata.\n+    num_text (`int`, *optional*):\n+        Number of text queries for the text encoder, used as task-guiding prompts.\n+    num_labels (`int`, *optional*):\n+        Number of semantic classes for segmentation, determining the output layer's size.\n+    ignore_index (`int`, *optional*):\n+        Label to ignore in segmentation maps, often used for padding.\n+    do_reduce_labels (`bool`, *optional*, defaults to `False`):\n+        Whether to decrement all label values by 1, mapping the background class to `ignore_index`.\n+    \"\"\"\n+\n+    repo_path: Optional[str]\n+    class_info_file: Optional[str]\n+    num_text: Optional[int]\n+    num_labels: Optional[int]\n+    ignore_index: Optional[int]\n+    do_reduce_labels: Optional[bool]\n+\n+\n+@auto_docstring\n+class OneFormerImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+    crop_size = None\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    default_to_square = False\n+    do_center_crop = False\n+    do_convert_rgb = True\n+    rescale_factor = 1 / 255\n+    ignore_index = None\n+    do_reduce_labels = False\n+    repo_path = \"shi-labs/oneformer_demo\"\n+    class_info_file = None\n+    num_text = None\n+    num_labels = None\n+    valid_kwargs = OneFormerFastImageProcessorKwargs\n+    model_input_names = [\"pixel_values\", \"pixel_mask\", \"task_inputs\"]\n+\n+    def __init__(self, **kwargs: Unpack[OneFormerFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+        if self.class_info_file:\n+            self.metadata = prepare_metadata(load_metadata(self.repo_path, self.class_info_file))\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        task_inputs: Optional[list[str]] = None,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n+        **kwargs: Unpack[OneFormerFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        task_inputs (`list[str]`, *optional*):\n+            List of tasks (`\"panoptic\"`, `\"instance\"`, `\"semantic\"`) for each image in the batch.\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps.\n+        instance_id_to_semantic_id (`Union[list[dict[int, int]], dict[int, int]]`, *optional*):\n+            A mapping from instance IDs to semantic IDs.\n+        \"\"\"\n+        return super().preprocess(\n+            images,\n+            task_inputs,\n+            segmentation_maps,\n+            instance_id_to_semantic_id,\n+            **kwargs,\n+        )\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        task_inputs: Optional[list[str]],\n+        segmentation_maps: ImageInput,\n+        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[OneFormerFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        It can be used for segmentation maps, depth maps, etc.\n+        \"\"\"\n+        # Prepare input images\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        if segmentation_maps is not None:\n+            segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+        return self._preprocess(images, task_inputs, segmentation_maps, instance_id_to_semantic_id, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        task_inputs: Optional[list[str]],\n+        segmentation_maps: list[\"torch.Tensor\"],\n+        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        ignore_index: Optional[int],\n+        do_reduce_labels: Optional[bool],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        processed_segmentation_maps = None\n+        if segmentation_maps is not None:\n+            grouped_segmentation_maps, grouped_segmentation_maps_index = group_images_by_shape(\n+                segmentation_maps, disable_grouping=disable_grouping\n+            )\n+            processed_segmentation_maps_grouped = {}\n+            for shape, stacked_segmentation_maps in grouped_segmentation_maps.items():\n+                if do_resize:\n+                    stacked_segmentation_maps = self.resize(\n+                        stacked_segmentation_maps, size=size, interpolation=F.InterpolationMode.NEAREST_EXACT\n+                    )\n+                processed_segmentation_maps_grouped[shape] = stacked_segmentation_maps\n+            processed_segmentation_maps = reorder_images(\n+                processed_segmentation_maps_grouped, grouped_segmentation_maps_index\n+            )\n+\n+        encoded_inputs = self._encode_inputs_fast(\n+            processed_images,\n+            task_inputs,\n+            segmentation_maps=processed_segmentation_maps,\n+            instance_id_to_semantic_id=instance_id_to_semantic_id,\n+            ignore_index=ignore_index,\n+            do_reduce_labels=do_reduce_labels,\n+            return_tensors=return_tensors,\n+        )\n+\n+        return encoded_inputs\n+\n+    def _pad_image_fast(\n+        self,\n+        image: \"torch.Tensor\",\n+        output_size: tuple[int, int],\n+        constant_values: float = 0,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pad an image with zeros to the given size using torch operations.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image tensor in channel-first format (C, H, W).\n+            output_size (`tuple[int, int]`):\n+                Target output size (height, width).\n+            constant_values (`float`, *optional*, defaults to 0):\n+                The value to use for padding.\n+\n+        Returns:\n+            `torch.Tensor`: The padded image.\n+        \"\"\"\n+        input_height, input_width = image.shape[1], image.shape[2]\n+        output_height, output_width = output_size\n+\n+        pad_bottom = output_height - input_height\n+        pad_right = output_width - input_width\n+\n+        padded_image = F.pad(image, padding=[0, 0, pad_right, pad_bottom], fill=constant_values)\n+\n+        return padded_image\n+\n+    def pad(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        return_pixel_mask: bool = True,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Pad a batch of images to the same size using torch operations.\n+\n+        Args:\n+            images (`List[torch.Tensor]`):\n+                List of image tensors in channel-first format.\n+            return_pixel_mask (`bool`, *optional*, defaults to `True`):\n+                Whether to return pixel masks.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return.\n+\n+        Returns:\n+            `BatchFeature`: Padded images and optional pixel masks.\n+        \"\"\"\n+        pad_size = get_max_height_width(images)\n+\n+        padded_images = []\n+        pixel_masks = []\n+\n+        for image in images:\n+            padded_image = self._pad_image_fast(\n+                image=image,\n+                output_size=pad_size,\n+                constant_values=0,\n+            )\n+            padded_images.append(padded_image)\n+\n+            if return_pixel_mask:\n+                input_height, input_width = image.shape[1], image.shape[2]\n+                mask = torch.zeros(pad_size, dtype=torch.int64, device=image.device)\n+                mask[:input_height, :input_width] = 1\n+                pixel_masks.append(mask)\n+\n+        if return_tensors:\n+            padded_images = torch.stack(padded_images, dim=0)\n+            if return_pixel_mask:\n+                pixel_masks = torch.stack(pixel_masks, dim=0)\n+\n+        data = {\"pixel_values\": padded_images}\n+        if return_pixel_mask:\n+            data[\"pixel_mask\"] = pixel_masks\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def convert_segmentation_map_to_binary_masks(\n+        self,\n+        segmentation_map: \"torch.Tensor\",\n+        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+        ignore_index: Optional[int] = None,\n+        do_reduce_labels: bool = False,\n+    ):\n+        return convert_segmentation_map_to_binary_masks_fast(\n+            segmentation_map=segmentation_map,\n+            instance_id_to_semantic_id=instance_id_to_semantic_id,\n+            ignore_index=ignore_index,\n+            do_reduce_labels=do_reduce_labels,\n+        )\n+\n+    def get_semantic_annotations(self, label, num_class_obj):\n+        annotation_classes = label[\"classes\"]\n+        annotation_masks = label[\"masks\"]\n+\n+        texts = [\"a semantic photo\"] * self.num_text\n+        classes = []\n+        masks = []\n+\n+        for idx in range(len(annotation_classes)):\n+            class_id = annotation_classes[idx]\n+            mask = annotation_masks[idx]\n+            if not torch.all(mask == 0):\n+                if class_id not in classes:\n+                    cls_name = self.metadata[str(class_id.cpu().item())]\n+                    classes.append(class_id)\n+                    masks.append(mask)\n+                    num_class_obj[cls_name] += 1\n+                else:\n+                    idx = classes.index(class_id)\n+                    masks[idx] += mask\n+                    masks[idx] = torch.clamp(masks[idx], 0, 1)\n+\n+        num = 0\n+        for i, cls_name in enumerate(self.metadata[\"class_names\"]):\n+            if num_class_obj[cls_name] > 0:\n+                for _ in range(num_class_obj[cls_name]):\n+                    if num >= len(texts):\n+                        break\n+                    texts[num] = f\"a photo with a {cls_name}\"\n+                    num += 1\n+\n+        classes = torch.stack(classes)\n+        masks = torch.stack(masks)\n+        return classes, masks, texts\n+\n+    def get_instance_annotations(self, label, num_class_obj):\n+        annotation_classes = label[\"classes\"]\n+        annotation_masks = label[\"masks\"]\n+\n+        texts = [\"an instance photo\"] * self.num_text\n+        classes = []\n+        masks = []\n+\n+        for idx in range(len(annotation_classes)):\n+            class_id = annotation_classes[idx]\n+            mask = annotation_masks[idx]\n+\n+            if class_id in self.metadata[\"thing_ids\"]:\n+                if not torch.all(mask == 0):\n+                    cls_name = self.metadata[str(class_id.cpu().item())]\n+                    classes.append(class_id)\n+                    masks.append(mask)\n+                    num_class_obj[cls_name] += 1\n+\n+        num = 0\n+        for i, cls_name in enumerate(self.metadata[\"class_names\"]):\n+            if num_class_obj[cls_name] > 0:\n+                for _ in range(num_class_obj[cls_name]):\n+                    if num >= len(texts):\n+                        break\n+                    texts[num] = f\"a photo with a {cls_name}\"\n+                    num += 1\n+\n+        classes = torch.stack(classes)\n+        masks = torch.stack(masks)\n+        return classes, masks, texts\n+\n+    def get_panoptic_annotations(self, label, num_class_obj):\n+        annotation_classes = label[\"classes\"]\n+        annotation_masks = label[\"masks\"]\n+\n+        texts = [\"an panoptic photo\"] * self.num_text\n+        classes = []\n+        masks = []\n+        for idx in range(len(annotation_classes)):\n+            class_id = annotation_classes[idx]\n+            mask = annotation_masks[idx] if hasattr(annotation_masks[idx], \"data\") else annotation_masks[idx]\n+            if not torch.all(mask == 0):\n+                cls_name = self.metadata[str(class_id.cpu().item())]\n+                classes.append(class_id)\n+                masks.append(mask)\n+                num_class_obj[cls_name] += 1\n+\n+        num = 0\n+        for i, cls_name in enumerate(self.metadata[\"class_names\"]):\n+            if num_class_obj[cls_name] > 0:\n+                for _ in range(num_class_obj[cls_name]):\n+                    if num >= len(texts):\n+                        break\n+                    texts[num] = f\"a photo with a {cls_name}\"\n+                    num += 1\n+\n+        classes = torch.stack(classes)\n+        masks = torch.stack(masks)\n+        return classes, masks, texts\n+\n+    def _encode_inputs_fast(\n+        self,\n+        pixel_values_list: list[\"torch.Tensor\"],\n+        task_inputs: Optional[list[str]] = None,\n+        segmentation_maps: Optional[list[\"torch.Tensor\"]] = None,\n+        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n+        ignore_index: Optional[int] = None,\n+        do_reduce_labels: bool = False,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+    ) -> BatchFeature:\n+        if task_inputs is None:\n+            task_inputs = [\"panoptic\"]\n+\n+        pad_size = get_max_height_width(pixel_values_list)\n+        encoded_inputs = self.pad(pixel_values_list, return_tensors=return_tensors)\n+\n+        annotations = None\n+        if segmentation_maps is not None:\n+            annotations = []\n+            for idx, segmentation_map in enumerate(segmentation_maps):\n+                # Use instance2class_id mapping per image\n+                if isinstance(instance_id_to_semantic_id, list):\n+                    instance_id = instance_id_to_semantic_id[idx]\n+                else:\n+                    instance_id = instance_id_to_semantic_id\n+\n+                # Convert segmentation map to binary masks using torch operations\n+                masks, classes = self.convert_segmentation_map_to_binary_masks(\n+                    segmentation_map,\n+                    instance_id,\n+                    ignore_index=ignore_index,\n+                    do_reduce_labels=do_reduce_labels,\n+                )\n+\n+                annotations.append({\"masks\": masks, \"classes\": classes})\n+\n+        if annotations is not None:\n+            mask_labels = []\n+            class_labels = []\n+            text_inputs = []\n+            num_class_obj = dict.fromkeys(self.metadata[\"class_names\"], 0)\n+\n+            for i, label in enumerate(annotations):\n+                task = task_inputs[i]\n+\n+                if task == \"semantic\":\n+                    classes, masks, texts = self.get_semantic_annotations(label, num_class_obj)\n+                elif task == \"instance\":\n+                    classes, masks, texts = self.get_instance_annotations(label, num_class_obj)\n+                elif task == \"panoptic\":\n+                    classes, masks, texts = self.get_panoptic_annotations(label, num_class_obj)\n+                else:\n+                    raise ValueError(f\"{task} was not expected, expected `semantic`, `instance` or `panoptic`\")\n+                # Pad masks to max size using torch operations\n+                padded_masks = [\n+                    self._pad_image_fast(image=mask, output_size=pad_size, constant_values=ignore_index)\n+                    for mask in masks\n+                ]\n+                padded_masks = torch.cat(padded_masks, dim=0)\n+                mask_labels.append(padded_masks)\n+                class_labels.append(classes)\n+                text_inputs.append(texts)\n+\n+            encoded_inputs[\"mask_labels\"] = mask_labels\n+            encoded_inputs[\"class_labels\"] = class_labels\n+            encoded_inputs[\"text_inputs\"] = text_inputs\n+\n+        encoded_inputs[\"task_inputs\"] = [f\"the task is {task_input}\" for task_input in task_inputs]\n+        return encoded_inputs\n+\n+    def post_process_semantic_segmentation(\n+        self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Converts the output of [`MaskFormerForInstanceSegmentation`] into semantic segmentation maps. Only supports\n+        PyTorch.\n+\n+        Args:\n+            outputs ([`MaskFormerForInstanceSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`List[Tuple[int, int]]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction. If left to None, predictions will not be resized.\n+        Returns:\n+            `List[torch.Tensor]`:\n+                A list of length `batch_size`, where each item is a semantic segmentation map of shape (height, width)\n+                corresponding to the target_sizes entry (if `target_sizes` is specified). Each entry of each\n+                `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+\n+        # Remove the null class `[..., :-1]`\n+        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n+        masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n+        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n+        batch_size = class_queries_logits.shape[0]\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if batch_size != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            semantic_segmentation = []\n+            for idx in range(batch_size):\n+                resized_logits = F.resize(\n+                    segmentation[idx].unsqueeze(dim=0),\n+                    size=target_sizes[idx],\n+                    interpolation=F.InterpolationMode.BILINEAR,\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = segmentation.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        task_type: str = \"instance\",\n+        is_demo: bool = True,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n+        return_coco_annotation: Optional[bool] = False,\n+    ):\n+        \"\"\"\n+        Converts the output of [`OneFormerForUniversalSegmentationOutput`] into image instance segmentation\n+        predictions. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`OneFormerForUniversalSegmentationOutput`]):\n+                The outputs from [`OneFormerForUniversalSegmentationOutput`].\n+            task_type (`str`, *optional*, defaults to \"instance\"):\n+                The post processing depends on the task token input. If the `task_type` is \"panoptic\", we need to\n+                ignore the stuff predictions.\n+            is_demo (`bool`, *optional)*, defaults to `True`):\n+                Whether the model is in demo mode. If true, use threshold to predict final masks.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            target_sizes (`List[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction in batch. If left to None, predictions will not be\n+                resized.\n+            return_coco_annotation (`bool`, *optional)*, defaults to `False`):\n+                Whether to return predictions in COCO format.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- a tensor of shape `(height, width)` where each pixel represents a `segment_id`, set\n+              to `None` if no mask if found above `threshold`. If `target_sizes` is specified, segmentation is resized\n+              to the corresponding `target_sizes` entry.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- an integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **was_fused** -- a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False` otherwise.\n+                  Multiple instances of the same class / label were fused and assigned a single `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+\n+        device = masks_queries_logits.device\n+        batch_size = class_queries_logits.shape[0]\n+        num_queries = class_queries_logits.shape[1]\n+        num_classes = class_queries_logits.shape[-1] - 1\n+\n+        # Loop over items in batch size\n+        results: list[dict[str, torch.Tensor]] = []\n+\n+        for i in range(batch_size):\n+            # [Q, K]\n+            scores = nn.functional.softmax(class_queries_logits[i], dim=-1)[:, :-1]\n+            labels = torch.arange(num_classes, device=device).unsqueeze(0).repeat(num_queries, 1).flatten(0, 1)\n+\n+            # scores_per_image, topk_indices = scores.flatten(0, 1).topk(self.num_queries, sorted=False)\n+            scores_per_image, topk_indices = scores.flatten(0, 1).topk(num_queries, sorted=False)\n+            labels_per_image = labels[topk_indices]\n+\n+            topk_indices = torch.div(topk_indices, num_classes, rounding_mode=\"floor\")\n+            # mask_pred = mask_pred.unsqueeze(1).repeat(1, self.sem_seg_head.num_classes, 1).flatten(0, 1)\n+            mask_pred = masks_queries_logits[i][topk_indices]\n+\n+            # Only consider scores with confidence over [threshold] for demo\n+            if is_demo:\n+                keep = scores_per_image > threshold\n+                scores_per_image = scores_per_image[keep]\n+                labels_per_image = labels_per_image[keep]\n+                mask_pred = mask_pred[keep]\n+\n+            # if this is panoptic segmentation, we only keep the \"thing\" classes\n+            if task_type == \"panoptic\":\n+                keep = torch.zeros_like(scores_per_image).bool()\n+                for j, lab in enumerate(labels_per_image):\n+                    keep[j] = lab in self.metadata[\"thing_ids\"]\n+\n+                scores_per_image = scores_per_image[keep]\n+                labels_per_image = labels_per_image[keep]\n+                mask_pred = mask_pred[keep]\n+\n+            if mask_pred.shape[0] <= 0:\n+                height, width = target_sizes[i] if target_sizes is not None else mask_pred.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            if \"ade20k\" in self.class_info_file and not is_demo and \"instance\" in task_type:\n+                for j in range(labels_per_image.shape[0]):\n+                    labels_per_image[j] = self.metadata[\"thing_ids\"].index(labels_per_image[j].item())\n+\n+            # Get segmentation map and segment information of batch item\n+            target_size = target_sizes[i] if target_sizes is not None else None\n+            segmentation, segments = compute_segments(\n+                mask_pred,\n+                scores_per_image,\n+                labels_per_image,\n+                mask_threshold,\n+                overlap_mask_area_threshold,\n+                set(),\n+                target_size,\n+            )\n+\n+            # Return segmentation map in run-length encoding (RLE) format\n+            if return_coco_annotation:\n+                segmentation = convert_segmentation_to_rle(segmentation)\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+    # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.post_process_panoptic_segmentation\n+    def post_process_panoptic_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        label_ids_to_fuse: Optional[set[int]] = None,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n+    ) -> list[dict]:\n+        \"\"\"\n+        Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into image panoptic segmentation\n+        predictions. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`MaskFormerForInstanceSegmentationOutput`]):\n+                The outputs from [`MaskFormerForInstanceSegmentation`].\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            label_ids_to_fuse (`Set[int]`, *optional*):\n+                The labels in this state will have all their instances be fused together. For instance we could say\n+                there can only be one sky in an image, but several persons, so the label ID for sky would be in that\n+                set, but not the one for person.\n+            target_sizes (`list[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction in batch. If left to None, predictions will not be\n+                resized.\n+\n+        Returns:\n+            `list[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- a tensor of shape `(height, width)` where each pixel represents a `segment_id`, set\n+              to `None` if no mask if found above `threshold`. If `target_sizes` is specified, segmentation is resized\n+              to the corresponding `target_sizes` entry.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- an integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **was_fused** -- a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False` otherwise.\n+                  Multiple instances of the same class / label were fused and assigned a single `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+\n+        if label_ids_to_fuse is None:\n+            logger.warning(\"`label_ids_to_fuse` unset. No instance will be fused.\")\n+            label_ids_to_fuse = set()\n+\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        mask_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Predicted label and score of each query (batch_size, num_queries)\n+        pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n+\n+        # Loop over items in batch size\n+        results: list[dict[str, TensorType]] = []\n+\n+        for i in range(batch_size):\n+            mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects(\n+                mask_probs[i], pred_scores[i], pred_labels[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs_item.shape[0] <= 0:\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs_item.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            # Get segmentation map and segment information of batch item\n+            target_size = target_sizes[i] if target_sizes is not None else None\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs_item,\n+                pred_scores=pred_scores_item,\n+                pred_labels=pred_labels_item,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                label_ids_to_fuse=label_ids_to_fuse,\n+                target_size=target_size,\n+            )\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+\n+__all__ = [\"OneFormerImageProcessorFast\"]"
        },
        {
            "sha": "76321cc22eda24b20f39cc8be31a556e0abc9851",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -264,10 +264,10 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "85fb1d142d13f40e1e60c0baad205a99afc3ceca",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -493,10 +493,10 @@ def resize_annotation(\n                 The target size of the image, as returned by the preprocessing `resize` step.\n             threshold (`float`, *optional*, defaults to 0.5):\n                 The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "ecdb4bca1ffe63c6a0a392930a60756c2dc86ad2",
            "filename": "tests/models/oneformer/test_image_processing_oneformer.py",
            "status": "modified",
            "additions": 202,
            "deletions": 115,
            "changes": 317,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d0500d15b9eedc33e9131a6bec6db56282b875/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d0500d15b9eedc33e9131a6bec6db56282b875/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_image_processing_oneformer.py?ref=c6d0500d15b9eedc33e9131a6bec6db56282b875",
            "patch": "@@ -19,9 +19,10 @@\n import unittest\n \n import numpy as np\n+from datasets import load_dataset\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -31,6 +32,9 @@\n \n     if is_vision_available():\n         from transformers import OneFormerImageProcessor\n+\n+    if is_torchvision_available():\n+        from transformers import OneFormerImageProcessorFast\n         from transformers.models.oneformer.image_processing_oneformer import binary_mask_to_rle, prepare_metadata\n         from transformers.models.oneformer.modeling_oneformer import OneFormerForUniversalSegmentationOutput\n \n@@ -152,12 +156,24 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n         )\n \n \n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs\n+def prepare_semantic_single_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n+\n+\n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs\n+def prepare_semantic_batch_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n+\n+\n @require_torch\n @require_vision\n class OneFormerImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = OneFormerImageProcessor if (is_vision_available() and is_torch_available()) else None\n-    # only for test_image_processing_common.test_image_proc_to_json_string\n-    image_processing_class = image_processing_class\n+    fast_image_processing_class = OneFormerImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -168,23 +184,24 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_proc_properties(self):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processor, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processor, \"image_std\"))\n-        self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processor, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processor, \"size\"))\n-        self.assertTrue(hasattr(image_processor, \"ignore_index\"))\n-        self.assertTrue(hasattr(image_processor, \"class_info_file\"))\n-        self.assertTrue(hasattr(image_processor, \"num_text\"))\n-        self.assertTrue(hasattr(image_processor, \"repo_path\"))\n-        self.assertTrue(hasattr(image_processor, \"metadata\"))\n-        self.assertTrue(hasattr(image_processor, \"do_reduce_labels\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processor, \"image_std\"))\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"ignore_index\"))\n+            self.assertTrue(hasattr(image_processor, \"class_info_file\"))\n+            self.assertTrue(hasattr(image_processor, \"num_text\"))\n+            self.assertTrue(hasattr(image_processor, \"repo_path\"))\n+            self.assertTrue(hasattr(image_processor, \"metadata\"))\n+            self.assertTrue(hasattr(image_processor, \"do_reduce_labels\"))\n \n     def comm_get_image_processor_inputs(\n-        self, with_segmentation_maps=False, is_instance_map=False, segmentation_type=\"np\"\n+        self, with_segmentation_maps=False, is_instance_map=False, segmentation_type=\"np\", image_processing_class=None\n     ):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n+        image_processor = image_processing_class(**self.image_processor_dict)\n         # prepare image and target\n         num_labels = self.image_processor_tester.num_labels\n         annotations = None\n@@ -218,21 +235,25 @@ def test_init_without_params(self):\n \n     def test_call_with_segmentation_maps(self):\n         def common(is_instance_map=False, segmentation_type=None):\n-            inputs = self.comm_get_image_processor_inputs(\n-                with_segmentation_maps=True, is_instance_map=is_instance_map, segmentation_type=segmentation_type\n-            )\n-\n-            mask_labels = inputs[\"mask_labels\"]\n-            class_labels = inputs[\"class_labels\"]\n-            pixel_values = inputs[\"pixel_values\"]\n-            text_inputs = inputs[\"text_inputs\"]\n-\n-            # check the batch_size\n-            for mask_label, class_label, text_input in zip(mask_labels, class_labels, text_inputs):\n-                self.assertEqual(mask_label.shape[0], class_label.shape[0])\n-                # this ensure padding has happened\n-                self.assertEqual(mask_label.shape[1:], pixel_values.shape[2:])\n-                self.assertEqual(len(text_input), self.image_processor_tester.num_text)\n+            for image_processing_class in self.image_processor_list:\n+                inputs = self.comm_get_image_processor_inputs(\n+                    with_segmentation_maps=True,\n+                    is_instance_map=is_instance_map,\n+                    segmentation_type=segmentation_type,\n+                    image_processing_class=image_processing_class,\n+                )\n+\n+                mask_labels = inputs[\"mask_labels\"]\n+                class_labels = inputs[\"class_labels\"]\n+                pixel_values = inputs[\"pixel_values\"]\n+                text_inputs = inputs[\"text_inputs\"]\n+\n+                # check the batch_size\n+                for mask_label, class_label, text_input in zip(mask_labels, class_labels, text_inputs):\n+                    self.assertEqual(mask_label.shape[0], class_label.shape[0])\n+                    # this ensure padding has happened\n+                    self.assertEqual(mask_label.shape[1:], pixel_values.shape[2:])\n+                    self.assertEqual(len(text_input), self.image_processor_tester.num_text)\n \n         common()\n         common(is_instance_map=True)\n@@ -251,86 +272,89 @@ def test_binary_mask_to_rle(self):\n         self.assertEqual(rle[1], 45)\n \n     def test_post_process_semantic_segmentation(self):\n-        fature_extractor = self.image_processing_class(\n-            num_labels=self.image_processor_tester.num_classes,\n-            max_seq_length=77,\n-            task_seq_length=77,\n-            class_info_file=\"ade20k_panoptic.json\",\n-            num_text=self.image_processor_tester.num_text,\n-            repo_path=\"shi-labs/oneformer_demo\",\n-        )\n-        outputs = self.image_processor_tester.get_fake_oneformer_outputs()\n+        for image_processing_class in self.image_processor_list:\n+            fature_extractor = image_processing_class(\n+                num_labels=self.image_processor_tester.num_classes,\n+                max_seq_length=77,\n+                task_seq_length=77,\n+                class_info_file=\"ade20k_panoptic.json\",\n+                num_text=self.image_processor_tester.num_text,\n+                repo_path=\"shi-labs/oneformer_demo\",\n+            )\n+            outputs = self.image_processor_tester.get_fake_oneformer_outputs()\n \n-        segmentation = fature_extractor.post_process_semantic_segmentation(outputs)\n+            segmentation = fature_extractor.post_process_semantic_segmentation(outputs)\n \n-        self.assertEqual(len(segmentation), self.image_processor_tester.batch_size)\n-        self.assertEqual(\n-            segmentation[0].shape,\n-            (\n-                self.image_processor_tester.height,\n-                self.image_processor_tester.width,\n-            ),\n-        )\n+            self.assertEqual(len(segmentation), self.image_processor_tester.batch_size)\n+            self.assertEqual(\n+                segmentation[0].shape,\n+                (\n+                    self.image_processor_tester.height,\n+                    self.image_processor_tester.width,\n+                ),\n+            )\n \n-        target_sizes = [(1, 4) for i in range(self.image_processor_tester.batch_size)]\n-        segmentation = fature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n+            target_sizes = [(1, 4) for i in range(self.image_processor_tester.batch_size)]\n+            segmentation = fature_extractor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n \n-        self.assertEqual(segmentation[0].shape, target_sizes[0])\n+            self.assertEqual(segmentation[0].shape, target_sizes[0])\n \n     def test_post_process_instance_segmentation(self):\n-        image_processor = self.image_processing_class(\n-            num_labels=self.image_processor_tester.num_classes,\n-            max_seq_length=77,\n-            task_seq_length=77,\n-            class_info_file=\"ade20k_panoptic.json\",\n-            num_text=self.image_processor_tester.num_text,\n-            repo_path=\"shi-labs/oneformer_demo\",\n-        )\n-        outputs = self.image_processor_tester.get_fake_oneformer_outputs()\n-        segmentation = image_processor.post_process_instance_segmentation(outputs, threshold=0)\n-\n-        self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n-        for el in segmentation:\n-            self.assertTrue(\"segmentation\" in el)\n-            self.assertTrue(\"segments_info\" in el)\n-            self.assertEqual(type(el[\"segments_info\"]), list)\n-            self.assertEqual(\n-                el[\"segmentation\"].shape, (self.image_processor_tester.height, self.image_processor_tester.width)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(\n+                num_labels=self.image_processor_tester.num_classes,\n+                max_seq_length=77,\n+                task_seq_length=77,\n+                class_info_file=\"ade20k_panoptic.json\",\n+                num_text=self.image_processor_tester.num_text,\n+                repo_path=\"shi-labs/oneformer_demo\",\n             )\n-\n-        segmentation_with_opts = image_processor.post_process_instance_segmentation(\n-            outputs,\n-            threshold=0,\n-            target_sizes=[(1, 4) for _ in range(self.image_processor_tester.batch_size)],\n-            task_type=\"panoptic\",\n-        )\n-        self.assertTrue(len(segmentation_with_opts) == self.image_processor_tester.batch_size)\n-        for el in segmentation_with_opts:\n-            self.assertTrue(\"segmentation\" in el)\n-            self.assertTrue(\"segments_info\" in el)\n-            self.assertEqual(type(el[\"segments_info\"]), list)\n-            self.assertEqual(el[\"segmentation\"].shape, (1, 4))\n+            outputs = self.image_processor_tester.get_fake_oneformer_outputs()\n+            segmentation = image_processor.post_process_instance_segmentation(outputs, threshold=0)\n+\n+            self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n+            for el in segmentation:\n+                self.assertTrue(\"segmentation\" in el)\n+                self.assertTrue(\"segments_info\" in el)\n+                self.assertEqual(type(el[\"segments_info\"]), list)\n+                self.assertEqual(\n+                    el[\"segmentation\"].shape, (self.image_processor_tester.height, self.image_processor_tester.width)\n+                )\n+\n+            segmentation_with_opts = image_processor.post_process_instance_segmentation(\n+                outputs,\n+                threshold=0,\n+                target_sizes=[(1, 4) for _ in range(self.image_processor_tester.batch_size)],\n+                task_type=\"panoptic\",\n+            )\n+            self.assertTrue(len(segmentation_with_opts) == self.image_processor_tester.batch_size)\n+            for el in segmentation_with_opts:\n+                self.assertTrue(\"segmentation\" in el)\n+                self.assertTrue(\"segments_info\" in el)\n+                self.assertEqual(type(el[\"segments_info\"]), list)\n+                self.assertEqual(el[\"segmentation\"].shape, (1, 4))\n \n     def test_post_process_panoptic_segmentation(self):\n-        image_processor = self.image_processing_class(\n-            num_labels=self.image_processor_tester.num_classes,\n-            max_seq_length=77,\n-            task_seq_length=77,\n-            class_info_file=\"ade20k_panoptic.json\",\n-            num_text=self.image_processor_tester.num_text,\n-            repo_path=\"shi-labs/oneformer_demo\",\n-        )\n-        outputs = self.image_processor_tester.get_fake_oneformer_outputs()\n-        segmentation = image_processor.post_process_panoptic_segmentation(outputs, threshold=0)\n-\n-        self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n-        for el in segmentation:\n-            self.assertTrue(\"segmentation\" in el)\n-            self.assertTrue(\"segments_info\" in el)\n-            self.assertEqual(type(el[\"segments_info\"]), list)\n-            self.assertEqual(\n-                el[\"segmentation\"].shape, (self.image_processor_tester.height, self.image_processor_tester.width)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(\n+                num_labels=self.image_processor_tester.num_classes,\n+                max_seq_length=77,\n+                task_seq_length=77,\n+                class_info_file=\"ade20k_panoptic.json\",\n+                num_text=self.image_processor_tester.num_text,\n+                repo_path=\"shi-labs/oneformer_demo\",\n             )\n+            outputs = self.image_processor_tester.get_fake_oneformer_outputs()\n+            segmentation = image_processor.post_process_panoptic_segmentation(outputs, threshold=0)\n+\n+            self.assertTrue(len(segmentation) == self.image_processor_tester.batch_size)\n+            for el in segmentation:\n+                self.assertTrue(\"segmentation\" in el)\n+                self.assertTrue(\"segments_info\" in el)\n+                self.assertEqual(type(el[\"segments_info\"]), list)\n+                self.assertEqual(\n+                    el[\"segmentation\"].shape, (self.image_processor_tester.height, self.image_processor_tester.width)\n+                )\n \n     def test_can_load_with_local_metadata(self):\n         # Create a temporary json file\n@@ -340,28 +364,91 @@ def test_can_load_with_local_metadata(self):\n             \"2\": {\"isthing\": 1, \"name\": \"baz\"},\n         }\n         metadata = prepare_metadata(class_info)\n+        for image_processing_class in self.image_processor_list:\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                metadata_path = os.path.join(tmpdirname, \"metadata.json\")\n+                with open(metadata_path, \"w\") as f:\n+                    json.dump(class_info, f)\n \n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            metadata_path = os.path.join(tmpdirname, \"metadata.json\")\n-            with open(metadata_path, \"w\") as f:\n-                json.dump(class_info, f)\n+                config_dict = self.image_processor_dict\n+                config_dict[\"class_info_file\"] = metadata_path\n+                config_dict[\"repo_path\"] = tmpdirname\n+                image_processor = image_processing_class(**config_dict)\n \n-            config_dict = self.image_processor_dict\n-            config_dict[\"class_info_file\"] = metadata_path\n-            config_dict[\"repo_path\"] = tmpdirname\n-            image_processor = self.image_processing_class(**config_dict)\n-\n-        self.assertEqual(image_processor.metadata, metadata)\n+            self.assertEqual(image_processor.metadata, metadata)\n \n     def test_removed_deprecated_kwargs(self):\n         image_processor_dict = dict(self.image_processor_dict)\n         image_processor_dict.pop(\"do_reduce_labels\", None)\n         image_processor_dict[\"reduce_labels\"] = True\n-\n+        # Only test for OneFormerImageProcessor\n+        image_processing_class = self.image_processing_class\n         # test we are able to create the image processor with the deprecated kwargs\n-        image_processor = self.image_processing_class(**image_processor_dict)\n+        image_processor = image_processing_class(**image_processor_dict)\n         self.assertEqual(image_processor.do_reduce_labels, True)\n \n         # test we still support reduce_labels with config\n-        image_processor = self.image_processing_class.from_dict(image_processor_dict)\n+        image_processor = image_processing_class.from_dict(image_processor_dict)\n         self.assertEqual(image_processor.do_reduce_labels, True)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image, dummy_map = prepare_semantic_single_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        image_encoding_slow = image_processor_slow(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        image_encoding_fast = image_processor_fast(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values)\n+        for mask_label_slow, mask_label_fast in zip(image_encoding_slow.mask_labels, image_encoding_fast.mask_labels):\n+            self._assert_slow_fast_tensors_equivalence(mask_label_slow, mask_label_fast)\n+        for class_label_slow, class_label_fast in zip(\n+            image_encoding_slow.class_labels, image_encoding_fast.class_labels\n+        ):\n+            self._assert_slow_fast_tensors_equivalence(class_label_slow.float(), class_label_fast.float())\n+        self.assertEqual(image_encoding_slow.text_inputs, image_encoding_fast.text_inputs)\n+        self.assertEqual(image_encoding_slow.task_inputs, image_encoding_fast.task_inputs)\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images, dummy_maps = prepare_semantic_batch_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(\n+            dummy_images,\n+            segmentation_maps=dummy_maps,\n+            task_inputs=[\"instance\"] + [\"semantic\"] * (len(dummy_images) - 1),\n+            return_tensors=\"pt\",\n+        )\n+        encoding_fast = image_processor_fast(\n+            dummy_images,\n+            segmentation_maps=dummy_maps,\n+            task_inputs=[\"instance\"] + [\"semantic\"] * (len(dummy_images) - 1),\n+            return_tensors=\"pt\",\n+        )\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        for mask_label_slow, mask_label_fast in zip(encoding_slow.mask_labels, encoding_fast.mask_labels):\n+            self._assert_slow_fast_tensors_equivalence(mask_label_slow, mask_label_fast)\n+        for class_label_slow, class_label_fast in zip(encoding_slow.class_labels, encoding_fast.class_labels):\n+            self._assert_slow_fast_tensors_equivalence(class_label_slow.float(), class_label_fast.float())\n+        self.assertEqual(encoding_slow.text_inputs, encoding_fast.text_inputs)\n+        self.assertEqual(encoding_slow.task_inputs, encoding_fast.task_inputs)"
        }
    ],
    "stats": {
        "total": 1358,
        "additions": 1227,
        "deletions": 131
    }
}