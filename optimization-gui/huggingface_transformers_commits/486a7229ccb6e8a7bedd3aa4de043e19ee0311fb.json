{
    "author": "Cyrilvallez",
    "message": "Improve `_init_weights` tests (#43108)\n\nimprove tests",
    "sha": "486a7229ccb6e8a7bedd3aa4de043e19ee0311fb",
    "files": [
        {
            "sha": "fb725f060bf2be8dafe1fe5197dd263150cd35ff",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 36,
            "deletions": 40,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/486a7229ccb6e8a7bedd3aa4de043e19ee0311fb/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/486a7229ccb6e8a7bedd3aa4de043e19ee0311fb/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=486a7229ccb6e8a7bedd3aa4de043e19ee0311fb",
            "patch": "@@ -1145,39 +1145,42 @@ def test_can_init_all_missing_weights(self):\n             # For now, skip everything older than 2023 and \"important models\" (too much models to patch otherwise)\n             # TODO: relax this as we patch more and more models\n             if addition_year < 2023:\n-                self.skipTest(reason=f\"{model_class} is not a priorited model for now.\")\n+                self.skipTest(reason=f\"{model_class} is not a prioritized model for now.\")\n \n             # This context manager makes sure that we get the same results deterministically for random new weights\n             with seeded_weight_init():\n-                # First, initialize the model from config -> this ensure everything is correctly initialized, even if\n+                # First, initialize the model from __init__ -> this ensure everything is correctly initialized, even if\n                 # _init_weights() does not take all weights into account correctly\n-                model_from_config = model_class(copy.deepcopy(config))\n+                model_from_init = model_class(copy.deepcopy(config))\n                 # Here, passing an empty state dict will force all weights to be moved from meta to cpu, then be initialized\n                 # by _init_weights()\n                 model_from_pretrained = model_class.from_pretrained(None, config=copy.deepcopy(config), state_dict={})\n \n-            # First, check if any parameters are still on meta -> this is usually an issue with tied weights\n+            # First, check if any parameters/buffers are still on meta -> this is usually an issue with tied weights\n             params_on_meta = []\n             for k, v in model_from_pretrained.named_parameters():\n                 if v.device.type == \"meta\":\n                     params_on_meta.append(k)\n+            for k, v in model_from_pretrained.named_buffers():\n+                if v.device.type == \"meta\":\n+                    params_on_meta.append(k)\n \n             self.assertTrue(\n                 len(params_on_meta) == 0,\n-                f\"The following keys are still on the meta device, it probably comes from an issue in the tied weights:\\n{params_on_meta}\",\n+                f\"The following keys are still on the meta device, it probably comes from an issue in the tied weights or buffers:\\n{params_on_meta}\",\n             )\n \n             from_pretrained_state_dict = model_from_pretrained.state_dict()\n-            from_config_state_dict = model_from_config.state_dict()\n+            from_init_state_dict = model_from_init.state_dict()\n             self.assertEqual(\n                 sorted(from_pretrained_state_dict.keys()),\n-                sorted(from_config_state_dict.keys()),\n+                sorted(from_init_state_dict.keys()),\n                 \"The keys from each model should be the exact same\",\n             )\n \n             # Everything must be exactly the same as we set the same seed for each init\n             different_weights = set()\n-            for k1, v1 in from_config_state_dict.items():\n+            for k1, v1 in from_init_state_dict.items():\n                 # In case using torch.nn.utils.parametrizations on a module, we should skip the resulting keys\n                 if re.search(r\"\\.parametrizations\\..*?\\.original[01]\", k1):\n                     continue\n@@ -1187,27 +1190,12 @@ def test_can_init_all_missing_weights(self):\n                 if not (v1 == v2).all():\n                     different_weights.add(k1)\n \n-            # Buffers that are initialized randomly are ignored as they are not initialized on meta device anyway\n-            buffer_names = {name for name, _ in model_from_config.named_buffers()}\n-            different_weights = {k for k in different_weights if k not in buffer_names}\n-\n-            # Find the parent structure of the buffers that are different\n+            # Find the parent structure of the weights/buffers that are different for explicit error messages\n             unique_bad_module_traceback = set()\n             for weight in different_weights.copy():\n-                parent_name, weight_name = weight.rsplit(\".\", 1) if \".\" in weight else (\"\", weight)\n-                parent = model_from_config.get_submodule(parent_name)\n-                immediate_parent_class = type(parent).__name__\n-                # Go back recursively to find the first PreTrainedModel that triggered the _init_weights call\n-                while not isinstance(parent, PreTrainedModel):\n-                    parent_name = parent_name.rsplit(\".\", 1)[0] if \".\" in parent_name else \"\"\n-                    parent = model_from_config.get_submodule(parent_name)\n-                # Get the exact XXXPreTrainedModel\n-                pretrained_parent_class = next(\n-                    x.__name__ for x in type(parent).mro() if \"PreTrainedModel\" in x.__name__\n+                weight_name, immediate_parent_class, pretrained_parent_class = find_parent_traceback(\n+                    weight, model_from_init\n                 )\n-                # Some models directly inherit from `PreTrainedModel` instead of `XXXPreTrainedModel`\n-                if pretrained_parent_class == \"PreTrainedModel\":\n-                    pretrained_parent_class = type(parent).__name__\n \n                 # We cannot control timm model weights initialization, so skip in this case\n                 if (pretrained_parent_class == \"TimmWrapperPreTrainedModel\" and \"timm_model.\" in weight) or (\n@@ -1271,23 +1259,12 @@ def test_init_weights_can_init_buffers(self):\n                 if not (v1 == v2).all():\n                     different_buffers.add(k1)\n \n-            # Find the parent structure of the buffers that are different\n+            # Find the parent structure of the buffers that are different for explicit error messages\n             unique_bad_module_traceback = set()\n             for buffer in different_buffers.copy():\n-                parent_name, buf_name = buffer.rsplit(\".\", 1) if \".\" in buffer else (\"\", buffer)\n-                parent = model_from_init.get_submodule(parent_name)\n-                immediate_parent_class = type(parent).__name__\n-                # Go back recursively to find the first PreTrainedModel that triggered the _init_weights call\n-                while not isinstance(parent, PreTrainedModel):\n-                    parent_name = parent_name.rsplit(\".\", 1)[0] if \".\" in parent_name else \"\"\n-                    parent = model_from_init.get_submodule(parent_name)\n-                # Get the exact XXXPreTrainedModel\n-                pretrained_parent_class = next(\n-                    x.__name__ for x in type(parent).mro() if \"PreTrainedModel\" in x.__name__\n+                buf_name, immediate_parent_class, pretrained_parent_class = find_parent_traceback(\n+                    buffer, model_from_init\n                 )\n-                # Some models directly inherit from `PreTrainedModel` instead of `XXXPreTrainedModel`\n-                if pretrained_parent_class == \"PreTrainedModel\":\n-                    pretrained_parent_class = type(parent).__name__\n \n                 # We cannot control timm model weights initialization, so skip in this case\n                 if (pretrained_parent_class == \"TimmWrapperPreTrainedModel\" and \"timm_model.\" in buffer) or (\n@@ -4638,6 +4615,25 @@ def skip_initialize_weights(self, module):\n         PreTrainedModel._initialize_weights = original_initialize_weights\n \n \n+def find_parent_traceback(full_param_name: str, model: PreTrainedModel) -> tuple[str, str, str]:\n+    \"\"\"From a given parameter or buffer `full_param_name`, find its immediate parent class name and immediate\n+    PreTrainedModel parent class name.\"\"\"\n+    parent_name, name = full_param_name.rsplit(\".\", 1) if \".\" in full_param_name else (\"\", full_param_name)\n+    parent = model.get_submodule(parent_name)\n+    immediate_parent_class = type(parent).__name__\n+    # Go back recursively to find the first PreTrainedModel from which we inherit\n+    while not isinstance(parent, PreTrainedModel):\n+        parent_name = parent_name.rsplit(\".\", 1)[0] if \".\" in parent_name else \"\"\n+        parent = model.get_submodule(parent_name)\n+    # Get the exact XXXPreTrainedModel\n+    pretrained_parent_class = next(x.__name__ for x in type(parent).mro() if \"PreTrainedModel\" in x.__name__)\n+    # Some models directly inherit from `PreTrainedModel` instead of `XXXPreTrainedModel`\n+    if pretrained_parent_class == \"PreTrainedModel\":\n+        pretrained_parent_class = type(parent).__name__\n+\n+    return name, immediate_parent_class, pretrained_parent_class\n+\n+\n def ids_tensor(shape, vocab_size, rng=None, name=None):\n     #  Creates a random int32 tensor of the shape within the vocab size\n     if rng is None:"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 36,
        "deletions": 40
    }
}