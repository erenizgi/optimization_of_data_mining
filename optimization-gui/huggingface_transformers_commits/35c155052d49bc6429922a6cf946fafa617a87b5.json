{
    "author": "CL-ModelCloud",
    "message": "Fix PretrainedTokenizerFast check => Fix PretrainedTokenizerFast Save (#35835)\n\n* Fix the bug in tokenizer.save_pretrained when saving tokenizer_class to tokenizer_config.json\n\n* Update tokenization_utils_base.py\n\n* Update tokenization_utils_base.py\n\n* Update tokenization_utils_base.py\n\n* add tokenizer class type test\n\n* code review\n\n* code opt\n\n* fix bug\n\n* Update test_tokenization_fast.py\n\n* ruff check\n\n* make style\n\n* code opt\n\n* Update test_tokenization_fast.py\n\n---------\n\nCo-authored-by: Qubitium-ModelCloud <qubitium@modelcloud.ai>\nCo-authored-by: LRL-ModelCloud <165116337+LRL-ModelCloud@users.noreply.github.com>",
    "sha": "35c155052d49bc6429922a6cf946fafa617a87b5",
    "files": [
        {
            "sha": "fc31e72984f4228e7ed93f3656db2f408f90d8a0",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/35c155052d49bc6429922a6cf946fafa617a87b5/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/35c155052d49bc6429922a6cf946fafa617a87b5/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=35c155052d49bc6429922a6cf946fafa617a87b5",
            "patch": "@@ -2472,8 +2472,8 @@ def save_pretrained(\n \n         # Add tokenizer class to the tokenizer config to be able to reload it with from_pretrained\n         tokenizer_class = self.__class__.__name__\n-        # Remove the Fast at the end unless we have a special `PreTrainedTokenizerFast`\n-        if tokenizer_class.endswith(\"Fast\") and tokenizer_class != \"PreTrainedTokenizerFast\":\n+        # Remove the Fast at the end if we can save the slow tokenizer\n+        if tokenizer_class.endswith(\"Fast\") and getattr(self, \"can_save_slow_tokenizer\", False):\n             tokenizer_class = tokenizer_class[:-4]\n         tokenizer_config[\"tokenizer_class\"] = tokenizer_class\n         if getattr(self, \"_auto_map\", None) is not None:"
        },
        {
            "sha": "4bd9b046d406530fb283e40bfd8ed280f08ddf1a",
            "filename": "tests/tokenization/test_tokenization_fast.py",
            "status": "modified",
            "additions": 36,
            "deletions": 1,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/35c155052d49bc6429922a6cf946fafa617a87b5/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/35c155052d49bc6429922a6cf946fafa617a87b5/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_fast.py?ref=35c155052d49bc6429922a6cf946fafa617a87b5",
            "patch": "@@ -20,7 +20,7 @@\n import tempfile\n import unittest\n \n-from transformers import AutoTokenizer, PreTrainedTokenizerFast\n+from transformers import AutoTokenizer, LlamaTokenizerFast, PreTrainedTokenizerFast\n from transformers.testing_utils import require_tokenizers\n \n from ..test_tokenization_common import TokenizerTesterMixin\n@@ -170,6 +170,41 @@ def test_init_from_tokenizers_model(self):\n             # thus tok(sentences, truncation = True) does nothing and does not warn either\n             self.assertEqual(tok(sentences, truncation = True, max_length = 8), {'input_ids': [[8774, 6, 3, 63, 31, 1748, 55, 1],[ 571, 33, 25, 3, 2, 3, 58, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],[1, 1, 1, 1, 1, 1, 1, 1]]})  # fmt: skip\n \n+    def test_class_after_save_and_reload(self):\n+        # Model contains a `LlamaTokenizerFast` tokenizer with no slow fallback\n+        model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n+\n+        with tempfile.TemporaryDirectory() as temp_dir:\n+            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n+            self.assertTrue(\n+                isinstance(tokenizer, LlamaTokenizerFast),\n+                f\"Expected tokenizer(use_fast=True) type: `LlamaTokenizerFast`, actual=`{type(tokenizer)}`\",\n+            )\n+\n+            # Fast tokenizer will ignore `use_fast=False`\n+            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n+            self.assertTrue(\n+                isinstance(tokenizer, LlamaTokenizerFast),\n+                f\"Expected tokenizer type(use_fast=False): `LlamaTokenizerFast`, actual=`{type(tokenizer)}`\",\n+            )\n+\n+            # Save tokenizer\n+            tokenizer.save_pretrained(temp_dir)\n+\n+            tokenizer = AutoTokenizer.from_pretrained(temp_dir, use_fast=False)\n+            # Verify post save and reload the fast tokenizer class did not change\n+            self.assertTrue(\n+                isinstance(tokenizer, LlamaTokenizerFast),\n+                f\"Expected tokenizer type: `LlamaTokenizerFast`, actual=`{type(tokenizer)}`\",\n+            )\n+\n+            tokenizer = AutoTokenizer.from_pretrained(temp_dir, use_fast=True)\n+            # Verify post save and reload the fast tokenizer class did not change\n+            self.assertTrue(\n+                isinstance(tokenizer, LlamaTokenizerFast),\n+                f\"Expected tokenizer type: `LlamaTokenizerFast`, actual=`{type(tokenizer)}`\",\n+            )\n+\n \n @require_tokenizers\n class TokenizerVersioningTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 38,
        "deletions": 3
    }
}