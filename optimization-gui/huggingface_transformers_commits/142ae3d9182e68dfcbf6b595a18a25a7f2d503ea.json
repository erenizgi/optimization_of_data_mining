{
    "author": "Cyrilvallez",
    "message": "Fix PEFT integration with new weight loader (#42701)\n\nsimplify",
    "sha": "142ae3d9182e68dfcbf6b595a18a25a7f2d503ea",
    "files": [
        {
            "sha": "24eab78c14fc1ac68d2be4fa3033187f69fb3397",
            "filename": "src/transformers/conversion_mapping.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/142ae3d9182e68dfcbf6b595a18a25a7f2d503ea/src%2Ftransformers%2Fconversion_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/142ae3d9182e68dfcbf6b595a18a25a7f2d503ea/src%2Ftransformers%2Fconversion_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconversion_mapping.py?ref=142ae3d9182e68dfcbf6b595a18a25a7f2d503ea",
            "patch": "@@ -228,7 +228,7 @@ def get_model_conversion_mapping(\n     \"\"\"\n     weight_conversions = []\n \n-    # Load models with key mapping\n+    # Load models with explicit, user-provided key mapping\n     if key_mapping is not None:\n         weight_conversions = [WeightRenaming(source_patterns=k, target_patterns=v) for k, v in key_mapping.items()]\n     elif any("
        },
        {
            "sha": "e8519d608c6909846c1d2c09f766f15d20208a62",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 5,
            "deletions": 25,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/142ae3d9182e68dfcbf6b595a18a25a7f2d503ea/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/142ae3d9182e68dfcbf6b595a18a25a7f2d503ea/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=142ae3d9182e68dfcbf6b595a18a25a7f2d503ea",
            "patch": "@@ -17,6 +17,7 @@\n import os\n from typing import Any, Literal\n \n+from ..conversion_mapping import get_model_conversion_mapping\n from ..core_model_loading import WeightRenaming, rename_source_key\n from ..utils import (\n     CONFIG_NAME,\n@@ -46,26 +47,6 @@\n logger = logging.get_logger(__name__)\n \n \n-# DO NOT MODIFY, KEPT FOR BC ONLY\n-VLMS = [\n-    \"aria\",\n-    \"ayavision\",\n-    \"emu3\",\n-    \"fuyu\",\n-    \"gotocr2\",\n-    \"gemma3\",\n-    \"internvl\",\n-    \"llava\",  # all llava prefixed models fall under this check\n-    \"mistral3\",\n-    \"mllama\",\n-    \"paligemma\",\n-    \"qwen2vl\",\n-    \"qwen2_5_vl\",\n-    \"videollava\",\n-    \"vipllava\",\n-]\n-\n-\n class PeftAdapterMixin:\n     \"\"\"\n     A class containing all functions for loading and using adapters weights that are supported in PEFT library. For\n@@ -211,11 +192,10 @@ def load_adapter(\n             if any(conf.peft_type != PeftType.LORA for conf in self.peft_config.values()):\n                 raise ValueError(\"Hotswapping is currently only supported for LoRA, please set `hotswap=False`.\")\n \n+        key_mapping = adapter_kwargs.pop(\"key_mapping\", None) if adapter_kwargs is not None else None\n+        weight_conversions = get_model_conversion_mapping(self, key_mapping=key_mapping)\n         # peft only supports low_cpu_mem_usage starting from v0.13.0\n         peft_load_kwargs = {}\n-        key_mapping = adapter_kwargs.pop(\"key_mapping\", None) if adapter_kwargs is not None else None\n-        if key_mapping is None and any(allowed_name in self.__class__.__name__.lower() for allowed_name in VLMS):\n-            key_mapping = self._checkpoint_conversion_mapping\n         peft_load_kwargs[\"low_cpu_mem_usage\"] = low_cpu_mem_usage\n \n         adapter_name = adapter_name if adapter_name is not None else \"default\"\n@@ -292,8 +272,8 @@ def load_adapter(\n \n         # We need to pre-process the state dict to remove unneeded prefixes - for backward compatibility\n         renamings = []\n-        if key_mapping:\n-            renamings = [entry for entry in key_mapping if isinstance(entry, WeightRenaming)]\n+        if weight_conversions:\n+            renamings = [entry for entry in weight_conversions if isinstance(entry, WeightRenaming)]\n         processed_adapter_state_dict = {}\n         prefix = \"base_model.model.\"\n         state_dict = self.state_dict()"
        },
        {
            "sha": "630bae4b609992c229347653999714bde55bf6b2",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/142ae3d9182e68dfcbf6b595a18a25a7f2d503ea/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/142ae3d9182e68dfcbf6b595a18a25a7f2d503ea/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=142ae3d9182e68dfcbf6b595a18a25a7f2d503ea",
            "patch": "@@ -4046,7 +4046,7 @@ def from_pretrained(\n             hf_quantizer.postprocess_model(model, config=config)  # usually a no-op but sometimes needed\n \n         if _adapter_model_path is not None:\n-            adapter_kwargs[\"key_mapping\"] = weight_conversions  # TODO: Dynamic weight loader for adapters\n+            adapter_kwargs[\"key_mapping\"] = key_mapping\n             model.load_adapter(\n                 _adapter_model_path,\n                 adapter_name=adapter_name,"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 7,
        "deletions": 27
    }
}