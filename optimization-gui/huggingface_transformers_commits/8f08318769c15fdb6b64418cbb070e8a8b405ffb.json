{
    "author": "yao-matrix",
    "message": "enable generation fsdp/utils cases on XPU (#38009)\n\n* enable generation fsdp/utils test cases on XPU\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* xx\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* use backend_xx APIs\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>",
    "sha": "8f08318769c15fdb6b64418cbb070e8a8b405ffb",
    "files": [
        {
            "sha": "b3734d97887ef0a5ddc4ff671997a609d5ed809e",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f08318769c15fdb6b64418cbb070e8a8b405ffb/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f08318769c15fdb6b64418cbb070e8a8b405ffb/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=8f08318769c15fdb6b64418cbb070e8a8b405ffb",
            "patch": "@@ -2986,28 +2986,37 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n         \"cpu\": 0,\n         \"default\": 0,\n     }\n+    BACKEND_TORCH_ACCELERATOR_MODULE = {\n+        \"cuda\": torch.cuda,\n+        \"cpu\": None,\n+        \"default\": None,\n+    }\n else:\n     BACKEND_MANUAL_SEED = {\"default\": None}\n     BACKEND_EMPTY_CACHE = {\"default\": None}\n     BACKEND_DEVICE_COUNT = {\"default\": lambda: 0}\n     BACKEND_RESET_MAX_MEMORY_ALLOCATED = {\"default\": None}\n     BACKEND_MAX_MEMORY_ALLOCATED = {\"default\": 0}\n     BACKEND_MEMORY_ALLOCATED = {\"default\": 0}\n+    BACKEND_TORCH_ACCELERATOR_MODULE = {\"default\": None}\n \n \n if is_torch_hpu_available():\n     BACKEND_MANUAL_SEED[\"hpu\"] = torch.hpu.manual_seed\n     BACKEND_DEVICE_COUNT[\"hpu\"] = torch.hpu.device_count\n+    BACKEND_TORCH_ACCELERATOR_MODULE[\"hpu\"] = torch.hpu\n \n if is_torch_mlu_available():\n     BACKEND_EMPTY_CACHE[\"mlu\"] = torch.mlu.empty_cache\n     BACKEND_MANUAL_SEED[\"mlu\"] = torch.mlu.manual_seed\n     BACKEND_DEVICE_COUNT[\"mlu\"] = torch.mlu.device_count\n+    BACKEND_TORCH_ACCELERATOR_MODULE[\"mlu\"] = torch.mlu\n \n if is_torch_npu_available():\n     BACKEND_EMPTY_CACHE[\"npu\"] = torch.npu.empty_cache\n     BACKEND_MANUAL_SEED[\"npu\"] = torch.npu.manual_seed\n     BACKEND_DEVICE_COUNT[\"npu\"] = torch.npu.device_count\n+    BACKEND_TORCH_ACCELERATOR_MODULE[\"npu\"] = torch.npu\n \n if is_torch_xpu_available():\n     BACKEND_EMPTY_CACHE[\"xpu\"] = torch.xpu.empty_cache\n@@ -3016,6 +3025,8 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n     BACKEND_RESET_MAX_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.reset_peak_memory_stats\n     BACKEND_MAX_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.max_memory_allocated\n     BACKEND_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.memory_allocated\n+    BACKEND_TORCH_ACCELERATOR_MODULE[\"xpu\"] = torch.xpu\n+\n \n if is_torch_xla_available():\n     BACKEND_EMPTY_CACHE[\"xla\"] = torch.cuda.empty_cache\n@@ -3047,6 +3058,10 @@ def backend_memory_allocated(device: str):\n     return _device_agnostic_dispatch(device, BACKEND_MEMORY_ALLOCATED)\n \n \n+def backend_torch_accelerator_module(device: str):\n+    return _device_agnostic_dispatch(device, BACKEND_TORCH_ACCELERATOR_MODULE)\n+\n+\n if is_torch_available():\n     # If `TRANSFORMERS_TEST_DEVICE_SPEC` is enabled we need to import extra entries\n     # into device to function mappings."
        },
        {
            "sha": "90ccef3137db63c22449da75ce8946c35b6eee6f",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f08318769c15fdb6b64418cbb070e8a8b405ffb/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f08318769c15fdb6b64418cbb070e8a8b405ffb/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=8f08318769c15fdb6b64418cbb070e8a8b405ffb",
            "patch": "@@ -140,6 +140,7 @@\n     is_bitsandbytes_available,\n     is_bitsandbytes_multi_backend_available,\n     is_bs4_available,\n+    is_ccl_available,\n     is_coloredlogs_available,\n     is_compressed_tensors_available,\n     is_cv2_available,"
        },
        {
            "sha": "9ecb431573132403a528d440bf9a8f454280640a",
            "filename": "tests/generation/test_fsdp.py",
            "status": "modified",
            "additions": 19,
            "deletions": 21,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f08318769c15fdb6b64418cbb070e8a8b405ffb/tests%2Fgeneration%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f08318769c15fdb6b64418cbb070e8a8b405ffb/tests%2Fgeneration%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_fsdp.py?ref=8f08318769c15fdb6b64418cbb070e8a8b405ffb",
            "patch": "@@ -15,19 +15,29 @@\n import argparse\n from typing import Any, Callable\n \n-from transformers import is_torch_available, is_torch_mlu_available\n+from transformers import is_torch_available, is_torch_xpu_available\n from transformers.testing_utils import (\n     TestCasePlus,\n+    backend_device_count,\n+    backend_torch_accelerator_module,\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n     require_torch_multi_accelerator,\n+    torch_device,\n )\n+from transformers.utils import is_ccl_available, is_ipex_available\n \n \n if is_torch_available():\n     import functools\n \n     import torch\n+\n+    if is_torch_xpu_available():\n+        if is_ipex_available():\n+            import intel_extension_for_pytorch  # noqa: F401\n+        if is_ccl_available():\n+            import oneccl_bindings_for_pytorch  # noqa: F401\n     import torch.distributed\n     from torch.distributed._composable.fsdp import fully_shard, register_fsdp_forward_method\n     from torch.distributed.device_mesh import init_device_mesh\n@@ -46,10 +56,7 @@ def manage_process_group(func: Callable[..., Any]) -> Callable[..., Any]:\n         \"\"\"Manage the creation and destruction of the distributed process group for the wrapped function.\"\"\"\n \n         def wrapped(*args: Any, **kwargs: Any) -> Any:\n-            if is_torch_mlu_available():\n-                device_count = torch.mlu.device_count()\n-            else:\n-                device_count = torch.cuda.device_count()\n+            device_count = backend_device_count(torch_device)\n             torch.distributed.init_process_group(world_size=device_count)\n             try:\n                 return func(*args, **kwargs)\n@@ -60,10 +67,8 @@ def wrapped(*args: Any, **kwargs: Any) -> Any:\n \n     @manage_process_group\n     def fsdp_generate():\n-        if is_torch_mlu_available():\n-            torch.mlu.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n-        else:\n-            torch.cuda.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n+        torch_accelerator_module = backend_torch_accelerator_module(torch_device)\n+        torch_accelerator_module.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n \n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(device)\n \n@@ -86,10 +91,8 @@ def fsdp_generate():\n \n     @manage_process_group\n     def fsdp2_generate():\n-        if is_torch_mlu_available():\n-            torch.mlu.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n-        else:\n-            torch.cuda.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n+        torch_accelerator_module = backend_torch_accelerator_module(torch_device)\n+        torch_accelerator_module.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n \n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(device)\n \n@@ -114,10 +117,7 @@ def fsdp2_generate():\n class TestFSDPGeneration(TestCasePlus):\n     @require_torch_multi_accelerator\n     def test_fsdp_generate(self):\n-        if is_torch_mlu_available():\n-            device_count = torch.mlu.device_count()\n-        else:\n-            device_count = torch.cuda.device_count()\n+        device_count = backend_device_count(torch_device)\n         distributed_args = f\"\"\"--nproc_per_node={device_count}\n             --master_port={get_torch_dist_unique_port()}\n             {self.test_file_dir}/test_fsdp.py\n@@ -129,10 +129,8 @@ def test_fsdp_generate(self):\n \n     @require_torch_multi_accelerator\n     def test_fsdp2_generate(self):\n-        if is_torch_mlu_available():\n-            device_count = torch.mlu.device_count()\n-        else:\n-            device_count = torch.cuda.device_count()\n+        device_count = backend_device_count(torch_device)\n+\n         distributed_args = f\"\"\"--nproc_per_node={device_count}\n             --master_port={get_torch_dist_unique_port()}\n             {self.test_file_dir}/test_fsdp.py"
        },
        {
            "sha": "3807c84dade09047214062f62283a76672b3f8ee",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f08318769c15fdb6b64418cbb070e8a8b405ffb/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f08318769c15fdb6b64418cbb070e8a8b405ffb/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=8f08318769c15fdb6b64418cbb070e8a8b405ffb",
            "patch": "@@ -41,7 +41,6 @@\n     require_torch_gpu,\n     require_torch_greater_or_equal,\n     require_torch_multi_accelerator,\n-    require_torch_multi_gpu,\n     require_torch_sdpa,\n     set_config_for_less_flaky_test,\n     set_model_for_less_flaky_test,\n@@ -2954,7 +2953,7 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwa\n     def test_stop_sequence_stopping_criteria(self):\n         prompt = \"\"\"Hello I believe in\"\"\"\n         generator = pipeline(\"text-generation\", model=\"hf-internal-testing/tiny-random-bart\")\n-        output = generator(prompt)\n+        output = generator(prompt, max_new_tokens=10)\n         self.assertEqual(\n             output,\n             [{\"generated_text\": (\"Hello I believe in we we we we we we we we we\")}],\n@@ -3860,7 +3859,7 @@ def test_return_unprocessed_logit_scores(self):\n \n     @slow\n     @require_torch_multi_accelerator\n-    def test_assisted_decoding_in_different_gpu(self):\n+    def test_assisted_decoding_in_different_accelerator(self):\n         device_0 = f\"{torch_device}:0\" if torch_device != \"cpu\" else \"cpu\"\n         device_1 = f\"{torch_device}:1\" if torch_device != \"cpu\" else \"cpu\"\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(device_0)\n@@ -3885,7 +3884,7 @@ def test_assisted_decoding_in_different_gpu(self):\n \n     @slow\n     @require_torch_accelerator\n-    def test_assisted_decoding_model_in_gpu_assistant_in_cpu(self):\n+    def test_assisted_decoding_model_in_accelerator_assistant_in_cpu(self):\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\n             torch_device\n         )\n@@ -3970,10 +3969,10 @@ def test_speculative_decoding_equals_regular_decoding(self):\n         self.assertTrue((expected_out == predicted_out).all().item())\n \n     @pytest.mark.generate\n-    @require_torch_multi_gpu\n-    def test_generate_with_static_cache_multi_gpu(self):\n+    @require_torch_multi_accelerator\n+    def test_generate_with_static_cache_multi_accelerator(self):\n         \"\"\"\n-        Tests if the static cache has been set correctly and if generate works correctly when we are using multi-gpus.\n+        Tests if the static cache has been set correctly and if generate works correctly when we are using multi-acceleratorss.\n         \"\"\"\n         # need to split manually as auto doesn't work well with unbalanced model\n         device_map = {\"model.embed_tokens\": 0, \"model.layers.0\": 0, \"model.layers.1\": 1, \"model.norm\": 1, \"lm_head\": 0}\n@@ -4005,10 +4004,10 @@ def test_generate_with_static_cache_multi_gpu(self):\n         self.assertTrue(key_cache_1.device == value_cache_1.device == torch.device(1))\n \n     @pytest.mark.generate\n-    @require_torch_multi_gpu\n-    def test_generate_multi_gpu_causal_mask(self):\n+    @require_torch_multi_accelerator\n+    def test_generate_multi_accelerator_causal_mask(self):\n         \"\"\"\n-        Tests that cache position device doesn't clash with causal mask device when we are using multi-gpus.\n+        Tests that cache position device doesn't clash with causal mask device when we are using multi-accelerators.\n         In real life happens only when multimodal encoder size is big, so `embed_tokens` gets allocated to the next device.\n         The error will be triggered whenever a bacthed input is used, so that `causal_mask` is actually prepared instead of\n         being `None`.\n@@ -4033,10 +4032,10 @@ def test_generate_multi_gpu_causal_mask(self):\n         _ = model.generate(**inputs, max_new_tokens=20)\n \n     @pytest.mark.generate\n-    @require_torch_multi_gpu\n-    def test_init_static_cache_multi_gpu(self):\n+    @require_torch_multi_accelerator\n+    def test_init_static_cache_multi_accelerator(self):\n         \"\"\"\n-        Tests if the static cache has been set correctly when we initialize it manually in a multi-gpu setup.\n+        Tests if the static cache has been set correctly when we initialize it manually in a multi-accelerator setup.\n         \"\"\"\n         # need to split manually as auto doesn't work well with unbalanced model\n         device_map = {\"model.embed_tokens\": 0, \"model.layers.0\": 0, \"model.layers.1\": 1, \"model.norm\": 1, \"lm_head\": 0}\n@@ -4870,7 +4869,7 @@ def test_generate_vision2text_conditioning(self):\n \n     @require_read_token\n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_cache_device_map_with_vision_layer_device_map(self):\n         \"\"\"\n         Test that the cache device map is correctly set when the vision layer has a device map. Regression test for"
        }
    ],
    "stats": {
        "total": 83,
        "additions": 48,
        "deletions": 35
    }
}