{
    "author": "simonJJJ",
    "message": "simple align qwen2vl kv_seq_len calculation with qwen2 (#33161)\n\n* qwen2vl_align_kv_seqlen_to_qwen2\r\n\r\n* flash att test\r\n\r\n* [run-slow] qwen2_vl\r\n\r\n* [run-slow] qwen2_vl fix OOM\r\n\r\n* [run-slow] qwen2_vl\r\n\r\n* Update tests/models/qwen2_vl/test_modeling_qwen2_vl.py\r\n\r\nCo-authored-by: Raushan Turganbay <raushan.turganbay@alumni.nu.edu.kz>\r\n\r\n* Update tests/models/qwen2_vl/test_modeling_qwen2_vl.py\r\n\r\nCo-authored-by: Raushan Turganbay <raushan.turganbay@alumni.nu.edu.kz>\r\n\r\n* code quality\r\n\r\n---------\r\n\r\nCo-authored-by: baishuai.bs <1051314669@qq.com>\r\nCo-authored-by: ShuaiBai623 <baishuai623@icloud.com>\r\nCo-authored-by: ShuaiBai623 <43326198+ShuaiBai623@users.noreply.github.com>\r\nCo-authored-by: Raushan Turganbay <raushan.turganbay@alumni.nu.edu.kz>",
    "sha": "21fac7abba2a37fae86106f87fcf9974fd1e3830",
    "files": [
        {
            "sha": "5e7919a95a7dce56e9199549613f3a245e0df006",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 18,
            "deletions": 4,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/21fac7abba2a37fae86106f87fcf9974fd1e3830/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21fac7abba2a37fae86106f87fcf9974fd1e3830/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=21fac7abba2a37fae86106f87fcf9974fd1e3830",
            "patch": "@@ -550,8 +550,13 @@ def forward(\n \n         kv_seq_len = key_states.shape[-2]\n         if past_key_value is not None:\n-            kv_seq_len += cache_position[0] + 1\n-\n+            if self.layer_idx is None:\n+                raise ValueError(\n+                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n+                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n+                    \"with a layer index.\"\n+                )\n+            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n         cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n         query_states, key_states = apply_multimodal_rotary_pos_emb(\n             query_states, key_states, cos, sin, position_ids, self.rope_scaling[\"mrope_section\"]\n@@ -632,10 +637,19 @@ def forward(\n \n         kv_seq_len = key_states.shape[-2]\n         if past_key_value is not None:\n-            kv_seq_len += cache_position[0] + 1\n+            if self.layer_idx is None:\n+                raise ValueError(\n+                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n+                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n+                    \"with a layer index.\"\n+                )\n+            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n \n         # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        rotary_seq_len = cache_position[-1]\n+        rotary_seq_len = (\n+            max(kv_seq_len, position_ids[:, -1].max().item() + 1) if position_ids is not None else kv_seq_len\n+        )\n+\n         cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len)\n \n         query_states, key_states = apply_multimodal_rotary_pos_emb("
        },
        {
            "sha": "536e0ab54abc45a8758d849c4e027c1ad902a943",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 98,
            "deletions": 51,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/21fac7abba2a37fae86106f87fcf9974fd1e3830/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21fac7abba2a37fae86106f87fcf9974fd1e3830/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=21fac7abba2a37fae86106f87fcf9974fd1e3830",
            "patch": "@@ -27,8 +27,9 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n-    require_bitsandbytes,\n+    require_flash_attn,\n     require_torch,\n+    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -311,19 +312,17 @@ def setUp(self):\n                 ],\n             }\n         ]\n-        url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n+        url = \"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/demo_small.jpg\"\n         self.image = Image.open(requests.get(url, stream=True).raw)\n \n     def tearDown(self):\n         gc.collect()\n         torch.cuda.empty_cache()\n \n     @slow\n-    @require_bitsandbytes\n     def test_small_model_integration_test(self):\n         model = Qwen2VLForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2-VL-7B-Instruct\",\n-            load_in_4bit=True,\n+            \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n         )\n \n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n@@ -334,33 +333,34 @@ def test_small_model_integration_test(self):\n \n         expected_pixel_slice = torch.tensor(\n             [\n-                [0.8501, 0.8647, 0.8647],\n-                [1.0106, 1.0106, 1.0252],\n-                [0.9960, 1.0106, 1.0252],\n-                [1.0982, 1.1128, 1.1274],\n-                [1.0836, 1.0982, 1.0982],\n-                [1.1858, 1.1858, 1.1858],\n+                [0.8792, 0.8792, 0.9084],\n+                [1.1858, 1.1858, 1.2296],\n+                [1.2004, 1.2004, 1.2150],\n+                [1.4340, 1.4340, 1.4194],\n+                [1.3902, 1.4048, 1.4194],\n+                [1.5216, 1.5362, 1.5362],\n             ],\n             dtype=torch.float32,\n             device=\"cpu\",\n         )\n-        assert torch.allclose(expected_pixel_slice, inputs.pixel_values[:6, :3], atol=1e-3)\n+        assert torch.allclose(expected_pixel_slice, inputs.pixel_values[:6, :3], atol=3e-3)\n \n         # verify generation\n         inputs = inputs.to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=30)\n-        EXPECTED_DECODED_TEXT = \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?assistant\\nThe dog in the picture appears to be a Labrador Retriever or a similar breed. Labradors are known for their friendly and intelligent nature,\"\n+        EXPECTED_DECODED_TEXT = \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets\"\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n         )\n \n     @slow\n-    @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n-        model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", load_in_4bit=True)\n+        model = Qwen2VLForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n         inputs = self.processor(text=[text, text], images=[self.image, self.image], return_tensors=\"pt\").to(\n             torch_device\n@@ -370,78 +370,125 @@ def test_small_model_integration_test_batch(self):\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?assistant\\nThe dog in the picture appears to be a Labrador Retriever or a similar breed. Labradors are known for their friendly and intelligent nature,\",\n-            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?assistant\\nThe dog in the image appears to be a Labrador Retriever or a similar breed. Labradors are known for their friendly and outgoing nature,\",\n-        ]\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular choices',\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets'\n+        ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n         )\n-        self.assertEqual(\n-            self.processor.batch_decode(output, skip_special_tokens=True)[0],\n-            self.processor.batch_decode(output, skip_special_tokens=True)[1],\n-        )\n \n     @slow\n-    @require_bitsandbytes\n     def test_small_model_integration_test_batch_wo_image(self):\n-        model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", load_in_4bit=True)\n+        model = Qwen2VLForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n         messages2 = [\n             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n             {\"role\": \"user\", \"content\": \"Who are you?\"},\n         ]\n         text2 = self.processor.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)\n-        inputs = self.processor(text=[text, text2], images=[self.image], return_tensors=\"pt\").to(torch_device)\n+        inputs = self.processor(text=[text, text2], images=[self.image], padding=True, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=30)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets',\n+            'system\\nYou are a helpful assistant.\\nuser\\nWho are you?\\nassistant\\nI am Qwen, a large language model created by Alibaba Cloud. I am designed to assist with various tasks and answer questions to the best of my'\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    def test_small_model_integration_test_batch_different_resolutions(self):\n+        model = Qwen2VLForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        text2 = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        image2 = self.image.resize((224, 224))\n+        inputs = self.processor(text=[text, text2], images=[self.image, image2], padding=True, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n \n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?assistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and outgoing personalities, as well as their\",\n-            \"system\\nYou are a helpful assistant.user\\nWho are you?assistant\\nI am Qwen, a large language model created by Alibaba Cloud. I am designed to assist with various tasks and answer a wide range of questions to\",\n+            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets\",\n+            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets\",\n+        ]\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_flash_attn\n+    @require_torch_gpu\n+    def test_small_model_integration_test_batch_flashatt2(self):\n+        model = Qwen2VLForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2-VL-7B-Instruct\",\n+            torch_dtype=torch.bfloat16,\n+            attn_implementation=\"flash_attention_2\",\n+            device_map=\"auto\",\n+        )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        inputs = self.processor(text=[text, text], images=[self.image, self.image], return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=30)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets\",\n+            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets\",\n         ]\n \n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n         )\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True)[0],\n+            self.processor.batch_decode(output, skip_special_tokens=True)[1],\n+        )\n \n     @slow\n-    @require_bitsandbytes\n-    def test_small_model_integration_test_batch_different_resolutions(self):\n-        model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", load_in_4bit=True)\n-        text, vision_infos = self.processor.apply_chat_template(\n-            self.messages, tokenize=False, add_generation_prompt=True\n+    @require_flash_attn\n+    @require_torch_gpu\n+    def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n+        model = Qwen2VLForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2-VL-7B-Instruct\",\n+            torch_dtype=torch.bfloat16,\n+            attn_implementation=\"flash_attention_2\",\n+            device_map=\"auto\",\n         )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n         messages2 = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\n-                        \"type\": \"image\",\n-                        \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n-                        \"resized_height\": 504,\n-                        \"resized_width\": 252,\n-                    },\n-                    {\"type\": \"text\", \"text\": \"What kind of dog is this?\"},\n-                ],\n-            }\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\"role\": \"user\", \"content\": \"Who are you?\"},\n         ]\n-        text2, vision_infos2 = self.processor.apply_chat_template(\n-            messages2, tokenize=False, add_generation_prompt=True\n+        text2 = self.processor.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)\n+        inputs = self.processor(text=[text, text2], images=[self.image], padding=True, return_tensors=\"pt\").to(\n+            torch_device\n         )\n-        inputs = self.processor(\n-            text=[text, text2], vision_infos=[vision_infos, vision_infos2], return_tensors=\"pt\"\n-        ).to(torch_device)\n \n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?assistant\\nThe dog in the picture appears to be a Labrador Retriever or a similar breed. Labradors are known for their friendly and intelligent nature,\",\n-            \"system\\nYou are a helpful assistant.\\nuser\\nWho are you?assistant\\nI am a large language model created by Alibaba Cloud. I am called Qwen.\",\n+            \"system\\nYou are a helpful assistant.\\nuser\\nWhat kind of dog is this?\\nassistant\\nThe dog in the picture appears to be a Labrador Retriever. Labradors are known for their friendly and intelligent nature, making them popular pets\",\n+            \"system\\nYou are a helpful assistant.\\nuser\\nWho are you?\\nassistant\\nI am Qwen, a large language model created by Alibaba Cloud. I am designed to answer a wide range of questions and provide information on various topics\",\n         ]\n+\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,"
        }
    ],
    "stats": {
        "total": 171,
        "additions": 116,
        "deletions": 55
    }
}