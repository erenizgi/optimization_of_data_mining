{
    "author": "cyyever",
    "message": "Remove repeated import (#40937)\n\n* Remove repeated import\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix conflict\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "c9939b3ab6f8dda4dea8a29a479755659c36ac96",
    "files": [
        {
            "sha": "368135e15f1593f0e63928734b6056469314e329",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=c9939b3ab6f8dda4dea8a29a479755659c36ac96",
            "patch": "@@ -737,8 +737,6 @@ def get_generator(self, seed):\n \n             return torch.Generator().manual_seed(seed)\n         else:\n-            import numpy as np\n-\n             return np.random.default_rng(seed)\n \n     def create_rng(self):"
        },
        {
            "sha": "267b1be82fd19ad07cb9b913da76a4d151382612",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=c9939b3ab6f8dda4dea8a29a479755659c36ac96",
            "patch": "@@ -544,8 +544,6 @@ def run_hp_search_sigopt(trainer, n_trials: int, direction: str, **kwargs) -> Be\n \n \n def run_hp_search_wandb(trainer, n_trials: int, direction: str, **kwargs) -> BestRun:\n-    from ..integrations import is_wandb_available\n-\n     if not is_wandb_available():\n         raise ImportError(\"This function needs wandb installed: `pip install wandb`\")\n     import wandb"
        },
        {
            "sha": "5855acd09a7e6f4f372210f326702440e7ac5506",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=c9939b3ab6f8dda4dea8a29a479755659c36ac96",
            "patch": "@@ -1103,8 +1103,6 @@ def distribute_model(model, distributed_config, device_mesh, tp_size):\n                 raise ValueError(f\"Unsupported tensor parallel style {v}. Supported styles are {ALL_PARALLEL_STYLES}\")\n         for name, module in model.named_modules():\n             if not getattr(module, \"_is_hooked\", False):\n-                from transformers.integrations.tensor_parallel import add_tensor_parallel_hooks_to_module\n-\n                 plan = _get_parameter_tp_plan(parameter_name=name, tp_plan=model_plan, is_weight=False)\n                 add_tensor_parallel_hooks_to_module(\n                     model=model,"
        },
        {
            "sha": "1096a990b8e372326ee8a19938b0965d8ca44a1c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c9939b3ab6f8dda4dea8a29a479755659c36ac96",
            "patch": "@@ -2242,8 +2242,6 @@ def tp_plan(self, plan: dict[str, str]):\n                                         flexible_matched = True\n                                         break\n                             if not flexible_matched:\n-                                import warnings\n-\n                                 warnings.warn(\n                                     f\"Layer pattern '{layer_pattern}' does not match any parameters in the model. \"\n                                     f\"This rule may not be applied during tensor parallelization.\""
        },
        {
            "sha": "21209042192afcb4b53c838518fee8bb7e8e6f55",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=c9939b3ab6f8dda4dea8a29a479755659c36ac96",
            "patch": "@@ -15,6 +15,7 @@\n import ast\n import collections\n import contextlib\n+import copy\n import doctest\n import functools\n import gc\n@@ -2752,8 +2753,6 @@ def wrapper(*args, **kwargs):\n         else:\n             test = \" \".join(os.environ.get(\"PYTEST_CURRENT_TEST\").split(\" \")[:-1])\n             try:\n-                import copy\n-\n                 env = copy.deepcopy(os.environ)\n                 env[\"_INSIDE_SUB_PROCESS\"] = \"1\"\n                 # This prevents the entries in `short test summary info` given by the subprocess being truncated. so the"
        },
        {
            "sha": "d056fc0e877874e784550ce0d00b281010e4e8bd",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=c9939b3ab6f8dda4dea8a29a479755659c36ac96",
            "patch": "@@ -1084,7 +1084,6 @@ def get_checkpoint_shard_files(\n     For the description of each arg, see [`PreTrainedModel.from_pretrained`]. `index_filename` is the full path to the\n     index (downloaded and cached if `pretrained_model_name_or_path` is a model ID on the Hub).\n     \"\"\"\n-    import json\n \n     use_auth_token = deprecated_kwargs.pop(\"use_auth_token\", None)\n     if use_auth_token is not None:"
        },
        {
            "sha": "3703ddaca1fbe46227a4e4b1b296275330db1442",
            "filename": "src/transformers/utils/metrics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Futils%2Fmetrics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9939b3ab6f8dda4dea8a29a479755659c36ac96/src%2Ftransformers%2Futils%2Fmetrics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fmetrics.py?ref=c9939b3ab6f8dda4dea8a29a479755659c36ac96",
            "patch": "@@ -105,8 +105,6 @@ def decorator(func):\n         if not _has_opentelemetry:\n             return func\n \n-        import functools\n-\n         @functools.wraps(func)\n         def wrapper(*args, **kwargs):\n             instance = args[0] if args and (hasattr(func, \"__self__\") and func.__self__ is not None) else None"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 1,
        "deletions": 13
    }
}