{
    "author": "gante",
    "message": "[qwen2 audio] remove redundant code and update docs (#36282)",
    "sha": "957b05b4136a965bf31a90c62450ff9f9a9c8ef9",
    "files": [
        {
            "sha": "e53f94b10eb111e526869dd7bd84461b91763b95",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/957b05b4136a965bf31a90c62450ff9f9a9c8ef9/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/957b05b4136a965bf31a90c62450ff9f9a9c8ef9/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=957b05b4136a965bf31a90c62450ff9f9a9c8ef9",
            "patch": "@@ -29,7 +29,7 @@ The Qwen2-Audio is the new model series of large audio-language models from the\n * voice chat: users can freely engage in voice interactions with Qwen2-Audio without text input\n * audio analysis: users could provide audio and text instructions for analysis during the interaction\n \n-It was proposed in [Qwen2-Audio Technical Report](https://arxiv.org/abs/2407.10759) by Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou. \n+It was proposed in [Qwen2-Audio Technical Report](https://arxiv.org/abs/2407.10759) by Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou.\n \n The abstract from the paper is the following:\n \n@@ -100,7 +100,7 @@ for message in conversation:\n         for ele in message[\"content\"]:\n             if ele[\"type\"] == \"audio\":\n                 audios.append(librosa.load(\n-                    BytesIO(urlopen(ele['audio_url']).read()), \n+                    BytesIO(urlopen(ele['audio_url']).read()),\n                     sr=processor.feature_extractor.sampling_rate)[0]\n                 )\n \n@@ -125,7 +125,7 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n \n conversation = [\n-    {'role': 'system', 'content': 'You are a helpful assistant.'}, \n+    {'role': 'system', 'content': 'You are a helpful assistant.'},\n     {\"role\": \"user\", \"content\": [\n         {\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n         {\"type\": \"text\", \"text\": \"What's that sound?\"},\n@@ -148,7 +148,7 @@ for message in conversation:\n             if ele[\"type\"] == \"audio\":\n                 audios.append(\n                     librosa.load(\n-                        BytesIO(urlopen(ele['audio_url']).read()), \n+                        BytesIO(urlopen(ele['audio_url']).read()),\n                         sr=processor.feature_extractor.sampling_rate)[0]\n                 )\n \n@@ -203,7 +203,7 @@ for conversation in conversations:\n                 if ele[\"type\"] == \"audio\":\n                     audios.append(\n                         librosa.load(\n-                            BytesIO(urlopen(ele['audio_url']).read()), \n+                            BytesIO(urlopen(ele['audio_url']).read()),\n                             sr=processor.feature_extractor.sampling_rate)[0]\n                     )\n \n@@ -221,14 +221,19 @@ response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_\n \n [[autodoc]] Qwen2AudioConfig\n \n-## Qwen2AudioConfig\n+## Qwen2AudioEncoderConfig\n \n [[autodoc]] Qwen2AudioEncoderConfig\n \n ## Qwen2AudioProcessor\n \n [[autodoc]] Qwen2AudioProcessor\n \n+## Qwen2AudioEncoder\n+\n+[[autodoc]] Qwen2AudioEncoder\n+    - forward\n+\n ## Qwen2AudioForConditionalGeneration\n \n [[autodoc]] Qwen2AudioForConditionalGeneration"
        },
        {
            "sha": "a013d0225cabe448931460a34e03b45e25cf6b55",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 47,
            "deletions": 195,
            "changes": 242,
            "blob_url": "https://github.com/huggingface/transformers/blob/957b05b4136a965bf31a90c62450ff9f9a9c8ef9/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/957b05b4136a965bf31a90c62450ff9f9a9c8ef9/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=957b05b4136a965bf31a90c62450ff9f9a9c8ef9",
            "patch": "@@ -16,14 +16,14 @@\n \n import math\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import PreTrainedModel\n@@ -35,6 +35,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_qwen2_audio import Qwen2AudioConfig, Qwen2AudioEncoderConfig\n \n@@ -58,12 +59,15 @@ class Qwen2AudioCausalLMOutputWithPast(ModelOutput):\n             Language modeling loss (for next-token prediction).\n         logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n             Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n+        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Pre-computed hidden-states that can be used to speed up auto-regressive (sequential) decoding. There are\n+            two sets of pre-computed hidden-states: key and values states in the self-attention blocks.\n+            The `past_key_values` are returned when `use_cache=True` is passed or when `config.use_cache=True`.\n+            It is a [`~cache_utils.Cache`] instance.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those\n+            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n+            all `input_ids` of shape `(batch_size, sequence_length)`.\n         hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n             Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n             one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n@@ -81,16 +85,16 @@ class Qwen2AudioCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: torch.FloatTensor = None\n-    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     attention_mask: Optional[torch.FloatTensor] = None\n \n \n-# Copied from transformers.models.whisper.modeling_whisper.WhisperAttention with Whisper->Qwen2Audio\n class Qwen2AudioAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n+    # Copied from transformers.models.whisper.modeling_whisper.WhisperAttention.__init__ with Whisper->Qwen2Audio\n     def __init__(\n         self,\n         embed_dim: int,\n@@ -135,50 +139,27 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n+    @deprecate_kwarg(\"key_value_states\", version=\"4.52\")\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.52\")\n+    @deprecate_kwarg(\"cache_position\", version=\"4.52\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n         query_states = self._shape(self.q_proj(hidden_states) * self.scaling, tgt_len, bsz)\n-\n-        if past_key_value is not None:\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n-            if is_cross_attention:\n-                # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                past_key_value.is_updated[self.layer_idx] = True\n-                past_key_value = past_key_value.cross_attention_cache\n-            else:\n-                past_key_value = past_key_value.self_attention_cache\n-\n-        # use key_value_states if cross attention\n-        current_states = key_value_states if key_value_states is not None else hidden_states\n-        if is_cross_attention and past_key_value and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value.key_cache[self.layer_idx]\n-            value_states = past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self._shape(self.k_proj(current_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(current_states), -1, bsz)\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n+        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n \n         attn_weights = torch.matmul(query_states, key_states.transpose(2, 3))\n \n@@ -212,17 +193,17 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights, None\n \n \n-# Copied from transformers.models.whisper.modeling_whisper.WhisperFlashAttention2 with Whisper->Qwen2Audio\n class Qwen2AudioFlashAttention2(Qwen2AudioAttention):\n     \"\"\"\n     Qwen2Audio flash attention module. This module inherits from `Qwen2AudioAttention` as the weights of the module stays\n     untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n+    # Copied from transformers.models.whisper.modeling_whisper.WhisperFlashAttention2.__init__ with Whisper->Qwen2Audio\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n@@ -231,57 +212,29 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n+    @deprecate_kwarg(\"key_value_states\", version=\"4.52\")\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.52\")\n+    @deprecate_kwarg(\"cache_position\", version=\"4.52\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"The `static` cache implementation is not compatible with `attn_implementation='flash_attention_2'`. \"\n-                \"Use `attn_implementation='sdpa'` in the meantime, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n         # Qwen2AudioFlashAttention2 attention does not support output_attentions\n         if output_attentions:\n             raise ValueError(\"Qwen2AudioFlashAttention2 attention does not support output_attentions\")\n \n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n         query_states = torch.reshape(self.q_proj(hidden_states), (bsz, tgt_len, self.num_heads, self.head_dim))\n-\n-        if past_key_value is not None:\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n-            if is_cross_attention:\n-                # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                past_key_value.is_updated[self.layer_idx] = True\n-                past_key_value = past_key_value.cross_attention_cache\n-            else:\n-                past_key_value = past_key_value.self_attention_cache\n-\n-        # use key_value_states if cross attention\n-        current_states = key_value_states if key_value_states is not None else hidden_states\n-        if is_cross_attention and past_key_value and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value.key_cache[self.layer_idx]\n-            value_states = past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self._shape(self.k_proj(current_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(current_states), -1, bsz)\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n+        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n \n         # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]\n         #  We would need to refactor the KV cache to be able to avoid many of these transpose/reshape/view.\n@@ -335,16 +288,18 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights, None\n \n \n-# Copied from transformers.models.whisper.modeling_whisper.WhisperSdpaAttention with Whisper->Qwen2Audio\n class Qwen2AudioSdpaAttention(Qwen2AudioAttention):\n+    @deprecate_kwarg(\"key_value_states\", version=\"4.52\")\n+    @deprecate_kwarg(\"past_key_value\", version=\"4.52\")\n+    @deprecate_kwarg(\"cache_position\", version=\"4.52\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n@@ -359,46 +314,17 @@ def forward(\n             )\n             return super().forward(\n                 hidden_states,\n-                key_value_states=key_value_states,\n-                past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n                 layer_head_mask=layer_head_mask,\n                 output_attentions=output_attentions,\n-                cache_position=cache_position,\n             )\n \n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n         query_states = self._shape(self.q_proj(hidden_states), tgt_len, bsz)\n-\n-        if past_key_value is not None:\n-            is_updated = past_key_value.is_updated.get(self.layer_idx)\n-            if is_cross_attention:\n-                # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                past_key_value.is_updated[self.layer_idx] = True\n-                past_key_value = past_key_value.cross_attention_cache\n-            else:\n-                past_key_value = past_key_value.self_attention_cache\n-\n-        # use key_value_states if cross attention\n-        current_states = key_value_states if key_value_states is not None else hidden_states\n-        if is_cross_attention and past_key_value and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value.key_cache[self.layer_idx]\n-            value_states = past_key_value.value_cache[self.layer_idx]\n-        else:\n-            key_states = self._shape(self.k_proj(current_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(current_states), -1, bsz)\n-            if past_key_value is not None:\n-                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_states, value_states = past_key_value.update(\n-                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n+        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n \n         causal_mask = attention_mask\n         if attention_mask is not None:  # no matter the length, we just slice it\n@@ -434,7 +360,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None, None\n \n \n QWEN2AUDIO_ATTENTION_CLASSES = {\n@@ -815,16 +741,15 @@ def forward(self, audio_features):\n         position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Pre-computed hidden-states that can be used to speed up auto-regressive (sequential) decoding. There are\n+            two sets of pre-computed hidden-states: key and values states in the self-attention blocks.\n+            The `past_key_values` are returned when `use_cache=True` is passed or when `config.use_cache=True`.\n+            It is a [`~cache_utils.Cache`] instance.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those\n+            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n+            all `input_ids` of shape `(batch_size, sequence_length)`.shape `(batch_size, 1)` instead of all\n             `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n@@ -851,7 +776,7 @@ def forward(self, audio_features):\n class Qwen2AudioForConditionalGeneration(Qwen2AudioPreTrainedModel, GenerationMixin):\n     def __init__(self, config: Qwen2AudioConfig):\n         super().__init__(config)\n-        self.audio_tower = AutoModel.from_config(config.audio_config)\n+        self.audio_tower = AutoModel.from_config(config.audio_config)  # Usually a `Qwen2AudioEncoder` instance\n \n         self.multi_modal_projector = Qwen2AudioMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n@@ -1103,7 +1028,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         feature_attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1258,78 +1183,5 @@ def forward(\n             attention_mask=attention_mask,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        inputs_embeds=None,\n-        input_features=None,\n-        attention_mask=None,\n-        **kwargs,\n-    ):\n-        # Overwritten -- custom processing (note: might not be needed, but there are no generation tests running atm)\n-\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, Cache):\n-                cache_length = past_key_values.get_seq_length()\n-                past_length = past_key_values.seen_tokens\n-            else:\n-                cache_length = past_length = past_key_values[0][0].shape[2]\n-\n-            # Here, we get the attention_mask, which was previously stored in the state after _merge_input_ids_with_audio_features.\n-            if input_features is not None and kwargs.get(\"attention_mask\") is not None:\n-                attention_mask = kwargs[\"attention_mask\"]\n-                attention_mask = torch.cat(\n-                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n-                )\n-\n-            # Keep only the unprocessed tokens:\n-            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n-            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n-            # input)\n-            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n-                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n-            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n-            # input_ids based on the past_length.\n-            elif past_length < input_ids.shape[1]:\n-                input_ids = input_ids[:, past_length:]\n-            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n-            elif self.config.audio_token_index in input_ids:\n-                input_ids = input_ids[:, input_ids.shape[1] - 1 :]\n-            # If the cache has seen more tokens than it can hold, then the cache has a size limit. Let's discard the\n-            # older attention values, as their corresponding values are not part of the input.\n-            if cache_length < past_length and attention_mask is not None:\n-                attention_mask = attention_mask[:, -(cache_length + input_ids.shape[1]) :]\n-\n-        position_ids = kwargs.get(\"position_ids\", None)\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_key_values is None:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n-\n-        feature_attention_mask = kwargs.get(\"feature_attention_mask\", None)\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-                \"input_features\": input_features,\n-                \"feature_attention_mask\": feature_attention_mask,\n-            }\n-        )\n-        return model_inputs\n-\n-    def _reorder_cache(self, *args, **kwargs):\n-        return self.language_model._reorder_cache(*args, **kwargs)\n-\n \n __all__ = [\"Qwen2AudioForConditionalGeneration\", \"Qwen2AudioPreTrainedModel\", \"Qwen2AudioEncoder\"]"
        }
    ],
    "stats": {
        "total": 259,
        "additions": 58,
        "deletions": 201
    }
}