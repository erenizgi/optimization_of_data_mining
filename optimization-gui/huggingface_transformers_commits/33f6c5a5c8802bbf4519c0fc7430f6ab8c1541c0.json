{
    "author": "yao-matrix",
    "message": "enable several cases on XPU (#37516)\n\n* enable several cases on XPU\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* Update tests/test_modeling_common.py\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0",
    "files": [
        {
            "sha": "d34128ba067e98ca9ac1e318fc24136f0314c22e",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0",
            "patch": "@@ -22,6 +22,7 @@\n     require_bitsandbytes,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     slow,\n     torch_device,\n@@ -517,7 +518,7 @@ def test_batched_generation(self):\n \n         self.assertListEqual(out, EXPECTED_OUTPUT)\n \n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_training_kernel(self):\n         model_id = \"tiiuae/falcon-mamba-7b\"\n "
        },
        {
            "sha": "721ec7ea3f6cf6c2cb6e50f2e8f4ab99deae4578",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0",
            "patch": "@@ -35,7 +35,6 @@\n     require_peft,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -422,7 +421,7 @@ def test_delete_adapter(self):\n                 self.assertNotIn(\"adapter_1\", model.peft_config)\n                 self.assertIn(\"adapter_2\", model.peft_config)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     def test_peft_from_pretrained_kwargs(self):\n         \"\"\""
        },
        {
            "sha": "d234dd408a521a3a0035467dd70fdad3759c9456",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0",
            "patch": "@@ -24,6 +24,7 @@\n     require_intel_extension_for_pytorch,\n     require_torch_accelerator,\n     require_torch_gpu,\n+    require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     slow,\n     torch_device,\n@@ -202,6 +203,7 @@ def test_quantized_model_bf16(self):\n         output = quantized_model.generate(**input_ids, max_new_tokens=40)\n         self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT_BF16)\n \n+    @require_torch_gpu\n     def test_quantized_model_exllama(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with exllama backend\n@@ -240,7 +242,7 @@ def test_save_pretrained(self):\n             output = model.generate(**input_ids, max_new_tokens=40)\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_quantized_model_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple GPUs\n@@ -275,7 +277,7 @@ def test_quantized_model_no_k_proj_quantized(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_auto_awq\n @require_accelerate\n class AwqFusedTest(unittest.TestCase):"
        },
        {
            "sha": "3e360c05f0a1bdbef433f6a8c362f94e98c1c56e",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=33f6c5a5c8802bbf4519c0fc7430f6ab8c1541c0",
            "patch": "@@ -3825,7 +3825,7 @@ def test_eager_matches_sdpa_inference(\n                     )\n \n     @require_torch_sdpa\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_sdpa_can_dispatch_on_flash(self):\n         if not self.has_attentions:\n@@ -3836,8 +3836,8 @@ def test_sdpa_can_dispatch_on_flash(self):\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n         elif device_type == \"rocm\" and major < 9:\n             self.skipTest(reason=\"This test requires an AMD GPU with compute capability >= 9.0\")\n-        else:\n-            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU\")\n+        elif device_type not in [\"cuda\", \"rocm\", \"xpu\"]:\n+            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU, or an Intel XPU\")\n \n         torch.compiler.reset()\n "
        }
    ],
    "stats": {
        "total": 18,
        "additions": 10,
        "deletions": 8
    }
}