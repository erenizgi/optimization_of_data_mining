{
    "author": "kho",
    "message": "Add inputs_to_logits_ratio to LasrCTCConfig (#42720)\n\n* Add inputs_to_logits_ratio to LasrCTCConfig\n\n* changes\n\n* nit\n\n* update\n\n* Add an _align_to property to unify stride computation in AutomaticSpeechRecognition pipeline\n\n---------\n\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>",
    "sha": "65dc261512cbdb1ee72b88ae5b222f2605aad8e5",
    "files": [
        {
            "sha": "41f09ad9e00fe46f94065b49bd8dbdc254926a87",
            "filename": "src/transformers/models/lasr/configuration_lasr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/65dc261512cbdb1ee72b88ae5b222f2605aad8e5/src%2Ftransformers%2Fmodels%2Flasr%2Fconfiguration_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65dc261512cbdb1ee72b88ae5b222f2605aad8e5/src%2Ftransformers%2Fmodels%2Flasr%2Fconfiguration_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fconfiguration_lasr.py?ref=65dc261512cbdb1ee72b88ae5b222f2605aad8e5",
            "patch": "@@ -240,5 +240,9 @@ def from_encoder_config(cls, encoder_config: LasrEncoderConfig, **kwargs):\n \n         return cls(encoder_config=encoder_config.to_dict(), **kwargs)\n \n+    @property\n+    def inputs_to_logits_ratio(self):\n+        return self.encoder_config.subsampling_conv_stride**2\n+\n \n __all__ = [\"LasrEncoderConfig\", \"LasrCTCConfig\"]"
        },
        {
            "sha": "be4e1465370a31f88f6dd615371208bfde88c72c",
            "filename": "src/transformers/models/lasr/modular_lasr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/65dc261512cbdb1ee72b88ae5b222f2605aad8e5/src%2Ftransformers%2Fmodels%2Flasr%2Fmodular_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65dc261512cbdb1ee72b88ae5b222f2605aad8e5/src%2Ftransformers%2Fmodels%2Flasr%2Fmodular_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fmodular_lasr.py?ref=65dc261512cbdb1ee72b88ae5b222f2605aad8e5",
            "patch": "@@ -291,6 +291,10 @@ def __init__(\n             **kwargs,\n         )\n \n+    @property\n+    def inputs_to_logits_ratio(self):\n+        return self.encoder_config.subsampling_conv_stride**2\n+\n \n class LasrEncoderSubsampling(nn.Module):\n     def __init__(self, config: LasrEncoderConfig):"
        },
        {
            "sha": "8e6f8b5cafcddf179cf4ee62ad3c9a50ebecea79",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/65dc261512cbdb1ee72b88ae5b222f2605aad8e5/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65dc261512cbdb1ee72b88ae5b222f2605aad8e5/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=65dc261512cbdb1ee72b88ae5b222f2605aad8e5",
            "patch": "@@ -350,6 +350,20 @@ def _sanitize_parameters(\n \n         return preprocess_params, forward_params, postprocess_params\n \n+    @property\n+    def _align_to(self):\n+        \"\"\"Sample stride per output.\"\"\"\n+        # XXX: Carefully, this variable will not exist in `seq2seq` setting.\n+        # Currently chunking is not possible at this level for `seq2seq` so\n+        # it's ok.\n+        align_to = getattr(self.model.config, \"inputs_to_logits_ratio\", 1)\n+        if self.model.config.model_type == \"lasr_ctc\":\n+            # TODO: find a standard for that but not easy because input length -> mel length depends on the feature extractor\n+            # specific way of doing it\n+            # means the model take mel features as input, we align according to the hop length\n+            align_to *= self.feature_extractor.hop_length\n+        return align_to\n+\n     def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):\n         if isinstance(inputs, str):\n             if inputs.startswith(\"http://\") or inputs.startswith(\"https://\"):\n@@ -444,10 +458,7 @@ def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):\n             if isinstance(stride_length_s, (int, float)):\n                 stride_length_s = [stride_length_s, stride_length_s]\n \n-            # XXX: Carefully, this variable will not exist in `seq2seq` setting.\n-            # Currently chunking is not possible at this level for `seq2seq` so\n-            # it's ok.\n-            align_to = getattr(self.model.config, \"inputs_to_logits_ratio\", 1)\n+            align_to = self._align_to\n             chunk_len = int(round(chunk_length_s * self.feature_extractor.sampling_rate / align_to) * align_to)\n             stride_left = int(round(stride_length_s[0] * self.feature_extractor.sampling_rate / align_to) * align_to)\n             stride_right = int(round(stride_length_s[1] * self.feature_extractor.sampling_rate / align_to) * align_to)\n@@ -567,7 +578,7 @@ def _forward(self, model_inputs, return_timestamps=False, **generate_kwargs):\n                 # Send stride to `postprocess`.\n                 # it needs to be handled there where\n                 # the pieces are to be concatenated.\n-                ratio = 1 / self.model.config.inputs_to_logits_ratio\n+                ratio = 1 / self._align_to\n                 if isinstance(stride, tuple):\n                     out[\"stride\"] = rescale_stride([stride], ratio)[0]\n                 else:\n@@ -650,11 +661,12 @@ def postprocess(\n \n         if return_timestamps and self.type not in {\"seq2seq\", \"seq2seq_whisper\"}:\n             chunks = []\n+            align_to = self._align_to\n             for item in offsets:\n-                start = item[\"start_offset\"] * self.model.config.inputs_to_logits_ratio\n+                start = item[\"start_offset\"] * align_to\n                 start /= self.feature_extractor.sampling_rate\n \n-                stop = item[\"end_offset\"] * self.model.config.inputs_to_logits_ratio\n+                stop = item[\"end_offset\"] * align_to\n                 stop /= self.feature_extractor.sampling_rate\n \n                 chunks.append({\"text\": item[return_timestamps], \"timestamp\": (start, stop)})"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 27,
        "deletions": 7
    }
}