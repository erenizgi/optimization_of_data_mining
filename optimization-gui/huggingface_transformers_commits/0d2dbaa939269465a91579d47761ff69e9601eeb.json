{
    "author": "Cyrilvallez",
    "message": "Do not use global variable, and improve context manager coverage (#42957)\n\nrefactor",
    "sha": "0d2dbaa939269465a91579d47761ff69e9601eeb",
    "files": [
        {
            "sha": "9905b4b49bd7c697f95d12363daa12c9e29d60cb",
            "filename": "src/transformers/initialization.py",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d2dbaa939269465a91579d47761ff69e9601eeb/src%2Ftransformers%2Finitialization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d2dbaa939269465a91579d47761ff69e9601eeb/src%2Ftransformers%2Finitialization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Finitialization.py?ref=0d2dbaa939269465a91579d47761ff69e9601eeb",
            "patch": "@@ -206,3 +206,40 @@ def guard_torch_init_functions():\n         for module, functions in originals.items():\n             for func_name, func in functions.items():\n                 setattr(module, func_name, func)\n+\n+\n+@contextmanager\n+def no_init_weights():\n+    \"\"\"\n+    Disable weight initialization both at the torch-level, and at the transformers-level (`init_weights`).\n+    This is used to speed-up initializing an empty model with deepspeed, as we do not initialize the model on meta device\n+    with deepspeed, but we still don't need to run expensive weight initializations as we are loading params afterwards.\n+    \"\"\"\n+    from .modeling_utils import PreTrainedModel\n+\n+    def empty_func(*args, **kwargs):\n+        pass\n+\n+    originals = defaultdict(dict)\n+    try:\n+        # Replace all torch funcs by empty ones\n+        for module_name in TORCH_MODULES_TO_PATCH:\n+            if module_name in sys.modules:\n+                module = sys.modules[module_name]\n+                for func_name in TORCH_INIT_FUNCTIONS.keys():\n+                    if hasattr(module, func_name):\n+                        originals[module][func_name] = getattr(module, func_name)\n+                        setattr(module, func_name, empty_func)\n+\n+        # Also patch our own `init_weights`\n+        original_init_weights = PreTrainedModel.init_weights\n+        PreTrainedModel.init_weights = empty_func\n+\n+        yield\n+    finally:\n+        # Set back the original torch functions on all modules\n+        for module, functions in originals.items():\n+            for func_name, func in functions.items():\n+                setattr(module, func_name, func)\n+        # Set back `init_weights`\n+        PreTrainedModel.init_weights = original_init_weights"
        },
        {
            "sha": "79b3108f59737c43035f11b86b1ce564b67d4fb0",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 53,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d2dbaa939269465a91579d47761ff69e9601eeb/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d2dbaa939269465a91579d47761ff69e9601eeb/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0d2dbaa939269465a91579d47761ff69e9601eeb",
            "patch": "@@ -151,7 +151,6 @@\n XLA_USE_BF16 = os.environ.get(\"XLA_USE_BF16\", \"0\").upper()\n XLA_DOWNCAST_BF16 = os.environ.get(\"XLA_DOWNCAST_BF16\", \"0\").upper()\n SpecificPreTrainedModelType = TypeVar(\"SpecificPreTrainedModelType\", bound=\"PreTrainedModel\")\n-_init_weights = True\n _is_quantized = False\n _is_ds_init_called = False\n \n@@ -170,51 +169,6 @@ def is_local_dist_rank_0():\n     )\n \n \n-TORCH_INIT_FUNCTIONS = {\n-    \"uniform_\": nn.init.uniform_,\n-    \"normal_\": nn.init.normal_,\n-    \"trunc_normal_\": nn.init.trunc_normal_,\n-    \"constant_\": nn.init.constant_,\n-    \"xavier_uniform_\": nn.init.xavier_uniform_,\n-    \"xavier_normal_\": nn.init.xavier_normal_,\n-    \"kaiming_uniform_\": nn.init.kaiming_uniform_,\n-    \"kaiming_normal_\": nn.init.kaiming_normal_,\n-    \"uniform\": nn.init.uniform,\n-    \"normal\": nn.init.normal,\n-    \"xavier_uniform\": nn.init.xavier_uniform,\n-    \"xavier_normal\": nn.init.xavier_normal,\n-    \"kaiming_uniform\": nn.init.kaiming_uniform,\n-    \"kaiming_normal\": nn.init.kaiming_normal,\n-    \"orthogonal_\": nn.init.orthogonal_,\n-}\n-\n-\n-@contextmanager\n-def no_init_weights():\n-    \"\"\"\n-    Context manager to globally disable weight initialization to speed up loading large models.\n-    \"\"\"\n-    global _init_weights\n-    old_init_weights = _init_weights\n-\n-    _init_weights = False\n-\n-    def _skip_init(*args, **kwargs):\n-        pass\n-\n-    # Save the original initialization functions\n-    for name, init_func in TORCH_INIT_FUNCTIONS.items():\n-        setattr(torch.nn.init, name, _skip_init)\n-\n-    try:\n-        yield\n-    finally:\n-        _init_weights = old_init_weights\n-        # Restore the original initialization functions\n-        for name, init_func in TORCH_INIT_FUNCTIONS.items():\n-            setattr(torch.nn.init, name, init_func)\n-\n-\n @contextmanager\n def set_quantized_state():\n     global _is_quantized\n@@ -2987,12 +2941,12 @@ def init_weights(self):\n         Maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any\n         initialization logic in `_init_weights`.\n         \"\"\"\n-        if not _init_weights or get_torch_context_manager_or_global_device() == torch.device(\"meta\"):\n-            return\n-        # Initialize weights\n-        self.initialize_weights()\n-        # Tie weights needs to be called here, but it can use the pre-computed `all_tied_weights_keys`\n-        self.tie_weights(recompute_mapping=False)\n+        # If we are initializing on meta device, there is no point in trying to run inits\n+        if get_torch_context_manager_or_global_device() != torch.device(\"meta\"):\n+            # Initialize weights\n+            self.initialize_weights()\n+            # Tie weights needs to be called here, but it can use the pre-computed `all_tied_weights_keys`\n+            self.tie_weights(recompute_mapping=False)\n \n     def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n         \"\"\"\n@@ -3537,7 +3491,11 @@ def get_init_context(cls, dtype: torch.dtype, is_quantized: bool, _is_ds_init_ca\n             if not is_quantized and not _is_ds_init_called:\n                 logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n                 init_contexts.extend(\n-                    [no_init_weights(), deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()]\n+                    [\n+                        init.no_init_weights(),\n+                        deepspeed.zero.Init(config_dict_or_path=deepspeed_config()),\n+                        set_zero3_state(),\n+                    ]\n                 )\n             elif is_quantized:\n                 init_contexts.extend([torch.device(\"meta\"), set_quantized_state()])"
        }
    ],
    "stats": {
        "total": 101,
        "additions": 48,
        "deletions": 53
    }
}