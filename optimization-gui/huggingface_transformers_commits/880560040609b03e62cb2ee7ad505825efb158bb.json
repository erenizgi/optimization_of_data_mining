{
    "author": "zucchini-nlp",
    "message": "[qwen3] fix generation tests (#37142)\n\n* do not skip tests\n\n* fix qwen3-moe as well\n\n* fixup\n\n* fixup",
    "sha": "880560040609b03e62cb2ee7ad505825efb158bb",
    "files": [
        {
            "sha": "c682fe86b4d33b4268e27602afc127562ebe3252",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=880560040609b03e62cb2ee7ad505825efb158bb",
            "patch": "@@ -352,7 +352,6 @@ def test_torch_fx_output_loss(self):\n \n     def test_Mistral_sequence_classification_model(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        print(config)\n         config.num_labels = 3\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(1).to(torch_device)"
        },
        {
            "sha": "481f94425c466ff27973e93d212a048719a871b2",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=880560040609b03e62cb2ee7ad505825efb158bb",
            "patch": "@@ -351,7 +351,6 @@ def test_torch_fx_output_loss(self):\n \n     def test_Mixtral_sequence_classification_model(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        print(config)\n         config.num_labels = 3\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(1).to(torch_device)"
        },
        {
            "sha": "2757ba30a8e876c6fed80eed219b27441d806ee1",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=880560040609b03e62cb2ee7ad505825efb158bb",
            "patch": "@@ -363,7 +363,6 @@ def test_torch_fx_output_loss(self):\n \n     def test_Qwen2_sequence_classification_model(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        print(config)\n         config.num_labels = 3\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(1).to(torch_device)"
        },
        {
            "sha": "c1e2daee81e786bf9d0ba4fe7ac479ef6c35e299",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=880560040609b03e62cb2ee7ad505825efb158bb",
            "patch": "@@ -391,7 +391,6 @@ def test_torch_fx_output_loss(self):\n \n     def test_Qwen2Moe_sequence_classification_model(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        print(config)\n         config.num_labels = 3\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(1).to(torch_device)"
        },
        {
            "sha": "672bf51b4d60beed5669aec27357e723b87a8b7c",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 41,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=880560040609b03e62cb2ee7ad505825efb158bb",
            "patch": "@@ -62,7 +62,7 @@ def __init__(\n         use_token_type_ids=True,\n         use_labels=True,\n         vocab_size=99,\n-        hidden_size=32,\n+        hidden_size=64,\n         num_hidden_layers=5,\n         max_window_layers=3,\n         use_sliding_window=True,\n@@ -348,42 +348,6 @@ def setUp(self):\n         self.model_tester = Qwen3ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Qwen3Config, hidden_size=37)\n \n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_beam_search_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_assisted_decoding_matches_greedy_search_0_random(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_assisted_decoding_matches_greedy_search_1_same(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_greedy_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_generate_compilation_all_outputs(self):\n-        pass\n-\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n@@ -402,7 +366,6 @@ def test_torch_fx_output_loss(self):\n \n     def test_Qwen3_sequence_classification_model(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        print(config)\n         config.num_labels = 3\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(1).to(torch_device)\n@@ -461,9 +424,9 @@ def test_Qwen3_token_classification_model(self):\n     def test_save_load_fast_init_from_base(self):\n         pass\n \n-    @unittest.skip(reason=\"Qwen3 uses GQA on all models so the KV cache is a non standard format\")\n+    # Ignore copy\n     def test_past_key_values_format(self):\n-        pass\n+        super().test_past_key_values_format()\n \n     @require_flash_attn\n     @require_torch_gpu\n@@ -487,7 +450,6 @@ def test_model_600m_logits(self):\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([5.9062, 6.0938, 5.5625, 3.8594, 2.6094, 1.9531, 4.3125, 4.9375, 3.8906, 3.1094, 3.6719, 5.1562, 6.9062, 5.7500, 5.4062, 7.0625, 8.7500, 8.7500, 8.1250, 7.9375, 8.0625, 7.5312, 7.3750, 7.2188, 7.2500, 5.8750, 2.8750, 4.3438, 2.3438, 2.2500])  # fmt: skip\n-        print(out[0, 0, :30])\n         torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n         del model"
        },
        {
            "sha": "3c237b7ae1495779656400066e199e87a00a7b6d",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 37,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/880560040609b03e62cb2ee7ad505825efb158bb/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=880560040609b03e62cb2ee7ad505825efb158bb",
            "patch": "@@ -60,7 +60,7 @@ def __init__(\n         use_token_type_ids=True,\n         use_labels=True,\n         vocab_size=99,\n-        hidden_size=32,\n+        hidden_size=64,\n         num_hidden_layers=5,\n         max_window_layers=3,\n         use_sliding_window=True,\n@@ -367,38 +367,6 @@ def setUp(self):\n         self.model_tester = Qwen3MoeModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Qwen3MoeConfig, hidden_size=37)\n \n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_beam_search_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_assisted_decoding_matches_greedy_search_0_random(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_contrastive_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_assisted_decoding_matches_greedy_search_1_same(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_greedy_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(\"TODO: ask the contributor to take a look\")\n-    def test_prompt_lookup_decoding_matches_greedy_search(self):\n-        pass\n-\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n@@ -417,7 +385,6 @@ def test_torch_fx_output_loss(self):\n \n     def test_Qwen3Moe_sequence_classification_model(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        print(config)\n         config.num_labels = 3\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(1).to(torch_device)\n@@ -476,9 +443,9 @@ def test_Qwen3Moe_token_classification_model(self):\n     def test_save_load_fast_init_from_base(self):\n         pass\n \n-    @unittest.skip(reason=\"Qwen3Moe uses GQA on all models so the KV cache is a non standard format\")\n+    # Ignore copy\n     def test_past_key_values_format(self):\n-        pass\n+        super().test_past_key_values_format()\n \n     @require_flash_attn\n     @require_torch_gpu\n@@ -539,7 +506,6 @@ def test_model_15b_a2b_logits(self):\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([7.5938, 2.6094, 4.0312, 4.0938, 2.5156, 2.7812, 2.9688, 1.5547, 1.3984, 2.2344, 3.0156, 3.1562, 1.1953, 3.2500, 1.0938, 8.4375, 9.5625, 9.0625, 7.5625, 7.5625, 7.9062, 7.2188, 7.0312, 6.9375, 8.0625, 1.7266, 0.9141, 3.7969, 5.3438, 3.9844])  # fmt: skip\n-        print(out[0, 0, :30])\n         torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n         del model"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 6,
        "deletions": 82
    }
}