{
    "author": "Rocketknight1",
    "message": ":rotating_light: :rotating_light: Fix custom code saving (#37716)\n\n* Firstly: Better detection of when we're a custom class\n\n* Trigger tests\n\n* Let's break everything\n\n* make fixup\n\n* fix mistaken line doubling\n\n* Let's try to get rid of it from config classes at least\n\n* Let's try to get rid of it from config classes at least\n\n* Fixup image processor\n\n* no more circular import\n\n* Let's go back to setting `_auto_class` again\n\n* Let's go back to setting `_auto_class` again\n\n* stash commit\n\n* Revert the irrelevant changes until we figure out AutoConfig\n\n* Change tests since we're breaking expectations\n\n* make fixup\n\n* do the same for all custom classes\n\n* Cleanup for feature extractor tests\n\n* Cleanup tokenization tests too\n\n* typo\n\n* Fix tokenizer tests\n\n* make fixup\n\n* fix image processor test\n\n* make fixup\n\n* Remove warning from register_for_auto_class\n\n* Stop adding model info to auto map entirely\n\n* Remove todo\n\n* Remove the other todo\n\n* Let's start slapping _auto_class on models why not\n\n* Let's start slapping _auto_class on models why not\n\n* Make sure the tests know what's up\n\n* Make sure the tests know what's up\n\n* Completely remove add_model_info_to_*\n\n* Start adding _auto_class to models\n\n* Start adding _auto_class to models\n\n* Add a flaky decorator\n\n* Add a flaky decorator and import\n\n* stash commit\n\n* More message cleanup\n\n* make fixup\n\n* fix indent\n\n* Fix trust_remote_code prompts\n\n* make fixup\n\n* correct indentation\n\n* Reincorporate changes into dynamic_module_utils\n\n* Update call to trust_remote_code\n\n* make fixup\n\n* Fix video processors too\n\n* Fix video processors too\n\n* Remove is_flaky additions\n\n* make fixup",
    "sha": "ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
    "files": [
        {
            "sha": "205a7dde8f28048fb81dc668f8896604be4a3dd0",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -28,8 +28,6 @@\n from .utils import (\n     CONFIG_NAME,\n     PushToHubMixin,\n-    add_model_info_to_auto_map,\n-    add_model_info_to_custom_pipelines,\n     cached_file,\n     copy_func,\n     download_url,\n@@ -713,15 +711,6 @@ def _get_config_dict(\n         else:\n             logger.info(f\"loading configuration file {configuration_file} from cache at {resolved_config_file}\")\n \n-        if \"auto_map\" in config_dict and not is_local:\n-            config_dict[\"auto_map\"] = add_model_info_to_auto_map(\n-                config_dict[\"auto_map\"], pretrained_model_name_or_path\n-            )\n-        if \"custom_pipelines\" in config_dict and not is_local:\n-            config_dict[\"custom_pipelines\"] = add_model_info_to_custom_pipelines(\n-                config_dict[\"custom_pipelines\"], pretrained_model_name_or_path\n-            )\n-\n         # timm models are not saved with the model_type in the config file\n         if \"model_type\" not in config_dict and is_timm_config_dict(config_dict):\n             config_dict[\"model_type\"] = \"timm_wrapper\"\n@@ -1044,11 +1033,7 @@ def register_for_auto_class(cls, auto_class=\"AutoConfig\"):\n         Register this class with a given auto class. This should only be used for custom configurations as the ones in\n         the library are already mapped with `AutoConfig`.\n \n-        <Tip warning={true}>\n-\n-        This API is experimental and may have some slight breaking changes in the next releases.\n \n-        </Tip>\n \n         Args:\n             auto_class (`str` or `type`, *optional*, defaults to `\"AutoConfig\"`):"
        },
        {
            "sha": "660d0ac6d8d88f17b66bb6411be977f89a4b9b49",
            "filename": "src/transformers/dynamic_module_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 6,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fdynamic_module_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fdynamic_module_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdynamic_module_utils.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -667,7 +667,9 @@ def _raise_timeout_error(signum, frame):\n TIME_OUT_REMOTE_CODE = 15\n \n \n-def resolve_trust_remote_code(trust_remote_code, model_name, has_local_code, has_remote_code, error_message=None):\n+def resolve_trust_remote_code(\n+    trust_remote_code, model_name, has_local_code, has_remote_code, error_message=None, upstream_repo=None\n+):\n     \"\"\"\n     Resolves the `trust_remote_code` argument. If there is remote code to be loaded, the user must opt-in to loading\n     it.\n@@ -688,11 +690,25 @@ def resolve_trust_remote_code(trust_remote_code, model_name, has_local_code, has\n     Returns:\n         The resolved `trust_remote_code` value.\n     \"\"\"\n-    # Originally, `trust_remote_code` was used to load models with custom code.\n-    error_message = (\n-        error_message\n-        or f\"The repository `{model_name}` contains custom code which must be executed to correctly load the model.\"\n-    )\n+    if error_message is None:\n+        if upstream_repo is not None:\n+            error_message = (\n+                f\"The repository {model_name} references custom code contained in {upstream_repo} which \"\n+                f\"must be executed to correctly load the model. You can inspect the repository \"\n+                f\"content at https://hf.co/{upstream_repo} .\\n\"\n+            )\n+        elif os.path.isdir(model_name):\n+            error_message = (\n+                f\"The repository {model_name} contains custom code which must be executed \"\n+                f\"to correctly load the model. You can inspect the repository \"\n+                f\"content at {os.path.abspath(model_name)} .\\n\"\n+            )\n+        else:\n+            error_message = (\n+                f\"The repository {model_name} contains custom code which must be executed \"\n+                f\"to correctly load the model. You can inspect the repository \"\n+                f\"content at https://hf.co/{model_name} .\\n\"\n+            )\n \n     if trust_remote_code is None:\n         if has_local_code:"
        },
        {
            "sha": "51e882aefa83ede1663825895fa4d9d273d340ef",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -29,8 +29,6 @@\n     FEATURE_EXTRACTOR_NAME,\n     PushToHubMixin,\n     TensorType,\n-    add_model_info_to_auto_map,\n-    add_model_info_to_custom_pipelines,\n     cached_file,\n     copy_func,\n     download_url,\n@@ -551,16 +549,6 @@ def get_feature_extractor_dict(\n                 f\"loading configuration file {feature_extractor_file} from cache at {resolved_feature_extractor_file}\"\n             )\n \n-        if not is_local:\n-            if \"auto_map\" in feature_extractor_dict:\n-                feature_extractor_dict[\"auto_map\"] = add_model_info_to_auto_map(\n-                    feature_extractor_dict[\"auto_map\"], pretrained_model_name_or_path\n-                )\n-            if \"custom_pipelines\" in feature_extractor_dict:\n-                feature_extractor_dict[\"custom_pipelines\"] = add_model_info_to_custom_pipelines(\n-                    feature_extractor_dict[\"custom_pipelines\"], pretrained_model_name_or_path\n-                )\n-\n         return feature_extractor_dict, kwargs\n \n     @classmethod\n@@ -673,11 +661,7 @@ def register_for_auto_class(cls, auto_class=\"AutoFeatureExtractor\"):\n         Register this class with a given auto class. This should only be used for custom feature extractors as the ones\n         in the library are already mapped with `AutoFeatureExtractor`.\n \n-        <Tip warning={true}>\n-\n-        This API is experimental and may have some slight breaking changes in the next releases.\n \n-        </Tip>\n \n         Args:\n             auto_class (`str` or `type`, *optional*, defaults to `\"AutoFeatureExtractor\"`):"
        },
        {
            "sha": "42a6b7858418f66a1548c14ef3b574c2b3bdfb8a",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -28,8 +28,6 @@\n from .utils import (\n     IMAGE_PROCESSOR_NAME,\n     PushToHubMixin,\n-    add_model_info_to_auto_map,\n-    add_model_info_to_custom_pipelines,\n     cached_file,\n     copy_func,\n     download_url,\n@@ -380,14 +378,6 @@ def get_image_processor_dict(\n             logger.info(\n                 f\"loading configuration file {image_processor_file} from cache at {resolved_image_processor_file}\"\n             )\n-            if \"auto_map\" in image_processor_dict:\n-                image_processor_dict[\"auto_map\"] = add_model_info_to_auto_map(\n-                    image_processor_dict[\"auto_map\"], pretrained_model_name_or_path\n-                )\n-            if \"custom_pipelines\" in image_processor_dict:\n-                image_processor_dict[\"custom_pipelines\"] = add_model_info_to_custom_pipelines(\n-                    image_processor_dict[\"custom_pipelines\"], pretrained_model_name_or_path\n-                )\n \n         return image_processor_dict, kwargs\n \n@@ -508,11 +498,7 @@ def register_for_auto_class(cls, auto_class=\"AutoImageProcessor\"):\n         Register this class with a given auto class. This should only be used for custom image processors as the ones\n         in the library are already mapped with `AutoImageProcessor `.\n \n-        <Tip warning={true}>\n-\n-        This API is experimental and may have some slight breaking changes in the next releases.\n \n-        </Tip>\n \n         Args:\n             auto_class (`str` or `type`, *optional*, defaults to `\"AutoImageProcessor \"`):"
        },
        {
            "sha": "7a200bdda961afd068a6b42f5179255fc8fb5cc8",
            "filename": "src/transformers/modeling_flax_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_utils.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -1218,11 +1218,7 @@ def register_for_auto_class(cls, auto_class=\"FlaxAutoModel\"):\n         Register this class with a given auto class. This should only be used for custom models as the ones in the\n         library are already mapped with an auto class.\n \n-        <Tip warning={true}>\n \n-        This API is experimental and may have some slight breaking changes in the next releases.\n-\n-        </Tip>\n \n         Args:\n             auto_class (`str` or `type`, *optional*, defaults to `\"FlaxAutoModel\"`):"
        },
        {
            "sha": "ed7b018d89df97c325aa137d0b0ce035ee1792ef",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -3229,11 +3229,7 @@ def register_for_auto_class(cls, auto_class=\"TFAutoModel\"):\n         Register this class with a given auto class. This should only be used for custom models as the ones in the\n         library are already mapped with an auto class.\n \n-        <Tip warning={true}>\n \n-        This API is experimental and may have some slight breaking changes in the next releases.\n-\n-        </Tip>\n \n         Args:\n             auto_class (`str` or `type`, *optional*, defaults to `\"TFAutoModel\"`):"
        },
        {
            "sha": "ba02bd6c910ee3d95eb4273c734a13e1f404ce25",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -5321,11 +5321,7 @@ def register_for_auto_class(cls, auto_class=\"AutoModel\"):\n         Register this class with a given auto class. This should only be used for custom models as the ones in the\n         library are already mapped with an auto class.\n \n-        <Tip warning={true}>\n \n-        This API is experimental and may have some slight breaking changes in the next releases.\n-\n-        </Tip>\n \n         Args:\n             auto_class (`str` or `type`, *optional*, defaults to `\"AutoModel\"`):"
        },
        {
            "sha": "b32f2b711f1256f2b13cd479b3e5cbc8e2f9a16c",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 21,
            "deletions": 6,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -420,17 +420,23 @@ def from_config(cls, config, **kwargs):\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\n         has_local_code = type(config) in cls._model_mapping.keys()\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, config._name_or_path, has_local_code, has_remote_code\n-        )\n+        if has_remote_code:\n+            class_ref = config.auto_map[cls.__name__]\n+            if \"--\" in class_ref:\n+                upstream_repo = class_ref.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, config._name_or_path, has_local_code, has_remote_code, upstream_repo=upstream_repo\n+            )\n \n         if has_remote_code and trust_remote_code:\n-            class_ref = config.auto_map[cls.__name__]\n             if \"--\" in class_ref:\n                 repo_id, class_ref = class_ref.split(\"--\")\n             else:\n                 repo_id = config.name_or_path\n             model_class = get_class_from_dynamic_module(class_ref, repo_id, **kwargs)\n+            model_class.register_for_auto_class(auto_class=cls)\n             cls.register(config.__class__, model_class, exist_ok=True)\n             _ = kwargs.pop(\"code_revision\", None)\n             model_class = add_generation_mixin_to_remote_model(model_class)\n@@ -545,21 +551,30 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n \n         has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\n         has_local_code = type(config) in cls._model_mapping.keys()\n+        upstream_repo = None\n+        if has_remote_code:\n+            class_ref = config.auto_map[cls.__name__]\n+            if \"--\" in class_ref:\n+                upstream_repo = class_ref.split(\"--\")[0]\n         trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n+            trust_remote_code,\n+            pretrained_model_name_or_path,\n+            has_local_code,\n+            has_remote_code,\n+            upstream_repo=upstream_repo,\n         )\n         kwargs[\"trust_remote_code\"] = trust_remote_code\n \n         # Set the adapter kwargs\n         kwargs[\"adapter_kwargs\"] = adapter_kwargs\n \n         if has_remote_code and trust_remote_code:\n-            class_ref = config.auto_map[cls.__name__]\n             model_class = get_class_from_dynamic_module(\n                 class_ref, pretrained_model_name_or_path, code_revision=code_revision, **hub_kwargs, **kwargs\n             )\n             _ = hub_kwargs.pop(\"code_revision\", None)\n             cls.register(config.__class__, model_class, exist_ok=True)\n+            model_class.register_for_auto_class(auto_class=cls)\n             model_class = add_generation_mixin_to_remote_model(model_class)\n             return model_class.from_pretrained(\n                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs"
        },
        {
            "sha": "58a639bad3103c1f4aea236ba2ba09163786f921",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Auto Config class.\"\"\"\n \n import importlib\n-import os\n import re\n import warnings\n from collections import OrderedDict\n@@ -1155,17 +1154,21 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n         config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n         has_remote_code = \"auto_map\" in config_dict and \"AutoConfig\" in config_dict[\"auto_map\"]\n         has_local_code = \"model_type\" in config_dict and config_dict[\"model_type\"] in CONFIG_MAPPING\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n-        )\n+        if has_remote_code:\n+            class_ref = config_dict[\"auto_map\"][\"AutoConfig\"]\n+            if \"--\" in class_ref:\n+                upstream_repo = class_ref.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n+            )\n \n         if has_remote_code and trust_remote_code:\n-            class_ref = config_dict[\"auto_map\"][\"AutoConfig\"]\n             config_class = get_class_from_dynamic_module(\n                 class_ref, pretrained_model_name_or_path, code_revision=code_revision, **kwargs\n             )\n-            if os.path.isdir(pretrained_model_name_or_path):\n-                config_class.register_for_auto_class()\n+            config_class.register_for_auto_class()\n             return config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)\n         elif \"model_type\" in config_dict:\n             try:"
        },
        {
            "sha": "a0f171af2457077f1322356e65ee27f4396005e5",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -371,17 +371,21 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n         has_remote_code = feature_extractor_auto_map is not None\n         has_local_code = feature_extractor_class is not None or type(config) in FEATURE_EXTRACTOR_MAPPING\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n-        )\n+        if has_remote_code:\n+            if \"--\" in feature_extractor_auto_map:\n+                upstream_repo = feature_extractor_auto_map.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n+            )\n \n         if has_remote_code and trust_remote_code:\n             feature_extractor_class = get_class_from_dynamic_module(\n                 feature_extractor_auto_map, pretrained_model_name_or_path, **kwargs\n             )\n             _ = kwargs.pop(\"code_revision\", None)\n-            if os.path.isdir(pretrained_model_name_or_path):\n-                feature_extractor_class.register_for_auto_class()\n+            feature_extractor_class.register_for_auto_class()\n             return feature_extractor_class.from_dict(config_dict, **kwargs)\n         elif feature_extractor_class is not None:\n             return feature_extractor_class.from_dict(config_dict, **kwargs)"
        },
        {
            "sha": "739431b1ba1cb1099a74485a663a79ba73880e40",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 16,
            "deletions": 13,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -539,26 +539,29 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n \n         has_remote_code = image_processor_auto_map is not None\n         has_local_code = image_processor_class is not None or type(config) in IMAGE_PROCESSOR_MAPPING\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n-        )\n-\n-        if image_processor_auto_map is not None and not isinstance(image_processor_auto_map, tuple):\n-            # In some configs, only the slow image processor class is stored\n-            image_processor_auto_map = (image_processor_auto_map, None)\n+        if has_remote_code:\n+            if image_processor_auto_map is not None and not isinstance(image_processor_auto_map, tuple):\n+                # In some configs, only the slow image processor class is stored\n+                image_processor_auto_map = (image_processor_auto_map, None)\n+            if use_fast and image_processor_auto_map[1] is not None:\n+                class_ref = image_processor_auto_map[1]\n+            else:\n+                class_ref = image_processor_auto_map[0]\n+            if \"--\" in class_ref:\n+                upstream_repo = class_ref.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n+            )\n \n         if has_remote_code and trust_remote_code:\n             if not use_fast and image_processor_auto_map[1] is not None:\n                 _warning_fast_image_processor_available(image_processor_auto_map[1])\n \n-            if use_fast and image_processor_auto_map[1] is not None:\n-                class_ref = image_processor_auto_map[1]\n-            else:\n-                class_ref = image_processor_auto_map[0]\n             image_processor_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)\n             _ = kwargs.pop(\"code_revision\", None)\n-            if os.path.isdir(pretrained_model_name_or_path):\n-                image_processor_class.register_for_auto_class()\n+            image_processor_class.register_for_auto_class()\n             return image_processor_class.from_dict(config_dict, **kwargs)\n         elif image_processor_class is not None:\n             return image_processor_class.from_dict(config_dict, **kwargs)"
        },
        {
            "sha": "d0ca90caa5addd410f14beebbb82a610b90f58ff",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -17,7 +17,6 @@\n import importlib\n import inspect\n import json\n-import os\n import warnings\n from collections import OrderedDict\n \n@@ -358,17 +357,21 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n         has_remote_code = processor_auto_map is not None\n         has_local_code = processor_class is not None or type(config) in PROCESSOR_MAPPING\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n-        )\n+        if has_remote_code:\n+            if \"--\" in processor_auto_map:\n+                upstream_repo = processor_auto_map.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n+            )\n \n         if has_remote_code and trust_remote_code:\n             processor_class = get_class_from_dynamic_module(\n                 processor_auto_map, pretrained_model_name_or_path, **kwargs\n             )\n             _ = kwargs.pop(\"code_revision\", None)\n-            if os.path.isdir(pretrained_model_name_or_path):\n-                processor_class.register_for_auto_class()\n+            processor_class.register_for_auto_class()\n             return processor_class.from_pretrained(\n                 pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n             )"
        },
        {
            "sha": "c0d449ee4cf2a1b20f9c1d141fc084a290d955bb",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -982,19 +982,23 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                 or tokenizer_class_from_name(config_tokenizer_class + \"Fast\") is not None\n             )\n         )\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n-        )\n-\n-        if has_remote_code and trust_remote_code:\n+        if has_remote_code:\n             if use_fast and tokenizer_auto_map[1] is not None:\n                 class_ref = tokenizer_auto_map[1]\n             else:\n                 class_ref = tokenizer_auto_map[0]\n+            if \"--\" in class_ref:\n+                upstream_repo = class_ref.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n+            )\n+\n+        if has_remote_code and trust_remote_code:\n             tokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)\n             _ = kwargs.pop(\"code_revision\", None)\n-            if os.path.isdir(pretrained_model_name_or_path):\n-                tokenizer_class.register_for_auto_class()\n+            tokenizer_class.register_for_auto_class()\n             return tokenizer_class.from_pretrained(\n                 pretrained_model_name_or_path, *inputs, trust_remote_code=trust_remote_code, **kwargs\n             )"
        },
        {
            "sha": "507930df720a1da977b2a5c93f55999946ec6013",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -339,8 +339,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             class_ref = video_processor_auto_map\n             video_processor_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)\n             _ = kwargs.pop(\"code_revision\", None)\n-            if os.path.isdir(pretrained_model_name_or_path):\n-                video_processor_class.register_for_auto_class()\n+            video_processor_class.register_for_auto_class()\n             return video_processor_class.from_dict(config_dict, **kwargs)\n         elif video_processor_class is not None:\n             return video_processor_class.from_dict(config_dict, **kwargs)"
        },
        {
            "sha": "91d9fe0aab89fba9b2c2bec08e71353c03ecbae6",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -54,8 +54,6 @@\n     PROCESSOR_NAME,\n     PushToHubMixin,\n     TensorType,\n-    add_model_info_to_auto_map,\n-    add_model_info_to_custom_pipelines,\n     cached_file,\n     copy_func,\n     direct_transformers_import,\n@@ -938,16 +936,6 @@ def get_processor_dict(\n         if \"chat_template\" in kwargs:\n             processor_dict[\"chat_template\"] = kwargs.pop(\"chat_template\")\n \n-        if not is_local:\n-            if \"auto_map\" in processor_dict:\n-                processor_dict[\"auto_map\"] = add_model_info_to_auto_map(\n-                    processor_dict[\"auto_map\"], pretrained_model_name_or_path\n-                )\n-            if \"custom_pipelines\" in processor_dict:\n-                processor_dict[\"custom_pipelines\"] = add_model_info_to_custom_pipelines(\n-                    processor_dict[\"custom_pipelines\"], pretrained_model_name_or_path\n-                )\n-\n         return processor_dict, kwargs\n \n     @classmethod\n@@ -1192,11 +1180,7 @@ def register_for_auto_class(cls, auto_class=\"AutoProcessor\"):\n         Register this class with a given auto class. This should only be used for custom feature extractors as the ones\n         in the library are already mapped with `AutoProcessor`.\n \n-        <Tip warning={true}>\n \n-        This API is experimental and may have some slight breaking changes in the next releases.\n-\n-        </Tip>\n \n         Args:\n             auto_class (`str` or `type`, *optional*, defaults to `\"AutoProcessor\"`):"
        },
        {
            "sha": "560158828024b40982b191bc4f85d3f7f8e21ecb",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -43,8 +43,6 @@\n     PushToHubMixin,\n     TensorType,\n     add_end_docstrings,\n-    add_model_info_to_auto_map,\n-    add_model_info_to_custom_pipelines,\n     cached_file,\n     copy_func,\n     download_url,\n@@ -2116,13 +2114,6 @@ def _from_pretrained(\n                 # For backward compatibility with odl format.\n                 if isinstance(init_kwargs[\"auto_map\"], (tuple, list)):\n                     init_kwargs[\"auto_map\"] = {\"AutoTokenizer\": init_kwargs[\"auto_map\"]}\n-                init_kwargs[\"auto_map\"] = add_model_info_to_auto_map(\n-                    init_kwargs[\"auto_map\"], pretrained_model_name_or_path\n-                )\n-            if \"custom_pipelines\" in init_kwargs:\n-                init_kwargs[\"custom_pipelines\"] = add_model_info_to_custom_pipelines(\n-                    init_kwargs[\"custom_pipelines\"], pretrained_model_name_or_path\n-                )\n \n         if config_tokenizer_class is None:\n             # Matt: This entire block is only used to decide if the tokenizer class matches the class in the repo.\n@@ -3973,11 +3964,7 @@ def register_for_auto_class(cls, auto_class=\"AutoTokenizer\"):\n         Register this class with a given auto class. This should only be used for custom tokenizers as the ones in the\n         library are already mapped with `AutoTokenizer`.\n \n-        <Tip warning={true}>\n \n-        This API is experimental and may have some slight breaking changes in the next releases.\n-\n-        </Tip>\n \n         Args:\n             auto_class (`str` or `type`, *optional*, defaults to `\"AutoTokenizer\"`):"
        },
        {
            "sha": "f0881c290219bd5758fe0c38edd3794ae2152bfe",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -50,8 +50,6 @@\n     ModelOutput,\n     PaddingStrategy,\n     TensorType,\n-    add_model_info_to_auto_map,\n-    add_model_info_to_custom_pipelines,\n     cached_property,\n     can_return_loss,\n     can_return_tuple,"
        },
        {
            "sha": "060140f31ebd8a43caa2c9362109550909de55fb",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -727,32 +727,6 @@ def tensor_size(array):\n         raise ValueError(f\"Type not supported for tensor_size: {type(array)}.\")\n \n \n-def add_model_info_to_auto_map(auto_map, repo_id):\n-    \"\"\"\n-    Adds the information of the repo_id to a given auto map.\n-    \"\"\"\n-    for key, value in auto_map.items():\n-        if isinstance(value, (tuple, list)):\n-            auto_map[key] = [f\"{repo_id}--{v}\" if (v is not None and \"--\" not in v) else v for v in value]\n-        elif value is not None and \"--\" not in value:\n-            auto_map[key] = f\"{repo_id}--{value}\"\n-\n-    return auto_map\n-\n-\n-def add_model_info_to_custom_pipelines(custom_pipeline, repo_id):\n-    \"\"\"\n-    Adds the information of the repo_id to a given custom pipeline.\n-    \"\"\"\n-    # {custom_pipelines : {task: {\"impl\": \"path.to.task\"},...} }\n-    for task in custom_pipeline.keys():\n-        if \"impl\" in custom_pipeline[task]:\n-            module = custom_pipeline[task][\"impl\"]\n-            if \"--\" not in module:\n-                custom_pipeline[task][\"impl\"] = f\"{repo_id}--{module}\"\n-    return custom_pipeline\n-\n-\n def infer_framework(model_class):\n     \"\"\"\n     Infers the framework of a given model without using isinstance(), because we cannot guarantee that the relevant"
        },
        {
            "sha": "108dbada648097f0a3c6a9280cacaafaa8f41cbc",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -36,8 +36,6 @@\n from .utils import (\n     VIDEO_PROCESSOR_NAME,\n     TensorType,\n-    add_model_info_to_auto_map,\n-    add_model_info_to_custom_pipelines,\n     add_start_docstrings,\n     cached_file,\n     copy_func,\n@@ -629,16 +627,6 @@ def get_video_processor_dict(\n             logger.info(\n                 f\"loading configuration file {video_processor_file} from cache at {resolved_video_processor_file}\"\n             )\n-\n-        if not is_local:\n-            if \"auto_map\" in video_processor_dict:\n-                video_processor_dict[\"auto_map\"] = add_model_info_to_auto_map(\n-                    video_processor_dict[\"auto_map\"], pretrained_model_name_or_path\n-                )\n-            if \"custom_pipelines\" in video_processor_dict:\n-                video_processor_dict[\"custom_pipelines\"] = add_model_info_to_custom_pipelines(\n-                    video_processor_dict[\"custom_pipelines\"], pretrained_model_name_or_path\n-                )\n         return video_processor_dict, kwargs\n \n     @classmethod"
        },
        {
            "sha": "9751c4f13035b10e61fe35e72866db274ad6763b",
            "filename": "tests/models/auto/test_configuration_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_configuration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_configuration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_configuration_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -122,19 +122,11 @@ def test_from_pretrained_dynamic_config(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             config.save_pretrained(tmp_dir)\n             reloaded_config = AutoConfig.from_pretrained(tmp_dir, trust_remote_code=True)\n+            self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"configuration.py\")))  # Assert we saved config code\n+            # Assert we're pointing at local code and not another remote repo\n+            self.assertEqual(reloaded_config.auto_map[\"AutoConfig\"], \"configuration.NewModelConfig\")\n         self.assertEqual(reloaded_config.__class__.__name__, \"NewModelConfig\")\n \n-        # The configuration file is cached in the snapshot directory. So the module file is not changed after dumping\n-        # to a temp dir. Because the revision of the configuration file is not changed.\n-        # Test the dynamic module is loaded only once if the configuration file is not changed.\n-        self.assertIs(config.__class__, reloaded_config.__class__)\n-\n-        # Test the dynamic module is reloaded if we force it.\n-        reloaded_config = AutoConfig.from_pretrained(\n-            \"hf-internal-testing/test_dynamic_model\", trust_remote_code=True, force_download=True\n-        )\n-        self.assertIsNot(config.__class__, reloaded_config.__class__)\n-\n     def test_from_pretrained_dynamic_config_conflict(self):\n         class NewModelConfigLocal(BertConfig):\n             model_type = \"new-model\""
        },
        {
            "sha": "7858c770eb02591688ffd22d903c441220dfac46",
            "filename": "tests/models/auto/test_feature_extraction_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_feature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_feature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_feature_extraction_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import json\n+import os\n import sys\n import tempfile\n import unittest\n@@ -125,19 +126,12 @@ def test_from_pretrained_dynamic_feature_extractor(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             feature_extractor.save_pretrained(tmp_dir)\n             reloaded_feature_extractor = AutoFeatureExtractor.from_pretrained(tmp_dir, trust_remote_code=True)\n+            self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"feature_extractor.py\")))  # Assert we saved code\n+            self.assertEqual(\n+                reloaded_feature_extractor.auto_map[\"AutoFeatureExtractor\"], \"feature_extractor.NewFeatureExtractor\"\n+            )\n         self.assertEqual(reloaded_feature_extractor.__class__.__name__, \"NewFeatureExtractor\")\n \n-        # The feature extractor file is cached in the snapshot directory. So the module file is not changed after dumping\n-        # to a temp dir. Because the revision of the module file is not changed.\n-        # Test the dynamic module is loaded only once if the module file is not changed.\n-        self.assertIs(feature_extractor.__class__, reloaded_feature_extractor.__class__)\n-\n-        # Test the dynamic module is reloaded if we force it.\n-        reloaded_feature_extractor = AutoFeatureExtractor.from_pretrained(\n-            \"hf-internal-testing/test_dynamic_feature_extractor\", trust_remote_code=True, force_download=True\n-        )\n-        self.assertIsNot(feature_extractor.__class__, reloaded_feature_extractor.__class__)\n-\n     def test_new_feature_extractor_registration(self):\n         try:\n             AutoConfig.register(\"custom\", CustomConfig)"
        },
        {
            "sha": "a8f43711894a611f525edc3fd29e2893a41e8137",
            "filename": "tests/models/auto/test_image_processing_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_image_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_image_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_image_processing_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \n import json\n+import os\n import sys\n import tempfile\n import unittest\n@@ -190,13 +191,12 @@ def test_from_pretrained_dynamic_image_processor(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             image_processor.save_pretrained(tmp_dir)\n             reloaded_image_processor = AutoImageProcessor.from_pretrained(tmp_dir, trust_remote_code=True)\n+            self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"image_processor.py\")))  # Assert we saved custom code\n+            self.assertEqual(\n+                reloaded_image_processor.auto_map[\"AutoImageProcessor\"], \"image_processor.NewImageProcessor\"\n+            )\n         self.assertEqual(reloaded_image_processor.__class__.__name__, \"NewImageProcessor\")\n \n-        # The image processor file is cached in the snapshot directory. So the module file is not changed after dumping\n-        # to a temp dir. Because the revision of the module file is not changed.\n-        # Test the dynamic module is loaded only once if the module file is not changed.\n-        self.assertIs(image_processor.__class__, reloaded_image_processor.__class__)\n-\n         # Test the dynamic module is reloaded if we force it.\n         reloaded_image_processor = AutoImageProcessor.from_pretrained(\n             \"hf-internal-testing/test_dynamic_image_processor\", trust_remote_code=True, force_download=True"
        },
        {
            "sha": "cfc0191c02b3f1506ee72622c37024ff46f88623",
            "filename": "tests/models/auto/test_modeling_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -332,11 +332,6 @@ def test_from_pretrained_dynamic_model_distant(self):\n         for p1, p2 in zip(model.parameters(), reloaded_model.parameters()):\n             self.assertTrue(torch.equal(p1, p2))\n \n-        # The model file is cached in the snapshot directory. So the module file is not changed after dumping\n-        # to a temp dir. Because the revision of the module file is not changed.\n-        # Test the dynamic module is loaded only once if the module file is not changed.\n-        self.assertIs(model.__class__, reloaded_model.__class__)\n-\n         # Test the dynamic module is reloaded if we force it.\n         reloaded_model = AutoModel.from_pretrained(\n             \"hf-internal-testing/test_dynamic_model\", trust_remote_code=True, force_download=True\n@@ -362,11 +357,6 @@ def test_from_pretrained_dynamic_model_distant(self):\n         for p1, p2 in zip(model.parameters(), reloaded_model.parameters()):\n             self.assertTrue(torch.equal(p1, p2))\n \n-        # The model file is cached in the snapshot directory. So the module file is not changed after dumping\n-        # to a temp dir. Because the revision of the module file is not changed.\n-        # Test the dynamic module is loaded only once if the module file is not changed.\n-        self.assertIs(model.__class__, reloaded_model.__class__)\n-\n         # Test the dynamic module is reloaded if we force it.\n         reloaded_model = AutoModel.from_pretrained(\n             \"hf-internal-testing/test_dynamic_model_with_util\", trust_remote_code=True, force_download=True"
        },
        {
            "sha": "5d6c4254785f4449e7ecafcfa07ee14f370c5f0f",
            "filename": "tests/models/auto/test_tokenization_auto.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -342,17 +342,20 @@ def test_from_pretrained_dynamic_tokenizer(self):\n             with tempfile.TemporaryDirectory() as tmp_dir:\n                 tokenizer.save_pretrained(tmp_dir)\n                 reloaded_tokenizer = AutoTokenizer.from_pretrained(tmp_dir, trust_remote_code=True, use_fast=False)\n+                self.assertTrue(\n+                    os.path.exists(os.path.join(tmp_dir, \"tokenization.py\"))\n+                )  # Assert we saved tokenizer code\n+                self.assertEqual(reloaded_tokenizer._auto_class, \"AutoTokenizer\")\n+                with open(os.path.join(tmp_dir, \"tokenizer_config.json\"), \"r\") as f:\n+                    tokenizer_config = json.load(f)\n+                # Assert we're pointing at local code and not another remote repo\n+                self.assertEqual(tokenizer_config[\"auto_map\"][\"AutoTokenizer\"], [\"tokenization.NewTokenizer\", None])\n             self.assertEqual(reloaded_tokenizer.__class__.__name__, \"NewTokenizer\")\n             self.assertTrue(reloaded_tokenizer.special_attribute_present)\n         else:\n             self.assertEqual(tokenizer.__class__.__name__, \"NewTokenizer\")\n             self.assertEqual(reloaded_tokenizer.__class__.__name__, \"NewTokenizer\")\n \n-        # The tokenizer file is cached in the snapshot directory. So the module file is not changed after dumping\n-        # to a temp dir. Because the revision of the module file is not changed.\n-        # Test the dynamic module is loaded only once if the module file is not changed.\n-        self.assertIs(tokenizer.__class__, reloaded_tokenizer.__class__)\n-\n         # Test the dynamic module is reloaded if we force it.\n         reloaded_tokenizer = AutoTokenizer.from_pretrained(\n             \"hf-internal-testing/test_dynamic_tokenizer\", trust_remote_code=True, force_download=True"
        },
        {
            "sha": "a66ed720056ac28e2388920959f4c8c41566184e",
            "filename": "tests/models/auto/test_video_processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_video_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba6d72226d30527cd9f27d594b58fb9c5ee35aed/tests%2Fmodels%2Fauto%2Ftest_video_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_video_processing_auto.py?ref=ba6d72226d30527cd9f27d594b58fb9c5ee35aed",
            "patch": "@@ -174,17 +174,6 @@ def test_from_pretrained_dynamic_video_processor(self):\n             reloaded_video_processor = AutoVideoProcessor.from_pretrained(tmp_dir, trust_remote_code=True)\n         self.assertEqual(reloaded_video_processor.__class__.__name__, \"NewVideoProcessor\")\n \n-        # The image processor file is cached in the snapshot directory. So the module file is not changed after dumping\n-        # to a temp dir. Because the revision of the module file is not changed.\n-        # Test the dynamic module is loaded only once if the module file is not changed.\n-        self.assertIs(video_processor.__class__, reloaded_video_processor.__class__)\n-\n-        # Test the dynamic module is reloaded if we force it.\n-        reloaded_video_processor = AutoVideoProcessor.from_pretrained(\n-            \"hf-internal-testing/test_dynamic_video_processor\", trust_remote_code=True, force_download=True\n-        )\n-        self.assertIsNot(video_processor.__class__, reloaded_video_processor.__class__)\n-\n     def test_new_video_processor_registration(self):\n         try:\n             AutoConfig.register(\"custom\", CustomConfig)"
        }
    ],
    "stats": {
        "total": 351,
        "additions": 120,
        "deletions": 231
    }
}