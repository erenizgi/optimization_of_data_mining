{
    "author": "MekkCyber",
    "message": "Fix : model used to test ggml conversion of Falcon-7b is incorrect (#35083)\n\nfixing test model",
    "sha": "85eb3392318fc91a97692f23e1ce69b916567185",
    "files": [
        {
            "sha": "508975865c27afa6c383ac729c661081f45e45d1",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/85eb3392318fc91a97692f23e1ce69b916567185/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85eb3392318fc91a97692f23e1ce69b916567185/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=85eb3392318fc91a97692f23e1ce69b916567185",
            "patch": "@@ -45,7 +45,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     phi3_model_id = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n     bloom_model_id = \"afrideva/bloom-560m-GGUF\"\n     original_bloom_model_id = \"bigscience/bloom-560m\"\n-    falcon7b_model_id = \"xaviviro/falcon-7b-quantized-gguf\"\n+    falcon7b_model_id_q2 = \"xaviviro/falcon-7b-quantized-gguf\"\n+    falcon7b_model_id_fp16 = \"medmekk/falcon-7b-gguf\"\n     falcon40b_model_id = \"maddes8cht/tiiuae-falcon-40b-gguf\"\n     original_flacon7b_model_id = \"tiiuae/falcon-7b\"\n     t5_model_id = \"repetitio/flan-t5-small\"\n@@ -615,9 +616,9 @@ def test_falcon40b_q2_k(self):\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n     def test_falcon7b_q2_k(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.falcon7b_model_id, gguf_file=self.q2_k_falcon7b_model_id)\n+        tokenizer = AutoTokenizer.from_pretrained(self.falcon7b_model_id_q2, gguf_file=self.q2_k_falcon7b_model_id)\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.falcon7b_model_id,\n+            self.falcon7b_model_id_q2,\n             gguf_file=self.q2_k_falcon7b_model_id,\n             device_map=\"auto\",\n             torch_dtype=torch.float16,\n@@ -631,7 +632,7 @@ def test_falcon7b_q2_k(self):\n \n     def test_falcon7b_weights_conversion_fp16(self):\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n-            self.falcon7b_model_id,\n+            self.falcon7b_model_id_fp16,\n             gguf_file=self.fp16_falcon7b_model_id,\n             device_map=\"auto\",\n             torch_dtype=torch.float16,"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}