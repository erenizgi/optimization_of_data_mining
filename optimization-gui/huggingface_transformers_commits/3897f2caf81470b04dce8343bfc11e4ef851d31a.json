{
    "author": "ydshieh",
    "message": "Enable pytest live log and show warning logs on GitHub Actions CI runs (#35912)\n\n* fix\r\n\r\n* remove\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "3897f2caf81470b04dce8343bfc11e4ef851d31a",
    "files": [
        {
            "sha": "79a6d9e70ae84cf5322a31f4f9006b13475cadb1",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3897f2caf81470b04dce8343bfc11e4ef851d31a/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/3897f2caf81470b04dce8343bfc11e4ef851d31a/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=3897f2caf81470b04dce8343bfc11e4ef851d31a",
            "patch": "@@ -52,3 +52,5 @@ markers = [\n     \"bitsandbytes: select (or deselect with `not`) bitsandbytes integration tests\",\n     \"generate: marks tests that use the GenerationTesterMixin\"\n ]\n+log_cli = 1\n+log_cli_level = \"WARNING\"\n\\ No newline at end of file"
        },
        {
            "sha": "3b01607fd04846dd9623cc32fa0ea292aca6bd42",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3897f2caf81470b04dce8343bfc11e4ef851d31a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3897f2caf81470b04dce8343bfc11e4ef851d31a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=3897f2caf81470b04dce8343bfc11e4ef851d31a",
            "patch": "@@ -785,8 +785,7 @@ def validate(self, is_init=False):\n             for arg_name in (\"cache_implementation\", \"cache_config\", \"return_legacy_cache\"):\n                 if getattr(self, arg_name) is not None:\n                     logger.warning_once(\n-                        no_cache_warning.format(cache_arg=arg_name, cache_arg_value=getattr(self, arg_name)),\n-                        UserWarning,\n+                        no_cache_warning.format(cache_arg=arg_name, cache_arg_value=getattr(self, arg_name))\n                     )\n \n         # 6.  check watermarking arguments"
        },
        {
            "sha": "67f70b96eddcb29fb4c1978c5132622e94c5ffea",
            "filename": "src/transformers/utils/logging.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3897f2caf81470b04dce8343bfc11e4ef851d31a/src%2Ftransformers%2Futils%2Flogging.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3897f2caf81470b04dce8343bfc11e4ef851d31a/src%2Ftransformers%2Futils%2Flogging.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Flogging.py?ref=3897f2caf81470b04dce8343bfc11e4ef851d31a",
            "patch": "@@ -101,7 +101,8 @@ def _configure_library_root_logger() -> None:\n             formatter = logging.Formatter(\"[%(levelname)s|%(pathname)s:%(lineno)s] %(asctime)s >> %(message)s\")\n             _default_handler.setFormatter(formatter)\n \n-        library_root_logger.propagate = False\n+        is_ci = os.getenv(\"CI\") is not None and os.getenv(\"CI\").upper() in {\"1\", \"ON\", \"YES\", \"TRUE\"}\n+        library_root_logger.propagate = True if is_ci else False\n \n \n def _reset_library_root_logger() -> None:"
        },
        {
            "sha": "2b17b6a7c43a04072a6872401a45f9d924a9bbdd",
            "filename": "tests/generation/test_streamers.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3897f2caf81470b04dce8343bfc11e4ef851d31a/tests%2Fgeneration%2Ftest_streamers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3897f2caf81470b04dce8343bfc11e4ef851d31a/tests%2Fgeneration%2Ftest_streamers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_streamers.py?ref=3897f2caf81470b04dce8343bfc11e4ef851d31a",
            "patch": "@@ -16,6 +16,7 @@\n import unittest\n from queue import Empty\n from threading import Thread\n+from unittest.mock import patch\n \n import pytest\n \n@@ -27,6 +28,7 @@\n     is_torch_available,\n )\n from transformers.testing_utils import CaptureStdout, require_torch, torch_device\n+from transformers.utils.logging import _get_library_root_logger\n \n from ..test_modeling_common import ids_tensor\n \n@@ -102,9 +104,12 @@ def test_text_streamer_decode_kwargs(self):\n         model.config.eos_token_id = -1\n \n         input_ids = torch.ones((1, 5), device=torch_device).long() * model.config.bos_token_id\n-        with CaptureStdout() as cs:\n-            streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n-            model.generate(input_ids, max_new_tokens=1, do_sample=False, streamer=streamer)\n+\n+        root = _get_library_root_logger()\n+        with patch.object(root, \"propagate\", False):\n+            with CaptureStdout() as cs:\n+                streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n+                model.generate(input_ids, max_new_tokens=1, do_sample=False, streamer=streamer)\n \n         # The prompt contains a special token, so the streamer should not print it. As such, the output text, when\n         # re-tokenized, must only contain one token"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 13,
        "deletions": 6
    }
}