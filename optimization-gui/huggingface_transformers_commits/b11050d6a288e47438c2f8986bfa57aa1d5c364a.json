{
    "author": "yao-matrix",
    "message": "enable OffloadedCache on XPU from PyTorch 2.7 (#36654)\n\n* fix \"Cannot copy out of meta tensor; no data!\" issue for BartForConditionalGeneration model\n\n* follow Marc's suggestion to use _tie_weights to fix\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* enable OffloadedCache on XPU since PyTorch 2.7\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* don't change bart\n\nSigned-off-by: root <root@a4bf01945cfe.jf.intel.com>\n\n* make code more concise per review comments\n\nSigned-off-by: N <matrix.yao@intel.com>\n\n* fix review comments\n\nSigned-off-by: root <root@a4bf01945cfe.jf.intel.com>\n\n* Revert \"fix review comments\"\n\nThis reverts commit acf1484b86c7cc58b2dee69e7008c0eeb4c97b1b.\n\n* fix review comments\n\nSigned-off-by: root <root@a4bf01945cfe.jf.intel.com>\n\n* fix style\n\nSigned-off-by: root <root@a4bf01945cfe.jf.intel.com>\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\nSigned-off-by: root <root@a4bf01945cfe.jf.intel.com>\nSigned-off-by: N <matrix.yao@intel.com>\nCo-authored-by: root <root@a4bf01945cfe.jf.intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "b11050d6a288e47438c2f8986bfa57aa1d5c364a",
    "files": [
        {
            "sha": "558bcfb2e280252e489d66b9c199ebdfaedcf6a9",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 8,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/b11050d6a288e47438c2f8986bfa57aa1d5c364a/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b11050d6a288e47438c2f8986bfa57aa1d5c364a/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=b11050d6a288e47438c2f8986bfa57aa1d5c364a",
            "patch": "@@ -9,7 +9,7 @@\n from packaging import version\n \n from .configuration_utils import PretrainedConfig\n-from .utils import is_hqq_available, is_optimum_quanto_available, logging\n+from .utils import is_hqq_available, is_optimum_quanto_available, is_torch_greater_or_equal, logging\n \n \n if is_hqq_available():\n@@ -537,10 +537,10 @@ def batch_select_indices(self, indices: torch.Tensor):\n \n class OffloadedCache(DynamicCache):\n     \"\"\"\n-    A drop-in replacement for DynamicCache that conserves GPU memory at the expense of more CPU memory.\n+    A drop-in replacement for DynamicCache that conserves accelerator(GPU, XPU) memory at the expense of more CPU memory.\n     Useful for generating from models with very long context.\n \n-    In addition to the default CUDA stream, where all forward() computations happen,\n+    In addition to the default accelerator stream, where all forward() computations happen,\n     this class uses another stream, the prefetch stream, which it creates itself.\n     Since scheduling of operations on separate streams happens independently, this class uses\n     the prefetch stream to asynchronously prefetch the KV cache of layer k+1 when layer k is executing.\n@@ -549,17 +549,21 @@ class OffloadedCache(DynamicCache):\n     \"\"\"\n \n     def __init__(self) -> None:\n-        if not torch.cuda.is_available():\n-            raise RuntimeError(\"OffloadedCache can only be used with a GPU\")\n+        if not (torch.cuda.is_available() or (is_torch_greater_or_equal(\"2.7\") and torch.xpu.is_available())):\n+            raise RuntimeError(\n+                \"OffloadedCache can only be used with a GPU\" + (\" or XPU\" if is_torch_greater_or_equal(\"2.7\") else \"\")\n+            )\n+\n         super().__init__()\n         self.original_device = []\n-        self.prefetch_stream = torch.cuda.Stream()\n+        self.prefetch_stream = None\n+        self.prefetch_stream = torch.Stream() if is_torch_greater_or_equal(\"2.7\") else torch.cuda.Stream()\n         self.beam_idx = None  # used to delay beam search operations\n \n     def prefetch_layer(self, layer_idx: int):\n         \"Starts prefetching the next layer cache\"\n         if layer_idx < len(self):\n-            with torch.cuda.stream(self.prefetch_stream):\n+            with self.prefetch_stream if is_torch_greater_or_equal(\"2.7\") else torch.cuda.stream(self.prefetch_stream):\n                 # Prefetch next layer tensors to GPU\n                 device = self.original_device[layer_idx]\n                 self.key_cache[layer_idx] = self.key_cache[layer_idx].to(device, non_blocking=True)\n@@ -577,7 +581,10 @@ def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n         \"Gets the cache for this layer to the device. Prefetches the next and evicts the previous layer.\"\n         if layer_idx < len(self):\n             # Evict the previous layer if necessary\n-            torch.cuda.current_stream().synchronize()\n+            if is_torch_greater_or_equal(\"2.7\"):\n+                torch.accelerator.current_stream().synchronize()\n+            else:\n+                torch.cuda.current_stream().synchronize()\n             self.evict_previous_layer(layer_idx)\n             # Load current layer cache to its original device if not already there\n             original_device = self.original_device[layer_idx]"
        },
        {
            "sha": "ad4e685b24f43f981eb04d5c24c0b936a5ae513c",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b11050d6a288e47438c2f8986bfa57aa1d5c364a/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b11050d6a288e47438c2f8986bfa57aa1d5c364a/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=b11050d6a288e47438c2f8986bfa57aa1d5c364a",
            "patch": "@@ -1062,7 +1062,9 @@ def is_torch_greater_or_equal(library_version: str):\n     if not _is_package_available(\"torch\"):\n         return False\n \n-    return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n+    return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n+        library_version\n+    )\n \n \n def is_torchdistx_available():"
        },
        {
            "sha": "efe4e6af5c1826df7b83b3f44a9b55e2b0aec9f9",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 26,
            "deletions": 9,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/b11050d6a288e47438c2f8986bfa57aa1d5c364a/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b11050d6a288e47438c2f8986bfa57aa1d5c364a/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=b11050d6a288e47438c2f8986bfa57aa1d5c364a",
            "patch": "@@ -27,6 +27,7 @@\n     require_non_xpu,\n     require_read_token,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n@@ -48,7 +49,7 @@\n         StaticCache,\n         convert_and_export_with_cache,\n     )\n-    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_3\n+    from transformers.utils import is_torch_greater_or_equal\n \n \n @require_torch\n@@ -179,7 +180,7 @@ def test_static_cache_exportability(self):\n         \"\"\"\n         Tests that static cache works with `torch.export()`\n         \"\"\"\n-        if not is_torch_greater_or_equal_than_2_3:\n+        if not is_torch_greater_or_equal(\"2.3\"):\n             self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n \n         set_seed(0)\n@@ -230,7 +231,7 @@ def test_static_cache_exportability(self):\n         self.assertEqual(n_static_value_caches, model.config.num_hidden_layers)\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n @slow\n class CacheIntegrationTest(unittest.TestCase):\n     def test_dynamic_cache_hard(self):\n@@ -542,13 +543,17 @@ def test_static_cache_extra_left_padding(self, cache_implementation):\n     def test_static_cache_beam_search(self):\n         pass\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_offloaded_cache_equivalent_to_dynamic_cache(self):\n         \"\"\"Tests that OffloadedCache produces the same result as the default DynamicCache\"\"\"\n         model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n         model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n         device = model.device\n+\n+        if not is_torch_greater_or_equal(\"2.7\") and device.type == \"xpu\":\n+            self.skipTest(reason=\"This test requires torch >= 2.7 to run on xpu.\")\n+\n         input_text = \"Fun fact:\"\n         inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n         common = {\n@@ -566,13 +571,17 @@ def test_offloaded_cache_equivalent_to_dynamic_cache(self):\n         for original_output, offloaded_output in zip(original_outputs, offloaded_outputs):\n             assert torch.all(original_output == offloaded_output).item()\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n         \"\"\"Tests that OffloadedCache uses less memory than the default DynamicCache\"\"\"\n         model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n         model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n         device = model.device\n+\n+        if not is_torch_greater_or_equal(\"2.7\") and device.type == \"xpu\":\n+            self.skipTest(reason=\"This test requires torch >= 2.7 to run on xpu.\")\n+\n         input_text = \"Fun fact:\"\n         inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n         common = {\n@@ -585,12 +594,20 @@ def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n         }\n         original = GenerationConfig(**common)\n         offloaded = GenerationConfig(cache_implementation=\"offloaded\", **common)\n-        torch.cuda.reset_peak_memory_stats(device)\n+\n+        torch_accelerator_module = None\n+        if device.type == \"cuda\":\n+            torch_accelerator_module = torch.cuda\n+        elif device.type == \"xpu\":\n+            torch_accelerator_module = torch.xpu\n+\n+        torch_accelerator_module.reset_peak_memory_stats(device)\n         model.generate(generation_config=original, **inputs)\n-        original_peak_memory = torch.cuda.max_memory_allocated(device)\n-        torch.cuda.reset_peak_memory_stats(device)\n+        original_peak_memory = torch_accelerator_module.max_memory_allocated(device)\n+        torch_accelerator_module.reset_peak_memory_stats(device)\n         model.generate(generation_config=offloaded, **inputs)\n-        offloaded_peak_memory = torch.cuda.max_memory_allocated(device)\n+        offloaded_peak_memory = torch_accelerator_module.max_memory_allocated(device)\n+        print(f\"original_peak_memory: {original_peak_memory}, offloaded_peak_memory: {offloaded_peak_memory}\")\n         assert offloaded_peak_memory < original_peak_memory\n \n     @require_torch_gpu"
        }
    ],
    "stats": {
        "total": 62,
        "additions": 44,
        "deletions": 18
    }
}