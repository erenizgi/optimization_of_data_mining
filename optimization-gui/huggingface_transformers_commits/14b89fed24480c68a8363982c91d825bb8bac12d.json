{
    "author": "Kurt232",
    "message": "fix to accept cumulative_seqlens from TransformersKwargs in FA (#40194)\n\n* fix to the typings which are unmatched to FA function signature\n\ncumulative_seqlens_q/k -> cu_seq_lens_q/k:\n- in the FlashAttentionKwargs in modeling_flash_attention_utils\n- in the TransformersKwargs in generic\n- in the PagedAttentionArgs in continuous_batching\n\nIt is **BC**, because they are created in `ContinuousBatchProcessor.setup_static_tensors:L762`, used in `ContinuousBatchingManager._model_forward:L1233` and destroyed with `ContinuousBatchProcessor`\n\n* format changes by ruff\n\n* Update src/transformers/integrations/flash_paged.py\r\n\r\nunused function arg in `PagedAttentionCache.update`\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* revert continuous_batching signiture, which is more meaningful\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "14b89fed24480c68a8363982c91d825bb8bac12d",
    "files": [
        {
            "sha": "2d903da05b620a67aa7e325c0cb5800417910cc4",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=14b89fed24480c68a8363982c91d825bb8bac12d",
            "patch": "@@ -801,8 +801,8 @@ def get_model_kwargs(self) -> PagedAttentionArgs:\n             \"input_ids\": self.input_ids,\n             \"position_ids\": self.position_ids,\n             \"attention_mask\": self.attention_mask,\n-            \"cumulative_seqlens_q\": self.cumulative_seqlens_q,\n-            \"cumulative_seqlens_k\": self.cumulative_seqlens_k,\n+            \"cu_seq_lens_q\": self.cumulative_seqlens_q,\n+            \"cu_seq_lens_k\": self.cumulative_seqlens_k,\n             \"write_index\": self.write_index,\n             \"read_index\": self.read_index,\n             \"logits_indices\": self.logits_indices,\n@@ -1238,7 +1238,7 @@ def _process_logit(self, batch_data, logits):\n         # Pass continuous batching context to logits processor if it supports it. TODO we should find a way to make this a little bit cleaner!\n         if hasattr(self.logit_processor, \"set_continuous_batching_context\"):\n             self.logit_processor.set_continuous_batching_context(\n-                batch_data[\"logits_indices\"], batch_data[\"cumulative_seqlens_q\"]\n+                batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"]\n             )\n         return self.logit_processor(batch_data[\"input_ids\"], logits)\n "
        },
        {
            "sha": "14b4b54aa1c570e6dc6bf90ece2fa8a415f3afb9",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=14b89fed24480c68a8363982c91d825bb8bac12d",
            "patch": "@@ -356,26 +356,26 @@ def __init__(self, penalty: float, prompt_ignore_length: Optional[int] = None):\n         self.penalty = penalty\n         self.prompt_ignore_length = prompt_ignore_length\n         self.logits_indices = None\n-        self.cumulative_seqlens_q = None\n+        self.cu_seq_lens_q = None\n \n-    def set_continuous_batching_context(self, logits_indices: torch.Tensor, cumulative_seqlens_q: torch.Tensor):\n+    def set_continuous_batching_context(self, logits_indices: torch.Tensor, cu_seq_lens_q: torch.Tensor):\n         self.logits_indices = logits_indices\n-        self.cumulative_seqlens_q = cumulative_seqlens_q\n+        self.cu_seq_lens_q = cu_seq_lens_q\n \n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         if self.prompt_ignore_length:\n             input_ids = input_ids[:, self.prompt_ignore_length :]\n \n         if scores.dim() == 3:\n-            if self.logits_indices is not None and self.cumulative_seqlens_q is not None:\n+            if self.logits_indices is not None and self.cu_seq_lens_q is not None:\n                 batch_size, seq_len, vocab_size = scores.shape\n                 last_positions = self.logits_indices\n                 last_scores = scores[0, last_positions, :]\n \n                 # Prepare token mask\n                 token_mask = torch.zeros_like(last_scores, dtype=torch.bool)\n-                cu_seq_lens = self.cumulative_seqlens_q\n+                cu_seq_lens = self.cu_seq_lens_q\n                 lengths = cu_seq_lens[1:] - cu_seq_lens[:-1]\n                 seq_indices = torch.repeat_interleave(torch.arange(len(lengths), device=input_ids.device), lengths)\n                 token_mask[seq_indices, input_ids] = True"
        },
        {
            "sha": "b3c9ff7cc6409334b4a2c9eaad97741d87ca7f3c",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=14b89fed24480c68a8363982c91d825bb8bac12d",
            "patch": "@@ -18,8 +18,8 @@ def paged_attention_forward(\n     v: torch.Tensor,\n     attention_mask: torch.Tensor = None,\n     cache: PagedAttentionCache = None,\n-    cumulative_seqlens_q=None,\n-    cumulative_seqlens_k=None,\n+    cu_seq_lens_q=None,\n+    cu_seq_lens_k=None,\n     max_seqlen_q=None,\n     max_seqlen_k=None,\n     block_tables=None,\n@@ -35,9 +35,9 @@ def paged_attention_forward(\n         q: (total_q, nheads, headdim), where total_q = total number of query tokens in the batch.\n         k: (total_k, nheads_k, headdim), where total_k = total number of key tokens in the batch.  but if there is a block table it can be the full k\n         v: (total_k, nheads_k, headdim), where total_k = total number of key tokens in the batch.  but if there is a block table it can be the full v\n-        cumulative_seqlens_q: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n+        cu_seq_lens_q: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n            of the sequences in the batch, used to index into q.\n-        cumulative_seqlens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n+        cu_seq_lens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n            of the sequences in the batch, used to index into kv.\n         max_seqlen_q: int. Maximum query sequence length in the batch.\n         max_seqlen_k: int. Maximum key sequence length in the batch.\n@@ -48,7 +48,7 @@ def paged_attention_forward(\n         window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n         softcap: float. Anything > 0 activates softcapping attention.\n     \"\"\"\n-    k, v = cache.update(k, v, module.layer_idx, cumulative_seqlens_k=cumulative_seqlens_k, **kwargs)\n+    k, v = cache.update(k, v, module.layer_idx, **kwargs)\n \n     sliding_window = (-1, -1) if not getattr(module, \"sliding_window\", False) else (module.sliding_window, 0)\n     if implementation is not None:\n@@ -58,8 +58,8 @@ def paged_attention_forward(\n         q.transpose(1, 2).squeeze(0).contiguous(),\n         k.transpose(1, 2).squeeze(0).contiguous(),\n         v.transpose(1, 2).squeeze(0).contiguous(),\n-        cumulative_seqlens_q.to(torch.int32),\n-        cumulative_seqlens_k.to(torch.int32).clone(),\n+        cu_seq_lens_q.to(torch.int32),\n+        cu_seq_lens_k.to(torch.int32).clone(),\n         max_seqlen_q,\n         max_seqlen_k,\n         softmax_scale=module.scaling,"
        },
        {
            "sha": "f5b506d3021645592177a2128bd4f1157da662fb",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=14b89fed24480c68a8363982c91d825bb8bac12d",
            "patch": "@@ -462,18 +462,18 @@ class FlashAttentionKwargs(TypedDict, total=False):\n     Keyword arguments for Flash Attention with Compile.\n \n     Attributes:\n-        cumulative_seqlens_q (`torch.LongTensor`, *optional*)\n+        cu_seq_lens_q (`torch.LongTensor`, *optional*)\n             Gets cumulative sequence length for query state.\n-        cumulative_seqlens_k (`torch.LongTensor`, *optional*)\n+        cu_seq_lens_k (`torch.LongTensor`, *optional*)\n             Gets cumulative sequence length for key state.\n         max_length_q (`int`, *optional*):\n             Maximum sequence length for query state.\n         max_length_k (`int`, *optional*):\n             Maximum sequence length for key state.\n     \"\"\"\n \n-    cumulative_seqlens_q: Optional[torch.LongTensor]\n-    cumulative_seqlens_k: Optional[torch.LongTensor]\n+    cu_seq_lens_q: Optional[torch.LongTensor]\n+    cu_seq_lens_k: Optional[torch.LongTensor]\n     max_length_q: Optional[int]\n     max_length_k: Optional[int]\n "
        },
        {
            "sha": "b13e7f33d9ff4057640b7ef6cafc90560cd1f700",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14b89fed24480c68a8363982c91d825bb8bac12d/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=14b89fed24480c68a8363982c91d825bb8bac12d",
            "patch": "@@ -848,9 +848,9 @@ class TransformersKwargs(TypedDict, total=False):\n             Turn this on to return the intermediary attention scores.\n         output_router_logits (`Optional[bool]`, *optional*):\n             For MoE models, this allows returning the router logits to compute the loss.\n-        cumulative_seqlens_q (`torch.LongTensor`, *optional*)\n+        cu_seq_lens_q (`torch.LongTensor`, *optional*)\n             Gets cumulative sequence length for query state.\n-        cumulative_seqlens_k (`torch.LongTensor`, *optional*)\n+        cu_seq_lens_k (`torch.LongTensor`, *optional*)\n             Gets cumulative sequence length for key state.\n         max_length_q (`int`, *optional*):\n             Maximum sequence length for query state.\n@@ -862,8 +862,8 @@ class TransformersKwargs(TypedDict, total=False):\n     output_hidden_states: Optional[bool]\n     output_attentions: Optional[bool]\n     output_router_logits: Optional[bool]\n-    cumulative_seqlens_q: Optional[\"torch.LongTensor\"]\n-    cumulative_seqlens_k: Optional[\"torch.LongTensor\"]\n+    cu_seq_lens_q: Optional[\"torch.LongTensor\"]\n+    cu_seq_lens_k: Optional[\"torch.LongTensor\"]\n     max_length_q: Optional[int]\n     max_length_k: Optional[int]\n "
        }
    ],
    "stats": {
        "total": 46,
        "additions": 23,
        "deletions": 23
    }
}