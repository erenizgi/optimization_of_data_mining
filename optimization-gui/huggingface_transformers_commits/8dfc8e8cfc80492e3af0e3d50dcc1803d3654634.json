{
    "author": "ArthurZucker",
    "message": ":facepalm: CB nit!  (#41413)\n\n* :facepalm:\n\n* updates\n\n* update cb simple\n\n* merge\n\n* up\n\n* update\n\n* fix\n\n* up\n\n* nit\n\n* rumble this is annoying\n\n* update\n\n* update\n\n* up\n\n* fix\n\n* ....\n\n* cleanup a bit\n\n* nit\n\n* typo\n\n* typing and typo\n\n* nit\n\n* updates\n\n* up\n\n* final fix!\n\n* update\n\n* fix more import issues\n\n* nuke is paged\n\n* up",
    "sha": "8dfc8e8cfc80492e3af0e3d50dcc1803d3654634",
    "files": [
        {
            "sha": "dca0c0a4587b40a0c77ecec1733c6eaad2c7b780",
            "filename": "examples/pytorch/continuous_batching_simple.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching_simple.py?ref=8dfc8e8cfc80492e3af0e3d50dcc1803d3654634",
            "patch": "@@ -33,15 +33,18 @@\n     parser.add_argument(\"--max-batch-tokens\", \"-b\", type=int, default=None)\n     parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n     parser.add_argument(\"--samples\", type=int, default=500)\n+    parser.add_argument(\"--max-new-tokens\", type=int, default=32)\n+\n     args = parser.parse_args()\n \n     # Prepare model\n     model = AutoModelForCausalLM.from_pretrained(\n         MODEL_ID,\n         attn_implementation=args.attn,\n+        device_map=\"cuda\",\n         dtype=torch.bfloat16,\n     )\n-    model = model.cuda().eval()\n+    model = model.eval()\n \n     # Prepare tokenizer and dataset\n     tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n@@ -52,7 +55,7 @@\n \n     # Prepare generation config\n     generation_config = GenerationConfig(\n-        max_new_tokens=512,\n+        max_new_tokens=args.max_new_tokens,\n         use_cuda_graph=False,  # Not supported for simple version\n         eos_token_id=tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,"
        },
        {
            "sha": "1f70639323031563566f69971792f4ba7fb02735",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=8dfc8e8cfc80492e3af0e3d50dcc1803d3654634",
            "patch": "@@ -173,7 +173,7 @@ def __init__(\n         page_size = self.head_dim * self.num_key_value_heads\n \n         if \"flash\" in self.config._attn_implementation:\n-            num_attention_masks = 0\n+            num_attention_masks = 1  # only used to compute the default meme args\n         else:\n             # TODO: when we generalize to allow for block-attn, we can use `num_attention_masks=sum(set(group_types))`\n             num_attention_masks = 2 if \"sliding_attention\" in group_types else 1"
        },
        {
            "sha": "8e79f0229ff04d90b29be5fe3d5d51413956a7b7",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=8dfc8e8cfc80492e3af0e3d50dcc1803d3654634",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...configuration_utils import PreTrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n+from ...integrations.hub_kernels import load_and_register_kernel\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n from .cache import PagedAttentionCache\n@@ -598,21 +599,27 @@ def __init__(\n         streaming: bool = True,\n         slice_inputs: bool = True,\n     ):\n-        \"\"\"Initialize the continuous batching manager.\n+        \"\"\"\n+        Initialize the continuous batching manager.\n \n         Args:\n             model: The language model for generation\n             generation_config: Configuration for generation parameters\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n             streaming: Whether to stream tokens as they are generated\n         \"\"\"\n-        self.model = model.eval()\n         if \"paged|\" not in model.config._attn_implementation:\n-            from ...integrations.hub_kernels import load_and_register_kernel\n+            attn_implementation = f\"paged|{model.config._attn_implementation}\"\n+\n+            from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n \n-            attn_implementation = \"paged|\" + self.model.config._attn_implementation\n-            load_and_register_kernel(attn_implementation)\n-            model.set_attn_implementation(attn_implementation)\n+            if attn_implementation not in ALL_ATTENTION_FUNCTIONS._global_mapping:  # when its a kernel\n+                from ...integrations.flash_paged import paged_attention_forward\n+\n+                load_and_register_kernel(attn_implementation, paged_attention_forward)\n+\n+            model.config._attn_implementation = attn_implementation\n+        self.model = model.eval()\n         generation_config = model.generation_config if generation_config is None else generation_config\n         self.generation_config = generation_config\n         self.input_queue = queue.Queue(maxsize=max_queue_size)"
        },
        {
            "sha": "d588be75b0bbaac68f0b2451be4267244550570b",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=8dfc8e8cfc80492e3af0e3d50dcc1803d3654634",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n import re\n from functools import partial\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n from ..modeling_flash_attention_utils import lazy_import_flash_attention\n from .flash_attention import flash_attention_forward\n@@ -168,27 +168,29 @@ def is_kernel(attn_implementation: Optional[str]) -> bool:\n     )\n \n \n-def load_and_register_kernel(attn_implementation: str) -> None:\n-    \"\"\"Load and register the kernel associated to `attn_implementation`.\"\"\"\n-    if not is_kernel(attn_implementation):\n+def load_and_register_kernel(attn_implementation: str, attention_wrapper: Optional[Callable] = None) -> None:\n+    \"\"\"\n+    Load and register the kernel associated to `attn_implementation`.\n+\n+    Args:\n+        attn_implementation: A string, usually a kernel repo like \"kernels-community/flash-mla\".\n+        attn_wrapper: a callable for the wrapper around the attention implementation. In `transformers` we\n+            have a wrapper around the `flash_attn_var_len` call, and the same goes for `sdpa` and `eager`.\n+            They just prepare the arguments properly. This is mostly used for continious batching, where we\n+            want the `paged` wrapper, which calls the paged cache.\n+    \"\"\"\n+    from ..masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n+    from ..modeling_utils import ALL_ATTENTION_FUNCTIONS\n+\n+    actual_attn_name = attn_implementation.split(\"|\")[1] if \"|\" in attn_implementation else attn_implementation\n+    if not is_kernel(actual_attn_name):\n         return\n     if not _kernels_available:\n         raise ImportError(\n             \"`kernels` is either not installed or uses an incompatible version. \"\n             \"Please install the latest version with `pip install -U kernels`.\"\n         )\n \n-    # Need to be imported here as otherwise we have a circular import in `modeling_utils`\n-    from ..masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n-    from ..modeling_utils import ALL_ATTENTION_FUNCTIONS\n-\n-    attention_wrapper = None\n-    # FIXME: @ArthurZucker this is dirty, did not want to do a lof of extra work\n-    actual_attn_name = attn_implementation\n-    if \"|\" in attn_implementation:\n-        attention_wrapper, actual_attn_name = attn_implementation.split(\"|\")\n-        # `transformers` has wrapper for sdpa, paged, flash, flex etc.\n-        attention_wrapper = ALL_ATTENTION_FUNCTIONS.get(attention_wrapper)\n     # Extract repo_id and kernel_name from the string\n     if \":\" in actual_attn_name:\n         repo_id, kernel_name = actual_attn_name.split(\":\")"
        },
        {
            "sha": "b992239b6b11e25fefbf37ca77c9c9ff3b6d99e5",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8dfc8e8cfc80492e3af0e3d50dcc1803d3654634/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8dfc8e8cfc80492e3af0e3d50dcc1803d3654634",
            "patch": "@@ -2576,12 +2576,10 @@ def _check_and_adjust_attn_implementation(\n             # preload flash attention here to allow compile with fullgraph\n             if applicable_attn_implementation.startswith(\"flash_attention\"):\n                 lazy_import_flash_attention(applicable_attn_implementation, force_import=True)\n-\n         return applicable_attn_implementation\n \n     def get_correct_attn_implementation(self, requested_attention: Optional[str], is_init_check: bool = False) -> str:\n         applicable_attention = \"sdpa\" if requested_attention is None else requested_attention\n-\n         if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n             message = (\n                 f'Specified `attn_implementation=\"{applicable_attention}\"` is not supported. The only possible arguments are '"
        }
    ],
    "stats": {
        "total": 62,
        "additions": 36,
        "deletions": 26
    }
}