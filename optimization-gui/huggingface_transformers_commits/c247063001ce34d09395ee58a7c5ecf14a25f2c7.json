{
    "author": "Cyrilvallez",
    "message": "Reapply modular examples (#42846)\n\n* reapply\n\n* fix\n\n* fix",
    "sha": "c247063001ce34d09395ee58a7c5ecf14a25f2c7",
    "files": [
        {
            "sha": "f51b08cff4a7c37506af446f356d68b6db5e1b5f",
            "filename": "examples/modular-transformers/configuration_duplicated_method.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -8,7 +8,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class DuplicatedMethodConfig(PreTrainedConfig):\n@@ -129,7 +129,7 @@ def __init__(\n         eos_token_id: Optional[int] = 2,\n         pretraining_tp: Optional[int] = 1,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,\n@@ -157,14 +157,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "d143b7df70de1a2ad26b4483665a9ddf0f7fe0e2",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -8,7 +8,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class MyNewModelConfig(PreTrainedConfig):\n@@ -165,7 +165,7 @@ def __init__(\n         eos_token_id: Optional[int] = 2,\n         pretraining_tp: Optional[int] = 1,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias=True,\n@@ -194,14 +194,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "155807daf4527877eccc4d3f433fcb4643c08192",
            "filename": "examples/modular-transformers/configuration_my_new_model2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -7,7 +7,7 @@\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+from ...modeling_rope_utils import RopeParameters\n \n \n class MyNewModel2Config(PreTrainedConfig):\n@@ -68,7 +68,7 @@ def __init__(\n         eos_token_id: Optional[int] = 2,\n         pretraining_tp: Optional[int] = 1,\n         tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n         attention_bias: Optional[bool] = False,\n         attention_dropout: Optional[float] = 0.0,\n         mlp_bias: Optional[bool] = False,\n@@ -96,14 +96,7 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.mlp_bias = mlp_bias\n         self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n-        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n-        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n-        self.rope_parameters = rope_scaling or rope_parameters\n-\n-        # Validate the correctness of rotary position embeddings parameters\n-        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n-        standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        self.rope_parameters = rope_parameters\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "99329c9e562ce42ad8470c914becef7bd6ff5cd7",
            "filename": "examples/modular-transformers/configuration_new_model.py",
            "status": "modified",
            "additions": 15,
            "deletions": 19,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_new_model.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -6,7 +6,8 @@\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # Example where we only want to overwrite the defaults of an init\n \n-from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+\n+from ...configuration_utils import PreTrainedConfig\n \n \n class NewModelConfig(PreTrainedConfig):\n@@ -59,14 +60,14 @@ class NewModelConfig(PreTrainedConfig):\n             Beginning of stream token id.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        layer_types (`list`, *optional*):\n-            Attention pattern for each layer.\n         use_bidirectional_attention (`bool`, *optional*):\n             If True, the model will attend to all text tokens instead of using a causal mask.\n \n@@ -116,20 +117,12 @@ def __init__(\n         eos_token_id=1,\n         bos_token_id=2,\n         tie_word_embeddings=True,\n-        rope_theta=10000.0,\n+        rope_parameters=None,\n         attention_bias=False,\n         attention_dropout=0.0,\n         use_bidirectional_attention=False,\n-        layer_types=None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -142,15 +135,18 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.rms_norm_eps = rms_norm_eps\n         self.use_cache = use_cache\n-        self.rope_theta = rope_theta\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.use_bidirectional_attention = use_bidirectional_attention\n+        self.rope_parameters = rope_parameters\n \n-        self.layer_types = layer_types\n-        if self.layer_types is None:\n-            self.layer_types = [\"full_attention\" for _ in range(self.num_hidden_layers)]\n-        layer_type_validation(self.layer_types, self.num_hidden_layers)\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n \n     @property\n     def num_heads(self):"
        },
        {
            "sha": "8837b7561df30086dfcda6f86a6c55aa5af7805a",
            "filename": "examples/modular-transformers/modeling_add_function.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_add_function.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -10,6 +10,8 @@\n import torch\n from torch import nn\n \n+from ...integrations import use_kernel_func_from_hub\n+\n \n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n@@ -18,6 +20,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "d9584d7966b386ff1ecc6ecd3ae6e2386fd765a2",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 56,
            "deletions": 164,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -10,24 +10,20 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available\n+from ...pytorch_utils import apply_chunking_to_forward\n+from ...utils import TransformersKwargs, auto_docstring\n from ...utils.generic import check_model_inputs\n from .configuration_dummy_bert import DummyBertConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n class DummyBertEmbeddings(nn.Module):\n     \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n@@ -106,7 +102,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n@@ -148,7 +144,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -160,14 +156,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only dummy_bert can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -221,7 +217,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -234,22 +230,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -290,33 +286,14 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = DummyBertCrossAttention if is_cross_attention else DummyBertSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = DummyBertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -325,7 +302,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -388,14 +365,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -413,7 +390,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -452,7 +429,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n@@ -503,7 +480,6 @@ def __init__(self, config):\n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n-\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n     def forward(self, hidden_states):\n@@ -527,21 +503,12 @@ class DummyBertPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": DummyBertCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, DummyBertLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, DummyBertLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n @auto_docstring(\n@@ -582,14 +549,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -615,19 +574,22 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if input_ids is not None:\n             device = input_ids.device\n-            input_shape = input_ids.shape\n+            seq_length = input_ids.shape[1]\n         else:\n             device = inputs_embeds.device\n-            input_shape = inputs_embeds.shape[:-1]\n+            seq_length = inputs_embeds.shape[1]\n \n-        seq_length = input_shape[1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:\n             cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n@@ -641,7 +603,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -672,103 +633,34 @@ def forward(\n \n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n-\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask"
        },
        {
            "sha": "a6341445c8c827f15c656b469b151496a92b005c",
            "filename": "examples/modular-transformers/modeling_from_uppercase_model.py",
            "status": "modified",
            "additions": 12,
            "deletions": 42,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -4,6 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_from_uppercase_model.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+\n from collections.abc import Callable\n from typing import Optional, Union\n \n@@ -13,6 +14,8 @@\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs\n from .configuration_from_uppercase_model import FromUppercaseModelTextConfig, FromUppercaseModelVisionConfig\n \n \n@@ -24,8 +27,7 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    output_attentions: bool = True,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n     if attention_mask is not None:\n@@ -35,8 +37,6 @@ def eager_attention_forward(\n \n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n-    if not output_attentions:\n-        attn_weights = None\n     return attn_output, attn_weights\n \n \n@@ -67,8 +67,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -81,15 +80,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # FROM_UPPERCASE_MODEL text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -101,17 +91,14 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n         return attn_output, attn_weights\n \n \n@@ -143,27 +130,15 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-                `(config.encoder_attention_heads,)`.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -172,9 +147,4 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states"
        },
        {
            "sha": "cf488172e2bf25cf798e3b715242b47c19a1c579",
            "filename": "examples/modular-transformers/modeling_global_indexing.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -13,6 +13,7 @@\n from transformers.modeling_utils import AttentionInterface\n \n from ...cache_utils import Cache\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs\n from .configuration_global_indexing import GlobalIndexingConfig\n@@ -25,6 +26,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -100,6 +102,7 @@ def custom_flex(x, **kwargs):\n ALL_ATTENTION_FUNCTIONS[\"flex_attention\"] = custom_flex\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class GlobalIndexingAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -129,8 +132,8 @@ def __init__(self, config: GlobalIndexingConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],"
        },
        {
            "sha": "86585c6c19543d97c4a077f734afa4fe615bcdb2",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 29,
            "deletions": 177,
            "changes": 206,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -17,7 +17,9 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, torch_int\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, torch_int\n+from ...utils.generic import check_model_inputs\n from .configuration_multimodal2 import Multimodal2Config, Multimodal2TextConfig, Multimodal2VisionConfig\n \n \n@@ -29,8 +31,7 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    output_attentions: bool = True,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n     if attention_mask is not None:\n@@ -40,8 +41,6 @@ def eager_attention_forward(\n \n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n-    if not output_attentions:\n-        attn_weights = None\n     return attn_output, attn_weights\n \n \n@@ -72,8 +71,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -86,15 +84,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # MULTIMODAL2_VISION text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -106,17 +95,14 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n         return attn_output, attn_weights\n \n \n@@ -135,86 +121,11 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class Multimodal2Attention(nn.Module):\n-    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n-\n-    def __init__(self, config: Union[Multimodal2VisionConfig, Multimodal2TextConfig]):\n-        super().__init__()\n-        self.config = config\n-        self.embed_dim = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.embed_dim // self.num_heads\n-        if self.head_dim * self.num_heads != self.embed_dim:\n-            raise ValueError(\n-                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n-                f\" {self.num_heads}).\"\n-            )\n-        self.scale = self.head_dim**-0.5\n-        self.dropout = config.attention_dropout\n-        self.is_causal = False\n-\n-        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n-        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n-        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n-        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        batch_size, seq_length, embed_dim = hidden_states.shape\n-\n-        queries = self.q_proj(hidden_states)\n-        keys = self.k_proj(hidden_states)\n-        values = self.v_proj(hidden_states)\n-\n-        queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # MULTIMODAL2 text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n-\n-        attention_interface: Callable = eager_attention_forward\n-        if self.config._attn_implementation != \"eager\":\n-            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n-\n-        attn_output, attn_weights = attention_interface(\n-            self,\n-            queries,\n-            keys,\n-            values,\n-            attention_mask,\n-            is_causal=self.is_causal,\n-            scaling=self.scale,\n-            dropout=0.0 if not self.training else self.dropout,\n-            output_attentions=output_attentions,\n-        )\n-\n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n-        attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-        return attn_output, attn_weights\n-\n-\n class Multimodal2VisionEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n-        self.self_attn = Multimodal2Attention(config)\n+        self.self_attn = Multimodal2VisionAttention(config)\n         self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = Multimodal2VisionMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n@@ -223,27 +134,15 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-                `(config.encoder_attention_heads,)`.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -252,12 +151,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class Multimodal2VisionEncoder(nn.Module):\n@@ -279,9 +173,7 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n         Args:\n@@ -296,53 +188,17 @@ def forward(\n                 - 0 for tokens that are **masked**.\n \n                 [What are attention masks?](../glossary#attention-mask)\n-            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Causal mask for the text model. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                causal_attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n         )\n \n \n@@ -444,15 +300,9 @@ def __init__(self, config):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -461,8 +311,7 @@ def forward(\n \n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs.last_hidden_state\n@@ -472,21 +321,25 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n @auto_docstring\n class Multimodal2VisionPreTrainedModel(PreTrainedModel):\n     config: Multimodal2Config\n     base_model_prefix = \"multimodal2_vision\"\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Multimodal2VisionEncoderLayer,\n+        \"attentions\": Multimodal2VisionAttention,\n+    }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Multimodal2VisionMLP):\n@@ -500,6 +353,7 @@ def _init_weights(self, module):\n class Multimodal2VisionModel(Multimodal2VisionPreTrainedModel):\n     config: Multimodal2VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"Multimodal2VisionEncoderLayer\"]\n \n     def __init__(self, config: Multimodal2VisionConfig):\n@@ -511,14 +365,13 @@ def __init__(self, config: Multimodal2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Example:\n@@ -543,7 +396,6 @@ def forward(\n \n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )"
        },
        {
            "sha": "1d190d134b6031b097ccf35178e87ee7fd016b10",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -10,8 +10,10 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -62,6 +64,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -127,6 +130,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class MyNewModel2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -260,12 +264,12 @@ class MyNewModel2PreTrainedModel(PreTrainedModel):\n         \"attentions\": MyNewModel2Attention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n-\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         if \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.zero_()\n+            init.zeros_(module.weight)\n \n \n class MyNewModel2ForSequenceClassification(GenericForSequenceClassification, MyNewModel2PreTrainedModel):"
        },
        {
            "sha": "2dba06c1b872f828905afe2efa1a96f5061269da",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 37,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -87,27 +87,17 @@ def forward(self, image_features):\n @auto_docstring\n class NewTaskModelPreTrainedModel(PreTrainedModel):\n     config: NewTaskModelConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"NewTaskModelMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-\n     _can_compile_fullgraph = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    def _init_weights(self, module):\n-        # important: this ported version of NewTaskModelisn't meant for training from scratch - only\n-        # inference and fine-tuning\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-\n \n def token_type_ids_mask_function(\n     token_type_ids: Optional[torch.Tensor],\n@@ -249,12 +239,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.language_model = decoder\n-\n-    def get_decoder(self):\n-        return self.language_model\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -457,28 +441,9 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.model.get_decoder()\n-\n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)\n \n-    # Make modules available through conditional class for BC\n-    @property\n-    def language_model(self):\n-        return self.model.language_model\n-\n-    @property\n-    def vision_tower(self):\n-        return self.model.vision_tower\n-\n-    @property\n-    def multi_modal_projector(self):\n-        return self.model.multi_modal_projector\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "35d4bd90e3a265cca39c173d4df87bc9b5f1ed53",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 56,
            "deletions": 164,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -10,24 +10,20 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available\n+from ...pytorch_utils import apply_chunking_to_forward\n+from ...utils import TransformersKwargs, auto_docstring\n from ...utils.generic import check_model_inputs\n from .configuration_roberta import RobertaConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n class RobertaEmbeddings(nn.Module):\n     \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n@@ -109,7 +105,7 @@ def eager_attention_forward(\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n     attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n+    if attention_mask is not None:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n         attn_weights = attn_weights + attention_mask\n \n@@ -151,7 +147,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -163,14 +159,14 @@ def forward(\n         key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n         value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        if past_key_values is not None:\n             # decoder-only roberta can have a simple dynamic cache for example\n-            current_past_key_value = past_key_value\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                current_past_key_value = past_key_value.self_attention_cache\n+            current_past_key_values = past_key_values\n+            if isinstance(past_key_values, EncoderDecoderCache):\n+                current_past_key_values = past_key_values.self_attention_cache\n \n             # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-            key_layer, value_layer = current_past_key_value.update(\n+            key_layer, value_layer = current_past_key_values.update(\n                 key_layer,\n                 value_layer,\n                 self.layer_idx,\n@@ -224,7 +220,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[EncoderDecoderCache] = None,\n+        past_key_values: Optional[EncoderDecoderCache] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -237,22 +233,22 @@ def forward(\n         # get query proj\n         query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n-        if past_key_value is not None and is_updated:\n+        is_updated = past_key_values.is_updated.get(self.layer_idx) if past_key_values is not None else False\n+        if past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n-            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+            key_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_values.cross_attention_cache.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n             value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_value is not None:\n+            if past_key_values is not None:\n                 # save all states to the cache\n-                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                key_layer, value_layer = past_key_values.cross_attention_cache.update(\n                     key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                past_key_value.is_updated[self.layer_idx] = True\n+                past_key_values.is_updated[self.layer_idx] = True\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -293,33 +289,14 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = RobertaCrossAttention if is_cross_attention else RobertaSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = RobertaSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n@@ -328,7 +305,7 @@ def forward(\n             hidden_states,\n             encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -391,14 +368,14 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n+        past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n             hidden_states,\n             attention_mask,\n-            past_key_value=past_key_value,\n+            past_key_values=past_key_values,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -416,7 +393,7 @@ def forward(\n                 None,  # attention_mask\n                 encoder_hidden_states,\n                 encoder_attention_mask,\n-                past_key_value=past_key_value,\n+                past_key_values=past_key_values,\n                 **kwargs,\n             )\n             attention_output = cross_attention_output\n@@ -455,7 +432,7 @@ def forward(\n                 attention_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_value=past_key_values,\n+                past_key_values=past_key_values,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n@@ -506,7 +483,6 @@ def __init__(self, config):\n         # The output weights are the same as the input embeddings, but there is\n         # an output-only bias for each token.\n         self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n-\n         self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n \n     def forward(self, hidden_states):\n@@ -530,21 +506,12 @@ class RobertaPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": RobertaCrossAttention,\n     }\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, RobertaLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, RobertaLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n @auto_docstring(\n@@ -585,14 +552,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -615,19 +574,22 @@ def forward(\n             use_cache = False\n \n         if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+                if encoder_hidden_states is not None or self.config.is_encoder_decoder\n+                else DynamicCache(config=self.config)\n+            )\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if input_ids is not None:\n             device = input_ids.device\n-            input_shape = input_ids.shape\n+            seq_length = input_ids.shape[1]\n         else:\n             device = inputs_embeds.device\n-            input_shape = inputs_embeds.shape[:-1]\n+            seq_length = inputs_embeds.shape[1]\n \n-        seq_length = input_shape[1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:\n             cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n@@ -641,7 +603,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -672,103 +633,34 @@ def forward(\n \n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n-\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask"
        },
        {
            "sha": "028add7be639a7061c6fd6869a8a95a10867e8e6",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 44,
            "deletions": 13,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -14,13 +14,13 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_super import SuperConfig\n \n \n@@ -50,20 +50,49 @@ class SuperRotaryEmbedding(nn.Module):\n \n     def __init__(self, config: SuperConfig, device=None):\n         super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_parameters\") and isinstance(config.rope_parameters, dict):\n-            self.rope_type = config.rope_parameters.get(\"rope_type\", config.rope_parameters.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n         self.max_seq_len_cached = config.max_position_embeddings\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[SuperConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n \n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n@@ -72,7 +101,7 @@ def forward(self, x, position_ids):\n         position_ids_expanded = position_ids[:, None, :].float()\n \n         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos() * self.attention_scaling\n@@ -104,6 +133,7 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -169,6 +199,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class SuperAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -198,8 +229,8 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],"
        },
        {
            "sha": "bf357b00c61ead617930c3f055322c7834d2d3df",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -12,6 +12,7 @@\n from torch import nn\n \n from ...cache_utils import Cache\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs\n@@ -26,6 +27,7 @@ def rotate_half(x):\n     return rot_x\n \n \n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -91,6 +93,7 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n+@use_kernelized_func(apply_rotary_pos_emb)\n class SwitchFunctionAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -120,8 +123,8 @@ def __init__(self, config: SwitchFunctionConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],"
        },
        {
            "sha": "d0c9678f1aaa54a38e665689a34c89409b44f9c7",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 31,
            "deletions": 26,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -4,6 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_test_detr.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+\n import math\n import warnings\n from dataclasses import dataclass\n@@ -13,6 +14,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n@@ -203,10 +205,10 @@ def replace_batch_norm(model):\n             new_module = TestDetrFrozenBatchNorm2d(module.num_features)\n \n             if module.weight.device != torch.device(\"meta\"):\n-                new_module.weight.data.copy_(module.weight)\n-                new_module.bias.data.copy_(module.bias)\n-                new_module.running_mean.data.copy_(module.running_mean)\n-                new_module.running_var.data.copy_(module.running_var)\n+                new_module.weight.copy_(module.weight)\n+                new_module.bias.copy_(module.bias)\n+                new_module.running_mean.copy_(module.running_mean)\n+                new_module.running_var.copy_(module.running_var)\n \n             model._modules[name] = new_module\n \n@@ -810,21 +812,23 @@ class TestDetrPreTrainedModel(PreTrainedModel):\n     config: TestDetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         r\"TestDetrConvEncoder\",\n         r\"TestDetrEncoderLayer\",\n         r\"TestDetrDecoderLayer\",\n     ]\n \n+    @torch.no_grad()\n     def _init_weights(self, module):\n         std = self.config.init_std\n \n         if isinstance(module, TestDetrLearnedPositionEmbedding):\n-            nn.init.uniform_(module.row_embeddings.weight)\n-            nn.init.uniform_(module.column_embeddings.weight)\n+            init.uniform_(module.row_embeddings.weight)\n+            init.uniform_(module.column_embeddings.weight)\n         elif isinstance(module, TestDetrMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -837,27 +841,28 @@ def _init_weights(self, module):\n             )\n             for i in range(module.n_points):\n                 grid_init[:, :, i, :] *= i + 1\n-            with torch.no_grad():\n-                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n-            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight.data)\n-            nn.init.constant_(module.value_proj.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight.data)\n-            nn.init.constant_(module.output_proj.bias.data, 0.0)\n+\n+            init.copy_(module.sampling_offsets.bias, grid_init.view(-1))\n+            init.constant_(module.attention_weights.weight, 0.0)\n+            init.constant_(module.attention_weights.bias, 0.0)\n+            init.xavier_uniform_(module.value_proj.weight)\n+            init.constant_(module.value_proj.bias, 0.0)\n+            init.xavier_uniform_(module.output_proj.weight)\n+            init.constant_(module.output_proj.bias, 0.0)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         if hasattr(module, \"reference_points\") and not self.config.two_stage:\n-            nn.init.xavier_uniform_(module.reference_points.weight.data, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias.data, 0.0)\n+            init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            init.constant_(module.reference_points.bias, 0.0)\n         if hasattr(module, \"level_embed\"):\n-            nn.init.normal_(module.level_embed)\n+            init.normal_(module.level_embed)\n \n \n class TestDetrEncoder(TestDetrPreTrainedModel):\n@@ -924,6 +929,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1046,6 +1052,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1267,9 +1274,6 @@ def __init__(self, config: TestDetrConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)\n@@ -1379,6 +1383,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], TestDetrModelOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):"
        },
        {
            "sha": "8f5d564f799b8f3eaa4d5c674068343fa0106e56",
            "filename": "examples/modular-transformers/modular_multimodal2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodular_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodular_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_multimodal2.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -35,6 +35,7 @@ class Multimodal2VisionEncoderLayer(CLIPEncoderLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.mlp = Multimodal2VisionMLP(config)\n+        self.self_attn = Multimodal2VisionAttention(config)\n \n \n class Multimodal2VisionEncoder(CLIPEncoder):\n@@ -43,14 +44,20 @@ def __init__(self, config):\n         self.layers = nn.ModuleList([Multimodal2VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n \n \n-# Finally here the `Vision` part was correct in CLIP, but we still need to tell it that the encoder arg should use it as well\n+# Finally here the `Vision` part was correct in CLIP, but we still need to tell it that the encoder and attn arg should\n+# use it as well\n class Multimodal2VisionTransformer(CLIPVisionTransformer):\n     def __init__(self, config):\n         super().__init__(config)\n         self.encoder = Multimodal2VisionEncoder(config)\n \n \n class Multimodal2VisionPreTrainedModel(CLIPPreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": Multimodal2VisionEncoderLayer,\n+        \"attentions\": Multimodal2VisionAttention,\n+    }\n+\n     def _init_weights(self, module):\n         if isinstance(module, Multimodal2VisionMLP):\n             pass"
        },
        {
            "sha": "698babb9d95940870285b6624d12de9494a5eabb",
            "filename": "examples/modular-transformers/modular_new_model.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodular_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c247063001ce34d09395ee58a7c5ecf14a25f2c7/examples%2Fmodular-transformers%2Fmodular_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_model.py?ref=c247063001ce34d09395ee58a7c5ecf14a25f2c7",
            "patch": "@@ -23,11 +23,10 @@ def __init__(\n         eos_token_id=1,\n         bos_token_id=2,\n         tie_word_embeddings=True,\n-        rope_theta=10000.0,\n+        rope_parameters=None,\n         attention_bias=False,\n         attention_dropout=0.0,\n         use_bidirectional_attention=False,\n-        layer_types=None,\n         **kwargs,\n     ):\n         super().__init__(self, **kwargs)"
        }
    ],
    "stats": {
        "total": 963,
        "additions": 282,
        "deletions": 681
    }
}