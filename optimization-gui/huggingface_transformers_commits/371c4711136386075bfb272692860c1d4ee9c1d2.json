{
    "author": "bvantuan",
    "message": "Fix initialization of OneFormer (#38901)\n\n* fix initialization of OneFormer\n\n* remove redundant initializations\n\n* remove redundant initializations\n\n* remove redundant initializations\n\n* keep BC",
    "sha": "371c4711136386075bfb272692860c1d4ee9c1d2",
    "files": [
        {
            "sha": "28eadd3a489e760f42f6c009bee87f6f5e2cd776",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 41,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c4711136386075bfb272692860c1d4ee9c1d2/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c4711136386075bfb272692860c1d4ee9c1d2/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=371c4711136386075bfb272692860c1d4ee9c1d2",
            "patch": "@@ -2773,7 +2773,6 @@ def _init_weights(self, module: nn.Module):\n         elif isinstance(module, OneFormerTransformerDecoder):\n             nn.init.xavier_uniform_(module.query_input_projection.weight, gain=xavier_std)\n             nn.init.constant_(module.query_input_projection.bias, 0)\n-            module.query_input_projection._is_hf_initialized = True\n         elif isinstance(module, OneFormerPixelDecoderEncoderMultiscaleDeformableAttention):\n             nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).float() * (2.0 * math.pi / module.n_heads)\n@@ -2793,46 +2792,16 @@ def _init_weights(self, module: nn.Module):\n             nn.init.constant_(module.value_proj.bias.data, 0.0)\n             nn.init.xavier_uniform_(module.output_proj.weight.data)\n             nn.init.constant_(module.output_proj.bias.data, 0.0)\n-        elif isinstance(module, OneFormerPixelDecoderEncoderOnly):\n-            for p in module.parameters():\n-                if p.dim() > 1:\n-                    nn.init.xavier_uniform_(p)\n         elif isinstance(module, OneFormerPixelDecoder):\n-            for p in module.parameters():\n-                if p.dim() > 1:\n-                    nn.init.xavier_uniform_(p)\n             nn.init.normal_(module.level_embed, std=0)\n-        elif isinstance(module, OneFormerTransformerDecoderSelfAttentionLayer):\n-            for p in module.parameters():\n-                if p.dim() > 1:\n-                    nn.init.xavier_uniform_(p, gain=xavier_std)\n-        elif isinstance(module, OneFormerTransformerDecoderCrossAttentionLayer):\n-            for p in module.parameters():\n-                if p.dim() > 1:\n-                    nn.init.xavier_uniform_(p, gain=xavier_std)\n-        elif isinstance(module, OneFormerTransformerDecoderFFNLayer):\n+        elif isinstance(module, OneFormerTransformerDecoderLayer):\n             for p in module.parameters():\n                 if p.dim() > 1:\n                     nn.init.xavier_uniform_(p, gain=xavier_std)\n         elif isinstance(module, OneFormerTransformerDecoderQueryTransformer):\n             for p in module.parameters():\n                 if p.dim() > 1:\n                     nn.init.xavier_uniform_(p, gain=xavier_std)\n-        elif isinstance(module, OneFormerPixelLevelModule):\n-            for submodule in module.modules():\n-                if isinstance(submodule, (nn.Conv2d, nn.Linear)):\n-                    submodule.weight.data.normal_(mean=0.0, std=std)\n-                    if submodule.bias is not None:\n-                        submodule.bias.data.zero_()\n-        elif isinstance(module, OneFormerTextContextDecoder):\n-            for submodule in module.modules():\n-                if isinstance(submodule, nn.Linear):\n-                    nn.init.trunc_normal_(submodule.weight, std=0.02)\n-                    if isinstance(submodule, nn.Linear) and submodule.bias is not None:\n-                        nn.init.constant_(submodule.bias, 0)\n-                elif isinstance(submodule, nn.LayerNorm):\n-                    nn.init.constant_(submodule.bias, 0)\n-                    nn.init.constant_(submodule.weight, 1.0)\n         elif isinstance(module, OneFormerTextTransformer):\n             proj_std = (module.width**-0.5) * ((2 * module.num_layers) ** -0.5)\n             attn_std = module.width**-0.5\n@@ -2848,27 +2817,27 @@ def _init_weights(self, module: nn.Module):\n         if hasattr(module, \"reference_points\"):\n             nn.init.xavier_uniform_(module.reference_points.weight.data, gain=1.0)\n             nn.init.constant_(module.reference_points.bias.data, 0.0)\n-        elif isinstance(module, OneFormerTaskModel):\n+        elif isinstance(module, OneFormerMLPPredictionHead):\n             for submodule in module.modules():\n-                if isinstance(module, OneFormerMLPPredictionHead):\n-                    for submodule in module.modules():\n-                        if isinstance(submodule, nn.Linear):\n-                            nn.init.xavier_uniform_(submodule.weight, gain=xavier_std)\n-                            nn.init.constant_(submodule.bias, 0)\n-                        elif isinstance(module, nn.LayerNorm):\n-                            module.bias.data.zero_()\n-                            module.weight.data.fill_(1.0)\n+                if isinstance(submodule, nn.Linear):\n+                    nn.init.xavier_uniform_(submodule.weight, gain=xavier_std)\n+                    nn.init.constant_(submodule.bias, 0)\n         elif isinstance(module, nn.MultiheadAttention):\n             module.in_proj_weight.data.normal_(mean=0.0, std=std)\n             module.in_proj_bias.data.zero_()\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, OneFormerLoss):\n+            module.logit_scale.data.fill_(np.log(1 / self.config.contrastive_temperature))\n \n \n @auto_docstring"
        },
        {
            "sha": "58a93a8c4fa3524543b3238176a284e19eccf5f3",
            "filename": "tests/models/oneformer/test_modeling_oneformer.py",
            "status": "modified",
            "additions": 47,
            "deletions": 11,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/371c4711136386075bfb272692860c1d4ee9c1d2/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/371c4711136386075bfb272692860c1d4ee9c1d2/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py?ref=371c4711136386075bfb272692860c1d4ee9c1d2",
            "patch": "@@ -13,14 +13,13 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch OneFormer model.\"\"\"\n \n-import copy\n import inspect\n import unittest\n \n import numpy as np\n \n from tests.test_modeling_common import floats_tensor\n-from transformers import OneFormerConfig, is_torch_available, is_vision_available\n+from transformers import AutoModelForImageClassification, OneFormerConfig, is_torch_available, is_vision_available\n from transformers.testing_utils import (\n     is_flaky,\n     require_timm,\n@@ -35,7 +34,7 @@\n from transformers.utils import cached_property\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -51,14 +50,6 @@\n     from PIL import Image\n \n \n-def _config_zero_init(config):\n-    configs_no_init = copy.deepcopy(config)\n-    for key in configs_no_init.__dict__.keys():\n-        if \"_range\" in key or \"_std\" in key or \"initializer_factor\" in key or \"layer_scale\" in key:\n-            setattr(configs_no_init, key, 1e-10)\n-    return configs_no_init\n-\n-\n class OneFormerModelTester:\n     def __init__(\n         self,\n@@ -375,19 +366,64 @@ def test_attention_outputs(self):\n \n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.is_training = True\n         config.contrastive_temperature = 1\n \n         configs_no_init = _config_zero_init(config)\n         for model_class in self.all_model_classes:\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n+                    if (\n+                        \"self_attn.sampling_offsets.bias\" in name\n+                        or \"self_attn.value_proj.weight\" in name\n+                        or \"self_attn.output_proj.weight\" in name\n+                        or \"self_attn.in_proj_weight\" in name\n+                        or \"self_attn.out_proj.weight\" in name\n+                        or \"mlp.fc1.weight\" in name\n+                        or \"mlp.fc2.weight\" in name\n+                        or \"text_mapper.text_encoder.positional_embedding\" in name\n+                        or \"text_mapper.text_encoder.token_embedding.weight\" in name\n+                    ):\n+                        continue\n                     self.assertIn(\n                         ((param.data.mean() * 1e9).round() / 1e9).item(),\n                         [0.0, 1.0],\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n+    def test_initialization_pretrained_backbone(self):\n+        backbone_name = \"microsoft/resnet-18\"\n+\n+        # load OneFormerConfig config with a pretrained backbone\n+        config = OneFormerConfig(\n+            backbone=backbone_name,\n+            use_pretrained_backbone=True,\n+        )\n+\n+        # load pretrained backbone\n+        backbone_model = AutoModelForImageClassification.from_pretrained(backbone_name, device_map=torch_device)\n+\n+        def params_match(params1, params2):\n+            return all((p1 == p2).all() for p1, p2 in zip(params1, params2))\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device).eval()\n+            if model.__class__.__name__ == \"OneFormerModel\":\n+                self.assertTrue(\n+                    params_match(\n+                        backbone_model.base_model.encoder.parameters(),\n+                        model.pixel_level_module.encoder.encoder.parameters(),\n+                    )\n+                )\n+            elif model.__class__.__name__ == \"OneFormerForUniversalSegmentation\":\n+                self.assertTrue(\n+                    params_match(\n+                        backbone_model.base_model.encoder.parameters(),\n+                        model.model.pixel_level_module.encoder.encoder.parameters(),\n+                    )\n+                )\n+\n     def test_training(self):\n         if not self.model_tester.is_training:\n             self.skipTest(reason=\"model_tester.is_training is set to False\")"
        }
    ],
    "stats": {
        "total": 109,
        "additions": 57,
        "deletions": 52
    }
}