{
    "author": "stevhliu",
    "message": "[docs] inference engines (#42932)\n\n* inference servers\n\n* feedback\n\n* fix",
    "sha": "c7e5b749a6392ea2f42fea983af41f825b0bc78d",
    "files": [
        {
            "sha": "cc4d8719e1e768991e2e108f3d544850bd36a8b2",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7e5b749a6392ea2f42fea983af41f825b0bc78d/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7e5b749a6392ea2f42fea983af41f825b0bc78d/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=c7e5b749a6392ea2f42fea983af41f825b0bc78d",
            "patch": "@@ -127,8 +127,6 @@\n     title: Agents\n   - local: tools\n     title: Tools\n-  - local: transformers_as_backend\n-    title: Transformers as modeling backend\n   title: Inference\n - isExpanded: false\n   sections:\n@@ -228,6 +226,15 @@\n   - local: quantization/contribute\n     title: Contribute\n   title: Quantization\n+- isExpanded: false\n+  sections:\n+  - local: community_integrations/vllm\n+    title: vLLM\n+  - local: community_integrations/sglang\n+    title: SGLang\n+  - local: community_integrations/transformers_as_backend\n+    title: Building a compatible model backend for inference\n+  title: Community integrations\n - isExpanded: false\n   sections:\n   - sections:"
        },
        {
            "sha": "489a031123d32206446cc51e82ba077a50b36c0f",
            "filename": "docs/source/en/community_integrations/sglang.md",
            "status": "added",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7e5b749a6392ea2f42fea983af41f825b0bc78d/docs%2Fsource%2Fen%2Fcommunity_integrations%2Fsglang.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7e5b749a6392ea2f42fea983af41f825b0bc78d/docs%2Fsource%2Fen%2Fcommunity_integrations%2Fsglang.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcommunity_integrations%2Fsglang.md?ref=c7e5b749a6392ea2f42fea983af41f825b0bc78d",
            "patch": "@@ -0,0 +1,52 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# SGLang\n+\n+[SGLang](https://docs.sglang.ai) is a low-latency, high-throughput inference engine for large language models (LLMs). It also includes a frontend language for building agentic workflows.\n+\n+Set `model-impl=\"transformers\"` to load a Transformers modeling backend.\n+\n+```py\n+import sglang as sgl\n+\n+llm = sgl.Engine(\"meta-llama/Llama-3.2-1B-Instruct\", model-impl=\"transformers\")\n+print(llm.generate([\"The capital of France is\"], {\"max_new_tokens\": 20})[0])\n+```\n+\n+Pass `--model-impl transformers` to the `sglang.launch_server` command for online serving.\n+\n+```bash\n+python3 -m sglang.launch_server \\\n+  --model-path meta-llama/Llama-3.2-1B-Instruct \\\n+  --model-impl transformers \\\n+  --host 0.0.0.0 \\\n+  --port 30000\n+```\n+\n+Setting `model-impl=\"transformers\"` tells SGLang to skip its native model matching and use the `TransformersModel` backend instead. [`PretrainedConfig.from_pretrained`] loads the config and [`AutoModel.config`] resolves the model class.\n+\n+During loading, `_attn_implementation` is set to `\"sglang\"`. This routes attention calls through SGLang. RadixAttention kernels replace standard attention layers. SGLang's parallel linear class replaces linear layers to support tensor parallelism. The model benefits from all SGLang optimizations.\n+\n+> [!WARNING]\n+> Compatible models require `_supports_attention_backend=True` so SGLang can control attention execution. See the [Building a compatible model backend for inference](./transformers_as_backend#model-implementation) guide for details.\n+\n+The [load_weights](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/transformers.py#L277) function populates the model with weights.\n+\n+## Resources\n+\n+- [SGLang docs](https://docs.sglang.ai/supported_models/transformers_fallback.html) has more usage examples and tips for using Transformers as a backend.\n+- [Transformers backend integration in SGLang](https://huggingface.co/blog/transformers-backend-sglang) blog post explains what this integration enables.\n\\ No newline at end of file"
        },
        {
            "sha": "a1129843cd3612184d15b2cb5c080a7e5f9603ad",
            "filename": "docs/source/en/community_integrations/transformers_as_backend.md",
            "status": "added",
            "additions": 140,
            "deletions": 0,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7e5b749a6392ea2f42fea983af41f825b0bc78d/docs%2Fsource%2Fen%2Fcommunity_integrations%2Ftransformers_as_backend.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7e5b749a6392ea2f42fea983af41f825b0bc78d/docs%2Fsource%2Fen%2Fcommunity_integrations%2Ftransformers_as_backend.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcommunity_integrations%2Ftransformers_as_backend.md?ref=c7e5b749a6392ea2f42fea983af41f825b0bc78d",
            "patch": "@@ -0,0 +1,140 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Building a compatible model backend for inference\n+\n+Transformers models are compatible with inference engines like [vLLM](https://github.com/vllm-project/vllm) and [SGLang](https://docs.sglang.ai). Use the same Transformers model anywhere and avoid reimplementing a model from scratch for each inference engine. Models in Transformers that aren't natively supported by either inference engine work too.\n+\n+This guide shows you how to implement a model in Transformers that works as a backend for any inference engine.\n+\n+## Model implementation\n+\n+1. Follow the model [contribution guidelines](./add_new_model) or the [custom model contribution guidelines](./custom_models). The model must have a valid `config.json` in its directory and a valid `auto_map` field pointing to the model class in the config.\n+\n+2. Use the [`AttentionInterface`] class for custom and optimized attention functions. This interface unlocks each inference engine's performance features. \n+\n+   Use `ALL_ATTENTION_FUNCTIONS` when defining the attention layer and propagate `**kwargs**` from the base `MyModel` class to the attention layers. Set `_supports_attention_backend` to `True` in [`PreTrainedModel`].\n+   \n+   Expand the code below for an example.\n+\n+    <details>\n+    <summary>modeling_my_model.py</summary>\n+\n+    ```python\n+    from transformers import PreTrainedModel\n+    from torch import nn\n+\n+    class MyAttention(nn.Module):\n+\n+        def forward(self, hidden_states, **kwargs):\n+            ...\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attn_output, attn_weights = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                **kwargs,\n+            )\n+            ...\n+\n+    class MyModel(PreTrainedModel):\n+        _supports_attention_backend = True\n+    ```\n+\n+    </details>\n+\n+3. Enable optional tensor or pipeline parallelism by adding the following keys to [`PreTrainedConfig`].\n+\n+    * `base_model_tp_plan` enables [tensor parallelism](./perf_infer_gpu_multi) by mapping fully qualified layer name patterns to tensor parallel styles. Supports only the `\"colwise\"` and `\"rowwise\"` partitioning strategies.\n+    * `base_model_pp_plan` enables pipeline parallelism by mapping direct child layer names to tuples of lists of strings. The first element of the tuple contains the names of the input arguments. The last element contains the variable names of the layer outputs in the modeling code.\n+\n+    Expand the code below for an example.\n+\n+    <details>\n+    <summary>configuration_my_model.py</summary>\n+\n+    ```python\n+\n+    from transformers import PreTrainedConfig\n+\n+    class MyConfig(PreTrainedConfig):\n+        base_model_tp_plan = {\n+            \"layers.*.self_attn.k_proj\": \"colwise\",\n+            \"layers.*.self_attn.v_proj\": \"colwise\",\n+            \"layers.*.self_attn.o_proj\": \"rowwise\",\n+            \"layers.*.mlp.gate_proj\": \"colwise\",\n+            \"layers.*.mlp.up_proj\": \"colwise\",\n+            \"layers.*.mlp.down_proj\": \"rowwise\",\n+        }\n+        base_model_pp_plan = {\n+            \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+            \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+            \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+        }\n+    ```\n+\n+    </details>\n+\n+## Multimodal models\n+\n+Multimodal models require additional changes beyond the [vision language model contribution checklist](./contributing#vision-language-model-contribution-checklist). These changes ensure multimodal inputs are properly processed.\n+\n+1. The [`ProcessorMixin`] class must include the `self.image_token` and `self.image_token_ids` attributes. These placeholder tokens indicate image positions in the input. The same token appears in the input prompt for images and in the model code to scatter image features.\n+\n+2. The [`ProcessorMixin`] class must include a `self._get_num_multimodal_tokens` method. This method computes the number of placeholder tokens required for multimodal inputs with given sizes. It returns a [`MultiModalData`] object. Placeholders between `<image>` tokens, such as row or column tokens, don't count as image placeholders. Count only tokens replaced by image features later in the modeling code.\n+\n+3. The [`ProcessorMixin`] class must check the value of `return_mm_token_type_ids` and return `mm_token_type_ids`. This indicates whether each position is a text token (`0`), image placeholder token (`1`), or a video placeholder token (`2`). Multimodal token type id sequences must be contiguous with no breaks between consecutive tokens. Treat special tokens for beginning, ending, row, and column tokens as placeholders.\n+\n+Expand the code below for an example.\n+\n+<details>\n+<summary>modeling_my_multimodal_model.py</summary>\n+\n+```python\n+class MyMultimodalProcessor(ProcessorMixin):\n+\n+    def __call__(self, images=None, text=None, **kwargs):\n+        if return_mm_token_type_ids:\n+            mm_token_type_ids = np.zeros_like(input_ids)\n+            mm_token_type_ids[input_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+        vision_data = {}\n+        if image_sizes is not None:\n+            num_image_tokens = [256] * len(image_sizes) # 256 placeholder tokens for each image always\n+            num_image_patches = [1] * len(image_sizes) # no patching, thus each image is processed as a single base image\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+        return MultiModalData(**vision_data)\n+```\n+\n+</details>\n+\n+## Resources\n+\n+* Read the [Transformers backend integration in vLLM](https://blog.vllm.ai/2025/04/11/transformers-backend.html) blog post for more details.\n+* Read the [Transformers backend integration in SGLang](https://huggingface.co/blog/transformers-backend-sglang) blog post for more details."
        },
        {
            "sha": "b24f060c9e2a8e344a6dd0c6ccd163f8ff4423ef",
            "filename": "docs/source/en/community_integrations/vllm.md",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7e5b749a6392ea2f42fea983af41f825b0bc78d/docs%2Fsource%2Fen%2Fcommunity_integrations%2Fvllm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7e5b749a6392ea2f42fea983af41f825b0bc78d/docs%2Fsource%2Fen%2Fcommunity_integrations%2Fvllm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcommunity_integrations%2Fvllm.md?ref=c7e5b749a6392ea2f42fea983af41f825b0bc78d",
            "patch": "@@ -0,0 +1,47 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# vLLM\n+\n+[vLLM](https://github.com/vllm-project/vllm) is a high-throughput inference engine for serving LLMs at scale. It continuously batches requests and keeps KV cache memory compact with PagedAttention.\n+\n+Set `model_impl=\"transformers\"` to load a model using the Transformers modeling backend.\n+\n+```py\n+from vllm import LLM\n+\n+llm = LLM(model=\"meta-llama/Llama-3.2-1B\", model_impl=\"transformers\")\n+print(llm.generate([\"The capital of France is\"]))\n+```\n+\n+Pass `--model-impl transformers` to the `vllm serve` command for online serving.\n+\n+```bash\n+vllm serve meta-llama/Llama-3.2-1B \\\n+    --task generate \\\n+    --model-impl transformers\n+```\n+\n+vLLM uses [`AutoConfig.from_pretrained`] to load a model's `config.json` file from the Hub or your Hugging Face cache. It checks the `architectures` field against its internal model registry to determine which vLLM model class to load. If the model isn't in the registry, vLLM calls [`AutoModel.from_config`] to load the Transformers model implementation.\n+\n+Setting `model_impl=\"transformers\"` bypasses the vLLM model registry and loads directly from Transformers. vLLM replaces most model modules (MoE, attention, linear, etc.) with its own optimized versions.\n+\n+[`AutoTokenizer.from_pretrained`] loads tokenizer files. vLLM caches some tokenizer internals to reduce overhead during inference. Model weights download from the Hub in safetensors format.\n+\n+## Resources\n+\n+- [vLLM docs](https://docs.vllm.ai/en/latest/models/supported_models.html#transformers) for more usage examples and tips.\n+- [Integration with Hugging Face](https://docs.vllm.ai/en/latest/design/huggingface_integration/) explains how vLLM integrates with Transformers.\n\\ No newline at end of file"
        },
        {
            "sha": "984964d38fe5b04651026e20ed2f18ee64ac38c7",
            "filename": "docs/source/en/transformers_as_backend.md",
            "status": "removed",
            "additions": 0,
            "deletions": 206,
            "changes": 206,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f5cc5df90e0b5e99b47460729bc1ab11b816dd1/docs%2Fsource%2Fen%2Ftransformers_as_backend.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f5cc5df90e0b5e99b47460729bc1ab11b816dd1/docs%2Fsource%2Fen%2Ftransformers_as_backend.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftransformers_as_backend.md?ref=2f5cc5df90e0b5e99b47460729bc1ab11b816dd1",
            "patch": "@@ -1,206 +0,0 @@\n-<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Transformers as modeling backend\n-\n-Transformers' models are compatible with different inference servers like vLLM and SGLang. Instead of implementing a new model architecture from scratch for each inference server, you only need a model definition in `transformers`, which can be plugged into any inference server. It simplifies maintenance and makes it easy for users to use different inference servers for different use cases.\n-\n-With Transformers as a backend, you can also serve any model - including custom and Hub-hosted models - without waiting for native support.\n-\n-This guide shows how to use Transformers' models as a backend to some popular inference servers and how to build a model that supports all inference servers.\n-\n-## vLLM\n-\n-[vLLM](https://github.com/vllm-project/vllm) is a high-performance inference engine optimized for serving LLMs at scale. It supports many Transformers' models, including all decoder-only LLMs and several vision-language models (VLMs). VLMs currently support image inputs only, with video support planned.\n-\n-vLLM automatically selects the best backend, and if a model isn't natively supported, it falls back to the Transformers model. To explicitly use a Transformers' model, set `model_impl=\"transformers\"`.\n-\n-```python\n-from vllm import LLM\n-llm = LLM(model=\"meta-llama/Llama-3.2-1B\", model_impl=\"transformers\")\n-```\n-\n-Add `--model-impl transformers` to `vllm serve` to launch a server with a Transformers' model.\n-\n-```bash\n-vllm serve meta-llama/Llama-3.2-1B \\\n-    --task generate \\\n-    --model-impl transformers\n-```\n-\n-Refer to the [vLLM docs](https://docs.vllm.ai/en/latest/models/supported_models.html#transformers) for more usage examples and tips on using a Transformers as the backend.\n-\n-## SGLang\n-\n-[SGLang](https://github.com/InternLM/sglang) is a high-performance, OpenAI-compatible server and runtime designed for chat-based LLMs. It offers fast inference, role-based conversation handling, and support for custom pipelines, making it great for building real-world LLM apps.\n-\n-SGLang automatically falls back to the Transformers backend if a model isn't natively supported. To explicitly use a Transformers' model, set `impl=\"transformers\"`.\n-\n-```python\n-import sglang as sgl\n-\n-llm = sgl.Engine(\"meta-llama/Llama-3.2-1B-Instruct\", impl=\"transformers\")\n-print(llm.generate([\"The capital of France is\"], {\"max_new_tokens\": 20})[0])\n-```\n-\n-Add `impl transformers` to `sglang.launch_server` to launch a server with a Transformers' model.\n-\n-```bash\n-python3 -m sglang.launch_server \\\n-  --model-path kyutai/helium-1-preview-2b \\\n-  --impl transformers \\\n-  --host 0.0.0.0 \\\n-  --port 30000\n-```\n-\n-Refer to the [SGLang docs](https://docs.sglang.ai/supported_models/transformers_fallback.html) for more usage examples and tips on using a Transformers as the backend.\n-\n-## TGI\n-\n-[TGI](https://huggingface.co/docs/text-generation-inference/index) can serve models that aren't [natively implemented](https://huggingface.co/docs/text-generation-inference/supported_models) by falling back on the Transformers implementation of the model. Some of TGIs high-performance features aren't available in the Transformers implementation, but other features like continuous batching and streaming are still supported.\n-\n-> [!TIP]\n-> Refer to the [Non-core model serving](https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models) guide for more details.\n-\n-Serve a Transformers implementation the same way you'd serve a TGI model.\n-\n-```docker\n-docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id gpt2\n-```\n-\n-Add `--trust-remote_code` to the command to serve a custom Transformers model.\n-\n-```docker\n-docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id <CUSTOM_MODEL_ID> --trust-remote-code\n-```\n-\n-## Building a compatible model backend\n-\n-To ensure a model is compatible as a backend to any inference server, make sure it is compatible with Transformers and supports the [AttentionInterface](./attention_interface) class.\n-\n-1. A model must be Transformers-compatible following the model [contribution guidelines](./add_new_model) or the [custom model contribution guidelines](./custom_models). Make sure the model has a valid `config.json` in its directory and a valid `auto_map` field pointing to the model class in the config.\n-\n-2. A model's attentions needs to be configurable with the [AttentionInterface](./attention_interface) to allow custom and optimized attention functions. This is important for enabling the performance features of the different inference servers.\n-   Use `ALL_ATTENTION_FUNCTIONS` when defining the attention layer and propagate `**kwargs**` from the base `MyModel` class to the attention layers. Set `_supports_attention_backend` to `True` in [`PreTrainedModel`]. Expand the code below for an example.\n-\n-<details>\n-<summary>modeling_my_model.py</summary>\n-\n-```python\n-\n-from transformers import PreTrainedModel\n-from torch import nn\n-\n-class MyAttention(nn.Module):\n-\n-    def forward(self, hidden_states, **kwargs):\n-        ...\n-        attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n-        attn_output, attn_weights = attention_interface(\n-            self,\n-            query_states,\n-            key_states,\n-            value_states,\n-            **kwargs,\n-        )\n-        ...\n-\n-class MyModel(PreTrainedModel):\n-    _supports_attention_backend = True\n-```\n-\n-</details>\n-\n-3. This step is optional, but if you want to support tensor parallel and/or pipeline parallel features, add the following keys to the config.\n-    * `base_model_tp_plan` enables [tensor parallelism](./perf_infer_gpu_multi) by mapping fully qualified layer name patterns to tensor parallel styles. Only the `\"colwise\"` and `\"rowwise\"` partitioning strategies are currently supported.\n-    * `base_model_pp_plan` enables pipeline parallelism by mapping direct child layer names to tuples of lists of strings. The list in the first element of the tuple contains the names of the input arguments. The list in the last element of the tuple contains the names of the variables the layer outputs to in the modeling code.\n-\n- Expand the code below for an example.\n-\n-<details>\n-<summary>configuration_my_model.py</summary>\n-\n-```python\n-\n-from transformers import PreTrainedConfig\n-\n-class MyConfig(PreTrainedConfig):\n-    base_model_tp_plan = {\n-        \"layers.*.self_attn.k_proj\": \"colwise\",\n-        \"layers.*.self_attn.v_proj\": \"colwise\",\n-        \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.up_proj\": \"colwise\",\n-        \"layers.*.mlp.down_proj\": \"rowwise\",\n-    }\n-    base_model_pp_plan = {\n-        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n-        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n-        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n-    }\n-```\n-\n-</details>\n-\n-### Multimodal models\n-\n-For multimodal models, you need to include a few more changes on top of the general recommendations outlined in [\"contribuiting a model\"](./contributing#vision-language-model-contribution-checklist). These rules ensure that your model integrates properly and enables processing multimodal data.\n-\n-1. A multimodal model's processing class must have the `self.image_token` and `self.image_token_ids` attributes. These are placeholder tokens used to indicate image positions in the input. This placeholder token is the same token used in the input prompt to denote images and used in model code to scatter image features.\n-\n-2. The processing class needs `self._get_num_multimodal_tokens` method to compute the number of placeholder tokens needed for multimodal inputs with given sizes and to return a [`MultiModalData`] object. The placeholders between `<image>` tokens such as row or column tokens don't count as image placeholders. Only tokens that are actually replaced by image features later in modeling should be counted!\n-\n-3. The processor needs to check the value of `return_mm_token_type_ids` and return `mm_token_type_ids` to indicate whether each position is a text token (`0`), image placeholder token (`1`) or video placeholder token (`2`). Each multimodal token type ID sequence must be contiguous without breaks between consecutive tokens, therefore special tokens for begin/end/row/column must be treated as placeholders.\n-\n-Expand the code below for an example.\n-\n-<details>\n-<summary>processing_my_multimodal_model.py</summary>\n-\n-```python\n-class MyMultimodalProcessor(ProcessorMixin):\n-\n-    def __call__(self, images=None, text=None, **kwargs):\n-        if return_mm_token_type_ids:\n-            mm_token_type_ids = np.zeros_like(input_ids)\n-            mm_token_type_ids[input_ids == self.image_token_id] = 1\n-            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n-        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n-\n-    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n-        \"\"\"\n-        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n-        Args:\n-            image_sizes (`list[list[int]]`, *optional*):\n-                The input sizes formatted as (height, width) per each image.\n-        Returns:\n-            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n-            input modalities, along with other useful data.\n-        \"\"\"\n-        vision_data = {}\n-        if image_sizes is not None:\n-            num_image_tokens = [256] * len(image_sizes) # 256 placeholder tokens for each image always\n-            num_image_patches = [1] * len(image_sizes) # no patching, thus each image is processed as a single base image\n-            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n-        return MultiModalData(**vision_data)\n-```\n-\n-</details>\n-\n-## Resources\n-\n-* Read the [Transformers modeling backend integration in vLLM](https://blog.vllm.ai/2025/04/11/transformers-backend.html) blog post for more details about the Transformers modeling backend in vLLM.\n-* Read the [Transformers modeling  backend integration in SGLang](https://huggingface.co/blog/transformers-backend-sglang) blog post for more details about the Transformers modeling backend in SGLang."
        }
    ],
    "stats": {
        "total": 456,
        "additions": 248,
        "deletions": 208
    }
}