{
    "author": "gante",
    "message": "[tests] reorganize cache tests and clean memory between tests (#37684)",
    "sha": "755b0fa2fe85d13726585609efeac593d394783e",
    "files": [
        {
            "sha": "e5b43bec921e1e52ff274bd66456173378e76e35",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 140,
            "deletions": 156,
            "changes": 296,
            "blob_url": "https://github.com/huggingface/transformers/blob/755b0fa2fe85d13726585609efeac593d394783e/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/755b0fa2fe85d13726585609efeac593d394783e/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=755b0fa2fe85d13726585609efeac593d394783e",
            "patch": "@@ -20,6 +20,7 @@\n from transformers import set_seed\n from transformers.testing_utils import (\n     CaptureStderr,\n+    cleanup,\n     get_gpu_count,\n     is_torch_available,\n     require_gptq,\n@@ -53,6 +54,8 @@\n \n @require_torch\n class CacheTest(unittest.TestCase):\n+    \"\"\"Cache tests that don't require loading models\"\"\"\n+\n     def test_dynamic_cache_retrocompatibility(self):\n         \"\"\"Tests that we can convert back and forth between the legacy cache format and DynamicCache\"\"\"\n         legacy_cache = ()\n@@ -173,120 +176,17 @@ def _random_kvs(config):\n         self.assertTrue(cached_keys.shape == (1, 1, 10, 128))\n         self.assertTrue(cached_values.shape == (1, 1, 10, 128))\n \n-    def test_dynamic_cache_exportability(self):\n-        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n-        model = model.eval()\n-        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n-        prompt = \"What is the best way to debug python script?\"\n-        inputs = tokenizer(prompt, return_tensors=\"pt\")\n-        attention_mask = inputs.attention_mask\n-        input_ids = inputs.input_ids\n-\n-        past_key_values = DynamicCache()\n-        ep = torch.export.export(\n-            model,\n-            (),\n-            {\n-                \"input_ids\": input_ids,\n-                \"attention_mask\": attention_mask,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": True,\n-            },\n-            strict=False,\n-        )\n-        res = ep.module()(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            past_key_values=past_key_values,\n-            use_cache=True,\n-        )\n-        self.assertTrue(len(res.past_key_values.key_cache) == model.config.num_hidden_layers)\n-        self.assertEqual(2 * model.config.num_hidden_layers + 1, len(ep.graph_signature.output_specs))\n-        self.assertEqual(\n-            3,\n-            len(\n-                [\n-                    x\n-                    for x in ep.graph_signature.input_specs\n-                    if x.kind == torch.export.graph_signature.InputKind.USER_INPUT\n-                ]\n-            ),\n-        )\n \n-        past_key_values_eager = DynamicCache()\n-        res_eager = model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            past_key_values=past_key_values_eager,\n-            use_cache=True,\n-        )\n-        self.assertTrue(torch.allclose(res.logits, res_eager.logits))\n-        for k1, k2 in zip(res.past_key_values.key_cache, res_eager.past_key_values.key_cache):\n-            self.assertTrue(torch.allclose(k1, k2))\n+@require_torch_accelerator\n+class CacheIntegrationTest(unittest.TestCase):\n+    \"\"\"Cache tests that require loading models\"\"\"\n \n-        for v1, v2 in zip(res.past_key_values.value_cache, res_eager.past_key_values.value_cache):\n-            self.assertTrue(torch.allclose(v1, v2))\n+    def tearDown(self):\n+        # Some tests use large models, which might result in suboptimal torch re-allocation if we run multiple tests\n+        # in a row\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n-    @require_read_token\n-    def test_static_cache_exportability(self):\n-        \"\"\"\n-        Tests that static cache works with `torch.export()`\n-        \"\"\"\n-        if not is_torch_greater_or_equal(\"2.3\"):\n-            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n-\n-        set_seed(0)\n-        device = \"cpu\"\n-        dtype = \"bfloat16\"\n-        cache_implementation = \"static\"\n-        attn_implementation = \"sdpa\"  # Export and ExecuTorch only works for SdpaAttention\n-        batch_size = 1\n-        max_cache_len = 1234\n-        model = AutoModelForCausalLM.from_pretrained(\n-            \"google/gemma-2b\",\n-            device_map=device,\n-            torch_dtype=dtype,\n-            attn_implementation=attn_implementation,\n-            generation_config=GenerationConfig(\n-                use_cache=True,\n-                cache_implementation=cache_implementation,\n-                max_length=max_cache_len,\n-                cache_config={\n-                    \"batch_size\": batch_size,\n-                    \"max_cache_len\": max_cache_len,\n-                    \"device\": device,\n-                },\n-            ),\n-        )\n-        # Check if cache config is passed through correctly\n-        self.assertEqual(model.generation_config.use_cache, True)\n-        self.assertEqual(model.generation_config.cache_implementation, cache_implementation)\n-        self.assertEqual(model.generation_config.max_length, max_cache_len)\n-        self.assertTrue(model.generation_config.cache_config is not None)\n-        self.assertEqual(model.generation_config.cache_config.batch_size, batch_size)\n-        self.assertEqual(model.generation_config.cache_config.max_cache_len, max_cache_len)\n-\n-        exported_program = convert_and_export_with_cache(model)\n-\n-        # Check if the exported model is configured with the `StaticCache` correctly\n-        n_static_key_caches = n_static_value_caches = 0\n-        for buffer_name, buffer in exported_program.named_buffers():\n-            if buffer_name.startswith(\"key_cache\"):\n-                self.assertTrue(buffer.shape[0] == batch_size)\n-                self.assertTrue(buffer.shape[2] == max_cache_len)\n-                n_static_key_caches = n_static_key_caches + 1\n-            if buffer_name.startswith(\"value_cache\"):\n-                self.assertTrue(buffer.shape[0] == batch_size)\n-                self.assertTrue(buffer.shape[2] == max_cache_len)\n-                n_static_value_caches = n_static_value_caches + 1\n-        self.assertEqual(n_static_key_caches, model.config.num_hidden_layers)\n-        self.assertEqual(n_static_value_caches, model.config.num_hidden_layers)\n-\n-\n-@require_torch_accelerator\n-@slow\n-class CacheIntegrationTest(unittest.TestCase):\n     def test_dynamic_cache_hard(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", padding_side=\"left\")\n         model = AutoModelForCausalLM.from_pretrained(\n@@ -316,6 +216,7 @@ def test_dynamic_cache_hard(self):\n         )\n         self.assertEqual(decoded[0], expected_text)\n \n+    @slow\n     def test_dynamic_cache_batched(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", padding_side=\"left\")\n         tokenizer.pad_token = tokenizer.eos_token\n@@ -331,6 +232,7 @@ def test_dynamic_cache_batched(self):\n         expected_text = [\"A sequence: 1, 2, 3, 4, 5, 6, 7, 8,\", \"A sequence: A, B, C, D, E, F, G, H\"]\n         self.assertListEqual(decoded, expected_text)\n \n+    @slow\n     def test_dynamic_cache_beam_search(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", padding_side=\"left\")\n         model = AutoModelForCausalLM.from_pretrained(\n@@ -352,6 +254,7 @@ def test_dynamic_cache_beam_search(self):\n         ]\n         self.assertListEqual(decoded, expected_text)\n \n+    @slow\n     def test_hybrid_cache_n_sequences(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n         model = AutoModelForCausalLM.from_pretrained(\n@@ -379,6 +282,7 @@ def test_hybrid_cache_n_sequences(self):\n \n     @require_non_xpu\n     @require_gptq\n+    @slow\n     def test_sink_cache_hard(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/LLaMa-7B-GPTQ\")\n         model = AutoModelForCausalLM.from_pretrained(\"TheBloke/LLaMa-7B-GPTQ\", device_map=\"auto\")\n@@ -392,6 +296,7 @@ def test_sink_cache_hard(self):\n         decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n         self.assertTrue(decoded[0].endswith(\"to perform a variety of tasks. The Transformer is a neural network\"))\n \n+    @slow\n     def test_sink_cache_iterative_prompts(self):\n         \"\"\"Tests that SinkCache supports more than one new token at once, when shifting the cache\"\"\"\n         tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n@@ -434,13 +339,14 @@ def test_sink_cache_iterative_prompts(self):\n         )\n         self.assertTrue(decoded[0].endswith(last_output))\n \n-    @require_torch_gpu\n     @parameterized.expand(\n         [\n             (\"eager\", \"static\"),\n             (\"sdpa\", \"static\"),\n         ]\n     )\n+    @require_torch_gpu\n+    @slow\n     def test_static_cache_greedy_decoding_pad_left(self, attn_implementation, cache_implementation):\n         EXPECTED_GENERATION = [\n             \"The best color is the one that complements the skin tone of the\",\n@@ -479,44 +385,7 @@ def test_static_cache_greedy_decoding_pad_left(self, attn_implementation, cache_\n         with self.subTest(f\"{attn_implementation}, static, compiled\"):\n             self.assertListEqual(decoded, EXPECTED_GENERATION)\n \n-    @require_torch_gpu\n-    @parameterized.expand(\n-        [\n-            (\"eager\", \"static\"),\n-            (\"sdpa\", \"static\"),\n-        ]\n-    )\n-    def test_static_cache_greedy_decoding_pad_right(self, attn_implementation, cache_implementation):\n-        EXPECTED_GENERATION = [\n-            \"The best color is–ã the one that complements the skin tone of\",\n-            \"We should not undermind the issues at hand.\\nWe should not undermind the issues\",\n-        ]\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            \"NousResearch/Llama-2-7b-chat-hf\", padding_side=\"right\", pad_token=\"<s>\"\n-        )\n-        model = AutoModelForCausalLM.from_pretrained(\n-            \"NousResearch/Llama-2-7b-chat-hf\",\n-            torch_dtype=torch.bfloat16,\n-            attn_implementation=attn_implementation,\n-        ).to(torch_device)\n-        inputs = tokenizer(\n-            [\"The best color is\", \"We should not undermind the issues at hand\"], padding=True, return_tensors=\"pt\"\n-        ).to(model.device)\n-\n-        set_seed(0)\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        with self.subTest(f\"{attn_implementation}, dynamic\"):\n-            self.assertListEqual(decoded, EXPECTED_GENERATION)\n-\n-        set_seed(0)\n-        model.generation_config.cache_implementation = cache_implementation\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        with self.subTest(f\"{attn_implementation}, static, eager\"):\n-            self.assertListEqual(decoded, EXPECTED_GENERATION)\n-\n+    @slow\n     def test_dynamic_cache_extra_left_padding(self):\n         \"\"\"Tests that adding extra left-padding does not affect the generation with the dynamic cache\"\"\"\n         EXPECTED_GENERATION = [\n@@ -551,12 +420,8 @@ def test_dynamic_cache_extra_left_padding(self):\n         decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n         self.assertListEqual(decoded, EXPECTED_GENERATION)\n \n-    @parameterized.expand(\n-        [\n-            \"static\",\n-        ]\n-    )\n-    def test_static_cache_extra_left_padding(self, cache_implementation):\n+    @slow\n+    def test_static_cache_extra_left_padding(self):\n         \"\"\"Tests that adding extra left-padding does not affect the generation with the static cache\"\"\"\n         EXPECTED_GENERATION = [\n             \"The best color is the one that complements the skin tone of the\",\n@@ -574,7 +439,7 @@ def test_static_cache_extra_left_padding(self, cache_implementation):\n             [\"The best color is\", \"We should not undermind the issues at hand\"], padding=True, return_tensors=\"pt\"\n         ).to(model.device)\n \n-        model.generation_config.cache_implementation = cache_implementation\n+        model.generation_config.cache_implementation = \"static\"\n \n         gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n         decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n@@ -597,6 +462,7 @@ def test_static_cache_beam_search(self):\n         pass\n \n     @require_torch_accelerator\n+    @slow\n     def test_offloaded_cache_equivalent_to_dynamic_cache(self):\n         \"\"\"Tests that OffloadedCache produces the same result as the default DynamicCache\"\"\"\n         model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n@@ -625,6 +491,7 @@ def test_offloaded_cache_equivalent_to_dynamic_cache(self):\n             assert torch.all(original_output == offloaded_output).item()\n \n     @require_torch_accelerator\n+    @slow\n     def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n         \"\"\"Tests that OffloadedCache uses less memory than the default DynamicCache\"\"\"\n         model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n@@ -664,6 +531,7 @@ def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n         assert offloaded_peak_memory < original_peak_memory\n \n     @require_torch_gpu\n+    @slow\n     def test_cache_copy(self):\n         model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n@@ -745,6 +613,7 @@ def test_static_cache_no_cuda_graph_skips(self):\n         self.assertEqual(cap.err, \"\")\n \n     @require_torch_multi_gpu\n+    @slow\n     def test_static_cache_multi_gpu(self):\n         \"\"\"Regression test for #35164: static cache with multi-gpu\"\"\"\n \n@@ -764,3 +633,118 @@ def test_static_cache_multi_gpu(self):\n         inputs = tokenizer(\"Today is a beautiful day!\", return_tensors=\"pt\").to(0)\n         _ = model(**inputs)\n         _ = model.generate(**inputs, max_new_tokens=2, cache_implementation=\"hybrid\")\n+\n+\n+@require_torch\n+class CacheExportIntegrationTest(unittest.TestCase):\n+    \"\"\"Cache tests that rely on `torch.export()` and model loading\"\"\"\n+\n+    def test_dynamic_cache_exportability(self):\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+        model = model.eval()\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+        prompt = \"What is the best way to debug python script?\"\n+        inputs = tokenizer(prompt, return_tensors=\"pt\")\n+        attention_mask = inputs.attention_mask\n+        input_ids = inputs.input_ids\n+\n+        past_key_values = DynamicCache()\n+        ep = torch.export.export(\n+            model,\n+            (),\n+            {\n+                \"input_ids\": input_ids,\n+                \"attention_mask\": attention_mask,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": True,\n+            },\n+            strict=False,\n+        )\n+        res = ep.module()(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            use_cache=True,\n+        )\n+        self.assertTrue(len(res.past_key_values.key_cache) == model.config.num_hidden_layers)\n+        self.assertEqual(2 * model.config.num_hidden_layers + 1, len(ep.graph_signature.output_specs))\n+        self.assertEqual(\n+            3,\n+            len(\n+                [\n+                    x\n+                    for x in ep.graph_signature.input_specs\n+                    if x.kind == torch.export.graph_signature.InputKind.USER_INPUT\n+                ]\n+            ),\n+        )\n+\n+        past_key_values_eager = DynamicCache()\n+        res_eager = model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values_eager,\n+            use_cache=True,\n+        )\n+        self.assertTrue(torch.allclose(res.logits, res_eager.logits))\n+        for k1, k2 in zip(res.past_key_values.key_cache, res_eager.past_key_values.key_cache):\n+            self.assertTrue(torch.allclose(k1, k2))\n+\n+        for v1, v2 in zip(res.past_key_values.value_cache, res_eager.past_key_values.value_cache):\n+            self.assertTrue(torch.allclose(v1, v2))\n+\n+    @slow\n+    @require_read_token\n+    def test_static_cache_exportability(self):\n+        \"\"\"\n+        Tests that static cache works with `torch.export()`\n+        \"\"\"\n+        if not is_torch_greater_or_equal(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        set_seed(0)\n+        device = \"cpu\"\n+        dtype = \"bfloat16\"\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"  # Export and ExecuTorch only works for SdpaAttention\n+        batch_size = 1\n+        max_cache_len = 1234\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"google/gemma-2b\",\n+            device_map=device,\n+            torch_dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_cache_len,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_cache_len,\n+                    \"device\": device,\n+                },\n+            ),\n+        )\n+        # Check if cache config is passed through correctly\n+        self.assertEqual(model.generation_config.use_cache, True)\n+        self.assertEqual(model.generation_config.cache_implementation, cache_implementation)\n+        self.assertEqual(model.generation_config.max_length, max_cache_len)\n+        self.assertTrue(model.generation_config.cache_config is not None)\n+        self.assertEqual(model.generation_config.cache_config.batch_size, batch_size)\n+        self.assertEqual(model.generation_config.cache_config.max_cache_len, max_cache_len)\n+\n+        exported_program = convert_and_export_with_cache(model)\n+\n+        # Check if the exported model is configured with the `StaticCache` correctly\n+        n_static_key_caches = n_static_value_caches = 0\n+        for buffer_name, buffer in exported_program.named_buffers():\n+            if buffer_name.startswith(\"key_cache\"):\n+                self.assertTrue(buffer.shape[0] == batch_size)\n+                self.assertTrue(buffer.shape[2] == max_cache_len)\n+                n_static_key_caches = n_static_key_caches + 1\n+            if buffer_name.startswith(\"value_cache\"):\n+                self.assertTrue(buffer.shape[0] == batch_size)\n+                self.assertTrue(buffer.shape[2] == max_cache_len)\n+                n_static_value_caches = n_static_value_caches + 1\n+        self.assertEqual(n_static_key_caches, model.config.num_hidden_layers)\n+        self.assertEqual(n_static_value_caches, model.config.num_hidden_layers)"
        }
    ],
    "stats": {
        "total": 296,
        "additions": 140,
        "deletions": 156
    }
}