{
    "author": "abhi-glitchhg",
    "message": "Bug fix for drop path decay rate in swin transformer (#34291)\n\n* potential bug fix for drop path\r\n\r\n* variable name change\r\n\r\n* forgot to rename the variables\r\n\r\n* back to original\r\n\r\n* modify dpr properly\r\n\r\n* check_copies auto fix\r\n\r\n* corresponsing swin2 changes\r\n\r\n* auto fix\r\n\r\n* linting\r\n\r\n* default value for drop_path_rate as 0.0\r\n\r\n* Update src/transformers/models/glm/modeling_glm.py\r\n\r\n* maskformer fix\r\n\r\n* ruff format\r\n\r\n* changes made to tf code as well\r\n\r\n* lint\r\n\r\n---------\r\n\r\nCo-authored-by: abhijit deo <167164474+deo-abhijit@users.noreply.github.com>",
    "sha": "56c45d575786de60acba02838fb2b0d1176b4ff7",
    "files": [
        {
            "sha": "f422b17b204f13475068f8d906ffe631bae51e2a",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=56c45d575786de60acba02838fb2b0d1176b4ff7",
            "patch": "@@ -575,15 +575,15 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n # Copied from transformers.models.swin.modeling_swin.SwinLayer with SwinDropPath->ClapDropPath, Swin->ClapAudio\n class ClapAudioLayer(nn.Module):\n-    def __init__(self, config, dim, input_resolution, num_heads, shift_size=0):\n+    def __init__(self, config, dim, input_resolution, num_heads, drop_path_rate=0.0, shift_size=0):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.shift_size = shift_size\n         self.window_size = config.window_size\n         self.input_resolution = input_resolution\n         self.layernorm_before = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.attention = ClapAudioAttention(config, dim, num_heads, window_size=self.window_size)\n-        self.drop_path = ClapDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+        self.drop_path = ClapDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n         self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.intermediate = ClapAudioIntermediate(config, dim)\n         self.output = ClapAudioOutput(config, dim)\n@@ -712,6 +712,7 @@ def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, d\n                     dim=dim,\n                     input_resolution=input_resolution,\n                     num_heads=num_heads,\n+                    drop_path_rate=drop_path[i],\n                     shift_size=0 if (i % 2 == 0) else config.window_size // 2,\n                 )\n                 for i in range(depth)"
        },
        {
            "sha": "2d5272e8642ee53a93e2e591c7c4db432e5eb456",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=56c45d575786de60acba02838fb2b0d1176b4ff7",
            "patch": "@@ -558,15 +558,15 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n # Copied from transformers.models.swin.modeling_swin.SwinLayer with Swin->DonutSwin\n class DonutSwinLayer(nn.Module):\n-    def __init__(self, config, dim, input_resolution, num_heads, shift_size=0):\n+    def __init__(self, config, dim, input_resolution, num_heads, drop_path_rate=0.0, shift_size=0):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.shift_size = shift_size\n         self.window_size = config.window_size\n         self.input_resolution = input_resolution\n         self.layernorm_before = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.attention = DonutSwinAttention(config, dim, num_heads, window_size=self.window_size)\n-        self.drop_path = DonutSwinDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+        self.drop_path = DonutSwinDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n         self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.intermediate = DonutSwinIntermediate(config, dim)\n         self.output = DonutSwinOutput(config, dim)\n@@ -695,6 +695,7 @@ def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, d\n                     dim=dim,\n                     input_resolution=input_resolution,\n                     num_heads=num_heads,\n+                    drop_path_rate=drop_path[i],\n                     shift_size=0 if (i % 2 == 0) else config.window_size // 2,\n                 )\n                 for i in range(depth)"
        },
        {
            "sha": "598e1d8186a24a0e4abc837bda4a8994d1929c90",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=56c45d575786de60acba02838fb2b0d1176b4ff7",
            "patch": "@@ -520,16 +520,14 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class MaskFormerSwinLayer(nn.Module):\n-    def __init__(self, config, dim, input_resolution, num_heads, shift_size=0):\n+    def __init__(self, config, dim, input_resolution, num_heads, drop_path_rate=0.0, shift_size=0):\n         super().__init__()\n         self.shift_size = shift_size\n         self.window_size = config.window_size\n         self.input_resolution = input_resolution\n         self.layernorm_before = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.attention = MaskFormerSwinAttention(config, dim, num_heads, self.window_size)\n-        self.drop_path = (\n-            MaskFormerSwinDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n-        )\n+        self.drop_path = MaskFormerSwinDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n         self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.intermediate = MaskFormerSwinIntermediate(config, dim)\n         self.output = MaskFormerSwinOutput(config, dim)\n@@ -644,6 +642,7 @@ def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, d\n                     dim=dim,\n                     input_resolution=input_resolution,\n                     num_heads=num_heads,\n+                    drop_path_rate=drop_path[i],\n                     shift_size=0 if (i % 2 == 0) else config.window_size // 2,\n                 )\n                 for i in range(depth)"
        },
        {
            "sha": "23f0ba6da620cd0b68cfc3366c58815a5247acbb",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=56c45d575786de60acba02838fb2b0d1176b4ff7",
            "patch": "@@ -635,15 +635,15 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class SwinLayer(nn.Module):\n-    def __init__(self, config, dim, input_resolution, num_heads, shift_size=0):\n+    def __init__(self, config, dim, input_resolution, num_heads, drop_path_rate=0.0, shift_size=0):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.shift_size = shift_size\n         self.window_size = config.window_size\n         self.input_resolution = input_resolution\n         self.layernorm_before = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.attention = SwinAttention(config, dim, num_heads, window_size=self.window_size)\n-        self.drop_path = SwinDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+        self.drop_path = SwinDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n         self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.intermediate = SwinIntermediate(config, dim)\n         self.output = SwinOutput(config, dim)\n@@ -771,6 +771,7 @@ def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, d\n                     dim=dim,\n                     input_resolution=input_resolution,\n                     num_heads=num_heads,\n+                    drop_path_rate=drop_path[i],\n                     shift_size=0 if (i % 2 == 0) else config.window_size // 2,\n                 )\n                 for i in range(depth)"
        },
        {
            "sha": "f1aa0bfef743ad6556a812f6cd77a04ecc3c63df",
            "filename": "src/transformers/models/swin/modeling_tf_swin.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py?ref=56c45d575786de60acba02838fb2b0d1176b4ff7",
            "patch": "@@ -742,7 +742,14 @@ def build(self, input_shape=None):\n \n class TFSwinLayer(keras.layers.Layer):\n     def __init__(\n-        self, config, dim, input_resolution: Tuple[int, int], num_heads: int, shift_size: int = 0, **kwargs\n+        self,\n+        config,\n+        dim,\n+        input_resolution: Tuple[int, int],\n+        num_heads: int,\n+        drop_path_rate: float = 0.0,\n+        shift_size: int = 0,\n+        **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -754,8 +761,8 @@ def __init__(\n         self.layernorm_before = keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layernorm_before\")\n         self.attention = TFSwinAttention(config, dim, num_heads, name=\"attention\")\n         self.drop_path = (\n-            TFSwinDropPath(config.drop_path_rate, name=\"drop_path\")\n-            if config.drop_path_rate > 0.0\n+            TFSwinDropPath(drop_path_rate, name=\"drop_path\")\n+            if drop_path_rate > 0.0\n             else keras.layers.Activation(\"linear\", name=\"drop_path\")\n         )\n         self.layernorm_after = keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"layernorm_after\")\n@@ -913,6 +920,7 @@ def __init__(\n                 input_resolution=input_resolution,\n                 num_heads=num_heads,\n                 shift_size=0 if (i % 2 == 0) else config.window_size // 2,\n+                drop_path_rate=drop_path[i],\n                 name=f\"blocks.{i}\",\n             )\n             for i in range(depth)"
        },
        {
            "sha": "d6bd8da9bed638b4e0cd80c2736e7d5923193391",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=56c45d575786de60acba02838fb2b0d1176b4ff7",
            "patch": "@@ -482,7 +482,9 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n # Copied from transformers.models.swinv2.modeling_swinv2.Swinv2Layer with Swinv2->Swin2SR\n class Swin2SRLayer(nn.Module):\n-    def __init__(self, config, dim, input_resolution, num_heads, shift_size=0, pretrained_window_size=0):\n+    def __init__(\n+        self, config, dim, input_resolution, num_heads, drop_path_rate=0.0, shift_size=0, pretrained_window_size=0\n+    ):\n         super().__init__()\n         self.input_resolution = input_resolution\n         window_size, shift_size = self._compute_window_shift(\n@@ -500,7 +502,7 @@ def __init__(self, config, dim, input_resolution, num_heads, shift_size=0, pretr\n             else (pretrained_window_size, pretrained_window_size),\n         )\n         self.layernorm_before = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n-        self.drop_path = Swin2SRDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+        self.drop_path = Swin2SRDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n         self.intermediate = Swin2SRIntermediate(config, dim)\n         self.output = Swin2SROutput(config, dim)\n         self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)"
        },
        {
            "sha": "191923958cfbdedfa195c14727986ad6071975f2",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56c45d575786de60acba02838fb2b0d1176b4ff7/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=56c45d575786de60acba02838fb2b0d1176b4ff7",
            "patch": "@@ -683,7 +683,9 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class Swinv2Layer(nn.Module):\n-    def __init__(self, config, dim, input_resolution, num_heads, shift_size=0, pretrained_window_size=0):\n+    def __init__(\n+        self, config, dim, input_resolution, num_heads, drop_path_rate=0.0, shift_size=0, pretrained_window_size=0\n+    ):\n         super().__init__()\n         self.input_resolution = input_resolution\n         window_size, shift_size = self._compute_window_shift(\n@@ -701,7 +703,7 @@ def __init__(self, config, dim, input_resolution, num_heads, shift_size=0, pretr\n             else (pretrained_window_size, pretrained_window_size),\n         )\n         self.layernorm_before = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n-        self.drop_path = Swinv2DropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+        self.drop_path = Swinv2DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n         self.intermediate = Swinv2Intermediate(config, dim)\n         self.output = Swinv2Output(config, dim)\n         self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n@@ -819,6 +821,7 @@ def __init__(\n                 dim=dim,\n                 input_resolution=input_resolution,\n                 num_heads=num_heads,\n+                drop_path_rate=drop_path[i],\n                 shift_size=0 if (i % 2 == 0) else config.window_size // 2,\n                 pretrained_window_size=pretrained_window_size,\n             )"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 32,
        "deletions": 17
    }
}