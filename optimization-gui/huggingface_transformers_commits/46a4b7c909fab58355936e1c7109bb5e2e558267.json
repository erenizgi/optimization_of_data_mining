{
    "author": "S1ro1",
    "message": "Feat: save_pretrained for tensor parallel (and other parallelisms) models (#37919)\n\n* tmp: initial save pretrained with dtensors\n\n* Feat: add correctness tests\n\n* Refactor: version checks\n\n* Temp: 1:1 checkpoint llama4\n\n* refactor\n\n* Tests\n\n* Feat: works\n\n* Style\n\n* Feat: version checks + minor fixes\n\n* Style\n\n* Fix: version checks in tests\n\n* Feat: move more stuff into tensor_parallel.py",
    "sha": "46a4b7c909fab58355936e1c7109bb5e2e558267",
    "files": [
        {
            "sha": "e788321b49bd9d5248f3daf6f8bfaa679a92d37d",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 126,
            "deletions": 6,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=46a4b7c909fab58355936e1c7109bb5e2e558267",
            "patch": "@@ -61,6 +61,22 @@ def _blocks_to_block_sizes(total_size: int, blocks: Union[int, List[int]]) -> Li\n         return [single_size] * blocks\n \n \n+def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str]) -> Optional[str]:\n+    \"\"\"\n+    Get the TP style for a parameter from the TP plan.\n+\n+    The TP plan is a dictionary that maps parameter names to TP styles.\n+    The parameter name can be a generic name with wildcards (e.g. \"*.weight\") or a specific name (e.g. \"layer_1.weight\").\n+    \"\"\"\n+    generic_param_name = re.sub(r\"\\d+\", \"*\", parameter_name)\n+    if generic_param_name in tp_plan:\n+        return tp_plan[generic_param_name]\n+    elif \".\" in generic_param_name and generic_param_name.rsplit(\".\", 1)[0] in tp_plan:\n+        return tp_plan[generic_param_name.rsplit(\".\", 1)[0]]\n+    else:\n+        return None\n+\n+\n str_to_torch_dtype = {\n     \"BOOL\": torch.bool,\n     \"U8\": torch.uint8,\n@@ -138,6 +154,71 @@ def get_packed_weights(param, empty_param, device_mesh, rank, dim):\n     return tensor.to(str_to_torch_dtype[slice_dtype])\n \n \n+def repack_weights(\n+    packed_parameter: torch.Tensor,\n+    sharded_dim: int,  # The dimension index in the global tensor that was sharded\n+    world_size: int,\n+    num_blocks: int = 2,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Reorders a tensor that was reconstructed from sharded packed weights into its canonical packed format.\n+\n+    For example, if a weight was packed (e.g., gate_proj and up_proj) and then sharded,\n+    DTensor.full_tensor() might produce an interleaved layout like [G0, U0, G1, U1, ...]\n+    along the sharded dimension. This function reorders it to [G0, G1, ..., U0, U1, ...].\n+    This is an inverse operation to get_packed_weights.\n+\n+    Args:\n+        reconstructed_tensor: The tensor reconstructed from DTensor (e.g., via .full_tensor().contiguous()).\n+        sharded_dim: The dimension index in the reconstructed_tensor that was originally sharded.\n+        world_size: The tensor parallel world size.\n+        num_packed_projs: The number of projections that were packed together (e.g., 2 for gate_up_proj).\n+\n+    Returns:\n+        The reordered tensor in canonical packed format.\n+    \"\"\"\n+\n+    if num_blocks != 2:\n+        raise ValueError(\n+            \"Num blocks different from 2 is not supported yet. This is most likely a bug in your implementation as we only pack gate and up projections together.\"\n+        )\n+\n+    actual_sharded_dim = sharded_dim if sharded_dim >= 0 else sharded_dim + packed_parameter.ndim\n+    total_size_on_sharded_dim = packed_parameter.shape[actual_sharded_dim]\n+    original_block_size_on_dim = total_size_on_sharded_dim // num_blocks\n+    shard_chunk_size = original_block_size_on_dim // world_size\n+\n+    prefix_shape = packed_parameter.shape[:actual_sharded_dim]\n+    suffix_shape = packed_parameter.shape[actual_sharded_dim + 1 :]\n+\n+    tensor_view = packed_parameter.view(\n+        *prefix_shape,\n+        world_size,\n+        num_blocks,\n+        shard_chunk_size,\n+        *suffix_shape,\n+    )\n+\n+    # Permute to bring num_packed_projs first, then world_size, then shard_chunk_size\n+    # This groups all chunks of G together, then all chunks of U together.\n+    # Target order of these middle dimensions: (num_packed_projs, world_size, shard_chunk_size)\n+    # Current order of view's middle dimensions: (world_size, num_packed_projs, shard_chunk_size)\n+    # Absolute indices of the dimensions to be permuted (world_size, num_packed_projs)\n+    axis_ws_abs = len(prefix_shape)\n+    axis_npp_abs = len(prefix_shape) + 1\n+\n+    permute_order = list(range(tensor_view.ndim))\n+    permute_order[axis_ws_abs], permute_order[axis_npp_abs] = permute_order[axis_npp_abs], permute_order[axis_ws_abs]\n+\n+    tensor_permuted = tensor_view.permute(*permute_order)\n+\n+    # Reshape back to the original tensor's ndim, with the sharded dimension now correctly ordered as [G_all, U_all].\n+    # The final shape should be the same as reconstructed_tensor.\n+    final_ordered_tensor = tensor_permuted.reshape_as(packed_parameter)\n+\n+    return final_ordered_tensor\n+\n+\n def get_tensor_shard(param, empty_param, device_mesh, rank, dim):\n     if dim == 0:\n         size_ = empty_param.shape[0]\n@@ -578,6 +659,49 @@ def translate_to_torch_parallel_style(style: str):\n         raise ValueError(f\"Unsupported parallel style value: {style}\")\n \n \n+def convert_local_tensor_to_dtensor(\n+    parameter: torch.Tensor, parameter_name: str, device_mesh, tp_plan: dict[str, str]\n+) -> DTensor:\n+    \"\"\"\n+    Converts a local variant of weights to a DTensor with corresponding placements. Shouldn't be done ever except of before saving the model.\n+    \"\"\"\n+    _, param_type = parameter_name.rsplit(\".\", 1) if \".\" in parameter_name else parameter_name\n+    tp_style = _get_parameter_tp_plan(parameter_name, tp_plan)\n+    if not tp_style:\n+        return parameter\n+\n+    if tp_style not in [\"local_packed_rowwise\", \"local_rowwise\", \"local_colwise\"]:\n+        return parameter\n+    # TODO: this logic should be wrapped in a function, this is copied from corresponding tp classes.\n+    if tp_style == \"local_packed_rowwise\":\n+        placements = [Shard(-1)]\n+    elif tp_style == \"local_rowwise\":\n+        if param_type == \"bias\":\n+            placements = [Replicate()]\n+        else:\n+            placements = [Shard(-1)]\n+    elif tp_style == \"local_colwise\":\n+        if param_type == \"bias\":\n+            placements = [Shard(-1)]\n+        else:\n+            placements = [Shard(-2)]\n+    return DTensor.from_local(parameter, device_mesh, placements, run_check=False)\n+\n+\n+def replace_state_dict_local_with_dtensor(\n+    state_dict: dict[str, torch.Tensor],\n+    tp_plan: dict[str, str],\n+    device_mesh,\n+) -> dict[str, torch.Tensor]:\n+    \"\"\"\n+    Replaces all tensors that were sharded with `local_*` strategy with DTensor to make determining their proper size possible.\n+    \"\"\"\n+    for key, value in state_dict.items():\n+        if isinstance(value, torch.Tensor) and not isinstance(value, DTensor):\n+            state_dict[key] = convert_local_tensor_to_dtensor(value, key, device_mesh, tp_plan)\n+    return state_dict\n+\n+\n def add_tensor_parallel_hooks_to_module(model, module, tp_plan, layer_name, current_module_plan, device_mesh):\n     \"\"\"\n     Add hooks to the module holding the layer. Meaning:\n@@ -632,13 +756,9 @@ def shard_and_distribute_module(\n     param_name, param_type = parameter_name.rsplit(\".\", 1) if \".\" in parameter_name else parameter_name\n     tp_plan = model._tp_plan\n     module_to_tp = model.get_submodule(param_name)\n-    current_module_plan = None\n     rank = int(rank)\n-    generic_param_name = re.sub(r\"\\d+\", \"*\", parameter_name)\n-    if generic_param_name in tp_plan:\n-        current_module_plan = tp_plan[generic_param_name]\n-    elif \".\" in generic_param_name and generic_param_name.rsplit(\".\", 1)[0] in tp_plan:\n-        current_module_plan = tp_plan[generic_param_name.rsplit(\".\", 1)[0]]\n+\n+    current_module_plan = _get_parameter_tp_plan(parameter_name, tp_plan)\n \n     # Add hooks to the module if not done yet\n     # add_tensor_parallel_hooks_to_module(model, module_to_tp, tp_plan, param_name, current_module_plan, device_mesh)"
        },
        {
            "sha": "b40b7cd2b388baff807959125caeeb2ce3e9f13e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 2,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=46a4b7c909fab58355936e1c7109bb5e2e558267",
            "patch": "@@ -63,6 +63,9 @@\n from .integrations.sdpa_attention import sdpa_attention_forward\n from .integrations.tensor_parallel import (\n     SUPPORTED_TP_STYLES,\n+    _get_parameter_tp_plan,\n+    repack_weights,\n+    replace_state_dict_local_with_dtensor,\n     shard_and_distribute_module,\n     verify_tp_plan,\n )\n@@ -123,6 +126,7 @@\n from .utils.hub import create_and_tag_model_card, get_checkpoint_shard_files\n from .utils.import_utils import (\n     ENV_VARS_TRUE_VALUES,\n+    is_huggingface_hub_greater_or_equal,\n     is_sagemaker_mp_enabled,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n@@ -168,6 +172,9 @@\n _is_ds_init_called = False\n _torch_distributed_available = torch.distributed.is_available()\n \n+if _torch_distributed_available and is_torch_greater_or_equal(\"2.5\"):\n+    from torch.distributed.tensor import DTensor\n+\n \n def is_fsdp_enabled():\n     return (\n@@ -3413,6 +3420,12 @@ def save_pretrained(\n         if safe_serialization and not is_safetensors_available():\n             raise ImportError(\"`safe_serialization` requires the `safetensors library: `pip install safetensors`.\")\n \n+        # we need to check against tp_size, not tp_plan, as tp_plan is substituted to the class one\n+        if self._tp_size is not None and not is_huggingface_hub_greater_or_equal(\"0.31.4\"):\n+            raise ImportError(\n+                \"Saving a model with tensor parallelism requires `huggingface_hub` version 0.31.4 or higher.\"\n+            )\n+\n         if os.path.isfile(save_directory):\n             logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n             return\n@@ -3540,6 +3553,10 @@ def save_pretrained(\n         # Rename state_dict keys before saving to file. Do nothing unless overridden in a particular model.\n         # (initially introduced with TimmWrapperModel to remove prefix and make checkpoints compatible with timm)\n         state_dict = self._fix_state_dict_keys_on_save(state_dict)\n+        # If model was sharded, we cannot properly determine sizes of tensors that `local_*` strategy was used,\n+        # therefore we replace them with DTensors that are equivalently sharded\n+        if self._tp_size is not None:\n+            state_dict = replace_state_dict_local_with_dtensor(state_dict, self._tp_plan, self._device_mesh)\n \n         if safe_serialization:\n             # Safetensors does not allow tensor aliasing.\n@@ -3548,7 +3565,7 @@ def save_pretrained(\n             for name, tensor in state_dict.items():\n                 # Sometimes in the state_dict we have non-tensor objects.\n                 # e.g. in bitsandbytes we have some `str` objects in the state_dict\n-                if isinstance(tensor, torch.Tensor):\n+                if isinstance(tensor, torch.Tensor) or isinstance(tensor, DTensor):\n                     ptrs[id_tensor_storage(tensor)].append(name)\n                 else:\n                     # In the non-tensor case, fall back to the pointer of the object itself\n@@ -3658,7 +3675,14 @@ def save_pretrained(\n         for shard_file, tensors in filename_to_tensors:\n             shard = {}\n             for tensor in tensors:\n-                shard[tensor] = state_dict[tensor].contiguous()\n+                if isinstance(state_dict[tensor], DTensor):\n+                    full_tensor = state_dict[tensor].full_tensor()\n+                    # to get the correctly ordered tensor we need to repack if packed\n+                    if _get_parameter_tp_plan(tensor, self._tp_plan) in (\"local_packed_rowwise\",):\n+                        full_tensor = repack_weights(full_tensor, -1, self._tp_size, 2)\n+                    shard[tensor] = full_tensor.contiguous()  # only do contiguous after it's permuted correctly\n+                else:\n+                    shard[tensor] = state_dict[tensor].contiguous()\n                 # delete reference, see https://github.com/huggingface/transformers/pull/34890\n                 del state_dict[tensor]\n \n@@ -4606,6 +4630,7 @@ def _assign_original_dtype(module):\n \n         # record tp degree the model sharded to\n         model._tp_size = tp_size\n+        model._device_mesh = device_mesh\n \n         # make sure token embedding weights are still tied if needed\n         model.tie_weights()"
        },
        {
            "sha": "ca60a05a2655b76593c65ef2aa3c6f06a56a10f2",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=46a4b7c909fab58355936e1c7109bb5e2e558267",
            "patch": "@@ -296,6 +296,13 @@ def id_tensor_storage(tensor: torch.Tensor) -> tuple[torch.device, int, int]:\n     guaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\n     non-overlapping lifetimes may have the same id.\n     \"\"\"\n+    if is_torch_greater_or_equal_than_2_0:\n+        from torch.distributed.tensor import DTensor\n+\n+        if isinstance(tensor, DTensor):\n+            local_tensor = tensor.to_local()\n+            return tensor.device, local_tensor.storage().data_ptr(), tensor.nbytes\n+\n     if tensor.device.type == \"xla\" and is_torch_xla_available():\n         # NOTE: xla tensors dont have storage\n         # use some other unique id to distinguish."
        },
        {
            "sha": "dd0a2a02d1f11297c5167f0d5eb171a7b8c8821c",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=46a4b7c909fab58355936e1c7109bb5e2e558267",
            "patch": "@@ -97,6 +97,7 @@\n     is_grokadamw_available,\n     is_hadamard_available,\n     is_hqq_available,\n+    is_huggingface_hub_greater_or_equal,\n     is_ipex_available,\n     is_jieba_available,\n     is_jinja_available,\n@@ -542,6 +543,21 @@ def decorator(test_case):\n     return decorator\n \n \n+def require_huggingface_hub_greater_or_equal(version: str):\n+    \"\"\"\n+    Decorator marking a test that requires huggingface_hub version >= `version`.\n+\n+    These tests are skipped when huggingface_hub version is less than `version`.\n+    \"\"\"\n+\n+    def decorator(test_case):\n+        return unittest.skipUnless(\n+            is_huggingface_hub_greater_or_equal(version), f\"test requires huggingface_hub version >= {version}\"\n+        )(test_case)\n+\n+    return decorator\n+\n+\n def require_flash_attn(test_case):\n     \"\"\"\n     Decorator marking a test that requires Flash Attention."
        },
        {
            "sha": "957662f920116076517810e19d5ebae5074b5a0a",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=46a4b7c909fab58355936e1c7109bb5e2e558267",
            "patch": "@@ -167,6 +167,7 @@\n     is_habana_gaudi1,\n     is_hadamard_available,\n     is_hqq_available,\n+    is_huggingface_hub_greater_or_equal,\n     is_in_notebook,\n     is_ipex_available,\n     is_jieba_available,"
        },
        {
            "sha": "58b8f9fad9f2b6d14d423dad4150f2156c478e4a",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46a4b7c909fab58355936e1c7109bb5e2e558267/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=46a4b7c909fab58355936e1c7109bb5e2e558267",
            "patch": "@@ -1077,6 +1077,19 @@ def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n         return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n \n \n+@lru_cache()\n+def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool = False):\n+    if not _is_package_available(\"huggingface_hub\"):\n+        return False\n+\n+    if accept_dev:\n+        return version.parse(\n+            version.parse(importlib.metadata.version(\"huggingface_hub\")).base_version\n+        ) >= version.parse(library_version)\n+    else:\n+        return version.parse(importlib.metadata.version(\"huggingface_hub\")) >= version.parse(library_version)\n+\n+\n def is_torchdistx_available():\n     return _torchdistx_available\n "
        },
        {
            "sha": "a8ca73263587f5b2edcd10f9031ec45ad35bd664",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 81,
            "deletions": 4,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/46a4b7c909fab58355936e1c7109bb5e2e558267/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46a4b7c909fab58355936e1c7109bb5e2e558267/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=46a4b7c909fab58355936e1c7109bb5e2e558267",
            "patch": "@@ -12,14 +12,17 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import os\n import subprocess\n import tempfile\n import textwrap\n \n from transformers import is_torch_available\n+from transformers.integrations.tensor_parallel import get_packed_weights, repack_weights\n from transformers.testing_utils import (\n     TestCasePlus,\n     get_torch_dist_unique_port,\n+    require_huggingface_hub_greater_or_equal,\n     require_torch_multi_gpu,\n )\n \n@@ -28,19 +31,51 @@\n     import torch\n \n \n+class TestTensorParallelUtils(TestCasePlus):\n+    def test_packed_unpacked_conversion(self):\n+        WORLD_SIZE = 2\n+        PACKED_BLOCK_SIZE = 800\n+        SHARDING_DIM = 2\n+        NUM_BLOCKS = 2\n+\n+        original_packed_weights = torch.randn(4, 512, 2 * PACKED_BLOCK_SIZE)\n+        original_packed_weights.get_dtype = lambda: \"F32\"  # get_packed_weights expects PySlice object\n+        empty_param = torch.empty(4, 512, 2 * PACKED_BLOCK_SIZE)\n+\n+        class MockDeviceMesh:\n+            def size(self):\n+                return WORLD_SIZE\n+\n+        mock_mesh = (\n+            MockDeviceMesh()\n+        )  # get_packed_weights only calls `.size()`, do this to avoid doing actual distributed run\n+\n+        packed_weights_0 = get_packed_weights(original_packed_weights, empty_param, mock_mesh, 0, SHARDING_DIM)\n+        packed_weights_1 = get_packed_weights(original_packed_weights, empty_param, mock_mesh, 1, SHARDING_DIM)\n+\n+        # simulate all gather of sharded weights\n+        packed_weights = torch.cat([packed_weights_0, packed_weights_1], dim=SHARDING_DIM)\n+        unpacked_weights = repack_weights(packed_weights, SHARDING_DIM, WORLD_SIZE, NUM_BLOCKS)\n+\n+        assert torch.allclose(unpacked_weights, original_packed_weights)\n+\n+\n # RUN_SLOW=1 pytest -sv tests/tensor_parallel/test_tensor_parallel.py\n class TestTensorParallel(TestCasePlus):\n     nproc_per_node = 2\n \n-    def torchrun(self, script: str):\n+    def torchrun(self, script: str, is_torchrun: bool = True):\n         \"\"\"Run the `script` using `torchrun` command for multi-processing in a subprocess. Captures errors as necessary.\"\"\"\n         with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as tmp:\n             tmp.write(script)\n             tmp.flush()\n             tmp.seek(0)\n-            cmd = (\n-                f\"torchrun --nproc_per_node {self.nproc_per_node} --master_port {get_torch_dist_unique_port()} {tmp.name}\"\n-            ).split()\n+            if is_torchrun:\n+                cmd = (\n+                    f\"torchrun --nproc_per_node {self.nproc_per_node} --master_port {get_torch_dist_unique_port()} {tmp.name}\"\n+                ).split()\n+            else:\n+                cmd = [\"python\", tmp.name]\n \n             # Note that the subprocess will be waited for here, and raise an error if not successful\n             try:\n@@ -88,6 +123,48 @@ def test_model_forward(self):\n         )\n         self.torchrun(script_to_run)\n \n+    @require_huggingface_hub_greater_or_equal(\"0.31.4\")\n+    def test_model_save(self):\n+        from safetensors import safe_open\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            for is_torchrun in [True, False]:\n+                script_to_run = textwrap.dedent(\n+                    f\"\"\"\n+                    import torch\n+                    import os\n+                    from transformers import AutoModelForCausalLM\n+\n+                    model_id = \"JackFram/llama-68m\"\n+                    kwargs = dict()\n+\n+                    if os.environ.get(\"RANK\", None) is not None:\n+                        kwargs[\"tp_plan\"] = \"auto\"\n+                        result_dir = \"{tmp_dir}/tp\"\n+                    else:\n+                        result_dir = \"{tmp_dir}/nontp\"\n+\n+                    model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n+                    model.save_pretrained(result_dir)\n+                    \"\"\"\n+                )\n+                self.torchrun(script_to_run, is_torchrun=is_torchrun)\n+\n+            non_tp_model_path = os.path.join(tmp_dir, \"nontp\")\n+            tp_model_path = os.path.join(tmp_dir, \"tp\")\n+\n+            for filename in os.listdir(non_tp_model_path):\n+                if not filename.endswith(\".safetensors\"):\n+                    continue\n+\n+                non_tp_model = safe_open(os.path.join(non_tp_model_path, filename), device=\"cpu\", framework=\"pt\")\n+                tp_model = safe_open(os.path.join(tp_model_path, filename), device=\"cpu\", framework=\"pt\")\n+                for non_tp_key in non_tp_model.keys():\n+                    non_tp_tensor = non_tp_model.get_tensor(non_tp_key)\n+                    tp_tensor = tp_model.get_tensor(non_tp_key)\n+                    assert torch.allclose(non_tp_tensor, tp_tensor), f\"Tensor with key: {non_tp_key} does not match\"\n+                    del non_tp_tensor, tp_tensor\n+\n \n @require_torch_multi_gpu\n class TestTensorParallelCuda(TestTensorParallel):"
        }
    ],
    "stats": {
        "total": 283,
        "additions": 271,
        "deletions": 12
    }
}