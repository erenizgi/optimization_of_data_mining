{
    "author": "gante",
    "message": "[tests] update `test_left_padding_compatibility` (and minimize overwrites) (#40980)\n\n* update test (and overwrites)\n\n* better test comment\n\n* 0 as a default for",
    "sha": "5bf633b32a1bc5cc5c56445247ecebd349b5d063",
    "files": [
        {
            "sha": "dcca71df7c2f71dc9173c976757451340e8a5ac1",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 74,
            "deletions": 36,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -23,6 +23,7 @@\n import unittest\n import warnings\n from pathlib import Path\n+from typing import Optional\n \n import numpy as np\n import pytest\n@@ -903,32 +904,44 @@ def test_prompt_lookup_decoding_stops_at_eos(self):\n         self.assertTrue(output_prompt_lookup.shape[-1] == 10)\n \n     @pytest.mark.generate\n-    def test_left_padding_compatibility(self):\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n+    def test_left_padding_compatibility(\n+        self, unpadded_custom_inputs: Optional[dict] = None, padded_custom_inputs: Optional[dict] = None\n+    ):\n+        \"\"\"\n+        Tests that adding left-padding yields the same logits as the original input. Exposes arguments for custom\n+        inputs for overwrites, to prevent full rewrites of the test when all we need is model-specific input handling.\n \n-        # First, filter out models that don't support left padding\n-        # - The model must have generative capabilities\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(reason=\"No generative architecture available for this model.\")\n+        ! If you overwrite this test, make sure to document why you need to overwrite it !\n \n-        # - The model must support padding\n+        NOTE: left-padding results in small numerical differences. This is expected.\n+        See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n+\n+        Args:\n+            unpadded_custom_inputs (`dict`, *optional*):\n+                Used in test overwrites. Custom inputs to add/overwrite over the default test inputs.\n+            padded_custom_inputs (`dict`, *optional*):\n+                Used in test overwrites. Custom inputs to add/overwrite over the padded test input handcrafted in this\n+                test. Commonly used e.g. with multimodal cross attention masks.\n+        \"\"\"\n+\n+        # First, filter out models that don't support left padding\n+        # 1. The model must support padding\n         if not self.has_attentions:\n             self.skipTest(reason=\"This model doesn't support padding.\")\n-\n-        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n+        # 2. [encoder-decoder] The model must be a decoder-only architecture. Encoder-based architectures can use\n+        # right-padding in their (encoder) inputs. Encoder-decoder may use left-padding on their decoder inputs\n+        # [TODO: lift this restriction? technically, we can test padding the decoder inputs.]\n         decoder_only_classes = []\n         for model_class in self.all_generative_model_classes:\n             config, _ = self.prepare_config_and_inputs_for_generate()\n-            if config.get_text_config(decoder=True).is_encoder_decoder:\n+            if config.is_encoder_decoder:\n                 continue\n             else:\n                 decoder_only_classes.append(model_class)\n         if len(decoder_only_classes) == 0:\n             self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n-\n-        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n-        #   added support for it yet. We skip these models for now.\n+        # 3. [old models] Decoder-only architectures derived from encoder-decoder models could support it in theory,\n+        # but we haven't added support for it yet. We skip these models for now.\n         has_encoder_attributes = any(\n             attr_name\n             for attr_name in config.to_dict()\n@@ -939,48 +952,73 @@ def test_left_padding_compatibility(self):\n                 reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n             )\n \n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n+        # Now we can start testing\n+        unpadded_custom_inputs = unpadded_custom_inputs or {}\n+        padded_custom_inputs = padded_custom_inputs or {}\n+\n+        def _prepare_model_kwargs(model_inputs, signature):\n+            model_kwargs = {\"input_ids\": model_inputs[\"input_ids\"], \"attention_mask\": model_inputs[\"attention_mask\"]}\n             if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n+                position_ids = torch.cumsum(model_inputs[\"attention_mask\"], dim=-1) - 1\n+                position_ids.masked_fill_(model_inputs[\"attention_mask\"] == 0, 1)\n                 model_kwargs[\"position_ids\"] = position_ids\n             if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[1], device=torch_device)\n+                cache_position = torch.arange(model_inputs[\"input_ids\"].shape[1], device=torch_device)\n                 model_kwargs[\"cache_position\"] = cache_position\n+            # forward all other inputs, if they are in the signature\n+            model_kwargs.update({k: v for k, v in model_inputs.items() if k not in model_kwargs and k in signature})\n             return model_kwargs\n \n         for model_class in decoder_only_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            input_ids = inputs_dict[\"input_ids\"]\n-            attention_mask = inputs_dict.get(\"attention_mask\")\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(input_ids)\n-\n             model = model_class(config).to(torch_device).eval()\n             signature = inspect.signature(model.forward).parameters.keys()\n \n-            # no cache as some models require special cache classes to be init outside forward\n+            # No cache to simplify the test (some models need careful init)\n             model.generation_config.use_cache = False\n+            inputs_dict.update(unpadded_custom_inputs)\n+            # special case: an inexistent `attention_mask` is a full mask\n+            inputs_dict[\"attention_mask\"] = inputs_dict.get(\"attention_mask\", None)\n+            if inputs_dict[\"attention_mask\"] is None:\n+                inputs_dict[\"attention_mask\"] = torch.ones_like(inputs_dict[\"input_ids\"])\n \n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n+            # Get output logits from inputs without padding\n+            model_kwargs_wo_padding = _prepare_model_kwargs(inputs_dict, signature)\n+            next_logits_wo_padding = model(**model_kwargs_wo_padding).logits[:, -1, :]\n \n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n+            # Prepare padding on common inputs (pad length 32)\n+            input_ids = inputs_dict[\"input_ids\"]\n+            attention_mask = inputs_dict[\"attention_mask\"]\n+            token_type_ids = inputs_dict.get(\"token_type_ids\", None)\n+            pad_token_id = getattr(config.get_text_config(decoder=True), \"pad_token_id\", None) or 0\n             pad_size = (input_ids.shape[0], 32, *input_ids.shape[2:])\n             padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n             padded_input_ids = torch.cat((padding, input_ids), dim=1)\n             padded_attention_mask = torch.cat(\n                 (torch.zeros(pad_size[:2], dtype=input_ids.dtype, device=torch_device), attention_mask), dim=1\n             )\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n+            if token_type_ids is not None:\n+                padded_token_type_ids = torch.cat(\n+                    (\n+                        # Assumption: `0` is a good default value for padding token type ids\n+                        torch.zeros(pad_size[:2], dtype=input_ids.dtype, device=torch_device),\n+                        token_type_ids,\n+                    ),\n+                    dim=1,\n+                )\n+            else:\n+                padded_token_type_ids = None\n+\n+            # Get output logits from inputs with left-padding (pad length 32)\n+            padded_inputs_dict = copy.deepcopy(inputs_dict)\n+            padded_inputs_dict[\"input_ids\"] = padded_input_ids\n+            padded_inputs_dict[\"attention_mask\"] = padded_attention_mask\n+            if padded_token_type_ids is not None:\n+                padded_inputs_dict[\"token_type_ids\"] = padded_token_type_ids\n+            padded_inputs_dict.update(padded_custom_inputs)\n+\n+            model_kwargs_with_padding = _prepare_model_kwargs(padded_inputs_dict, signature)\n+            next_logits_with_padding = model(**model_kwargs_with_padding).logits[:, -1, :]\n \n             # They should result in very similar logits\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)"
        },
        {
            "sha": "10eaf0efe2c3c9c4188ab363239dc84e28b81328",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 80,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -438,88 +438,11 @@ def test_batching_equivalence(self):\n         super().test_batching_equivalence()\n         self.model_tester.use_input_mask = orig\n \n-    # essentially the same test in test_utils, just adjustment for rtol for this model\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # First, filter out models that don't support left padding\n-        # - The model must have generative capabilities\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(reason=\"No generative architecture available for this model.\")\n-\n-        # - The model must support padding\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"This model doesn't support padding.\")\n-\n-        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n-        decoder_only_classes = []\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.prepare_config_and_inputs_for_generate()\n-            if config.is_encoder_decoder:\n-                continue\n-            else:\n-                decoder_only_classes.append(model_class)\n-        if len(decoder_only_classes) == 0:\n-            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n-\n-        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n-        #   added support for it yet. We skip these models for now.\n-        has_encoder_attributes = any(\n-            attr_name\n-            for attr_name in config.to_dict()\n-            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n-        )\n-        if has_encoder_attributes:\n-            self.skipTest(\n-                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n-            )\n-\n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in decoder_only_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            input_ids = inputs_dict[\"input_ids\"]\n-\n-            # - for left padding we absolutely need to use an all ones\n-            #   attention mask, so we do not use the one in inputs_dict\n-            attention_mask = torch.ones_like(input_ids)\n-\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n+        # TODO: document why a random attention mask causes this test to fail, but a full mask doesn't\n+        unpadded_custom_inputs = {\"attention_mask\": None}\n+        super().test_left_padding_compatibility(unpadded_custom_inputs=unpadded_custom_inputs)\n \n     @unittest.skip(\n         \"Bamba requires additionally specifying position_ids, seq_idx, and FlashAttentionKwargs for padding-free training.\""
        },
        {
            "sha": "5667b1a3fe19739575fb0e3316f336bf382aabc6",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 84,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -19,7 +19,6 @@\n import unittest\n \n import numpy as np\n-import pytest\n import requests\n from parameterized import parameterized\n \n@@ -597,89 +596,6 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n             output, config, use_cache=use_cache, num_return_sequences=num_return_sequences, num_beams=num_beams\n         )\n \n-    # overwrite because BLIP2 cannot generate only from input ids, and requires pixel values in all cases to be present\n-    @pytest.mark.generate\n-    def test_left_padding_compatibility(self):\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # First, filter out models that don't support left padding\n-        # - The model must have generative capabilities\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(reason=\"No generative architecture available for this model.\")\n-\n-        # - The model must support padding\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"This model doesn't support padding.\")\n-\n-        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n-        decoder_only_classes = []\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.prepare_config_and_inputs_for_generate()\n-            if config.is_encoder_decoder:\n-                continue\n-            else:\n-                decoder_only_classes.append(model_class)\n-        if len(decoder_only_classes) == 0:\n-            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n-\n-        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n-        #   added support for it yet. We skip these models for now.\n-        has_encoder_attributes = any(\n-            attr_name\n-            for attr_name in config.to_dict()\n-            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n-        )\n-        if has_encoder_attributes:\n-            self.skipTest(\n-                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n-            )\n-\n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in decoder_only_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            input_ids = inputs_dict[\"input_ids\"]\n-            attention_mask = inputs_dict.get(\"attention_mask\")\n-            pixel_values = inputs_dict[\"pixel_values\"]\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(input_ids)\n-\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n-\n \n # this class is based on `T5ModelTester` found in tests/models/t5/test_modeling_t5.py\n class Blip2TextModelTester:"
        },
        {
            "sha": "e5050df20901e142fd614831598e15c21a6e22ee",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 81,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch FalconH1 model.\"\"\"\n \n-import inspect\n import unittest\n \n import pytest\n@@ -413,88 +412,11 @@ def test_batching_equivalence(self):\n         super().test_batching_equivalence()\n         self.model_tester.use_input_mask = orig\n \n-    # essentially the same test in test_utils, just adjustment for rtol for this model\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # First, filter out models that don't support left padding\n-        # - The model must have generative capabilities\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(reason=\"No generative architecture available for this model.\")\n-\n-        # - The model must support padding\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"This model doesn't support padding.\")\n-\n-        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n-        decoder_only_classes = []\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.prepare_config_and_inputs_for_generate()\n-            if config.is_encoder_decoder:\n-                continue\n-            else:\n-                decoder_only_classes.append(model_class)\n-        if len(decoder_only_classes) == 0:\n-            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n-\n-        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n-        #   added support for it yet. We skip these models for now.\n-        has_encoder_attributes = any(\n-            attr_name\n-            for attr_name in config.to_dict()\n-            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n-        )\n-        if has_encoder_attributes:\n-            self.skipTest(\n-                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n-            )\n-\n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in decoder_only_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            input_ids = inputs_dict[\"input_ids\"]\n-\n-            # - for left padding we absolutely need to use an all ones\n-            #   attention mask, so we do not use the one in inputs_dict\n-            attention_mask = torch.ones_like(input_ids)\n-\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n+        # TODO: document why a random attention mask causes this test to fail, but a full mask doesn't\n+        unpadded_custom_inputs = {\"attention_mask\": None}\n+        super().test_left_padding_compatibility(unpadded_custom_inputs=unpadded_custom_inputs)\n \n \n @slow"
        },
        {
            "sha": "a517d69e18a614b55d1b224ced89df305fc0b8bc",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 27,
            "deletions": 62,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Idefics model.\"\"\"\n \n-import inspect\n import unittest\n from functools import cached_property\n \n@@ -327,7 +326,6 @@ class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMi\n     test_pruning = False\n     test_headmasking = False\n     test_torchscript = False\n-    has_attentions = False  # only supports SDOA and thus no attention probs returned\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n@@ -594,6 +592,33 @@ def test_generate_from_random_inputs_embeds(\n     ):\n         pass\n \n+    @pytest.mark.generate\n+    def test_left_padding_compatibility(self):\n+        # Overwrite -- Idefics needs to prepare `image_attention_mask`, and it must be padded accordingly\n+        _, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+        input_ids = inputs_dict[\"input_ids\"]\n+        image_attention_mask = inputs_dict[\"image_attention_mask\"]\n+\n+        pad_size_img = (input_ids.shape[0], 32, image_attention_mask.shape[-1])\n+        extra_img_mask = torch.zeros(pad_size_img, dtype=image_attention_mask.dtype, device=torch_device)\n+        padded_image_attention_mask = torch.cat([extra_img_mask, image_attention_mask], dim=1)\n+\n+        # `image_attention_mask` is randomly generated in `prepare_config_and_inputs_for_generate`, and it must match\n+        # its padded version for the test to be valid -- we need to pass both\n+        unpadded_custom_inputs = {\"image_attention_mask\": image_attention_mask}\n+        padded_custom_inputs = {\"image_attention_mask\": padded_image_attention_mask}\n+        super().test_left_padding_compatibility(\n+            unpadded_custom_inputs=unpadded_custom_inputs, padded_custom_inputs=padded_custom_inputs\n+        )\n+\n+    @unittest.skip(reason=\"Idefics can't do text-only inference (test filters non-text inputs)\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Idefics can't do text-only inference (test filters non-text inputs)\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n \n @require_torch\n class IdeficsForVisionText2TextTest(IdeficsModelTest, GenerationTesterMixin, unittest.TestCase):\n@@ -613,66 +638,6 @@ def test_eager_matches_sdpa_inference(\n     ):\n         pass\n \n-    @pytest.mark.generate\n-    def test_left_padding_compatibility(self):\n-        \"\"\"Overwrite because IDEFICS needs image attention mask to be also padded\"\"\"\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        def _prepare_model_kwargs(input_ids, attention_mask, image_attention_mask, signature):\n-            model_kwargs = {\n-                \"input_ids\": input_ids,\n-                \"attention_mask\": attention_mask,\n-                \"image_attention_mask\": image_attention_mask,\n-            }\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            input_ids = inputs_dict.pop(\"input_ids\")\n-            attention_mask = inputs_dict.pop(\"attention_mask\")\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(input_ids)\n-            image_attention_mask = inputs_dict.pop(\"image_attention_mask\", None)\n-\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, image_attention_mask, signature)\n-            next_logits_wo_padding = model(**model_kwargs, **inputs_dict).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-\n-            pad_size_img = (input_ids.shape[0], 32, image_attention_mask.shape[-1])\n-            extra_img_mask = torch.zeros(pad_size_img, dtype=image_attention_mask.dtype, device=torch_device)\n-            padded_image_attention_mask = torch.cat([extra_img_mask, image_attention_mask], dim=1)\n-            model_kwargs = _prepare_model_kwargs(\n-                padded_input_ids, padded_attention_mask, padded_image_attention_mask, signature\n-            )\n-            next_logits_with_padding = model(**model_kwargs, **inputs_dict).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n-\n     @pytest.mark.generate\n     def test_generate_continue_from_past_key_values(self):\n         \"\"\"Overwrite because IDEFICS needs image attention mask to be also processed\"\"\""
        },
        {
            "sha": "9a43671ad9751c2d8b3f8ae25fb80bec54455e84",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -316,10 +316,6 @@ def test_forward_signature(self):\n             expected_arg_names = [\"input_ids\"]\n             self.assertListEqual(arg_names[:1], expected_arg_names)\n \n-    @unittest.skip(reason=\"The model doesn't support left padding\")  # and it's not used enough to be worth fixing :)\n-    def test_left_padding_compatibility(self):\n-        pass\n-\n     @unittest.skip(reason=\"Model inputs don't fit test pattern\")  # and it's not used enough to be worth fixing :)\n     def test_past_key_values_format(self):\n         pass"
        },
        {
            "sha": "17a54da482a22fda5fd6752adc62855f4f502b5f",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 89,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -18,7 +18,6 @@\n import unittest\n \n import numpy as np\n-import pytest\n import requests\n \n from transformers import (\n@@ -566,94 +565,6 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n             output, config, use_cache=use_cache, num_return_sequences=num_return_sequences, num_beams=num_beams\n         )\n \n-    # overwrite because InstructBLIP cannot generate only from input ids, and requires `pixel` values and `qformer_input_ids` in all cases to be present\n-    @pytest.mark.generate\n-    def test_left_padding_compatibility(self):\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # First, filter out models that don't support left padding\n-        # - The model must have generative capabilities\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(reason=\"No generative architecture available for this model.\")\n-\n-        # - The model must support padding\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"This model doesn't support padding.\")\n-\n-        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n-        decoder_only_classes = []\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.prepare_config_and_inputs_for_generate()\n-            if config.is_encoder_decoder:\n-                continue\n-            else:\n-                decoder_only_classes.append(model_class)\n-        if len(decoder_only_classes) == 0:\n-            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n-\n-        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n-        #   added support for it yet. We skip these models for now.\n-        has_encoder_attributes = any(\n-            attr_name\n-            for attr_name in config.to_dict()\n-            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n-        )\n-        if has_encoder_attributes:\n-            self.skipTest(\n-                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n-            )\n-\n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in decoder_only_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            input_ids = inputs_dict[\"input_ids\"]\n-            attention_mask = inputs_dict.get(\"attention_mask\")\n-            pixel_values = inputs_dict[\"pixel_values\"]\n-            qformer_input_ids = inputs_dict[\"qformer_input_ids\"]\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(input_ids)\n-\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(\n-                **model_kwargs, pixel_values=pixel_values, qformer_input_ids=qformer_input_ids\n-            ).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(\n-                **model_kwargs, pixel_values=pixel_values, qformer_input_ids=qformer_input_ids\n-            ).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n-\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model."
        },
        {
            "sha": "d6336c8c6840e913e368b4c1a5cd2fee1d33e9b5",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 89,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -18,7 +18,6 @@\n import unittest\n \n import numpy as np\n-import pytest\n from huggingface_hub import hf_hub_download\n \n from transformers import (\n@@ -578,94 +577,6 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n             output, config, use_cache=use_cache, num_return_sequences=num_return_sequences, num_beams=num_beams\n         )\n \n-    # overwrite because InstructBLIPVideo cannot generate only from input ids, and requires `pixel` values and `qformer_input_ids` in all cases to be present\n-    @pytest.mark.generate\n-    def test_left_padding_compatibility(self):\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # First, filter out models that don't support left padding\n-        # - The model must have generative capabilities\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(reason=\"No generative architecture available for this model.\")\n-\n-        # - The model must support padding\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"This model doesn't support padding.\")\n-\n-        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n-        decoder_only_classes = []\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.prepare_config_and_inputs_for_generate()\n-            if config.is_encoder_decoder:\n-                continue\n-            else:\n-                decoder_only_classes.append(model_class)\n-        if len(decoder_only_classes) == 0:\n-            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n-\n-        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n-        #   added support for it yet. We skip these models for now.\n-        has_encoder_attributes = any(\n-            attr_name\n-            for attr_name in config.to_dict()\n-            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n-        )\n-        if has_encoder_attributes:\n-            self.skipTest(\n-                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n-            )\n-\n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in decoder_only_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            input_ids = inputs_dict[\"input_ids\"]\n-            attention_mask = inputs_dict.get(\"attention_mask\")\n-            pixel_values = inputs_dict[\"pixel_values\"]\n-            qformer_input_ids = inputs_dict[\"qformer_input_ids\"]\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(input_ids)\n-\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(\n-                **model_kwargs, pixel_values=pixel_values, qformer_input_ids=qformer_input_ids\n-            ).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(\n-                **model_kwargs, pixel_values=pixel_values, qformer_input_ids=qformer_input_ids\n-            ).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n-\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model."
        },
        {
            "sha": "38a7692299521c46e5461b52486987de7a0fd815",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 49,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -481,57 +481,24 @@ def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n \n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n-        # Overwrite because Kosmos-2 need to pad pixel values and pad image-attn-mask\n-\n-        def _prepare_model_kwargs(input_ids, attention_mask, pad_size, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            if \"image_embeds_position_mask\" in signature:\n-                image_embeds_position_mask = torch.zeros_like(input_ids)\n-                image_embeds_position_mask[:, (pad_size + 1) : pad_size + 1 + self.model_tester.latent_query_num] = 1\n-                model_kwargs[\"image_embeds_position_mask\"] = image_embeds_position_mask\n-            return model_kwargs\n+        # Overwrite -- kosmos2 needs to prepare `image_embeds_position_mask`, and it must be padded accordingly\n+        _, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+        input_ids = inputs_dict[\"input_ids\"]\n \n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            input_ids = inputs_dict[\"input_ids\"]\n-            pixel_values = inputs_dict[\"pixel_values\"]\n-            attention_mask = inputs_dict.get(\"attention_mask\")\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(input_ids)\n-\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, pad_size=0, signature=signature)\n-            next_logits_wo_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(\n-                padded_input_ids, padded_attention_mask, pad_size=32, signature=signature\n+        def _prepare_image_embeds_position_mask(input_ids, pad_size):\n+            image_embeds_position_mask = torch.zeros(\n+                input_ids.shape[0], input_ids.shape[1] + pad_size, device=torch_device, dtype=input_ids.dtype\n             )\n-            next_logits_with_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-3, atol=1e-3)\n+            image_embeds_position_mask[:, (pad_size + 1) : pad_size + 1 + self.model_tester.latent_query_num] = 1\n+            return image_embeds_position_mask\n+\n+        # `image_embeds_position_mask` is randomly generated in `prepare_config_and_inputs_for_generate`, and it must\n+        # match its padded version for the test to be valid -- we need to pass both\n+        unpadded_custom_inputs = {\"image_embeds_position_mask\": _prepare_image_embeds_position_mask(input_ids, 0)}\n+        padded_custom_inputs = {\"image_embeds_position_mask\": _prepare_image_embeds_position_mask(input_ids, 32)}\n+        super().test_left_padding_compatibility(\n+            unpadded_custom_inputs=unpadded_custom_inputs, padded_custom_inputs=padded_custom_inputs\n+        )\n \n     @slow\n     def test_model_from_pretrained(self):"
        },
        {
            "sha": "b3155915b03d1c6efc51bad7cd297881d52f3363",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 17,
            "deletions": 50,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -570,57 +570,24 @@ def test_generate_from_inputs_embeds(self):\n \n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n-        # Overwrite because Kosmos-2.5 need to pad pixel values and pad image-attn-mask\n-\n-        def _prepare_model_kwargs(input_ids, attention_mask, pad_size, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            if \"image_embeds_position_mask\" in signature:\n-                image_embeds_position_mask = torch.zeros_like(input_ids)\n-                image_embeds_position_mask[:, (pad_size + 1) : pad_size + 1 + self.model_tester.latent_query_num] = 1\n-                model_kwargs[\"image_embeds_position_mask\"] = image_embeds_position_mask\n-            return model_kwargs\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            input_ids = inputs_dict[\"input_ids\"]\n-            flattened_patches = inputs_dict[\"flattened_patches\"]\n-            attention_mask = inputs_dict.get(\"attention_mask\")\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(input_ids)\n-\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, pad_size=0, signature=signature)\n-            next_logits_wo_padding = model(**model_kwargs, flattened_patches=flattened_patches).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(\n-                padded_input_ids, padded_attention_mask, pad_size=32, signature=signature\n-            )\n-            next_logits_with_padding = model(**model_kwargs, flattened_patches=flattened_patches).logits[:, -1, :]\n+        # Overwrite -- Kosmos-2.5 needs to prepare `image_embeds_position_mask`, and it must be padded accordingly\n+        _, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+        input_ids = inputs_dict[\"input_ids\"]\n \n-            # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-3))\n+        def _prepare_image_embeds_position_mask(input_ids, pad_size):\n+            image_embeds_position_mask = torch.zeros(\n+                input_ids.shape[0], input_ids.shape[1] + pad_size, device=torch_device, dtype=input_ids.dtype\n+            )\n+            image_embeds_position_mask[:, (pad_size + 1) : pad_size + 1 + self.model_tester.latent_query_num] = 1\n+            return image_embeds_position_mask\n+\n+        # `image_embeds_position_mask` is randomly generated in `prepare_config_and_inputs_for_generate`, and it must\n+        # match its padded version for the test to be valid -- we need to pass both\n+        unpadded_custom_inputs = {\"image_embeds_position_mask\": _prepare_image_embeds_position_mask(input_ids, 0)}\n+        padded_custom_inputs = {\"image_embeds_position_mask\": _prepare_image_embeds_position_mask(input_ids, 32)}\n+        super().test_left_padding_compatibility(\n+            unpadded_custom_inputs=unpadded_custom_inputs, padded_custom_inputs=padded_custom_inputs\n+        )\n \n \n @require_vision"
        },
        {
            "sha": "c7ab9f9dc6dd3916fda3fc9dfdc23743b9627aa5",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 5,
            "deletions": 81,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch Moshi ASR model.\"\"\"\n \n import gc\n-import inspect\n import tempfile\n import unittest\n \n@@ -361,86 +360,11 @@ def test_disk_offload_safetensors(self):\n \n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # First, filter out models that don't support left padding\n-        # - The model must have generative capabilities\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(reason=\"No generative architecture available for this model.\")\n-\n-        # - The model must support padding\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"This model doesn't support padding.\")\n-\n-        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n-        decoder_only_classes = []\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.prepare_config_and_inputs_for_generate()\n-            if config.is_encoder_decoder:\n-                continue\n-            else:\n-                decoder_only_classes.append(model_class)\n-        if len(decoder_only_classes) == 0:\n-            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n-\n-        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n-        #   added support for it yet. We skip these models for now.\n-        has_encoder_attributes = any(\n-            attr_name\n-            for attr_name in config.to_dict()\n-            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n-        )\n-        if has_encoder_attributes:\n-            self.skipTest(\n-                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n-            )\n-\n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in decoder_only_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            input_ids = inputs_dict[\"input_ids\"]\n-            attention_mask = inputs_dict.get(\"attention_mask\")\n-            if attention_mask is None:\n-                attention_mask = torch.ones_like(input_ids)\n-\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32, *input_ids.shape[2:])\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat(\n-                (torch.zeros(pad_size[:2], dtype=input_ids.dtype, device=torch_device), attention_mask), dim=1\n-            )\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n+        # TODO: this tester has non-standard input monkey-patching in `prepare_config_and_inputs_for_generate`,\n+        # and the test fails with the monkey-patched test inputs (bad shapes for the test)  The base inputs work\n+        # fine, though.\n+        unpadded_custom_inputs = self.model_tester.prepare_config_and_inputs_for_common()[1]\n+        super().test_left_padding_compatibility(unpadded_custom_inputs=unpadded_custom_inputs)\n \n     def test_generate_continue_from_past_key_values(self):\n         # Tests that we can continue generating from past key values, returned from a previous `generate` call"
        },
        {
            "sha": "2330684d0d71a5a8c88451e62b67aa45a9168f2f",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -505,6 +505,25 @@ def test_generate_text_only_with_cache(self):\n \n             model.generate(input_ids, use_cache=True)\n \n+    @pytest.mark.generate\n+    def test_left_padding_compatibility(self):\n+        # Overwrite -- mllama needs to prepare `cross_attention_mask`, and it must be padded accordingly\n+        _, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+        input_ids = inputs_dict[\"input_ids\"]\n+        cross_attention_mask = inputs_dict[\"cross_attention_mask\"]\n+\n+        pad_cross_attn_size = (input_ids.shape[0], 32, *cross_attention_mask.shape[2:])\n+        extra_cross_attn_mask = torch.zeros(pad_cross_attn_size, dtype=cross_attention_mask.dtype, device=torch_device)\n+        padded_cross_attention_mask = torch.cat([extra_cross_attn_mask, cross_attention_mask], dim=1)\n+\n+        # `cross_attention_mask` is randomly generated in `prepare_config_and_inputs_for_generate`, and it must match\n+        # its padded version for the test to be valid -- we need to pass both\n+        unpadded_custom_inputs = {\"cross_attention_mask\": cross_attention_mask}\n+        padded_custom_inputs = {\"cross_attention_mask\": padded_cross_attention_mask}\n+        super().test_left_padding_compatibility(\n+            unpadded_custom_inputs=unpadded_custom_inputs, padded_custom_inputs=padded_custom_inputs\n+        )\n+\n \n @require_torch\n class MllamaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "d4815a140d69bc90047e2818ce4c80ca997b033a",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 24,
            "deletions": 48,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -629,54 +629,30 @@ def test_sdpa_can_compile_dynamic(self):\n \n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # Then, test left-padding\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask, input_dict = self._get_input_ids_and_config()\n-            model = model_class(config).to(torch_device).eval()\n-\n-            # no cache as some models require special cache classes to be init outside forward\n-            model.generation_config.use_cache = False\n-\n-            # Without padding\n-            next_logits_wo_padding = model(input_ids=input_ids, attention_mask=attention_mask, **input_dict).logits[\n-                :, -1, :\n-            ]\n-\n-            # With left-padding (length 32)\n-            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = (\n-                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n-            )\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-\n-            padding = (\n-                torch.ones(\n-                    (pad_size[0], self.model_tester.num_codebooks, 32), dtype=input_ids.dtype, device=torch_device\n-                )\n-                * config.audio_vocab_size\n-            )\n-            padded_moshi_audio_codes = torch.cat((padding, input_dict[\"moshi_audio_codes\"]), dim=2)\n-            padded_user_audio_codes = torch.cat((padding, input_dict[\"user_audio_codes\"]), dim=2)\n-\n-            model_kwargs = {\n-                \"input_ids\": padded_input_ids,\n-                \"attention_mask\": padded_attention_mask,\n-                \"moshi_audio_codes\": padded_moshi_audio_codes,\n-                \"user_audio_codes\": padded_user_audio_codes,\n-            }\n-\n-            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n+        # Overwrite -- Moshi needs to prepare the audio codes, and they must be padded accordingly\n+        config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+        input_ids = inputs_dict[\"input_ids\"]\n+        moshi_audio_codes = inputs_dict[\"moshi_audio_codes\"]\n+        user_audio_codes = inputs_dict[\"user_audio_codes\"]\n+\n+        pad_size = (input_ids.shape[0], 32)\n+        padding = (\n+            torch.ones((pad_size[0], self.model_tester.num_codebooks, 32), dtype=input_ids.dtype, device=torch_device)\n+            * config.audio_vocab_size\n+        )\n+        padded_moshi_audio_codes = torch.cat((padding, moshi_audio_codes), dim=2)\n+        padded_user_audio_codes = torch.cat((padding, user_audio_codes), dim=2)\n+\n+        # the audio codes are randomly generated in `prepare_config_and_inputs_for_generate`, and they must match\n+        # their padded version for the test to be valid -- we need to pass both\n+        unpadded_custom_inputs = {\"moshi_audio_codes\": moshi_audio_codes, \"user_audio_codes\": user_audio_codes}\n+        padded_custom_inputs = {\n+            \"moshi_audio_codes\": padded_moshi_audio_codes,\n+            \"user_audio_codes\": padded_user_audio_codes,\n+        }\n+        super().test_left_padding_compatibility(\n+            unpadded_custom_inputs=unpadded_custom_inputs, padded_custom_inputs=padded_custom_inputs\n+        )\n \n     @slow\n     @is_flaky(max_attempts=5, description=\"flaky on some models.\")"
        },
        {
            "sha": "538353fee44de8d3acb8bc232bad2f75570b2a4a",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -63,6 +63,7 @@ def __init__(\n             \"use_labels\": True,\n             \"use_mrope\": False,\n             \"vocab_size\": 99,\n+            \"pad_token_id\": 1,  # can't be the same as the audio token id\n         },\n         is_training=True,\n         audio_config={"
        },
        {
            "sha": "d6662ebd5532745415a50fe20a81dc8aa4a8ba21",
            "filename": "tests/models/voxtral/test_modeling_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -59,7 +59,7 @@ def __init__(\n             \"use_mrope\": False,\n             \"vocab_size\": 99,\n             \"head_dim\": 8,\n-            \"pad_token_id\": 0,\n+            \"pad_token_id\": 1,  # can't be the same as the audio token id\n         },\n         is_training=True,\n         audio_config={"
        },
        {
            "sha": "070e84733092b3cdeb6d373c81f639ec863750fa",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -480,51 +480,6 @@ def _get_input_ids_and_config(self):\n         ) = config_and_inputs\n         return config, input_ids, input_mask\n \n-    def test_left_padding_compatibility(self):\n-        r\"\"\"\n-        Overriding the test_left_padding_compatibility test as the mamba layers accentuate the numerical differences\n-        effect of the left padding discussed in the issue in the note. Using a more permissive tolerance value.\n-        \"\"\"\n-        import inspect\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # First, filter out models that don't support left padding - generative and decoder-only.\n-        # Zamba is a decoder-only architecture\n-        decoder_only_classes = self.all_generative_model_classes\n-\n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in decoder_only_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * config.pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=3e-3, atol=3e-3)\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @require_bitsandbytes"
        },
        {
            "sha": "99c6f5fc53d92e025aa4227ce9fa540421970fe4",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bf633b32a1bc5cc5c56445247ecebd349b5d063/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=5bf633b32a1bc5cc5c56445247ecebd349b5d063",
            "patch": "@@ -499,51 +499,6 @@ def _get_input_ids_and_config(self):\n         ) = config_and_inputs\n         return config, input_ids, input_mask\n \n-    def test_left_padding_compatibility(self):\n-        r\"\"\"\n-        Overriding the test_left_padding_compatibility test as the mamba layers accentuate the numerical differences\n-        effect of the left padding discussed in the issue in the note. Using a more permissive tolerance value.\n-        \"\"\"\n-        import inspect\n-        # NOTE: left-padding results in small numerical differences. This is expected.\n-        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n-\n-        # First, filter out models that don't support left padding - generative and decoder-only.\n-        # Zamba2 is a decoder-only architecture\n-        decoder_only_classes = self.all_generative_model_classes\n-\n-        # Then, test left-padding\n-        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n-            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-            if \"position_ids\" in signature:\n-                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n-                position_ids.masked_fill_(attention_mask == 0, 1)\n-                model_kwargs[\"position_ids\"] = position_ids\n-            if \"cache_position\" in signature:\n-                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n-                model_kwargs[\"cache_position\"] = cache_position\n-            return model_kwargs\n-\n-        for model_class in decoder_only_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n-            model = model_class(config).to(torch_device).eval()\n-            signature = inspect.signature(model.forward).parameters.keys()\n-\n-            # Without padding\n-            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n-            next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # With left-padding (length 32)\n-            pad_size = (input_ids.shape[0], 32)\n-            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * config.pad_token_id\n-            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n-            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n-            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n-            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n-\n-            # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=3e-3))\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @require_bitsandbytes"
        }
    ],
    "stats": {
        "total": 1034,
        "additions": 190,
        "deletions": 844
    }
}