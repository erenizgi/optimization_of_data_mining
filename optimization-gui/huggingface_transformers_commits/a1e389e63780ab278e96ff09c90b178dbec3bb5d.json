{
    "author": "qubvel",
    "message": "Refactor `return_dict` logic to remove complicated if/else paths (#36794)\n\n* SAM\n\n* CLIP\n\n* SigLIP\n\n* GOT-OCR2 (depends on SAM)\n\n* SigLIP2 (depends on SigLIP)\n\n* trigger tests\n\n* Fix SAM\n\n* Fix missed indexing, use named attributes\n\n* Llama\n\n* Aria\n\n* Bamba\n\n* Update llama: missed outputs return type\n\n* (fixup) Aria\n\n* DiffLlama\n\n* Emu3\n\n* Gemma\n\n* Gemma2\n\n* Paligemma\n\n* Fix paligemma\n\n* Gemma3\n\n* GLM\n\n* Helium\n\n* JetMoe\n\n* Jamba\n\n* Mistral\n\n* Mistral\n\n* Mixtral\n\n* Nemotron\n\n* Olmo\n\n* Olmo2\n\n* Persimmon\n\n* Phi\n\n* Phi3\n\n* PhiMoe\n\n* Qwen2\n\n* Qwen2_moe\n\n* StableLM\n\n* Starcoder2\n\n* Add return_dict decorator\n\n* SAM\n\n* Update decorator: compile, export, trace - friendly\n\n* Llama (decorator)\n\n* SAM (decorator)\n\n* Add decorator `can_return_tuple`\n\n* Llama\n\n* Update to decorator\n\n* Update CLIP\n\n* Update decorator to store `_is_top_level_module` in self\n\n* Update decorator to correctly handle compile/export\n\n* Remove is_torchdynamo_compiling constraint, all work fine with self attribute assignment\n\n* Typing\n\n* GPT NeoX\n\n* Fixup\n\n* Fix attribute Granite\n\n* Fix return type mixtral\n\n* Update Gemma3\n\n* Fix Cohere amd Cohere2\n\n* Fixup\n\n* Fix corner case for Phi4, when activation is shared\n\n* (fix-copies) deepseekv3, phi4\n\n* Fixup\n\n* Apply to qwen3/qwen3_moe\n\n* Fix",
    "sha": "a1e389e63780ab278e96ff09c90b178dbec3bb5d",
    "files": [
        {
            "sha": "5c3b50caef4bd7a1ebe5eb3b8424c6960d00cf64",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 12,
            "deletions": 25,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -35,6 +35,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -895,6 +896,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -906,16 +908,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -998,13 +998,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -1182,6 +1181,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1196,11 +1196,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1236,10 +1235,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1248,12 +1246,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1262,10 +1259,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1445,6 +1438,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n         return image_features\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n@@ -1461,11 +1455,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **loss_kwargs,\n-    ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n+    ) -> AriaCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1531,7 +1524,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n@@ -1562,31 +1554,26 @@ def forward(\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        outputs = self.language_model(\n+        outputs: CausalLMOutputWithPast = self.language_model(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             logits_to_keep=logits_to_keep,\n             cache_position=cache_position,\n         )\n \n-        logits = outputs[0]\n+        logits = outputs.logits\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(\n                 logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **loss_kwargs\n             )\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return AriaCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "afc61a02dda13ca91da55e7a81d5e4b5a656533f",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -33,6 +33,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...modeling_outputs import CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils import (\n@@ -43,6 +44,7 @@\n     TensorType,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1416,6 +1418,7 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n         return image_features\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n@@ -1432,11 +1435,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **loss_kwargs,\n-    ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n+    ) -> AriaCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1502,7 +1504,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n@@ -1533,31 +1534,26 @@ def forward(\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        outputs = self.language_model(\n+        outputs: CausalLMOutputWithPast = self.language_model(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             logits_to_keep=logits_to_keep,\n             cache_position=cache_position,\n         )\n \n-        logits = outputs[0]\n+        logits = outputs.logits\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(\n                 logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **loss_kwargs\n             )\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return AriaCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "953e1242c5671f0d0663c809f8348fa3fb43710a",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 16,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -43,6 +43,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1191,6 +1192,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1202,18 +1204,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -1298,8 +1297,6 @@ def forward(\n \n         next_cache = None if not use_cache else past_key_values\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1471,6 +1468,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1485,11 +1483,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1525,10 +1522,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1537,12 +1533,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1551,10 +1546,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "df4f0200576d4de6162d55f90a9b1443d62ceef0",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -51,6 +51,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -935,6 +936,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -946,18 +948,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -1042,8 +1041,6 @@ def forward(\n \n         next_cache = None if not use_cache else past_key_values\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1184,6 +1181,7 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n \n \n class BambaForCausalLM(LlamaForCausalLM):\n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1198,11 +1196,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1244,7 +1241,6 @@ def forward(\n             use_cache,\n             output_attentions,\n             output_hidden_states,\n-            return_dict,\n             cache_position,\n             logits_to_keep,\n             **kwargs,"
        },
        {
            "sha": "b06b6fdcf5f21a6f9a9a2515c2f8fc005cd1ab96",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 41,
            "deletions": 90,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch CLIP model.\"\"\"\n \n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Optional, Tuple\n \n import torch\n import torch.utils.checkpoint\n@@ -33,6 +33,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n@@ -819,15 +820,15 @@ def __init__(self, config: CLIPConfig):\n         self.layers = nn.ModuleList([CLIPEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n+    @can_return_tuple\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> BaseModelOutput:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -861,7 +862,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -894,10 +894,10 @@ def forward(\n         if output_hidden_states:\n             encoder_states = encoder_states + (hidden_states,)\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n         )\n \n \n@@ -916,6 +916,7 @@ def __init__(self, config: CLIPTextConfig):\n         # For attention mask, it differs between `flash_attention_2` and other attention implementations\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(CLIP_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPTextConfig)\n     def forward(\n@@ -925,8 +926,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -935,7 +935,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if input_ids is None:\n             raise ValueError(\"You have to specify input_ids\")\n@@ -956,16 +955,15 @@ def forward(\n             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n             causal_attention_mask=causal_attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.final_layer_norm(last_hidden_state)\n \n         if self.eos_token_id == 2:\n@@ -990,9 +988,6 @@ def forward(\n                 .argmax(dim=-1),\n             ]\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n@@ -1022,6 +1017,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(CLIP_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPTextConfig)\n     def forward(\n@@ -1031,8 +1027,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -1050,15 +1045,13 @@ def forward(\n         >>> last_hidden_state = outputs.last_hidden_state\n         >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         return self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n \n@@ -1073,16 +1066,16 @@ def __init__(self, config: CLIPVisionConfig):\n         self.encoder = CLIPEncoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(CLIP_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPVisionConfig)\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -1091,28 +1084,23 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n         hidden_states = self.pre_layrnorm(hidden_states)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n@@ -1139,6 +1127,7 @@ def __init__(self, config: CLIPVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(CLIP_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPVisionConfig)\n     def forward(\n@@ -1147,8 +1136,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -1171,13 +1159,11 @@ def forward(\n         >>> last_hidden_state = outputs.last_hidden_state\n         >>> pooled_output = outputs.pooler_output  # pooled CLS states\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         return self.vision_model(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n@@ -1230,7 +1216,6 @@ def get_text_features(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1253,18 +1238,16 @@ def get_text_features(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        pooled_output = text_outputs[1]\n+        pooled_output = text_outputs.pooler_output\n         text_features = self.text_projection(pooled_output)\n \n         return text_features\n@@ -1276,7 +1259,6 @@ def get_image_features(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1305,21 +1287,20 @@ def get_image_features(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n         )\n \n-        pooled_output = vision_outputs[1]  # pooled_output\n+        pooled_output = vision_outputs.pooler_output\n         image_features = self.visual_projection(pooled_output)\n \n         return image_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(CLIP_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CLIPOutput, config_class=CLIPConfig)\n     def forward(\n@@ -1332,8 +1313,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, CLIPOutput]:\n+    ) -> CLIPOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1363,29 +1343,26 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n         )\n \n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        image_embeds = vision_outputs[1]\n+        image_embeds = vision_outputs.pooler_output\n         image_embeds = self.visual_projection(image_embeds)\n \n-        text_embeds = text_outputs[1]\n+        text_embeds = text_outputs.pooler_output\n         text_embeds = self.text_projection(text_embeds)\n \n         # normalized features\n@@ -1402,10 +1379,6 @@ def forward(\n         if return_loss:\n             loss = clip_loss(logits_per_text)\n \n-        if not return_dict:\n-            output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n-            return ((loss,) + output) if loss is not None else output\n-\n         return CLIPOutput(\n             loss=loss,\n             logits_per_image=logits_per_image,\n@@ -1445,6 +1418,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(CLIP_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CLIPTextModelOutput, config_class=CLIPTextConfig)\n     def forward(\n@@ -1454,8 +1428,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, CLIPTextModelOutput]:\n+    ) -> CLIPTextModelOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1472,25 +1445,17 @@ def forward(\n         >>> outputs = model(**inputs)\n         >>> text_embeds = outputs.text_embeds\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = text_outputs[1]\n-\n+        pooled_output = text_outputs.pooler_output\n         text_embeds = self.text_projection(pooled_output)\n \n-        if not return_dict:\n-            outputs = (text_embeds, text_outputs[0]) + text_outputs[2:]\n-            return tuple(output for output in outputs if output is not None)\n-\n         return CLIPTextModelOutput(\n             text_embeds=text_embeds,\n             last_hidden_state=text_outputs.last_hidden_state,\n@@ -1523,6 +1488,7 @@ def __init__(self, config: CLIPVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(CLIP_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CLIPVisionModelOutput, config_class=CLIPVisionConfig)\n     def forward(\n@@ -1531,8 +1497,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, CLIPVisionModelOutput]:\n+    ) -> CLIPVisionModelOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1554,24 +1519,16 @@ def forward(\n         >>> outputs = model(**inputs)\n         >>> image_embeds = outputs.image_embeds\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n-            return_dict=return_dict,\n         )\n-\n-        pooled_output = vision_outputs[1]  # pooled_output\n-\n+        pooled_output = vision_outputs.pooler_output\n         image_embeds = self.visual_projection(pooled_output)\n \n-        if not return_dict:\n-            outputs = (image_embeds, vision_outputs[0]) + vision_outputs[2:]\n-            return tuple(output for output in outputs if output is not None)\n-\n         return CLIPVisionModelOutput(\n             image_embeds=image_embeds,\n             last_hidden_state=vision_outputs.last_hidden_state,\n@@ -1605,6 +1562,7 @@ def __init__(self, config: CLIPConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(CLIP_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_IMAGE_CLASS_CHECKPOINT,\n@@ -1618,8 +1576,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, ImageClassifierOutput]:\n+    ) -> ImageClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n@@ -1630,16 +1587,14 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.vision_model(\n+        outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         # average pool the patch tokens\n         sequence_output = torch.mean(sequence_output[:, 1:, :], dim=1)\n@@ -1671,10 +1626,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return ImageClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "eadc89697c24d8373de8f65d273cfdbacc08ed4b",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -46,6 +46,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -545,6 +546,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(COHERE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -556,16 +558,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -648,13 +648,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -822,6 +821,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(COHERE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -836,11 +836,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -876,10 +875,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -888,12 +886,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -903,10 +900,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "4c1ac5ff33f2cc260c2152fe76dc5924b3b8b4b3",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -30,7 +30,7 @@\n \n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import CausalLMOutputWithPast\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n@@ -315,11 +315,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -355,10 +354,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -367,12 +365,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -382,10 +379,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "21489e9b78dbd304cce3044fec0b83f645d0483c",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -37,6 +37,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -552,6 +553,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -563,17 +565,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -669,13 +669,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     @torch.no_grad()\n     def _update_causal_mask(\n@@ -808,6 +807,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -822,11 +822,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -862,10 +861,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -874,12 +872,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -889,10 +886,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "d76a92accbc54a016c055a3cf7a19d9ca5a0545d",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -462,7 +462,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n@@ -472,7 +471,6 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -568,13 +566,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class Cohere2ForCausalLM(CohereForCausalLM):"
        },
        {
            "sha": "67564fbca4da6bd349c91cc71f76058bcf284d51",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -25,6 +25,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -691,6 +692,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(DEEPSEEK_V3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -702,16 +704,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -794,13 +794,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -966,6 +965,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(DEEPSEEK_V3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -980,11 +980,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1020,10 +1019,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1032,12 +1030,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1046,10 +1043,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "933cf159637072b064283d0c40392b325453de7c",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 20,
            "deletions": 45,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -52,6 +52,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -787,6 +788,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -798,16 +800,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -890,13 +890,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -1062,6 +1061,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1076,11 +1076,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1116,10 +1115,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1128,12 +1126,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1142,10 +1139,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1186,6 +1179,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1198,17 +1192,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1217,9 +1209,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1249,10 +1240,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1286,6 +1273,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.transformer.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1298,9 +1286,8 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1311,20 +1298,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.transformer(\n+        outputs: BaseModelOutputWithPast = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1335,10 +1320,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,\n@@ -1378,6 +1359,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1395,17 +1377,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1414,20 +1394,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "607ac4e6b5a4fd1b5c84398ff086a294609cb66b",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -41,6 +41,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -1370,6 +1371,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1381,16 +1383,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -1473,13 +1473,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -1646,6 +1645,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n@@ -1660,11 +1660,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1700,10 +1699,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1712,12 +1710,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1726,10 +1723,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1873,6 +1866,7 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n         image = self.vqmodel.decode(image_tokens)\n         return image\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1887,11 +1881,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1946,7 +1939,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n@@ -1965,7 +1957,7 @@ def forward(\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.text_model(\n+        return self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1974,13 +1966,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n         )\n \n-        return outputs\n-\n     def prepare_inputs_for_generation(\n         self,\n         input_ids,"
        },
        {
            "sha": "d411ade67aeba0e8d7d226fa8bdb300c2455521d",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -32,6 +32,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1055,6 +1056,7 @@ def __init__(self, config: Emu3Config):\n             [Emu3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     def forward(self, **super_kwargs):\n         super().forward(**super_kwargs)\n@@ -1067,6 +1069,7 @@ def __init__(self, config):\n         super().__init__(config)\n         self.model = Emu3TextModel(config)\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n@@ -1160,6 +1163,7 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n         image = self.vqmodel.decode(image_tokens)\n         return image\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1174,11 +1178,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1233,7 +1236,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n@@ -1252,7 +1254,7 @@ def forward(\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.text_model(\n+        return self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1261,13 +1263,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n         )\n \n-        return outputs\n-\n     def prepare_inputs_for_generation(\n         self,\n         input_ids,"
        },
        {
            "sha": "dfa7aabfcf4046a35fcaea08068e14d7e9dd9e39",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 16,
            "deletions": 35,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -43,6 +43,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -510,6 +511,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -521,16 +523,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwarg for now\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -615,13 +615,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -787,6 +786,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -801,11 +801,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -841,10 +840,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -853,12 +851,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -867,10 +864,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -911,6 +904,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -923,17 +917,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -942,9 +934,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -974,10 +965,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1017,6 +1004,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1034,17 +1022,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1053,20 +1039,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "50b1d33dcc0f858ebb2747892ab53ea80e57b363",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -377,7 +377,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwarg for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n@@ -386,7 +385,6 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -471,13 +469,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class GemmaForCausalLM(LlamaForCausalLM):"
        },
        {
            "sha": "75e318009c5986cb1ae5224de7918597175bc5d2",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 35,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -42,6 +42,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -555,6 +556,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -566,17 +568,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -681,13 +681,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     @torch.no_grad()\n     def _update_causal_mask(\n@@ -815,6 +814,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -829,11 +829,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -875,9 +874,8 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -886,12 +884,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **loss_kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -904,10 +901,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1005,6 +998,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1017,17 +1011,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1036,9 +1028,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1068,10 +1059,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1111,6 +1098,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1128,17 +1116,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1147,20 +1133,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "4f8d2e1ba4d5ee2aa85af76366b1ad5d839e586e",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -412,7 +412,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n@@ -422,7 +421,6 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -527,13 +525,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     @torch.no_grad()\n     def _update_causal_mask(\n@@ -588,7 +585,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n@@ -634,9 +630,8 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -645,12 +640,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **loss_kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -663,10 +657,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "e17009f402f4118e7fe2b9759861c1d6ef1933d2",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 23,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -39,6 +39,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n@@ -643,6 +644,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -654,17 +656,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -770,13 +770,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     @torch.no_grad()\n     def _update_causal_mask(\n@@ -906,6 +905,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -920,11 +920,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -966,9 +965,8 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -977,12 +975,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **loss_kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -995,10 +992,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1222,6 +1215,7 @@ def get_image_features(self, pixel_values: torch.Tensor):\n         image_features = self.multi_modal_projector(vision_outputs)\n         return image_features\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Gemma3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1239,7 +1233,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n     ) -> Union[Tuple, Gemma3CausalLMOutputWithPast]:\n@@ -1304,7 +1297,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         is_training = token_type_ids is not None and labels is not None\n \n@@ -1358,21 +1350,20 @@ def forward(\n         causal_mask = self._update_causal_mask(\n             attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n         )\n-        outputs = self.language_model(\n+        outputs: CausalLMOutputWithPast = self.language_model(\n             attention_mask=causal_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        logits = outputs.logits\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues\n@@ -1394,9 +1385,6 @@ def forward(\n             flat_logits = shift_logits.view(-1, self.config.text_config.vocab_size)\n             flat_labels = shift_labels.view(-1).to(shift_logits.device)\n             loss = loss_fct(flat_logits, flat_labels)\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n \n         return Gemma3CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "001bbd8f19df8fa22c9290fcfa406c5d431e0043",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 12,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -28,13 +28,15 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n+    CausalLMOutputWithPast,\n     ModelOutput,\n )\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import (\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n@@ -592,7 +594,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n@@ -602,7 +603,6 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -708,13 +708,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class Gemma3ForCausalLM(Gemma2ForCausalLM):\n@@ -849,6 +848,7 @@ def _update_causal_mask(\n \n         return causal_mask\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Gemma3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -866,7 +866,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n     ) -> Union[Tuple, Gemma3CausalLMOutputWithPast]:\n@@ -931,7 +930,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         is_training = token_type_ids is not None and labels is not None\n \n@@ -985,21 +983,20 @@ def forward(\n         causal_mask = self._update_causal_mask(\n             attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n         )\n-        outputs = self.language_model(\n+        outputs: CausalLMOutputWithPast = self.language_model(\n             attention_mask=causal_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        logits = outputs.logits\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues\n@@ -1021,9 +1018,6 @@ def forward(\n             flat_logits = shift_logits.view(-1, self.config.text_config.vocab_size)\n             flat_labels = shift_labels.view(-1).to(shift_logits.device)\n             loss = loss_fct(flat_logits, flat_labels)\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n \n         return Gemma3CausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "8bd9031127aa425ecf01084365c75b33eafb2388",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 35,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -44,6 +44,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -526,6 +527,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GLM_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -537,16 +539,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -629,13 +629,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -801,6 +800,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GLM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -815,11 +815,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -855,10 +854,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -867,12 +865,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -881,10 +878,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -925,6 +918,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GLM_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -937,17 +931,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -956,9 +948,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -988,10 +979,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1031,6 +1018,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GLM_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1048,17 +1036,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1067,20 +1053,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "83fa36f0e1be531a7e0e754693ba8d4ee1843ab0",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 21,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -28,11 +28,18 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from transformers.modeling_outputs import CausalLMOutputWithPast\n+\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    replace_return_docstrings,\n+)\n from ..auto import AutoModelForCausalLM\n from .configuration_got_ocr2 import GotOcr2Config, GotOcr2VisionConfig\n \n@@ -438,18 +445,17 @@ def __init__(self, config: GotOcr2VisionConfig):\n     def get_input_embeddings(self):\n         return self.patch_embed\n \n+    @can_return_tuple\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, GotOcr2VisionEncoderOutput]:\n+    ) -> GotOcr2VisionEncoderOutput:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n@@ -483,14 +489,6 @@ def forward(\n \n         hidden_states = self.neck(hidden_states)\n \n-        if not return_dict:\n-            outputs = (hidden_states,)\n-            if output_hidden_states:\n-                outputs = outputs + (all_hidden_states,)\n-            if output_attentions:\n-                outputs = outputs + (all_self_attentions,)\n-            return outputs\n-\n         return GotOcr2VisionEncoderOutput(\n             last_hidden_state=hidden_states,\n             hidden_states=all_hidden_states,\n@@ -738,6 +736,7 @@ def get_image_features(\n         image_outputs = self.vision_tower(pixel_values).last_hidden_state\n         return self.multi_modal_projector(image_outputs)\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=GotOcr2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -752,7 +751,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, GotOcr2CausalLMOutputWithPast]:\n@@ -805,7 +803,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -831,20 +828,19 @@ def forward(\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        outputs = self.language_model(\n+        outputs: CausalLMOutputWithPast = self.language_model(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n         )\n \n-        logits = outputs[0]\n+        logits = outputs.logits\n \n         loss = None\n         if labels is not None:\n@@ -864,10 +860,6 @@ def forward(\n                 shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n             )\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return GotOcr2CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "aed41cc28549bc02f6d631b446b61157657768b0",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 14,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -14,12 +14,13 @@\n # limitations under the License.\n \n \n-from typing import List, Optional, Tuple, Union\n+from typing import List, Optional, Union\n \n import torch\n import torch.nn as nn\n import torch.utils.checkpoint\n \n+from transformers.modeling_outputs import CausalLMOutputWithPast\n from transformers.models.llava.modeling_llava import (\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n@@ -30,6 +31,7 @@\n from ...configuration_utils import PretrainedConfig\n from ...utils import (\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_vision_available,\n     logging,\n     replace_return_docstrings,\n@@ -226,9 +228,6 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-__all__ = [\"GotOcr2VisionConfig\", \"GotOcr2Config\"]\n-\n-\n class GotOcr2MLPBlock(SamMLPBlock):\n     pass\n \n@@ -381,6 +380,7 @@ def get_image_features(\n         image_outputs = self.vision_tower(pixel_values).last_hidden_state\n         return self.multi_modal_projector(image_outputs)\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=GotOcr2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -395,10 +395,9 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-    ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n+    ) -> LlavaCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -448,7 +447,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -474,20 +472,19 @@ def forward(\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        outputs = self.language_model(\n+        outputs: CausalLMOutputWithPast = self.language_model(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n         )\n \n-        logits = outputs[0]\n+        logits = outputs.logits\n \n         loss = None\n         if labels is not None:\n@@ -507,10 +504,6 @@ def forward(\n                 shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n             )\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return GotOcr2CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "11dfbab89d1ae2a004c1079eacd92e05ed72f0ea",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 19,
            "deletions": 44,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -29,6 +29,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -501,6 +502,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_in = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -519,15 +521,13 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -618,13 +618,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -781,6 +780,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.embed_out = new_embeddings\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -795,7 +795,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n@@ -827,9 +826,8 @@ def forward(\n \n         >>> prediction_logits = outputs.logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.gpt_neox(\n+        outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -839,12 +837,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.embed_out(hidden_states[:, slice_indices, :])\n@@ -853,10 +850,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -891,6 +884,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -909,17 +903,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.gpt_neox(\n+        outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -929,9 +921,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         batch_size = logits.shape[0]\n@@ -957,10 +948,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -982,6 +969,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=\"LarsJonasson/pythia-410m-deduped-sft-swedish\",\n@@ -1002,17 +990,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.gpt_neox(\n+        outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n@@ -1022,21 +1008,16 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         hidden_states = self.dropout(hidden_states)\n         logits = self.classifier(hidden_states)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1062,6 +1043,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1081,8 +1063,7 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1093,20 +1074,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.gpt_neox(\n+        outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1117,10 +1096,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "1399f1f18f198d943b24de10b0a2dcb24b82c369",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 18,
            "deletions": 43,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -22,6 +22,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -321,6 +322,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_in = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -339,15 +341,13 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -438,13 +438,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n@@ -473,6 +472,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.embed_out = new_embeddings\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -487,7 +487,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n@@ -519,9 +518,8 @@ def forward(\n \n         >>> prediction_logits = outputs.logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.gpt_neox(\n+        outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -531,12 +529,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.embed_out(hidden_states[:, slice_indices, :])\n@@ -545,10 +542,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -583,6 +576,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -601,17 +595,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.gpt_neox(\n+        outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -621,9 +613,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         batch_size = logits.shape[0]\n@@ -649,10 +640,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -674,6 +661,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=\"LarsJonasson/pythia-410m-deduped-sft-swedish\",\n@@ -694,17 +682,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.gpt_neox(\n+        outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n@@ -714,21 +700,16 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         hidden_states = self.dropout(hidden_states)\n         logits = self.classifier(hidden_states)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -754,6 +735,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GPT_NEOX_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -773,8 +755,7 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -785,20 +766,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.gpt_neox(\n+        outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -809,10 +788,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "6b64e18aa7baf60272d754693118cb7f7dca0983",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -38,6 +38,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -527,6 +528,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(GRANITE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -538,16 +540,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -628,13 +628,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -800,6 +799,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(GRANITE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -814,11 +814,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -854,10 +853,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -866,12 +864,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -881,10 +878,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "f6d99e1c30a1bbdaf5396a1c3e353ed6becebc7f",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -130,7 +130,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n@@ -139,7 +138,6 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -220,13 +218,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n@@ -244,7 +241,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n@@ -253,10 +249,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -265,12 +260,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -280,10 +274,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "5e66ce7298427b8ef1d21c13a466e361d4d894b9",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 16,
            "deletions": 35,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -45,6 +45,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -513,6 +514,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(HELIUM_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -524,16 +526,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -616,13 +616,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -788,6 +787,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(HELIUM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -802,11 +802,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -842,10 +841,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -854,12 +852,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -868,10 +865,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -912,6 +905,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(HELIUM_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -924,17 +918,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -943,9 +935,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -975,10 +966,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1018,6 +1005,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(HELIUM_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1035,17 +1023,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1054,20 +1040,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "4e4e5f71bf488bee7098ffecb0df321febedb654",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 34,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -43,6 +43,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1228,6 +1229,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(JAMBA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1240,9 +1242,8 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, MoeModelOutputWithPast]:\n+    ) -> MoeModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n@@ -1252,8 +1253,6 @@ def forward(\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -1339,12 +1338,6 @@ def forward(\n \n         next_cache = None if not use_cache else past_key_values\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_router_logits]\n-                if v is not None\n-            )\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1433,6 +1426,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(JAMBA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1448,11 +1442,10 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n-    ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1493,10 +1486,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1507,10 +1499,9 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             cache_position=cache_position,\n-            return_dict=return_dict,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n@@ -1521,20 +1512,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,\n@@ -1621,7 +1606,7 @@ def prepare_inputs_for_generation(\n     \"\"\",\n     JAMBA_START_DOCSTRING,\n )\n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification with Mixtral->Jamba, MIXTRAL->JAMBA\n+# Copied from transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification with Mixtral->Jamba, MIXTRAL->JAMBA, BaseModelOutputWithPast->MoeModelOutputWithPast\n class JambaForSequenceClassification(JambaPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1638,6 +1623,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(JAMBA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1650,17 +1636,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: MoeModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1669,9 +1653,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1701,10 +1684,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,"
        },
        {
            "sha": "b87ca1376e341150c3b5caf9b6e428cf20d0bee9",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 13,
            "deletions": 29,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -37,6 +37,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -982,6 +983,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(JETMOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -994,9 +996,8 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, MoeModelOutputWithPast]:\n+    ) -> MoeModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1005,7 +1006,6 @@ def forward(\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -1110,8 +1110,6 @@ def forward(\n         if return_legacy_cache:\n             next_cache = next_cache.to_legacy_cache()\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1289,6 +1287,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(JETMOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1304,11 +1303,10 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1329,10 +1327,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1341,11 +1338,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1372,20 +1368,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,\n@@ -1412,7 +1402,7 @@ def forward(\n     \"\"\",\n     JETMOE_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->JetMoe, LLAMA->JETMOE\n+# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->JetMoe, LLAMA->JETMOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n class JetMoeForSequenceClassification(JetMoePreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1429,6 +1419,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(JETMOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1441,17 +1432,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: MoeModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1460,9 +1449,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1492,10 +1480,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,"
        },
        {
            "sha": "f7d8c714d8eb6742b3f87be8a108b76f1ad746cc",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 20,
            "deletions": 45,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -45,6 +45,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -515,6 +516,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -526,16 +528,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -618,13 +618,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -790,6 +789,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -804,11 +804,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -844,10 +843,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -856,12 +854,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -870,10 +867,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -914,6 +907,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -926,17 +920,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -945,9 +937,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -977,10 +968,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1015,6 +1002,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.transformer.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1027,9 +1015,8 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1040,20 +1027,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.transformer(\n+        outputs: BaseModelOutputWithPast = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1064,10 +1049,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,\n@@ -1107,6 +1088,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1124,17 +1106,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1143,20 +1123,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "65044b4dbe99df4e62831b0059b4e065e881d117",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 20,
            "deletions": 45,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -30,6 +30,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -480,6 +481,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -491,16 +493,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -583,13 +583,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -779,6 +778,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -793,11 +793,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -833,10 +832,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -845,12 +843,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -859,10 +856,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -902,6 +895,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -919,17 +913,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -938,20 +930,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -991,6 +978,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1003,17 +991,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1022,9 +1008,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1054,10 +1039,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1091,6 +1072,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1103,9 +1085,8 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1116,20 +1097,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1140,10 +1119,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "4548e69a73886ea2f27ff29075b7291fdf685385",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -7,7 +7,7 @@\n from ...cache_utils import Cache, SlidingWindowCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import QuestionAnsweringModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, QuestionAnsweringModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n@@ -302,7 +302,6 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n@@ -315,20 +314,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -339,10 +336,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "7cfc266fd3accbae404621c6faef5a5a3cdd5691",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 21,
            "deletions": 49,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -53,6 +53,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -602,6 +603,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -614,10 +616,9 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n@@ -627,8 +628,6 @@ def forward(\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -712,14 +711,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = MoeModelOutputWithPast(\n+        return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -994,6 +992,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1009,11 +1008,10 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1054,10 +1052,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1067,12 +1064,11 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1084,20 +1080,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,\n@@ -1140,6 +1130,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1152,17 +1143,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1171,9 +1160,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1203,10 +1191,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1246,6 +1230,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1263,17 +1248,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1282,20 +1265,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1328,6 +1306,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1340,9 +1319,8 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1353,20 +1331,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1377,10 +1353,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "3b470667e99d008fbfd6e42606d2f7883aa607cd",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 19,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -342,10 +342,9 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, MoeModelOutputWithPast]:\n+    ) -> MoeModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n@@ -355,8 +354,6 @@ def forward(\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -440,14 +437,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = MoeModelOutputWithPast(\n+        return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n@@ -475,11 +471,10 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -520,10 +515,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -533,12 +527,11 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -550,20 +543,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,"
        },
        {
            "sha": "17e2b9584e4045ccdc25cdac6c2ace9004916bb0",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 16,
            "deletions": 31,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -47,6 +47,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -615,15 +616,15 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value: nn.Module):\n         self.conv1 = value\n \n+    @can_return_tuple\n     def forward(\n         self,\n         input_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         r\"\"\"\n         Args:\n             input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n@@ -650,7 +651,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if input_values is None:\n             raise ValueError(\"You must specify input_values.\")\n@@ -725,12 +725,11 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n MOONSHINE_INPUTS_DOCSTRING = r\"\"\"\n@@ -836,6 +835,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MOONSHINE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -847,12 +847,11 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         \"\"\"\n         Args:\n             encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n@@ -869,7 +868,6 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -977,14 +975,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPastAndCrossAttentions(\n+        return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -1399,6 +1396,7 @@ def _mask_input_features(\n \n         return input_features\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MOONSHINE_MODEL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1414,7 +1412,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n@@ -1442,43 +1439,37 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if encoder_outputs is None:\n-            encoder_outputs = self.encoder(\n+            encoder_outputs: BaseModelOutput = self.encoder(\n                 input_values,\n                 attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n             )\n         # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n-        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n+        elif not isinstance(encoder_outputs, BaseModelOutput):\n             encoder_outputs = BaseModelOutput(\n                 last_hidden_state=encoder_outputs[0],\n                 hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n         # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n-        decoder_outputs = self.decoder(\n+        decoder_outputs: BaseModelOutputWithPastAndCrossAttentions = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n             encoder_attention_mask=attention_mask,\n-            encoder_hidden_states=encoder_outputs[0],\n+            encoder_hidden_states=encoder_outputs.last_hidden_state,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             position_ids=decoder_position_ids,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        if not return_dict:\n-            return decoder_outputs + encoder_outputs\n-\n         return Seq2SeqModelOutput(\n             last_hidden_state=decoder_outputs.last_hidden_state,\n             past_key_values=decoder_outputs.past_key_values,\n@@ -1537,6 +1528,7 @@ def set_output_embeddings(self, new_embeddings):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.model.get_input_embeddings()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MOONSHINE_MODEL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1552,10 +1544,9 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n+    ) -> Seq2SeqLMOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`\n@@ -1585,15 +1576,14 @@ def forward(\n         >>> transcription\n         'Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if labels is not None:\n             if decoder_input_ids is None and decoder_inputs_embeds is None:\n                 decoder_input_ids = shift_tokens_right(\n                     labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                 )\n \n-        outputs = self.model(\n+        outputs: Seq2SeqModelOutput = self.model(\n             input_values,\n             attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n@@ -1605,19 +1595,14 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n-        logits = self.proj_out(outputs[0])\n+        logits = self.proj_out(outputs.last_hidden_state)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return Seq2SeqLMOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "02938e73d50d5a4ecb56e35b81d9430014e8d2c4",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 15,
            "deletions": 31,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -40,6 +40,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -605,15 +606,15 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value: nn.Module):\n         self.conv1 = value\n \n+    @can_return_tuple\n     def forward(\n         self,\n         input_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         r\"\"\"\n         Args:\n             input_values (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n@@ -640,7 +641,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if input_values is None:\n             raise ValueError(\"You must specify input_values.\")\n@@ -715,12 +715,11 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class MoonshineDecoder(LlamaModel):\n@@ -743,7 +742,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -765,7 +763,6 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -873,14 +870,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPastAndCrossAttentions(\n+        return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             cross_attentions=all_cross_attentions,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n MOONSHINE_MODEL_INPUTS_DOCSTRING = r\"\"\"\n@@ -978,6 +974,7 @@ def forward(\n     MOONSHINE_START_DOCSTRING,\n )\n class MoonshineModel(WhisperModel):\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MOONSHINE_MODEL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -993,9 +990,8 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n+    ) -> Seq2SeqModelOutput:\n         r\"\"\"\n         ```python\n         >>> import torch\n@@ -1017,43 +1013,37 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if encoder_outputs is None:\n-            encoder_outputs = self.encoder(\n+            encoder_outputs: BaseModelOutput = self.encoder(\n                 input_values,\n                 attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n             )\n         # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n-        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n+        elif not isinstance(encoder_outputs, BaseModelOutput):\n             encoder_outputs = BaseModelOutput(\n                 last_hidden_state=encoder_outputs[0],\n                 hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                 attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n             )\n \n         # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n-        decoder_outputs = self.decoder(\n+        decoder_outputs: BaseModelOutputWithPastAndCrossAttentions = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n             encoder_attention_mask=attention_mask,\n-            encoder_hidden_states=encoder_outputs[0],\n+            encoder_hidden_states=encoder_outputs.last_hidden_state,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             position_ids=decoder_position_ids,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        if not return_dict:\n-            return decoder_outputs + encoder_outputs\n-\n         return Seq2SeqModelOutput(\n             last_hidden_state=decoder_outputs.last_hidden_state,\n             past_key_values=decoder_outputs.past_key_values,\n@@ -1096,6 +1086,7 @@ def set_output_embeddings(self, new_embeddings):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.model.get_input_embeddings()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MOONSHINE_MODEL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1111,10 +1102,9 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:\n+    ) -> Seq2SeqLMOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`\n@@ -1144,15 +1134,14 @@ def forward(\n         >>> transcription\n         'Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if labels is not None:\n             if decoder_input_ids is None and decoder_inputs_embeds is None:\n                 decoder_input_ids = shift_tokens_right(\n                     labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                 )\n \n-        outputs = self.model(\n+        outputs: Seq2SeqModelOutput = self.model(\n             input_values,\n             attention_mask=attention_mask,\n             decoder_input_ids=decoder_input_ids,\n@@ -1164,19 +1153,14 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n-        logits = self.proj_out(outputs[0])\n+        logits = self.proj_out(outputs.last_hidden_state)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return Seq2SeqLMOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "6a362804a230da5f3819e163a33f4d3eb922c801",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 19,
            "deletions": 45,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -42,6 +42,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -760,6 +761,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(NEMOTRON_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -771,15 +773,13 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -866,8 +866,6 @@ def forward(\n \n         next_cache = next_decoder_cache if use_cache else None\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1037,6 +1035,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(NEMOTRON_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1052,11 +1051,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1092,10 +1090,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1104,11 +1101,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1117,10 +1113,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1162,6 +1154,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(NEMOTRON_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1174,17 +1167,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1193,9 +1184,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1225,10 +1215,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1264,6 +1250,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.transformer.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(NEMOTRON_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1276,9 +1263,8 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1289,20 +1275,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.transformer(\n+        outputs: BaseModelOutputWithPast = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1313,10 +1297,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,\n@@ -1357,6 +1337,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(NEMOTRON_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1374,17 +1355,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1393,20 +1372,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "79b8c28ca260fb81165999ac7f39788e72c6f96c",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -24,6 +24,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -491,6 +492,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(OLMO_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -502,16 +504,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -594,13 +594,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -766,6 +765,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(OLMO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -780,11 +780,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -820,10 +819,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -832,12 +830,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -846,10 +843,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "cc83dd4d17ed815e1fc499aff6d34119e1f39a40",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -23,6 +23,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -492,6 +493,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(OLMO2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -503,16 +505,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -595,13 +595,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -767,6 +766,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(OLMO2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -781,11 +781,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -821,10 +820,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -833,12 +831,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -847,10 +844,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "f14e2d4535ae1c93db96c9377ab788fd72b90437",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_outputs import CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -537,15 +538,15 @@ def forward(\n         causal_mask = self._update_causal_mask(\n             attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n         )\n-        outputs = self.language_model(\n+        outputs: CausalLMOutputWithPast = self.language_model(\n             attention_mask=causal_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n@@ -573,18 +574,16 @@ def forward(\n             flat_logits = shift_logits.view(-1, self.config.text_config.vocab_size)\n             flat_labels = shift_labels.view(-1).to(shift_logits.device)\n             loss = loss_fct(flat_logits, flat_labels)\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n \n-        return PaliGemmaCausalLMOutputWithPast(\n+        output = PaliGemmaCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n             image_hidden_states=image_features if pixel_values is not None else None,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def prepare_inputs_for_generation(\n         self,"
        },
        {
            "sha": "1e3a784026047756aa271c2883d6713bac09143d",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 15,
            "deletions": 36,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -42,6 +42,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -550,6 +551,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PERSIMMON_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -561,17 +563,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -667,8 +666,6 @@ def forward(\n         if return_legacy_cache:\n             next_cache = next_cache.to_legacy_cache()\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -844,6 +841,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PERSIMMON_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -858,11 +856,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -899,10 +896,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -911,11 +907,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # No upscaling to float was ever done for Persimmon\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -929,10 +924,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -974,6 +965,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PERSIMMON_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -986,17 +978,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1005,9 +995,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1037,10 +1026,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1081,6 +1066,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PERSIMMON_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1098,17 +1084,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1117,20 +1101,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "612bd70407d17f33f4991be1493bee52ffea9754",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 16,
            "deletions": 35,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -29,6 +29,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -488,6 +489,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -499,16 +501,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -588,13 +588,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -760,6 +759,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PHI_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -774,11 +774,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -814,10 +813,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -826,12 +824,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -840,10 +837,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -884,6 +877,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -896,17 +890,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -915,9 +907,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -947,10 +938,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -990,6 +977,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1007,17 +995,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1026,20 +1012,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "e01d433aa1e0500da013a4629c988e0f5e4f0e03",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -189,7 +189,6 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n@@ -198,7 +197,6 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -278,13 +276,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class PhiForCausalLM(LlamaForCausalLM):"
        },
        {
            "sha": "0d8238683eee14285948255931ab707c420ea7f1",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 35,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -45,6 +45,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -555,6 +556,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -566,16 +568,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -658,13 +658,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -854,6 +853,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PHI3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -868,11 +868,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -908,10 +907,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -920,12 +918,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -934,10 +931,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1017,6 +1010,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1029,17 +1023,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1048,9 +1040,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1080,10 +1071,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1123,6 +1110,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI3_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1140,17 +1128,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1159,20 +1145,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "5754459529fa15f92d943b5a8236bbc80c995b19",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 16,
            "deletions": 31,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -47,6 +47,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n@@ -212,14 +213,14 @@ def __init__(self, config: Phi4MultimodalVisionConfig):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n+    @can_return_tuple\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> BaseModelOutput:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -246,7 +247,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -277,10 +277,10 @@ def forward(\n         if output_hidden_states:\n             encoder_states = encoder_states + (hidden_states,)\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n         )\n \n \n@@ -567,13 +567,11 @@ def forward(\n         patch_attention_mask: Optional[torch.BoolTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         batch_size = pixel_values.size(0)\n         if patch_attention_mask is None:\n@@ -602,25 +600,21 @@ def forward(\n                 else patch_attention_mask\n             )\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooled_output = self.head(\n             hidden_state=last_hidden_state,\n             attention_mask=patch_attention_mask,\n         )\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n@@ -1845,6 +1839,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1862,18 +1857,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -1961,13 +1953,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -2154,6 +2145,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=Phi4MultimodalConfig)\n     def forward(\n@@ -2173,11 +2165,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -2209,10 +2200,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -2227,12 +2217,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -2241,10 +2230,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.vocab_size)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "c9e9f209adbc9ad51e20093244eaca1156dd27c5",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 12,
            "deletions": 25,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -27,13 +27,15 @@\n from ...cache_utils import DynamicCache\n from ...configuration_utils import PretrainedConfig\n from ...modeling_outputs import (\n+    BaseModelOutput,\n     BaseModelOutputWithPast,\n     BaseModelOutputWithPooling,\n     CausalLMOutputWithPast,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -668,13 +670,11 @@ def forward(\n         patch_attention_mask: Optional[torch.BoolTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         batch_size = pixel_values.size(0)\n         if patch_attention_mask is None:\n@@ -703,25 +703,21 @@ def forward(\n                 else patch_attention_mask\n             )\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooled_output = self.head(\n             hidden_state=last_hidden_state,\n             attention_mask=patch_attention_mask,\n         )\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n@@ -1549,6 +1545,7 @@ def __init__(self, config: Phi4MultimodalConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1566,18 +1563,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -1665,13 +1659,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class Phi4MultimodalForCausalLM(Phi3ForCausalLM, nn.Module):\n@@ -1686,6 +1679,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=Phi4MultimodalConfig)\n     def forward(\n@@ -1705,11 +1699,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1741,10 +1734,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1759,12 +1751,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1773,10 +1764,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.vocab_size)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "89e82b7d6bac968256f137bf102e3758b3d1fa29",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 13,
            "deletions": 34,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -37,6 +37,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1031,6 +1032,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHIMOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1043,9 +1045,8 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, MoeModelOutputWithPast]:\n+    ) -> MoeModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n@@ -1055,8 +1056,6 @@ def forward(\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n                 \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n@@ -1159,12 +1158,6 @@ def forward(\n         if return_legacy_cache:\n             next_cache = next_cache.to_legacy_cache()\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_router_logits]\n-                if v is not None\n-            )\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1366,6 +1359,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(PHIMOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1382,11 +1376,10 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n-    ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1429,10 +1422,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1442,11 +1434,10 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1458,20 +1449,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,\n@@ -1537,7 +1522,7 @@ def prepare_inputs_for_generation(\n     PHIMOE_START_DOCSTRING,\n )\n \n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Phimoe, LLAMA->PHIMOE\n+# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Phimoe, LLAMA->PHIMOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n class PhimoeForSequenceClassification(PhimoePreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1554,6 +1539,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(PHIMOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1566,17 +1552,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: MoeModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1585,9 +1569,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1617,10 +1600,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,"
        },
        {
            "sha": "232598b2314d6e00abb52f960f3b870f3d836213",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 45,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -30,6 +30,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -493,6 +494,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -504,16 +506,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -596,13 +596,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -792,6 +791,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -806,11 +806,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -846,10 +845,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -858,12 +856,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -872,10 +869,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -916,6 +909,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -928,17 +922,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -947,9 +939,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -979,10 +970,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1022,6 +1009,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1039,17 +1027,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1058,20 +1044,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1104,6 +1085,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.transformer.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1116,9 +1098,8 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1129,20 +1110,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.transformer(\n+        outputs: BaseModelOutputWithPast = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1153,10 +1132,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "8e1e8de79e94d9597316b0248626bca4d9d64e46",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 23,
            "deletions": 56,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -45,6 +45,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -919,6 +920,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -931,9 +933,8 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, MoeModelOutputWithPast]:\n+    ) -> MoeModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n@@ -943,8 +944,6 @@ def forward(\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -1046,12 +1045,6 @@ def forward(\n         if return_legacy_cache:\n             next_cache = next_cache.to_legacy_cache()\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_router_logits]\n-                if v is not None\n-            )\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1250,6 +1243,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1265,11 +1259,10 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n-    ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1309,10 +1302,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1322,11 +1314,10 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1338,20 +1329,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,\n@@ -1378,7 +1363,7 @@ def forward(\n     \"\"\",\n     QWEN2MOE_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Qwen2Moe, LLAMA->QWEN2MOE\n+# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Qwen2Moe, LLAMA->QWEN2MOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n class Qwen2MoeForSequenceClassification(Qwen2MoePreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1395,6 +1380,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1407,17 +1393,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: MoeModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1426,9 +1410,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1458,10 +1441,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1478,7 +1457,7 @@ def forward(\n     \"\"\",\n     QWEN2MOE_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->Qwen2Moe, LLAMA->QWEN2MOE\n+# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->Qwen2Moe, LLAMA->QWEN2MOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n class Qwen2MoeForTokenClassification(Qwen2MoePreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1502,6 +1481,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1519,17 +1499,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1538,20 +1516,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1567,7 +1540,7 @@ def forward(\n     \"\"\",\n     QWEN2MOE_START_DOCSTRING,\n )\n-# Copied from transformers.models.mistral.modeling_mistral.MistralForQuestionAnswering with Mistral->Qwen2Moe, MISTRAL->QWEN2MOE\n+# Copied from transformers.models.mistral.modeling_mistral.MistralForQuestionAnswering with Mistral->Qwen2Moe, MISTRAL->QWEN2MOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n class Qwen2MoeForQuestionAnswering(Qwen2MoePreTrainedModel):\n     base_model_prefix = \"model\"\n \n@@ -1585,6 +1558,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1597,9 +1571,8 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1610,20 +1583,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1634,10 +1605,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "b235c8acde1bf889173c8ccd66a35d257322ff45",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 20,
            "deletions": 45,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -45,6 +45,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -520,6 +521,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -531,16 +533,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -623,13 +623,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -819,6 +818,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -833,11 +833,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -873,10 +872,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -885,12 +883,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -899,10 +896,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -943,6 +936,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -955,17 +949,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -974,9 +966,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1006,10 +997,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1049,6 +1036,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1066,17 +1054,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1085,20 +1071,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1131,6 +1112,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.transformer.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1143,9 +1125,8 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1156,20 +1137,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.transformer(\n+        outputs: BaseModelOutputWithPast = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1180,10 +1159,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "3897cd44bd76fa865d98385ee993c7d66822d003",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 21,
            "deletions": 49,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -48,6 +48,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -615,6 +616,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN3_MOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -627,10 +629,9 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n@@ -640,8 +641,6 @@ def forward(\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -725,14 +724,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = MoeModelOutputWithPast(\n+        return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -1007,6 +1005,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(QWEN3_MOE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1022,11 +1021,10 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1067,10 +1065,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1080,12 +1077,11 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1097,20 +1093,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,\n@@ -1153,6 +1143,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN3_MOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1165,17 +1156,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1184,9 +1173,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1216,10 +1204,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1259,6 +1243,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN3_MOE_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1276,17 +1261,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1295,20 +1278,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1341,6 +1319,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.transformer.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(QWEN3_MOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1353,9 +1332,8 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1366,20 +1344,18 @@ def forward(\n             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n             are not taken into account for computing the loss.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.transformer(\n+        outputs: BaseModelOutputWithPast = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         logits = self.qa_outputs(sequence_output)\n         start_logits, end_logits = logits.split(1, dim=-1)\n@@ -1390,10 +1366,6 @@ def forward(\n         if start_positions is not None and end_positions is not None:\n             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "3e80e28a0c39a7ceb2544bfe81d1bd863cb7ea0b",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -23,7 +23,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import MoeCausalLMOutputWithPast\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import (\n     LossKwargs,\n@@ -254,7 +254,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n@@ -299,10 +298,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -312,12 +310,11 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -329,20 +326,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,"
        },
        {
            "sha": "b25d1b7318420bf59d90ba8191adbe748c581ef7",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 10,
            "deletions": 36,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -16,7 +16,7 @@\n \n import collections\n from dataclasses import dataclass\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -31,6 +31,7 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -406,13 +407,11 @@ def forward(\n         target_embedding=None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         all_attentions = ()\n \n@@ -1121,18 +1120,17 @@ def __init__(self, config: SamVisionConfig):\n     def get_input_embeddings(self):\n         return self.patch_embed\n \n+    @can_return_tuple\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SamVisionEncoderOutput]:\n+    ) -> SamVisionEncoderOutput:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n@@ -1166,14 +1164,6 @@ def forward(\n \n         hidden_states = self.neck(hidden_states)\n \n-        if not return_dict:\n-            outputs = (hidden_states,)\n-            if output_hidden_states:\n-                outputs = outputs + (all_hidden_states,)\n-            if output_attentions:\n-                outputs = outputs + (all_self_attentions,)\n-            return outputs\n-\n         return SamVisionEncoderOutput(\n             last_hidden_state=hidden_states,\n             hidden_states=all_hidden_states,\n@@ -1396,7 +1386,6 @@ def get_image_embeddings(\n         pixel_values,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ):\n         r\"\"\"\n         Returns the image embeddings by passing the pixel values through the vision encoder.\n@@ -1408,15 +1397,11 @@ def get_image_embeddings(\n                 Whether or not to return the attentions tensors of all attention layers.\n             output_hidden_states (`bool`, *optional*):\n                 Whether or not to return the hidden states of all layers.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\n         \"\"\"\n         vision_output = self.vision_encoder(\n             pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n         image_embeddings = vision_output[0]\n         return image_embeddings\n@@ -1454,6 +1439,7 @@ def get_prompt_embeddings(\n         )\n         return prompt_output\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SAM_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1468,9 +1454,8 @@ def forward(\n         target_embedding: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> List[Dict[str, torch.Tensor]]:\n+    ) -> SamImageSegmentationOutput:\n         r\"\"\"\n         Example:\n \n@@ -1500,7 +1485,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if pixel_values is None and image_embeddings is None:\n             raise ValueError(\"Either pixel_values or image_embeddings must be provided.\")\n@@ -1537,18 +1521,17 @@ def forward(\n         vision_hidden_states = None\n \n         if pixel_values is not None:\n-            vision_outputs = self.vision_encoder(\n+            vision_outputs: SamVisionEncoderOutput = self.vision_encoder(\n                 pixel_values,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n             )\n-            image_embeddings = vision_outputs[0]\n+            image_embeddings = vision_outputs.last_hidden_state\n \n             if output_hidden_states:\n-                vision_hidden_states = vision_outputs[1]\n+                vision_hidden_states = vision_outputs.hidden_states\n             if output_attentions:\n-                vision_attentions = vision_outputs[-1]\n+                vision_attentions = vision_outputs.attentions\n \n         if input_points is not None and input_labels is None:\n             input_labels = torch.ones_like(input_points[:, :, :, 0], dtype=torch.int, device=input_points.device)\n@@ -1580,15 +1563,6 @@ def forward(\n             output_attentions=output_attentions,\n         )\n \n-        if not return_dict:\n-            output = (iou_predictions, low_res_masks)\n-            if output_hidden_states:\n-                output = output + (vision_hidden_states,)\n-\n-            if output_attentions:\n-                output = output + (vision_attentions, mask_decoder_attentions)\n-            return output\n-\n         return SamImageSegmentationOutput(\n             iou_scores=iou_predictions,\n             pred_masks=low_res_masks,"
        },
        {
            "sha": "0288b78381fe86d245f23ace52cfcd1858519dc4",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 33,
            "deletions": 65,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Optional, Tuple\n \n import numpy as np\n import torch\n@@ -35,6 +35,7 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n@@ -848,14 +849,14 @@ def __init__(self, config: SiglipConfig):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n+    @can_return_tuple\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> BaseModelOutput:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -882,7 +883,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -913,10 +913,10 @@ def forward(\n         if output_hidden_states:\n             encoder_states = encoder_states + (hidden_states,)\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n         )\n \n \n@@ -932,6 +932,7 @@ def __init__(self, config: SiglipTextConfig):\n         self.head = nn.Linear(embed_dim, config.projection_size)\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=SiglipTextConfig)\n     def forward(\n@@ -941,8 +942,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -951,7 +951,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if input_ids is None:\n             raise ValueError(\"You have to specify input_ids\")\n@@ -967,24 +966,20 @@ def forward(\n             # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.final_layer_norm(last_hidden_state)\n \n         # Assuming \"sticky\" EOS tokenization, last token is always EOS.\n         pooled_output = last_hidden_state[:, -1, :]\n         pooled_output = self.head(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n@@ -1012,6 +1007,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=SiglipTextConfig)\n     def forward(\n@@ -1021,8 +1017,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -1041,15 +1036,13 @@ def forward(\n         >>> last_hidden_state = outputs.last_hidden_state\n         >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         return self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n \n@@ -1066,16 +1059,16 @@ def __init__(self, config: SiglipVisionConfig):\n         if self.use_head:\n             self.head = SiglipMultiheadAttentionPoolingHead(config)\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=SiglipVisionConfig)\n     def forward(\n         self,\n         pixel_values,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -1084,23 +1077,19 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooler_output = self.head(last_hidden_state) if self.use_head else None\n-        if not return_dict:\n-            return (last_hidden_state, pooler_output) + encoder_outputs[1:]\n \n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n@@ -1153,16 +1142,16 @@ def __init__(self, config: SiglipVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=SiglipVisionConfig)\n     def forward(\n         self,\n         pixel_values,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -1185,13 +1174,11 @@ def forward(\n         >>> last_hidden_state = outputs.last_hidden_state\n         >>> pooled_output = outputs.pooler_output  # pooled features\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         return self.vision_model(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n@@ -1240,7 +1227,6 @@ def get_text_features(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1266,18 +1252,16 @@ def get_text_features(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        pooled_output = text_outputs[1]\n+        pooled_output = text_outputs.pooler_output\n \n         return pooled_output\n \n@@ -1287,7 +1271,6 @@ def get_image_features(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -1319,20 +1302,19 @@ def get_image_features(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n-        pooled_output = vision_outputs[1]\n+        pooled_output = vision_outputs.pooler_output\n \n         return pooled_output\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=SiglipOutput, config_class=SiglipConfig)\n     def forward(\n@@ -1344,9 +1326,8 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple, SiglipOutput]:\n+    ) -> SiglipOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1381,27 +1362,24 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        image_embeds = vision_outputs[1]\n-        text_embeds = text_outputs[1]\n+        image_embeds = vision_outputs.pooler_output\n+        text_embeds = text_outputs.pooler_output\n \n         # normalized features\n         image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n@@ -1424,10 +1402,6 @@ def forward(\n             nll = -torch.sum(loglik, dim=-1)\n             loss = nll.mean()\n \n-        if not return_dict:\n-            output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SiglipOutput(\n             loss=loss,\n             logits_per_image=logits_per_image,\n@@ -1467,6 +1441,7 @@ def __init__(self, config: SiglipConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=ImageClassifierOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1475,9 +1450,8 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[tuple, ImageClassifierOutput]:\n+    ) -> ImageClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n@@ -1515,17 +1489,15 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.vision_model(\n+        outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         # average pool the patch tokens\n         sequence_output = torch.mean(sequence_output, dim=1)\n@@ -1557,10 +1529,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return ImageClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "663d9f3dd0970f8a346e2cd5477d8b8833968464",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 33,
            "deletions": 66,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -21,7 +21,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Optional, Tuple\n \n import numpy as np\n import torch\n@@ -39,6 +39,7 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -566,14 +567,14 @@ def __init__(self, config: Siglip2Config):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n+    @can_return_tuple\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> BaseModelOutput:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -600,7 +601,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -631,10 +631,10 @@ def forward(\n         if output_hidden_states:\n             encoder_states = encoder_states + (hidden_states,)\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n         )\n \n \n@@ -670,6 +670,7 @@ def __init__(self, config: Siglip2VisionConfig):\n             self.head = Siglip2MultiheadAttentionPoolingHead(config)\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP2_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Siglip2VisionConfig)\n     def forward(\n@@ -679,8 +680,7 @@ def forward(\n         spatial_shapes: torch.LongTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -689,7 +689,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         hidden_states = self.embeddings(pixel_values, spatial_shapes)\n \n@@ -699,20 +698,17 @@ def forward(\n         else:\n             encoder_attention_mask = attention_mask\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=encoder_attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n-        if not return_dict:\n-            return (last_hidden_state, pooler_output) + encoder_outputs[1:]\n \n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n@@ -902,6 +898,7 @@ def __init__(self, config: Siglip2TextConfig):\n         self.head = nn.Linear(embed_dim, config.projection_size)\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP2_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Siglip2TextConfig)\n     def forward(\n@@ -911,8 +908,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -921,7 +917,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if input_ids is None:\n             raise ValueError(\"You have to specify input_ids\")\n@@ -937,24 +932,20 @@ def forward(\n             # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.final_layer_norm(last_hidden_state)\n \n         # Assuming \"sticky\" EOS tokenization, last token is always EOS.\n         pooled_output = last_hidden_state[:, -1, :]\n         pooled_output = self.head(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n@@ -1104,6 +1095,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP2_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Siglip2TextConfig)\n     def forward(\n@@ -1113,8 +1105,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -1133,15 +1124,13 @@ def forward(\n         >>> last_hidden_state = outputs.last_hidden_state\n         >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         return self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n \n@@ -1195,6 +1184,7 @@ def __init__(self, config: Siglip2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP2_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Siglip2VisionConfig)\n     def forward(\n@@ -1204,8 +1194,7 @@ def forward(\n         spatial_shapes: torch.LongTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -1228,15 +1217,12 @@ def forward(\n         >>> last_hidden_state = outputs.last_hidden_state\n         >>> pooled_output = outputs.pooler_output  # pooled features\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         return self.vision_model(\n             pixel_values=pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n \n@@ -1284,7 +1270,6 @@ def get_text_features(\n         position_ids: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1310,18 +1295,16 @@ def get_text_features(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        pooled_output = text_outputs[1]\n+        pooled_output = text_outputs.pooler_output\n \n         return pooled_output\n \n@@ -1333,7 +1316,6 @@ def get_image_features(\n         spatial_shapes: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1364,21 +1346,20 @@ def get_image_features(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        pooled_output = vision_outputs[1]\n+        pooled_output = vision_outputs.pooler_output\n \n         return pooled_output\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Siglip2Output, config_class=Siglip2Config)\n     def forward(\n@@ -1392,8 +1373,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Siglip2Output]:\n+    ) -> Siglip2Output:\n         r\"\"\"\n         Returns:\n \n@@ -1428,28 +1408,25 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        image_embeds = vision_outputs[1]\n-        text_embeds = text_outputs[1]\n+        image_embeds = vision_outputs.pooler_output\n+        text_embeds = text_outputs.pooler_output\n \n         # normalized features\n         image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n@@ -1472,10 +1449,6 @@ def forward(\n             nll = -torch.sum(loglik, dim=-1)\n             loss = nll.mean()\n \n-        if not return_dict:\n-            output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n-            return ((loss,) + output) if loss is not None else output\n-\n         return Siglip2Output(\n             loss=loss,\n             logits_per_image=logits_per_image,\n@@ -1515,6 +1488,7 @@ def __init__(self, config: Siglip2Config) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(SIGLIP2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=ImageClassifierOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -1525,8 +1499,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, ImageClassifierOutput]:\n+    ) -> ImageClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n@@ -1564,18 +1537,16 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.vision_model(\n+        outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         # average pool the patch tokens\n         if pixel_attention_mask is not None:\n@@ -1612,10 +1583,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return ImageClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "92e106bc59bea504296844f5aa4ad57c9bfb271e",
            "filename": "src/transformers/models/siglip2/modular_siglip2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 40,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers.models.siglip.configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n from transformers.models.siglip.modeling_siglip import (\n+    BaseModelOutput,\n     BaseModelOutputWithPooling,\n     ImageClassifierOutput,\n     SiglipForImageClassification,\n@@ -242,7 +243,6 @@ def forward(\n         spatial_shapes: torch.LongTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Returns:\n@@ -252,7 +252,6 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         hidden_states = self.embeddings(pixel_values, spatial_shapes)\n \n@@ -262,20 +261,17 @@ def forward(\n         else:\n             encoder_attention_mask = attention_mask\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=encoder_attention_mask,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n-        if not return_dict:\n-            return (last_hidden_state, pooler_output) + encoder_outputs[1:]\n \n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n@@ -326,17 +322,13 @@ def forward(\n         spatial_shapes: torch.LongTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n+    ) -> BaseModelOutputWithPooling:\n         return self.vision_model(\n             pixel_values=pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n \n@@ -349,25 +341,22 @@ def get_image_features(\n         spatial_shapes: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         # Use Siglip2Model's config for some fields (if specified) instead of those of vision & text components.\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        pooled_output = vision_outputs[1]\n+        pooled_output = vision_outputs.pooler_output\n \n         return pooled_output\n \n@@ -383,35 +372,31 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Siglip2Output]:\n+    ) -> Siglip2Output:\n         # Use Siglip2 model's config for some fields (if specified) instead of those of vision & text components.\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        vision_outputs = self.vision_model(\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        text_outputs = self.text_model(\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        image_embeds = vision_outputs[1]\n-        text_embeds = text_outputs[1]\n+        image_embeds = vision_outputs.pooler_output\n+        text_embeds = text_outputs.pooler_output\n \n         # normalized features\n         image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n@@ -434,10 +419,6 @@ def forward(\n             nll = -torch.sum(loglik, dim=-1)\n             loss = nll.mean()\n \n-        if not return_dict:\n-            output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n-            return ((loss,) + output) if loss is not None else output\n-\n         return Siglip2Output(\n             loss=loss,\n             logits_per_image=logits_per_image,\n@@ -459,24 +440,21 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, ImageClassifierOutput]:\n+    ) -> ImageClassifierOutput:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.vision_model(\n+        outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values,\n             attention_mask=pixel_attention_mask,\n             spatial_shapes=spatial_shapes,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n \n         # average pool the patch tokens\n         if pixel_attention_mask is not None:\n@@ -513,10 +491,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return ImageClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "ab14bdb8e6cd6c109232e2cd8c6cfa98315fa2cd",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 15,
            "deletions": 36,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -43,6 +43,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -804,6 +805,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(STABLELM_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -815,17 +817,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -921,8 +920,6 @@ def forward(\n         if return_legacy_cache:\n             next_cache = next_cache.to_legacy_cache()\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=next_cache,\n@@ -1099,6 +1096,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(STABLELM_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -1114,11 +1112,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1155,9 +1152,8 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1166,11 +1162,10 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # No upscaling to float was ever done for StableLm\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1184,10 +1179,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -1229,6 +1220,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(STABLELM_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -1241,17 +1233,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1260,9 +1250,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1292,10 +1281,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1336,6 +1321,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(STABLELM_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1353,17 +1339,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1372,20 +1356,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "364f9a3d03f7a862676aaf325f7cbadee01bfae5",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 35,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -48,6 +48,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     logging,\n     replace_return_docstrings,\n )\n@@ -485,6 +486,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -496,16 +498,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -574,13 +574,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -770,6 +769,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n+    @can_return_tuple\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -784,11 +784,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -824,10 +823,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -836,12 +834,11 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -850,10 +847,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n@@ -894,6 +887,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -906,17 +900,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -925,9 +917,8 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n+        hidden_states = transformer_outputs.last_hidden_state\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -957,10 +948,6 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n@@ -1000,6 +987,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1017,17 +1005,15 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1036,20 +1022,15 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n+        sequence_output = outputs.last_hidden_state\n         sequence_output = self.dropout(sequence_output)\n         logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n             loss = self.loss_function(logits, labels, self.config)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "d6aaa08f2c0aced81a299a54fbbfce1b112762e9",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -33,7 +33,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import add_start_docstrings_to_model_forward, logging\n+from ...utils import add_start_docstrings_to_model_forward, can_return_tuple, logging\n from ..mistral.modeling_mistral import (\n     MistralAttention,\n     MistralDecoderLayer,\n@@ -155,6 +155,7 @@ def __init__(self, config: Starcoder2Config):\n         self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n         self.embedding_dropout = config.embedding_dropout\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(STARCODER2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -166,16 +167,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n@@ -244,13 +243,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        output = BaseModelOutputWithPast(\n+        return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-        return output if return_dict else output.to_tuple()\n \n \n class Starcoder2ForCausalLM(MistralForCausalLM):"
        },
        {
            "sha": "0209c85b85fd1407856ab6f537da900f9538e1ac",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -45,6 +45,7 @@\n     add_model_info_to_custom_pipelines,\n     cached_property,\n     can_return_loss,\n+    can_return_tuple,\n     expand_dims,\n     filter_out_non_signature_kwargs,\n     find_labels,"
        },
        {
            "sha": "721ecaa37f64fefa5c2a766879c190226ae2979e",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -41,6 +41,11 @@\n )\n \n \n+if is_torch_available():\n+    # required for @can_return_tuple decorator to work with torchdynamo\n+    import torch  # noqa: F401\n+\n+\n class cached_property(property):\n     \"\"\"\n     Descriptor that mimics @property but caches output in member variable.\n@@ -909,3 +914,62 @@ def is_timm_local_checkpoint(pretrained_model_path: str) -> bool:\n         return is_timm_config_dict(config_dict)\n \n     return False\n+\n+\n+def set_attribute_for_modules(module: \"torch.nn.Module\", key: str, value: Any):\n+    \"\"\"\n+    Set a value to a module and all submodules.\n+    \"\"\"\n+    setattr(module, key, value)\n+    for submodule in module.children():\n+        set_attribute_for_modules(submodule, key, value)\n+\n+\n+def del_attribute_from_modules(module: \"torch.nn.Module\", key: str):\n+    \"\"\"\n+    Delete a value from a module and all submodules.\n+    \"\"\"\n+    # because we might remove it previously in case it's a shared module, e.g. activation function\n+    if hasattr(module, key):\n+        delattr(module, key)\n+\n+    for submodule in module.children():\n+        del_attribute_from_modules(submodule, key)\n+\n+\n+def can_return_tuple(func):\n+    \"\"\"\n+    Decorator to wrap model method, to call output.to_tuple() if return_dict=False passed as a kwarg or\n+    use_return_dict=False is set in the config.\n+\n+    Note:\n+        output.to_tuple() convert output to tuple skipping all `None` values.\n+    \"\"\"\n+\n+    @wraps(func)\n+    def wrapper(self, *args, **kwargs):\n+        is_requested_to_return_tuple = kwargs.pop(\"return_dict\", True) is False\n+        is_configured_to_return_tuple = self.config.use_return_dict is False if hasattr(self, \"config\") else False\n+\n+        # The following allows to convert output to tuple ONLY on top level forward call,\n+        # while internal modules of the model will return Output objects\n+        # to be able to use name-based attribute access in modeling code.\n+\n+        # We will check if we are on top level module, if so, turn off to tuple conversion for all\n+        # underling calls.\n+        is_top_level_module = getattr(self, \"_is_top_level_module\", True)\n+        if is_configured_to_return_tuple and is_top_level_module:\n+            set_attribute_for_modules(self, \"_is_top_level_module\", False)\n+\n+        try:\n+            output = func(self, *args, **kwargs)\n+            if is_requested_to_return_tuple or (is_configured_to_return_tuple and is_top_level_module):\n+                output = output.to_tuple()\n+        finally:\n+            # Remove the flag after the model forward call is finished.\n+            if is_configured_to_return_tuple and is_top_level_module:\n+                del_attribute_from_modules(self, \"_is_top_level_module\")\n+\n+        return output\n+\n+    return wrapper"
        },
        {
            "sha": "4af8d7c5147c8c2bb8ce28798a217994271282a4",
            "filename": "tests/utils/test_generic.py",
            "status": "modified",
            "additions": 119,
            "deletions": 0,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1e389e63780ab278e96ff09c90b178dbec3bb5d/tests%2Futils%2Ftest_generic.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1e389e63780ab278e96ff09c90b178dbec3bb5d/tests%2Futils%2Ftest_generic.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_generic.py?ref=a1e389e63780ab278e96ff09c90b178dbec3bb5d",
            "patch": "@@ -18,8 +18,11 @@\n \n import numpy as np\n \n+from transformers.configuration_utils import PretrainedConfig\n+from transformers.modeling_outputs import BaseModelOutput\n from transformers.testing_utils import require_flax, require_tf, require_torch\n from transformers.utils import (\n+    can_return_tuple,\n     expand_dims,\n     filter_out_non_signature_kwargs,\n     flatten_dict,\n@@ -343,3 +346,119 @@ def func3(a, **kwargs):\n         with self.assertWarns(UserWarning):\n             kwargs = func3(1, extra_arg=2, extra_arg2=3, extra_arg3=4)\n         self.assertEqual(kwargs, {\"extra_arg\": 2, \"extra_arg2\": 3})\n+\n+\n+@require_torch\n+class CanReturnTupleDecoratorTester(unittest.TestCase):\n+    def _get_model(self, config, store_config=True, raise_in_forward=False):\n+        # Simple model class for testing can_return_tuple decorator.\n+        class SimpleTestModel(torch.nn.Module):\n+            def __init__(self, config):\n+                super().__init__()\n+                if store_config:\n+                    self.config = config\n+\n+            @can_return_tuple\n+            def forward(self, x):\n+                if raise_in_forward:\n+                    raise ValueError(\"Test error\")\n+                return BaseModelOutput(\n+                    last_hidden_state=x,\n+                    hidden_states=None,\n+                    attentions=None,\n+                )\n+\n+        return SimpleTestModel(config)\n+\n+    def test_decorator_eager(self):\n+        \"\"\"Test that the can_return_tuple decorator works with eager mode.\"\"\"\n+\n+        # test nothing is set\n+        config = PretrainedConfig()\n+        model = self._get_model(config)\n+        inputs = torch.tensor(10)\n+        output = model(inputs)\n+        self.assertIsInstance(\n+            output, BaseModelOutput, \"output should be a BaseModelOutput when return_dict is not set\"\n+        )\n+\n+        # test all explicit cases\n+        for config_return_dict in [True, False, None]:\n+            for return_dict in [True, False, None]:\n+                config = PretrainedConfig(return_dict=config_return_dict)\n+                model = self._get_model(config)\n+                output = model(torch.tensor(10), return_dict=return_dict)\n+\n+                expected_type = tuple if config_return_dict is False or return_dict is False else BaseModelOutput\n+                message = f\"output should be a {expected_type.__name__} when config.use_return_dict={config_return_dict} and return_dict={return_dict}\"\n+                self.assertIsInstance(output, expected_type, message)\n+\n+    def test_decorator_compiled(self):\n+        \"\"\"Test that the can_return_tuple decorator works with compiled mode.\"\"\"\n+        config = PretrainedConfig()\n+\n+        # Output object\n+        model = self._get_model(config)\n+        compiled_model = torch.compile(model)\n+        output = compiled_model(torch.tensor(10))\n+        self.assertIsInstance(output, BaseModelOutput)\n+\n+        # Tuple output\n+        model = self._get_model(config)\n+        compiled_model = torch.compile(model)\n+        output = compiled_model(torch.tensor(10), return_dict=False)\n+        self.assertIsInstance(output, tuple)\n+\n+    def test_decorator_torch_export(self):\n+        \"\"\"Test that the can_return_tuple decorator works with torch.export.\"\"\"\n+        config = PretrainedConfig()\n+        model = self._get_model(config)\n+        torch.export.export(model, args=(torch.tensor(10),))\n+\n+    def test_decorator_torchscript(self):\n+        \"\"\"Test that the can_return_tuple decorator works with torch.jit.trace.\"\"\"\n+        config = PretrainedConfig(return_dict=False)\n+        model = self._get_model(config)\n+        inputs = torch.tensor(10)\n+        traced_module = torch.jit.trace(model, inputs)\n+        output = traced_module(inputs)\n+        self.assertIsInstance(output, tuple)\n+\n+    def test_attribute_cleanup(self):\n+        \"\"\"Test that the `_is_top_level_module` attribute is removed after the forward call.\"\"\"\n+\n+        config = PretrainedConfig(return_dict=False)\n+        inputs = torch.tensor(10)\n+\n+        # working case\n+        model = self._get_model(config)\n+        output = model(inputs)\n+\n+        self.assertIsInstance(output, tuple)\n+        for name, module in model.named_modules():\n+            self.assertFalse(\n+                hasattr(module, \"_is_top_level_module\"),\n+                f\"Module `{name}` should not have `_is_top_level_module` attribute\",\n+            )\n+\n+        # model without config\n+        no_config_model = self._get_model(config, store_config=False)\n+        output = no_config_model(inputs)\n+\n+        self.assertIsInstance(output, BaseModelOutput)\n+        for name, module in no_config_model.named_modules():\n+            self.assertFalse(\n+                hasattr(module, \"_is_top_level_module\"),\n+                f\"Module `{name}` should not have `_is_top_level_module` attribute\",\n+            )\n+\n+        # model with raise in forward\n+        model_with_raise = self._get_model(config, raise_in_forward=True)\n+        with self.assertRaises(ValueError):\n+            model_with_raise(inputs)\n+\n+        for name, module in model_with_raise.named_modules():\n+            self.assertFalse(\n+                hasattr(module, \"_is_top_level_module\"),\n+                f\"Module `{name}` should not have `_is_top_level_module` attribute\",\n+            )"
        }
    ],
    "stats": {
        "total": 2635,
        "additions": 943,
        "deletions": 1692
    }
}