{
    "author": "jpizarrom",
    "message": "ðŸš¨ Add Blip2ForImageTextRetrieval (#29261)\n\n* add Blip2ForImageTextRetrieval\r\n\r\n* use one line and remove unnecessary space in tests\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* use  value from the config, rather than hardcoded\r\n\r\n* change order of params in Blip2QFormerModel.forward\r\n\r\n* update docstring\r\n\r\n* fix style\r\n\r\n* update test_inference_opt\r\n\r\n* move embeddings out of Blip2QFormerModel\r\n\r\n* remove from_vision_qformer_configs\r\n\r\n* remove autocast float16 in Blip2QFormerModel\r\n\r\n* rename fiels into vision_projection,text_projection,use_image_text_matching_head\r\n\r\n* use CLIPOutput for  Blip2ImageTextMatchingModelOutput\r\n\r\n* remove past_key_values_length from Blip2TextEmbeddings\r\n\r\n* fix small typo in the CLIPOutput docstring\r\n\r\n* add Blip2ForImageTextRetrieval to Zero Shot Image Classification mapping\r\n\r\n* update docstring and add require_torch_fp16\r\n\r\n* rollback test_inference_opt\r\n\r\n* use use_image_text_matching_head=True in convert\r\n\r\n* skip test_model_get_set_embeddings\r\n\r\n* fix create_rename_keys error on new itm fields\r\n\r\n* revert to do  scale after dot product between \"query\" and \"key\"\r\n\r\n* fix ValueError on convert script for blip2-opt-2.7b\r\n\r\n* update org of paths to Salesforce\r\n\r\n* add is_pipeline_test_to_skip for VisualQuestionAnsweringPipelineTests\r\n\r\n* [run_slow] blip_2\r\n\r\n* removed Blip2ForImageTextRetrieval from IGNORE_NON_AUTO_CONFIGURED\r\n\r\n* fix docstring of Blip2ImageTextMatchingModelOutput\r\n\r\n* [run_slow] blip_2\r\n\r\n* fix multi-gpu tests\r\n\r\n* [run_slow] blip_2\r\n\r\n* [run_slow] blip_2\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "7591ca5bc583aadbdfe6b2fb36a12c874233323c",
    "files": [
        {
            "sha": "b57c69ca6b321b4d293bf94d826d495c87afc089",
            "filename": "docs/source/en/model_doc/blip-2.md",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -87,4 +87,17 @@ If you're interested in submitting a resource to be included here, please feel f\n \n [[autodoc]] Blip2ForConditionalGeneration\n     - forward\n-    - generate\n\\ No newline at end of file\n+    - generate\n+\n+## Blip2ForImageTextRetrieval\n+\n+[[autodoc]] Blip2ForImageTextRetrieval\n+    - forward\n+\n+## Blip2TextModelWithProjection\n+\n+[[autodoc]] Blip2TextModelWithProjection\n+\n+## Blip2VisionModelWithProjection\n+\n+[[autodoc]] Blip2VisionModelWithProjection"
        },
        {
            "sha": "fef6610a5a42b2b1d465b3beb0a5a2d61a616f5f",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -1578,10 +1578,13 @@\n     _import_structure[\"models.blip_2\"].extend(\n         [\n             \"Blip2ForConditionalGeneration\",\n+            \"Blip2ForImageTextRetrieval\",\n             \"Blip2Model\",\n             \"Blip2PreTrainedModel\",\n             \"Blip2QFormerModel\",\n+            \"Blip2TextModelWithProjection\",\n             \"Blip2VisionModel\",\n+            \"Blip2VisionModelWithProjection\",\n         ]\n     )\n     _import_structure[\"models.bloom\"].extend(\n@@ -6327,10 +6330,13 @@\n         )\n         from .models.blip_2 import (\n             Blip2ForConditionalGeneration,\n+            Blip2ForImageTextRetrieval,\n             Blip2Model,\n             Blip2PreTrainedModel,\n             Blip2QFormerModel,\n+            Blip2TextModelWithProjection,\n             Blip2VisionModel,\n+            Blip2VisionModelWithProjection,\n         )\n         from .models.bloom import (\n             BloomForCausalLM,"
        },
        {
            "sha": "0d344cc54b137fa8b03a8f1d828d556d2cab6af9",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -161,19 +161,19 @@ class AltCLIPOutput(ModelOutput):\n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n             Contrastive loss for image-text similarity.\n-        logits_per_image:(`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n             The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n             similarity scores.\n-        logits_per_text:(`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n             The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n             similarity scores.\n-        text_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The text embeddings obtained by applying the projection layer to the pooled output of [`AltCLIPTextModel`].\n-        image_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The image embeddings obtained by applying the projection layer to the pooled output of [`AltCLIPVisionModel`].\n-        text_model_output(`BaseModelOutputWithPooling`):\n+        text_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`AltCLIPTextModel`].\n-        vision_model_output(`BaseModelOutputWithPooling`):\n+        vision_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`AltCLIPVisionModel`].\n     \"\"\"\n "
        },
        {
            "sha": "93eef37fbd91ca7543c901229a749b5437bba967",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -1266,6 +1266,7 @@\n         (\"align\", \"AlignModel\"),\n         (\"altclip\", \"AltCLIPModel\"),\n         (\"blip\", \"BlipModel\"),\n+        (\"blip-2\", \"Blip2ForImageTextRetrieval\"),\n         (\"chinese_clip\", \"ChineseCLIPModel\"),\n         (\"clip\", \"CLIPModel\"),\n         (\"clipseg\", \"CLIPSegModel\"),"
        },
        {
            "sha": "329ddfe19ac66ccc584c2dcd370c8dc760658a81",
            "filename": "src/transformers/models/blip_2/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fblip_2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fblip_2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2F__init__.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -33,10 +33,13 @@\n else:\n     _import_structure[\"modeling_blip_2\"] = [\n         \"Blip2Model\",\n+        \"Blip2VisionModelWithProjection\",\n         \"Blip2QFormerModel\",\n         \"Blip2PreTrainedModel\",\n         \"Blip2ForConditionalGeneration\",\n+        \"Blip2ForImageTextRetrieval\",\n         \"Blip2VisionModel\",\n+        \"Blip2TextModelWithProjection\",\n     ]\n \n if TYPE_CHECKING:\n@@ -55,10 +58,13 @@\n     else:\n         from .modeling_blip_2 import (\n             Blip2ForConditionalGeneration,\n+            Blip2ForImageTextRetrieval,\n             Blip2Model,\n             Blip2PreTrainedModel,\n             Blip2QFormerModel,\n+            Blip2TextModelWithProjection,\n             Blip2VisionModel,\n+            Blip2VisionModelWithProjection,\n         )\n \n else:"
        },
        {
            "sha": "16fa4aec38492b49376231982fe7828e706fa047",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"BLIP-2 model configuration\"\"\"\n \n import os\n-from typing import Union\n+from typing import Optional, Union\n \n from ...configuration_utils import PretrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n@@ -172,6 +172,8 @@ class Blip2QFormerConfig(PretrainedConfig):\n             The frequency of adding cross-attention to the Transformer layers.\n         encoder_hidden_size (`int`, *optional*, defaults to 1408):\n             The hidden size of the hidden states for cross-attention.\n+        use_qformer_text_input (`bool`, *optional*, defaults to `False`):\n+            Whether to use BERT-style embeddings.\n \n     Examples:\n \n@@ -206,6 +208,7 @@ def __init__(\n         position_embedding_type=\"absolute\",\n         cross_attention_frequency=2,\n         encoder_hidden_size=1408,\n+        use_qformer_text_input=False,\n         **kwargs,\n     ):\n         super().__init__(pad_token_id=pad_token_id, **kwargs)\n@@ -224,6 +227,7 @@ def __init__(\n         self.position_embedding_type = position_embedding_type\n         self.cross_attention_frequency = cross_attention_frequency\n         self.encoder_hidden_size = encoder_hidden_size\n+        self.use_qformer_text_input = use_qformer_text_input\n \n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n@@ -263,6 +267,8 @@ class Blip2Config(PretrainedConfig):\n             Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n         num_query_tokens (`int`, *optional*, defaults to 32):\n             The number of query tokens passed through the Transformer.\n+        image_text_hidden_size (`int`, *optional*, defaults to 256):\n+            Dimentionality of the hidden state of the image-text fusion layer.\n \n         image_token_index (`int`, *optional*):\n             Token index of special image token.\n@@ -307,6 +313,7 @@ def __init__(\n         qformer_config=None,\n         text_config=None,\n         num_query_tokens=32,\n+        image_text_hidden_size=256,\n         image_token_index=None,\n         **kwargs,\n     ):\n@@ -333,6 +340,7 @@ def __init__(\n         self.is_encoder_decoder = self.text_config.is_encoder_decoder\n \n         self.num_query_tokens = num_query_tokens\n+        self.image_text_hidden_size = image_text_hidden_size\n         self.image_token_index = image_token_index\n         self.qformer_config.encoder_hidden_size = self.vision_config.hidden_size\n         self.use_decoder_only_language_model = self.text_config.model_type in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n@@ -344,20 +352,28 @@ def from_vision_qformer_text_configs(\n         cls,\n         vision_config: Blip2VisionConfig,\n         qformer_config: Blip2QFormerConfig,\n-        text_config: PretrainedConfig,\n+        text_config: Optional[PretrainedConfig] = None,\n         **kwargs,\n     ):\n         r\"\"\"\n         Instantiate a [`Blip2Config`] (or a derived class) from a BLIP-2 vision model, Q-Former and language model\n         configurations.\n \n+        Args:\n+            vision_config (`dict`):\n+                Dictionary of configuration options used to initialize [`Blip2VisionConfig`].\n+            qformer_config (`dict`):\n+                Dictionary of configuration options used to initialize [`Blip2QFormerConfig`].\n+            text_config (`dict`, *optional*):\n+                Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n+\n         Returns:\n             [`Blip2Config`]: An instance of a configuration object\n         \"\"\"\n \n         return cls(\n             vision_config=vision_config.to_dict(),\n             qformer_config=qformer_config.to_dict(),\n-            text_config=text_config.to_dict(),\n+            text_config=text_config.to_dict() if text_config is not None else None,\n             **kwargs,\n         )"
        },
        {
            "sha": "5f972353c4f41e2bac44d639353ae189f0b9020e",
            "filename": "src/transformers/models/blip_2/convert_blip_2_original_to_pytorch.py",
            "status": "modified",
            "additions": 162,
            "deletions": 63,
            "changes": 225,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconvert_blip_2_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconvert_blip_2_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconvert_blip_2_original_to_pytorch.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -31,9 +31,12 @@\n \n from transformers import (\n     AutoTokenizer,\n+    BertTokenizer,\n     Blip2Config,\n     Blip2ForConditionalGeneration,\n+    Blip2ForImageTextRetrieval,\n     Blip2Processor,\n+    Blip2QFormerConfig,\n     Blip2VisionConfig,\n     BlipImageProcessor,\n     OPTConfig,\n@@ -51,7 +54,7 @@ def load_demo_image():\n \n \n # here we list all keys to be renamed (original name on the left, our name on the right)\n-def create_rename_keys(config):\n+def create_rename_keys(config, model_name):\n     rename_keys = []\n     # fmt: off\n \n@@ -79,6 +82,13 @@ def create_rename_keys(config):\n     # QFormer\n     rename_keys.append((\"Qformer.bert.embeddings.LayerNorm.weight\", \"qformer.layernorm.weight\"))\n     rename_keys.append((\"Qformer.bert.embeddings.LayerNorm.bias\", \"qformer.layernorm.bias\"))\n+    if \"itm\" in model_name:\n+        rename_keys.append((\"Qformer.bert.embeddings.word_embeddings.weight\", \"embeddings.word_embeddings.weight\"))\n+        rename_keys.append((\"Qformer.bert.embeddings.position_embeddings.weight\", \"embeddings.position_embeddings.weight\"))\n+        rename_keys.append((\"vision_proj.weight\", \"vision_projection.weight\"))\n+        rename_keys.append((\"vision_proj.bias\", \"vision_projection.bias\"))\n+        rename_keys.append((\"text_proj.weight\", \"text_projection.weight\"))\n+        rename_keys.append((\"text_proj.bias\", \"text_projection.bias\"))\n \n     # fmt: on\n     return rename_keys\n@@ -114,26 +124,47 @@ def get_blip2_config(model_name, eos_token_id):\n         text_config = T5Config.from_pretrained(\"google/flan-t5-xl\", dense_act_fn=\"gelu\", bos_token_id=1).to_dict()\n     elif \"t5-xxl\" in model_name:\n         text_config = T5Config.from_pretrained(\"google/flan-t5-xxl\", dense_act_fn=\"gelu\", bos_token_id=1).to_dict()\n-\n-    config = Blip2Config(vision_config=vision_config, text_config=text_config)\n+    elif \"itm\" in model_name:\n+        text_config = {}\n+    else:\n+        raise ValueError(\"Model name not supported\")\n+\n+    if \"itm\" in model_name:\n+        config = Blip2Config(\n+            vision_config=vision_config,\n+            qformer_config=Blip2QFormerConfig(vocab_size=30523, use_qformer_text_input=True).to_dict(),\n+        )\n+    else:\n+        config = Blip2Config(vision_config=vision_config, text_config=text_config)\n \n     return config, image_size\n \n \n @torch.no_grad()\n-def convert_blip2_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):\n+def convert_blip2_checkpoint(\n+    model_name, pytorch_dump_folder_path=None, push_to_hub=False, lavis_device=\"cpu\", hf_model_device=\"cpu\"\n+):\n     \"\"\"\n     Copy/paste/tweak model's weights to Transformers design.\n     \"\"\"\n-    tokenizer = (\n-        AutoTokenizer.from_pretrained(\"facebook/opt-2.7b\")\n-        if \"opt\" in model_name\n-        else AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n-    )\n-    eos_token_id = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\n+    if \"opt\" in model_name:\n+        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-2.7b\")\n+    elif \"itm\" in model_name:\n+        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", truncation_side=\"right\")\n+        tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})\n+    else:\n+        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n+\n+    if \"itm\" in model_name:\n+        eos_token_id = None\n+    else:\n+        eos_token_id = tokenizer(\"\\n\", add_special_tokens=False).input_ids[0]\n     config, image_size = get_blip2_config(model_name, eos_token_id=eos_token_id)\n \n-    hf_model = Blip2ForConditionalGeneration(config).eval()\n+    if \"itm\" in model_name:\n+        hf_model = Blip2ForImageTextRetrieval(config).eval()\n+    else:\n+        hf_model = Blip2ForConditionalGeneration(config).eval()\n \n     model_name_to_original = {\n         \"blip2-opt-2.7b\": (\"blip2_opt\", \"pretrain_opt2.7b\"),\n@@ -143,16 +174,12 @@ def convert_blip2_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_\n         \"blip2-flan-t5-xl\": (\"blip2_t5\", \"pretrain_flant5xl\"),\n         \"blip2-flan-t5-xl-coco\": (\"blip2_t5\", \"caption_coco_flant5xl\"),\n         \"blip2-flan-t5-xxl\": (\"blip2_t5\", \"pretrain_flant5xxl\"),\n+        \"blip2-itm-vit-g\": (\"blip2_image_text_matching\", \"pretrain\"),\n+        \"blip2-itm-vit-g-coco\": (\"blip2_image_text_matching\", \"coco\"),\n     }\n \n     name, type = model_name_to_original[model_name]\n \n-    # note: this script is tested on 2 GPUs, as models are compared in float32,\n-    # which requires quite some memory. Hence loading both on a\n-    # separate device is the easiest to compare\n-    hf_model_device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n-    lavis_device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n-\n     # load original model\n     print(\"Loading original model...\")\n     original_model, vis_processors, _ = load_model_and_preprocess(\n@@ -163,7 +190,7 @@ def convert_blip2_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_\n \n     # update state dict keys\n     state_dict = original_model.state_dict()\n-    rename_keys = create_rename_keys(config)\n+    rename_keys = create_rename_keys(config, model_name)\n     for src, dest in rename_keys:\n         rename_key(state_dict, src, dest)\n \n@@ -189,11 +216,15 @@ def convert_blip2_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_\n \n     missing_keys, unexpected_keys = hf_model.load_state_dict(state_dict, strict=False)\n     assert len(missing_keys) == 0\n-    assert unexpected_keys == [\"qformer.embeddings.position_ids\"]\n+\n+    if \"itm\" in model_name:\n+        unexpected_keys = list(filter(lambda x: not x.startswith(\"Qformer.cls\"), unexpected_keys))\n+        assert unexpected_keys == [\"temp\", \"qformer.embeddings.position_ids\"]\n+    else:\n+        assert unexpected_keys == [\"qformer.embeddings.position_ids\"]\n \n     image = load_demo_image()\n     original_pixel_values = vis_processors[\"eval\"](image).unsqueeze(0).to(lavis_device)\n-    input_ids = tokenizer([\"\\n\"], return_tensors=\"pt\").input_ids.to(hf_model_device)\n \n     # create processor\n     image_processor = BlipImageProcessor(\n@@ -207,50 +238,105 @@ def convert_blip2_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_\n \n     original_model.to(lavis_device)\n     hf_model.to(hf_model_device)\n-    with torch.no_grad():\n-        if \"opt\" in model_name:\n-            original_logits = original_model({\"image\": original_pixel_values, \"text_input\": [\"\"]}).logits\n-            logits = hf_model(pixel_values, input_ids).logits\n-        else:\n-            original_logits = original_model(\n-                {\"image\": original_pixel_values, \"text_input\": [\"\\n\"], \"text_output\": [\"\\n\"]}\n-            ).logits\n-            labels = input_ids.masked_fill(input_ids == tokenizer.pad_token_id, -100)\n-            logits = hf_model(pixel_values, input_ids, labels=labels).logits\n-\n-    assert original_logits.shape == logits.shape\n-    print(\"First values of original logits:\", original_logits[0, :3, :3])\n-    print(\"First values of HF logits:\", logits[0, :3, :3])\n \n-    # assert values\n-    assert torch.allclose(original_logits.to(logits.device), logits, atol=1e-4)\n-    print(\"Looks ok!\")\n+    if \"itm\" in model_name:\n+        caption = \"a large fountain spewing water into the air\"\n+        input_ids = tokenizer([caption], return_tensors=\"pt\").input_ids.to(hf_model_device)\n+        attention_mask = processor(text=caption, return_tensors=\"pt\").attention_mask.to(hf_model_device)\n \n-    print(\"Generating a caption...\")\n-    prompt = \"Question: what object is in this image? Answer:\"\n-    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(hf_model_device)\n-\n-    set_seed(42)\n-\n-    original_outputs = original_model.generate(\n-        {\"image\": original_pixel_values, \"prompt\": prompt}, use_nucleus_sampling=True\n-    )\n-    outputs = hf_model.generate(\n-        pixel_values,\n-        input_ids,\n-        do_sample=True,\n-        num_beams=5,\n-        max_length=30,\n-        min_length=1,\n-        top_p=0.9,\n-        repetition_penalty=1.0,\n-        length_penalty=1.0,\n-        temperature=1,\n-    )\n-    output_text = processor.batch_decode(outputs, skip_special_tokens=True)\n-    output_text = [text.strip() for text in output_text]\n-    print(\"Original generation:\", original_outputs)\n-    print(\"HF generation:\", output_text)\n+        with torch.no_grad():\n+            original_logits = original_model(\n+                {\"image\": original_pixel_values, \"text_input\": [caption]}, match_head=\"itm\"\n+            )\n+            logits = hf_model(\n+                pixel_values=original_pixel_values,\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                use_image_text_matching_head=True,\n+            )\n+\n+        assert original_logits.shape == logits.logits_per_image.shape\n+        print(\"First values of original logits:\", original_logits[0, :3])\n+        print(\"First values of HF logits:\", logits.logits_per_image[0, :3])\n+\n+        # assert values\n+        # cast to same type\n+        target_dtype = logits.logits_per_image.dtype\n+        assert torch.allclose(original_logits.to(target_dtype), logits.logits_per_image, atol=1e-4)\n+\n+        original_itm_scores = torch.nn.functional.softmax(original_logits, dim=1)\n+        itm_scores = torch.nn.functional.softmax(logits.logits_per_image, dim=1)\n+        assert torch.allclose(original_itm_scores.to(target_dtype), itm_scores, atol=1e-4)\n+        print(\"Looks ok!\")\n+\n+        with torch.no_grad():\n+            original_logits = original_model(\n+                {\"image\": original_pixel_values, \"text_input\": [caption]}, match_head=\"itc\"\n+            )\n+            logits = hf_model(\n+                pixel_values=original_pixel_values,\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                use_image_text_matching_head=False,\n+            )\n+\n+        assert original_logits.shape == logits.logits_per_image.shape\n+        print(\"First values of original logits:\", original_logits[0, :3])\n+        print(\"First values of HF logits:\", logits.logits_per_image[0, :3])\n+\n+        # assert values\n+        # cast to same type\n+        target_dtype = logits.logits_per_image.dtype\n+        assert torch.allclose(original_logits.to(target_dtype), logits.logits_per_image, atol=1e-4)\n+        print(\"Looks ok!\")\n+\n+    else:\n+        input_ids = tokenizer([\"\\n\"], return_tensors=\"pt\").input_ids.to(hf_model_device)\n+\n+        with torch.no_grad():\n+            if \"opt\" in model_name:\n+                original_logits = original_model({\"image\": original_pixel_values, \"text_input\": [\"\"]}).logits\n+                logits = hf_model(pixel_values, input_ids).logits\n+            else:\n+                original_logits = original_model(\n+                    {\"image\": original_pixel_values, \"text_input\": [\"\\n\"], \"text_output\": [\"\\n\"]}\n+                ).logits\n+                labels = input_ids.masked_fill(input_ids == tokenizer.pad_token_id, -100)\n+                logits = hf_model(pixel_values, input_ids, labels=labels).logits\n+\n+        assert original_logits.shape == logits.shape\n+        print(\"First values of original logits:\", original_logits[0, :3, :3])\n+        print(\"First values of HF logits:\", logits[0, :3, :3])\n+\n+        # assert values\n+        assert torch.allclose(original_logits.to(logits.device), logits, atol=1e-4)\n+        print(\"Looks ok!\")\n+\n+        print(\"Generating a caption...\")\n+        prompt = \"Question: what object is in this image? Answer:\"\n+        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(hf_model_device)\n+\n+        set_seed(42)\n+\n+        original_outputs = original_model.generate(\n+            {\"image\": original_pixel_values, \"prompt\": prompt}, use_nucleus_sampling=True, max_length=50\n+        )\n+        outputs = hf_model.generate(\n+            pixel_values,\n+            input_ids,\n+            do_sample=True,\n+            num_beams=5,\n+            max_length=30,\n+            min_length=1,\n+            top_p=0.9,\n+            repetition_penalty=1.0,\n+            length_penalty=1.0,\n+            temperature=1,\n+        )\n+        output_text = processor.batch_decode(outputs, skip_special_tokens=True)\n+        output_text = [text.strip() for text in output_text]\n+        print(\"Original generation:\", original_outputs)\n+        print(\"HF generation:\", output_text)\n \n     if pytorch_dump_folder_path is not None:\n         processor.save_pretrained(pytorch_dump_folder_path)\n@@ -271,6 +357,8 @@ def convert_blip2_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_\n         \"blip2-flan-t5-xl\",\n         \"blip2-flan-t5-xl-coco\",\n         \"blip2-flan-t5-xxl\",\n+        \"blip2-itm-vit-g\",\n+        \"blip2-itm-vit-g-coco\",\n     ]\n     parser.add_argument(\n         \"--model_name\",\n@@ -285,7 +373,18 @@ def convert_blip2_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_\n         action=\"store_true\",\n         help=\"Whether to push the model and processor to the hub after converting\",\n     )\n+    # note: this script is tested on 2 GPUs, as models are compared in float32,\n+    # which requires quite some memory. Hence loading both on a\n+    # separate device is the easiest to compare\n+    parser.add_argument(\n+        \"--lavis_device\", default=\"cpu\", type=str, help=\"Torch device to run the conversion, either cpu or cuda.\"\n+    )\n+    parser.add_argument(\n+        \"--hf_model_device\", default=\"cpu\", type=str, help=\"Torch device to run the conversion, either cpu or cuda.\"\n+    )\n \n     args = parser.parse_args()\n \n-    convert_blip2_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub)\n+    convert_blip2_checkpoint(\n+        args.model_name, args.pytorch_dump_folder_path, args.push_to_hub, args.lavis_device, args.hf_model_device\n+    )"
        },
        {
            "sha": "fba4c98696a0e3d8d51b88186dff83304e6b4552",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 593,
            "deletions": 2,
            "changes": 595,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -81,6 +81,103 @@ def to_tuple(self) -> Tuple[Any]:\n         )\n \n \n+@dataclass\n+class Blip2ImageTextMatchingModelOutput(ModelOutput):\n+    \"\"\"\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+            Contrastive loss for image-text similarity.\n+        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+            similarity scores.\n+        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+            similarity scores.\n+        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+            The text embeddings obtained by applying the projection layer to the pooled output.\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+            The image embeddings obtained by applying the projection layer to the pooled output.\n+        text_model_output (`BaseModelOutputWithPooling`):\n+            The output of the [`Blip2QFormerModel`].\n+        vision_model_output (`BaseModelOutputWithPooling`):\n+            The output of the [`Blip2VisionModel`].\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits_per_image: torch.FloatTensor = None\n+    logits_per_text: torch.FloatTensor = None\n+    text_embeds: torch.FloatTensor = None\n+    image_embeds: torch.FloatTensor = None\n+    text_model_output: BaseModelOutputWithPooling = None\n+    vision_model_output: BaseModelOutputWithPooling = None\n+\n+    def to_tuple(self) -> Tuple[Any]:\n+        return tuple(\n+            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n+            for k in self.keys()\n+        )\n+\n+\n+@dataclass\n+# Copied from transformers.models.clip.modeling_clip.CLIPTextModelOutput with CLIP->Blip2\n+class Blip2TextModelOutput(ModelOutput):\n+    \"\"\"\n+    Base class for text model's outputs that also contains a pooling of the last hidden states.\n+\n+    Args:\n+        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+            The text embeddings obtained by applying the projection layer to the pooler_output.\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    last_hidden_state: torch.FloatTensor = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@dataclass\n+# Copied from transformers.models.clip.modeling_clip.CLIPVisionModelOutput with CLIP->Blip2\n+class Blip2VisionModelOutput(ModelOutput):\n+    \"\"\"\n+    Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n+\n+    Args:\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+            The image embeddings obtained by applying the projection layer to the pooler_output.\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    last_hidden_state: torch.FloatTensor = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n # Copied from transformers.models.blip.modeling_blip.BlipVisionEmbeddings with Blip->Blip2\n class Blip2VisionEmbeddings(nn.Module):\n     def __init__(self, config: Blip2VisionConfig):\n@@ -304,7 +401,13 @@ class Blip2PreTrainedModel(PreTrainedModel):\n     config_class = Blip2Config\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"Blip2Attention\", \"T5Block\", \"OPTDecoderLayer\"]\n+    _no_split_modules = [\n+        \"Blip2Attention\",\n+        \"Blip2QFormerMultiHeadAttention\",\n+        \"Blip2TextEmbeddings\",\n+        \"T5Block\",\n+        \"OPTDecoderLayer\",\n+    ]\n     _skip_keys_device_placement = \"past_key_values\"\n     _keep_in_fp32_modules = [\"wo\"]\n \n@@ -398,6 +501,30 @@ def _init_weights(self, module):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n \n+BLIP_2_TEXT_WITH_PROJECTION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it. Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details. [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+            [What are attention masks?](../glossary#attention-mask)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n BLIP_2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n@@ -444,6 +571,43 @@ def _init_weights(self, module):\n             Whether to interpolate the pre-trained position encodings.\n \"\"\"\n \n+BLIP2_IMAGE_TEXT_RETRIEVAL_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for\n+            details.\n+\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n+            provided to serve as text prompt, which the language model can continue.\n+\n+            Indices can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+        use_image_text_matching_head (`bool`, *optional*):\n+            Whether to return the Image-Text Matching or Contrastive scores.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->Blip2\n class Blip2Encoder(nn.Module):\n@@ -842,6 +1006,10 @@ def __init__(self, config, layer_idx):\n         else:\n             self.has_cross_attention = False\n \n+        if config.use_qformer_text_input:\n+            self.intermediate = Blip2QFormerIntermediate(config)\n+            self.output = Blip2QFormerOutput(config)\n+\n         self.intermediate_query = Blip2QFormerIntermediate(config)\n         self.output_query = Blip2QFormerOutput(config)\n \n@@ -1022,6 +1190,49 @@ def forward(\n         )\n \n \n+class Blip2TextEmbeddings(nn.Module):\n+    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n+\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        query_embeds: Optional[torch.FloatTensor] = None,\n+    ) -> torch.Tensor:\n+        if input_ids is not None:\n+            seq_length = input_ids.size()[1]\n+        else:\n+            seq_length = 0\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, :seq_length]\n+\n+        if input_ids is not None:\n+            input_ids = input_ids.to(self.word_embeddings.weight.device)\n+            embeddings = self.word_embeddings(input_ids)\n+            if self.position_embedding_type == \"absolute\":\n+                position_embeddings = self.position_embeddings(position_ids)\n+                embeddings += position_embeddings\n+\n+            if query_embeds is not None:\n+                embeddings = torch.cat((query_embeds, embeddings), dim=1)\n+        else:\n+            embeddings = query_embeds\n+\n+        return embeddings\n+\n+\n class Blip2QFormerModel(Blip2PreTrainedModel):\n     \"\"\"\n     Querying Transformer (Q-Former), used in BLIP-2.\n@@ -1100,6 +1311,7 @@ def get_extended_attention_mask(\n     def forward(\n         self,\n         query_embeds: torch.FloatTensor,\n+        query_length: Optional[int] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n@@ -1140,7 +1352,9 @@ def forward(\n             past_key_values[0][0].shape[2] - self.config.query_length if past_key_values is not None else 0\n         )\n \n-        query_length = query_embeds.shape[1] if query_embeds is not None else 0\n+        query_length = (\n+            query_length if query_length is not None else query_embeds.shape[1] if query_embeds is not None else 0\n+        )\n \n         embedding_output = self.layernorm(query_embeds)\n         embedding_output = self.dropout(embedding_output)\n@@ -1567,6 +1781,206 @@ def forward(\n         )\n \n \n+@add_start_docstrings(\n+    \"\"\"\n+    BLIP-2 Text Model with a projection layer on top (a linear layer on top of the pooled output).\n+    \"\"\",\n+    BLIP_2_START_DOCSTRING,\n+)\n+class Blip2TextModelWithProjection(Blip2PreTrainedModel):\n+    supports_gradient_checkpointing = False\n+    _keep_in_fp32_modules = []\n+\n+    def __init__(self, config: Blip2Config):\n+        super().__init__(config)\n+\n+        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n+        self.embeddings = Blip2TextEmbeddings(config.qformer_config)\n+        self.qformer = Blip2QFormerModel(config.qformer_config)\n+\n+        # text projection layer\n+        self.text_projection = nn.Linear(config.qformer_config.hidden_size, config.image_text_hidden_size)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(BLIP_2_TEXT_WITH_PROJECTION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Blip2TextModelOutput, config_class=Blip2Config)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, Blip2TextModelOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoProcessor, Blip2TextModelWithProjection\n+\n+        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+        >>> model = Blip2TextModelWithProjection.from_pretrained(\n+        ...     \"Salesforce/blip2-itm-vit-g\", torch_dtype=torch.float16\n+        ... )\n+\n+        >>> model.to(device)  # doctest: +IGNORE_RESULT\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n+\n+        >>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], return_tensors=\"pt\").to(device)\n+\n+        >>> outputs = model(**inputs)\n+        >>> text_embeds = outputs.text_embeds\n+        >>> print(text_embeds.shape)\n+        torch.Size([2, 7, 256])\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        query_embeds = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+        )\n+\n+        text_outputs = self.qformer(\n+            query_embeds=query_embeds,\n+            query_length=0,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        pooled_output = text_outputs[0] if not return_dict else text_outputs.last_hidden_state\n+\n+        text_embeds = self.text_projection(pooled_output)\n+        text_embeds = nn.functional.normalize(text_embeds, dim=-1)\n+\n+        if not return_dict:\n+            outputs = (text_embeds, text_outputs[0]) + text_outputs[2:]\n+            return tuple(output for output in outputs if output is not None)\n+\n+        return Blip2TextModelOutput(\n+            text_embeds=text_embeds,\n+            last_hidden_state=text_outputs.last_hidden_state,\n+            hidden_states=text_outputs.hidden_states,\n+            attentions=text_outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    BLIP-2 Vision Model with a projection layer on top (a linear layer on top of the pooled output).\n+    \"\"\",\n+    BLIP_2_START_DOCSTRING,\n+)\n+class Blip2VisionModelWithProjection(Blip2PreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    _keep_in_fp32_modules = []\n+\n+    def __init__(self, config: Blip2Config):\n+        super().__init__(config)\n+\n+        self.vision_model = Blip2VisionModel(config.vision_config)\n+\n+        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n+        self.qformer = Blip2QFormerModel(config.qformer_config)\n+\n+        # vision projection layer\n+        self.vision_projection = nn.Linear(config.qformer_config.hidden_size, config.image_text_hidden_size)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_model.embeddings.patch_embedding\n+\n+    @add_start_docstrings_to_model_forward(BLIP_2_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Blip2VisionModelOutput, config_class=Blip2Config)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, Blip2VisionModelOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Blip2VisionModelWithProjection\n+\n+        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n+        >>> model = Blip2VisionModelWithProjection.from_pretrained(\n+        ...     \"Salesforce/blip2-itm-vit-g\", torch_dtype=torch.float16\n+        ... )\n+        >>> model.to(device)  # doctest: +IGNORE_RESULT\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n+\n+        >>> outputs = model(**inputs)\n+        >>> image_embeds = outputs.image_embeds\n+        >>> print(image_embeds.shape)\n+        torch.Size([1, 32, 256])\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        pooled_output = vision_outputs[0] if not return_dict else vision_outputs.last_hidden_state\n+\n+        image_attention_mask = torch.ones(pooled_output.size()[:-1], dtype=torch.long, device=pooled_output.device)\n+\n+        query_tokens = self.query_tokens.expand(pooled_output.shape[0], -1, -1)\n+\n+        query_outputs = self.qformer(\n+            query_embeds=query_tokens,\n+            encoder_hidden_states=pooled_output,\n+            encoder_attention_mask=image_attention_mask,\n+            return_dict=return_dict,\n+        )\n+\n+        embeds = query_outputs[0] if not return_dict else query_outputs.last_hidden_state\n+        image_embeds = self.vision_projection(embeds)\n+        image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n+\n+        if not return_dict:\n+            outputs = (image_embeds, vision_outputs[0]) + vision_outputs[2:]\n+            return tuple(output for output in outputs if output is not None)\n+\n+        return Blip2VisionModelOutput(\n+            image_embeds=image_embeds,\n+            last_hidden_state=vision_outputs.last_hidden_state,\n+            hidden_states=vision_outputs.hidden_states,\n+            attentions=vision_outputs.attentions,\n+        )\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     BLIP-2 Model for generating text given an image and an optional text prompt. The model consists of a vision\n@@ -1937,3 +2351,180 @@ def generate(\n             else:\n                 outputs = torch.cat([bos_tokens, outputs], dim=-1)\n         return outputs\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    BLIP-2 Model with a vision and text projector, and a classification head on top. The model is used in the context\n+    of image-text retrieval. Given an image and a text, the model returns the probability of the text being relevant to\n+    the image.\n+    \"\"\",\n+    BLIP_2_START_DOCSTRING,\n+)\n+class Blip2ForImageTextRetrieval(Blip2PreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    _keep_in_fp32_modules = []\n+\n+    def __init__(self, config: Blip2Config):\n+        super().__init__(config)\n+\n+        self.vision_model = Blip2VisionModel(config.vision_config)\n+\n+        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n+\n+        self.embeddings = Blip2TextEmbeddings(config.qformer_config)\n+        self.qformer = Blip2QFormerModel(config.qformer_config)\n+\n+        # vision projection layer\n+        self.vision_projection = nn.Linear(config.qformer_config.hidden_size, config.image_text_hidden_size)\n+\n+        # text projection layer\n+        self.text_projection = nn.Linear(config.qformer_config.hidden_size, config.image_text_hidden_size)\n+\n+        # image text matching head\n+        self.itm_head = nn.Linear(config.qformer_config.hidden_size, 2)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(BLIP2_IMAGE_TEXT_RETRIEVAL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Blip2ImageTextMatchingModelOutput, config_class=Blip2Config)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        input_ids: torch.LongTensor,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        use_image_text_matching_head: Optional[bool] = False,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, Blip2ImageTextMatchingModelOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Blip2ForImageTextRetrieval\n+\n+        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+        >>> model = Blip2ForImageTextRetrieval.from_pretrained(\"Salesforce/blip2-itm-vit-g\", torch_dtype=torch.float16)\n+        >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n+\n+        >>> model.to(device)  # doctest: +IGNORE_RESULT\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> text = \"two cats laying on a pink blanket\"\n+\n+        >>> inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device, torch.float16)\n+        >>> itm_out = model(**inputs, use_image_text_matching_head=True)\n+        >>> logits_per_image = torch.nn.functional.softmax(itm_out.logits_per_image, dim=1)\n+        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n+\n+        >>> print(f\"{probs[0][0]:.1%} that image 0 is not '{text}'\")\n+        26.9% that image 0 is not 'two cats laying on a pink blanket'\n+\n+        >>> print(f\"{probs[0][1]:.1%} that image 0 is '{text}'\")\n+        73.0% that image 0 is 'two cats laying on a pink blanket'\n+\n+        >>> texts = [\"a photo of a cat\", \"a photo of a dog\"]\n+\n+        >>> inputs = processor(images=image, text=texts, return_tensors=\"pt\").to(device, torch.float16)\n+        >>> itc_out = model(**inputs, use_image_text_matching_head=False)\n+        >>> logits_per_image = itc_out.logits_per_image  # this is the image-text similarity score\n+        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n+\n+        >>> print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n+        55.3% that image 0 is 'a photo of a cat'\n+\n+        >>> print(f\"{probs[0][1]:.1%} that image 0 is '{texts[1]}'\")\n+        44.7% that image 0 is 'a photo of a dog'\n+        ```\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        image_embeds = vision_outputs[0]\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        if use_image_text_matching_head:\n+            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n+            query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(query_tokens.device)\n+            attention_mask = torch.cat([query_attention_mask, attention_mask], dim=1)\n+\n+            query_embeds = self.embeddings(\n+                input_ids=input_ids,\n+                query_embeds=query_tokens,\n+            )\n+\n+            text_outputs = self.qformer(\n+                query_embeds=query_embeds,\n+                query_length=query_tokens.shape[1],\n+                attention_mask=attention_mask,\n+                encoder_hidden_states=image_embeds,\n+                encoder_attention_mask=image_attention_mask,\n+                return_dict=return_dict,\n+            )\n+            text_embeds = text_outputs[0] if not return_dict else text_outputs.last_hidden_state\n+\n+            output = self.itm_head(text_embeds[:, : query_tokens.size(1), :])\n+            logits_per_image = output.mean(dim=1)\n+            logits_per_text = logits_per_image.t()\n+        else:\n+            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n+            query_outputs = self.qformer(\n+                query_embeds=query_tokens,\n+                encoder_hidden_states=image_embeds,\n+                encoder_attention_mask=image_attention_mask,\n+                return_dict=return_dict,\n+            )\n+            image_embeds = query_outputs[0] if not return_dict else query_outputs.last_hidden_state\n+\n+            query_embeds = self.embeddings(\n+                input_ids=input_ids,\n+            )\n+            text_outputs = self.qformer(\n+                query_embeds=query_embeds,\n+                query_length=0,\n+                attention_mask=attention_mask,\n+                return_dict=return_dict,\n+            )\n+            question_embeds = text_outputs[0] if not return_dict else text_outputs.last_hidden_state\n+\n+            # normalized features\n+            image_embeds = nn.functional.normalize(self.vision_projection(image_embeds), dim=-1)\n+            text_embeds = nn.functional.normalize(self.text_projection(question_embeds[:, 0, :]), dim=-1)\n+\n+            # cosine similarity as logits\n+            logits_per_image = torch.matmul(image_embeds, text_embeds.t())\n+            logits_per_image, _ = logits_per_image.max(dim=1)\n+\n+            logits_per_text = logits_per_image.t()\n+\n+        if not return_dict:\n+            output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n+            return output\n+\n+        return Blip2ImageTextMatchingModelOutput(\n+            logits_per_image=logits_per_image,\n+            logits_per_text=logits_per_text,\n+            text_embeds=text_embeds,\n+            image_embeds=image_embeds,\n+            text_model_output=text_outputs,\n+            vision_model_output=vision_outputs,\n+        )"
        },
        {
            "sha": "d0224e3caa5b28028cfb8ffec237af0df7e1c4fe",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -195,19 +195,19 @@ class ClapOutput(ModelOutput):\n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n             Contrastive loss for audio-text similarity.\n-        logits_per_audio:(`torch.FloatTensor` of shape `(audio_batch_size, text_batch_size)`):\n+        logits_per_audio (`torch.FloatTensor` of shape `(audio_batch_size, text_batch_size)`):\n             The scaled dot product scores between `audio_embeds` and `text_embeds`. This represents the audio-text\n             similarity scores.\n-        logits_per_text:(`torch.FloatTensor` of shape `(text_batch_size, audio_batch_size)`):\n+        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, audio_batch_size)`):\n             The scaled dot product scores between `text_embeds` and `audio_embeds`. This represents the text-audio\n             similarity scores.\n-        text_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The text embeddings obtained by applying the projection layer to the pooled output of [`ClapTextModel`].\n-        audio_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        audio_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The audio embeddings obtained by applying the projection layer to the pooled output of [`ClapAudioModel`].\n-        text_model_output(`BaseModelOutputWithPooling`):\n+        text_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`ClapTextModel`].\n-        audio_model_output(`BaseModelOutputWithPooling`):\n+        audio_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`ClapAudioModel`].\n     \"\"\"\n "
        },
        {
            "sha": "64eb027e9e220c168ad430b4492ce4046edc2ec7",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -142,19 +142,19 @@ class CLIPOutput(ModelOutput):\n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n             Contrastive loss for image-text similarity.\n-        logits_per_image:(`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n             The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n             similarity scores.\n-        logits_per_text:(`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n             The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n             similarity scores.\n-        text_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The text embeddings obtained by applying the projection layer to the pooled output of [`CLIPTextModel`].\n-        image_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The image embeddings obtained by applying the projection layer to the pooled output of [`CLIPVisionModel`].\n-        text_model_output(`BaseModelOutputWithPooling`):\n+        text_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`CLIPTextModel`].\n-        vision_model_output(`BaseModelOutputWithPooling`):\n+        vision_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`CLIPVisionModel`].\n     \"\"\"\n "
        },
        {
            "sha": "a6507e431f68e2362dd5ce580d9b77896d92076d",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -63,19 +63,19 @@ class CLIPSegOutput(ModelOutput):\n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n             Contrastive loss for image-text similarity.\n-        logits_per_image:(`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n             The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n             similarity scores.\n-        logits_per_text:(`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n             The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n             similarity scores.\n-        text_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The text embeddings obtained by applying the projection layer to the pooled output of [`CLIPSegTextModel`].\n-        image_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The image embeddings obtained by applying the projection layer to the pooled output of [`CLIPSegVisionModel`].\n-        text_model_output(`BaseModelOutputWithPooling`):\n+        text_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`CLIPSegTextModel`].\n-        vision_model_output(`BaseModelOutputWithPooling`):\n+        vision_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`CLIPSegVisionModel`].\n     \"\"\"\n "
        },
        {
            "sha": "3439aa49dcb05c1ac6d59ab30ce3a6b731714333",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -215,19 +215,19 @@ class SiglipOutput(ModelOutput):\n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n             Contrastive loss for image-text similarity.\n-        logits_per_image:(`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n             The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n             similarity scores.\n-        logits_per_text:(`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n             The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n             similarity scores.\n-        text_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The text embeddings obtained by applying the projection layer to the pooled output of [`SiglipTextModel`].\n-        image_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n             The image embeddings obtained by applying the projection layer to the pooled output of [`SiglipVisionModel`].\n-        text_model_output(`BaseModelOutputWithPooling`):\n+        text_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`SiglipTextModel`].\n-        vision_model_output(`BaseModelOutputWithPooling`):\n+        vision_model_output (`BaseModelOutputWithPooling`):\n             The output of the [`SiglipVisionModel`].\n     \"\"\"\n "
        },
        {
            "sha": "61d5ff52227559e858ea95a017e2c5f0e1230e4d",
            "filename": "src/transformers/pipelines/zero_shot_image_classification.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -97,6 +97,9 @@ def __call__(self, images: Union[str, List[str], \"Image\", List[\"Image\"]], **kwar\n                 The maximum time in seconds to wait for fetching images from the web. If None, no timeout is set and\n                 the call may block forever.\n \n+            tokenizer_kwargs (`dict`, *optional*):\n+                Additional dictionary of keyword arguments passed along to the tokenizer.\n+\n         Return:\n             A list of dictionaries containing result, one dictionary per proposed label. The dictionaries contain the\n             following keys:\n@@ -106,26 +109,37 @@ def __call__(self, images: Union[str, List[str], \"Image\", List[\"Image\"]], **kwar\n         \"\"\"\n         return super().__call__(images, **kwargs)\n \n-    def _sanitize_parameters(self, **kwargs):\n+    def _sanitize_parameters(self, tokenizer_kwargs=None, **kwargs):\n         preprocess_params = {}\n         if \"candidate_labels\" in kwargs:\n             preprocess_params[\"candidate_labels\"] = kwargs[\"candidate_labels\"]\n         if \"timeout\" in kwargs:\n             preprocess_params[\"timeout\"] = kwargs[\"timeout\"]\n         if \"hypothesis_template\" in kwargs:\n             preprocess_params[\"hypothesis_template\"] = kwargs[\"hypothesis_template\"]\n+        if tokenizer_kwargs is not None:\n+            preprocess_params[\"tokenizer_kwargs\"] = tokenizer_kwargs\n \n         return preprocess_params, {}, {}\n \n-    def preprocess(self, image, candidate_labels=None, hypothesis_template=\"This is a photo of {}.\", timeout=None):\n+    def preprocess(\n+        self,\n+        image,\n+        candidate_labels=None,\n+        hypothesis_template=\"This is a photo of {}.\",\n+        timeout=None,\n+        tokenizer_kwargs=None,\n+    ):\n+        if tokenizer_kwargs is None:\n+            tokenizer_kwargs = {}\n         image = load_image(image, timeout=timeout)\n         inputs = self.image_processor(images=[image], return_tensors=self.framework)\n         if self.framework == \"pt\":\n             inputs = inputs.to(self.torch_dtype)\n         inputs[\"candidate_labels\"] = candidate_labels\n         sequences = [hypothesis_template.format(x) for x in candidate_labels]\n         padding = \"max_length\" if self.model.config.model_type == \"siglip\" else True\n-        text_inputs = self.tokenizer(sequences, return_tensors=self.framework, padding=padding)\n+        text_inputs = self.tokenizer(sequences, return_tensors=self.framework, padding=padding, **tokenizer_kwargs)\n         inputs[\"text_inputs\"] = [text_inputs]\n         return inputs\n "
        },
        {
            "sha": "41d164183dd89064bd2e027b6b4cd68a5fe4d7da",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -1603,6 +1603,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Blip2ForImageTextRetrieval(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class Blip2Model(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -1624,13 +1631,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Blip2TextModelWithProjection(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class Blip2VisionModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Blip2VisionModelWithProjection(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class BloomForCausalLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "cee5d710a85fb87b2811c684f027f4d40f6e7206",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 659,
            "deletions": 4,
            "changes": 663,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -24,6 +24,8 @@\n from transformers import CONFIG_MAPPING, Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n from transformers.testing_utils import (\n     require_torch,\n+    require_torch_fp16,\n+    require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_vision,\n     slow,\n@@ -47,7 +49,14 @@\n     import torch\n     from torch import nn\n \n-    from transformers import Blip2ForConditionalGeneration, Blip2Model, Blip2VisionModel\n+    from transformers import (\n+        Blip2ForConditionalGeneration,\n+        Blip2ForImageTextRetrieval,\n+        Blip2Model,\n+        Blip2TextModelWithProjection,\n+        Blip2VisionModel,\n+        Blip2VisionModelWithProjection,\n+    )\n \n \n if is_vision_available():\n@@ -243,6 +252,7 @@ def __init__(\n         initializer_range=0.02,\n         bos_token_id=0,\n         scope=None,\n+        use_qformer_text_input=False,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -262,6 +272,7 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.scope = scope\n         self.bos_token_id = bos_token_id\n+        self.use_qformer_text_input = use_qformer_text_input\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n@@ -294,6 +305,7 @@ def get_config(self):\n             max_position_embeddings=self.max_position_embeddings,\n             initializer_range=self.initializer_range,\n             bos_token_id=self.bos_token_id,\n+            use_qformer_text_input=self.use_qformer_text_input,\n         )\n \n \n@@ -489,7 +501,7 @@ def test_forward_signature(self):\n             self.assertListEqual(arg_names[:1], expected_arg_names)\n \n     def test_load_vision_qformer_text_config(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n         # Save Blip2Config and check if we can load Blip2VisionConfig from it\n         with tempfile.TemporaryDirectory() as tmp_dir_name:\n@@ -704,6 +716,16 @@ class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixi\n     test_attention_outputs = False\n     test_torchscript = False\n \n+    # TODO: Fix the failed tests\n+    def is_pipeline_test_to_skip(\n+        self, pipeline_test_casse_name, config_class, model_architecture, tokenizer_name, processor_name\n+    ):\n+        if pipeline_test_casse_name == \"VisualQuestionAnsweringPipelineTests\":\n+            # Get `RuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'`.\n+            return True\n+\n+        return False\n+\n     def setUp(self):\n         self.model_tester = Blip2ModelTester(self)\n \n@@ -752,7 +774,7 @@ def test_forward_signature(self):\n             self.assertListEqual(arg_names[:1], expected_arg_names)\n \n     def test_load_vision_qformer_text_config(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n         # Save Blip2Config and check if we can load Blip2VisionConfig from it\n         with tempfile.TemporaryDirectory() as tmp_dir_name:\n@@ -840,6 +862,549 @@ def test_initialization(self):\n                     )\n \n \n+class Blip2TextModelWithProjectionTester:\n+    def __init__(self, parent, vision_kwargs=None, qformer_kwargs=None, is_training=True):\n+        if vision_kwargs is None:\n+            vision_kwargs = {}\n+        if qformer_kwargs is None:\n+            qformer_kwargs = {\"use_qformer_text_input\": True}\n+\n+        self.parent = parent\n+        self.vision_model_tester = Blip2VisionModelTester(parent, **vision_kwargs)\n+        self.qformer_model_tester = Blip2QFormerModelTester(parent, **qformer_kwargs)\n+        self.is_training = is_training\n+        self.batch_size = self.vision_model_tester.batch_size  # need bs for batching_equivalence test\n+\n+    def get_config(self):\n+        return Blip2Config.from_vision_qformer_text_configs(\n+            vision_config=self.vision_model_tester.get_config(),\n+            qformer_config=self.qformer_model_tester.get_config(),\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        _, input_ids, attention_mask = self.qformer_model_tester.prepare_config_and_inputs()\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, attention_mask\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, attention_mask = config_and_inputs\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_model(self, config, input_ids, attention_mask):\n+        model = Blip2TextModelWithProjection(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(input_ids, attention_mask=attention_mask, output_attentions=True, output_hidden_states=True)\n+\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape,\n+            (self.vision_model_tester.batch_size, input_ids.shape[1], self.qformer_model_tester.hidden_size),\n+        )\n+        self.parent.assertEqual(\n+            result.text_embeds.shape,\n+            (\n+                self.vision_model_tester.batch_size,\n+                input_ids.shape[1],\n+                config.image_text_hidden_size,\n+            ),\n+        )\n+\n+        with torch.no_grad():\n+            result2 = model(\n+                input_ids,\n+                attention_mask=attention_mask,\n+                return_dict=not config.use_return_dict,\n+                output_attentions=True,\n+                output_hidden_states=True,\n+            )\n+\n+        self.parent.assertTrue(torch.allclose(result.text_embeds, result2[0]))\n+        self.parent.assertTrue(torch.allclose(result.last_hidden_state, result2[1]))\n+        self.parent.assertTrue(torch.allclose(result.hidden_states[0], result2[2][0]))\n+        self.parent.assertTrue(torch.allclose(result.hidden_states[1], result2[2][1]))\n+        self.parent.assertTrue(torch.allclose(result.attentions[0], result2[3][0]))\n+        self.parent.assertTrue(torch.allclose(result.attentions[1], result2[3][1]))\n+\n+\n+@require_torch\n+class Blip2TextModelWithProjectionTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (Blip2TextModelWithProjection,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_head_masking = False\n+\n+    test_resize_embeddings = False\n+    test_attention_outputs = False\n+    test_torchscript = False\n+\n+    def setUp(self):\n+        self.model_tester = Blip2TextModelWithProjectionTester(self)\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2TextModelWithProjection does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2TextModelWithProjection does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2TextModelWithProjection does not have input/output embeddings\")\n+    def test_model_common_attributes(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2TextModelWithProjection has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2TextModelWithProjection has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_to_base(self):\n+        pass\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n+            self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_model_from_pretrained(self):\n+        model_name = \"Salesforce/blip2-itm-vit-g\"\n+        model = Blip2TextModelWithProjection.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+        self.assertTrue(hasattr(model, \"text_projection\"))\n+\n+        _, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n+\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n+\n+        self.assertEqual(\n+            outputs.text_embeds.shape,\n+            (\n+                self.model_tester.qformer_model_tester.batch_size,\n+                input_ids.shape[1],\n+                model.config.image_text_hidden_size,\n+            ),\n+        )\n+\n+\n+class Blip2VisionModelWithProjectionTester:\n+    def __init__(self, parent, vision_kwargs=None, qformer_kwargs=None, is_training=True):\n+        if vision_kwargs is None:\n+            vision_kwargs = {}\n+        if qformer_kwargs is None:\n+            qformer_kwargs = {\"use_qformer_text_input\": True}\n+\n+        self.parent = parent\n+        self.vision_model_tester = Blip2VisionModelTester(parent, **vision_kwargs)\n+        self.qformer_model_tester = Blip2QFormerModelTester(parent, **qformer_kwargs)\n+        self.is_training = is_training\n+        self.num_hidden_layers = self.vision_model_tester.num_hidden_layers\n+        self.num_attention_heads = self.vision_model_tester.num_attention_heads\n+        self.seq_length = self.vision_model_tester.seq_length\n+        self.hidden_size = self.vision_model_tester.hidden_size\n+        self.batch_size = self.vision_model_tester.batch_size  # need bs for batching_equivalence test\n+\n+    def get_config(self):\n+        return Blip2Config.from_vision_qformer_text_configs(\n+            vision_config=self.vision_model_tester.get_config(),\n+            qformer_config=self.qformer_model_tester.get_config(),\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        _, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = Blip2VisionModelWithProjection(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values, output_attentions=True, output_hidden_states=True)\n+\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape,\n+            (\n+                self.vision_model_tester.batch_size,\n+                self.vision_model_tester.seq_length,\n+                self.qformer_model_tester.hidden_size,\n+            ),\n+        )\n+        self.parent.assertEqual(\n+            result.image_embeds.shape,\n+            (\n+                self.vision_model_tester.batch_size,\n+                config.vision_config.hidden_size,\n+                config.image_text_hidden_size,\n+            ),\n+        )\n+\n+        with torch.no_grad():\n+            result2 = model(\n+                pixel_values,\n+                return_dict=not config.use_return_dict,\n+                output_attentions=True,\n+                output_hidden_states=True,\n+            )\n+\n+        self.parent.assertTrue(torch.allclose(result.image_embeds, result2[0]))\n+        self.parent.assertTrue(torch.allclose(result.last_hidden_state, result2[1]))\n+        self.parent.assertTrue(torch.allclose(result.hidden_states[0], result2[2][0]))\n+        self.parent.assertTrue(torch.allclose(result.hidden_states[1], result2[2][1]))\n+        self.parent.assertTrue(torch.allclose(result.attentions[0], result2[3][0]))\n+        self.parent.assertTrue(torch.allclose(result.attentions[1], result2[3][1]))\n+\n+\n+@require_torch\n+class Blip2VisionModelWithProjectionTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (Blip2VisionModelWithProjection,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_head_masking = False\n+\n+    test_resize_embeddings = False\n+    test_torchscript = False\n+\n+    def setUp(self):\n+        self.model_tester = Blip2VisionModelWithProjectionTester(self)\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2VisionModelWithProjection does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2VisionModelWithProjection does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    def test_model_common_attributes(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    @unittest.skip(reason=\"Blip2VisionModelWithProjection has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2VisionModelWithProjection has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_to_base(self):\n+        pass\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_model_from_pretrained(self):\n+        model_name = \"Salesforce/blip2-itm-vit-g\"\n+        model = Blip2VisionModelWithProjection.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+        self.assertTrue(hasattr(model, \"vision_projection\"))\n+\n+        _, pixel_values = self.model_tester.prepare_config_and_inputs()\n+\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            outputs = model(pixel_values=pixel_values)\n+\n+        self.assertEqual(\n+            outputs.image_embeds.shape,\n+            (\n+                self.model_tester.vision_model_tester.batch_size,\n+                model.config.num_query_tokens,\n+                model.config.image_text_hidden_size,\n+            ),\n+        )\n+\n+\n+class Blip2TextRetrievalModelTester:\n+    def __init__(self, parent, vision_kwargs=None, qformer_kwargs=None, is_training=True):\n+        if vision_kwargs is None:\n+            vision_kwargs = {}\n+        if qformer_kwargs is None:\n+            qformer_kwargs = {\"use_qformer_text_input\": True}\n+\n+        self.parent = parent\n+        self.vision_model_tester = Blip2VisionModelTester(parent, **vision_kwargs)\n+        self.qformer_model_tester = Blip2QFormerModelTester(parent, **qformer_kwargs)\n+        self.is_training = is_training\n+        self.batch_size = self.vision_model_tester.batch_size  # need bs for batching_equivalence test\n+\n+    def get_config(self):\n+        return Blip2Config.from_vision_qformer_text_configs(\n+            vision_config=self.vision_model_tester.get_config(),\n+            qformer_config=self.qformer_model_tester.get_config(),\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        _, input_ids, attention_mask = self.qformer_model_tester.prepare_config_and_inputs()\n+        _, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, attention_mask, pixel_values\n+\n+    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n+        model = Blip2ForImageTextRetrieval(config).to(torch_device).eval()\n+        with torch.no_grad():\n+            result = model(pixel_values, input_ids, attention_mask, use_image_text_matching_head=True)\n+\n+        self.parent.assertEqual(\n+            result.logits_per_image.shape,\n+            (self.vision_model_tester.batch_size, 2),\n+        )\n+\n+        with torch.no_grad():\n+            result = model(pixel_values, input_ids, attention_mask)\n+\n+        self.parent.assertEqual(\n+            result.logits_per_image.shape,\n+            (self.vision_model_tester.batch_size, self.qformer_model_tester.batch_size),\n+        )\n+        self.parent.assertEqual(\n+            result.logits_per_text.shape, (self.qformer_model_tester.batch_size, self.vision_model_tester.batch_size)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, attention_mask, pixel_values = config_and_inputs\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"pixel_values\": pixel_values,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Blip2TextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (Blip2ForImageTextRetrieval,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_head_masking = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_attention_outputs = False\n+    test_torchscript = False\n+\n+    def setUp(self):\n+        self.model_tester = Blip2TextRetrievalModelTester(self)\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Inputs_embeds is tested in individual model tests\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2ForImageTextRetrieval does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Blip2Model does not have input/output embeddings\")\n+    def test_model_common_attributes(self):\n+        pass\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\", \"input_ids\", \"attention_mask\"]\n+            expected_arg_names.extend(\n+                [\"use_image_text_matching_head\"] if \"use_image_text_matching_head\" in arg_names else []\n+            )\n+            self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n+\n+    def test_load_vision_qformer_text_config(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # Save Blip2Config and check if we can load Blip2VisionConfig from it\n+        with tempfile.TemporaryDirectory() as tmp_dir_name:\n+            config.save_pretrained(tmp_dir_name)\n+            vision_config = Blip2VisionConfig.from_pretrained(tmp_dir_name)\n+            self.assertDictEqual(config.vision_config.to_dict(), vision_config.to_dict())\n+\n+        # Save Blip2Config and check if we can load Blip2QFormerConfig from it\n+        with tempfile.TemporaryDirectory() as tmp_dir_name:\n+            config.save_pretrained(tmp_dir_name)\n+            qformer_config = Blip2QFormerConfig.from_pretrained(tmp_dir_name)\n+            self.assertDictEqual(config.qformer_config.to_dict(), qformer_config.to_dict())\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_model_from_pretrained(self):\n+        model_name = \"Salesforce/blip2-itm-vit-g\"\n+        model = Blip2ForImageTextRetrieval.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+        _, input_ids, attention_mask, pixel_values = self.model_tester.prepare_config_and_inputs()\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        with torch.no_grad():\n+            outputs = model(\n+                pixel_values=pixel_values,\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                use_image_text_matching_head=True,\n+            )\n+        self.assertEqual(outputs.logits_per_image.shape, (self.model_tester.qformer_model_tester.batch_size, 2))\n+\n+        with torch.no_grad():\n+            outputs = model(\n+                pixel_values=pixel_values,\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+            )\n+        self.assertEqual(\n+            outputs.logits_per_image.shape,\n+            (self.model_tester.vision_model_tester.batch_size, self.model_tester.qformer_model_tester.batch_size),\n+        )\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Training is not yet supported\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    # check if `logit_scale` is initilized as per the original implementation\n+                    if name == \"logit_scale\":\n+                        self.assertAlmostEqual(\n+                            param.data.item(),\n+                            np.log(1 / 0.07),\n+                            delta=1e-3,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    elif name == \"temp\":\n+                        self.assertAlmostEqual(\n+                            param.data.item(),\n+                            0.07,\n+                            delta=1e-3,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    else:\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+\n+\n # We will verify our results on an image of cute cats\n def prepare_img():\n     url = \"https://huggingface.co/hf-internal-testing/blip-test-image/resolve/main/demo.jpg\"\n@@ -984,7 +1549,7 @@ def test_inference_opt_multi_accelerator(self):\n         prompt = \"Question: which city is this? Answer:\"\n         inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(0, dtype=torch.float16)\n \n-        predictions = model.generate(**inputs)\n+        predictions = model.generate(**inputs, max_new_tokens=11)\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n         # Test output\n@@ -1063,3 +1628,93 @@ def test_expansion_in_processing(self):\n         generated_text_expanded = processor.batch_decode(predictions_expanded, skip_special_tokens=True)[0].strip()\n \n         self.assertTrue(generated_text_expanded == generated_text)\n+\n+    @require_torch_gpu\n+    def test_inference_itm(self):\n+        model_name = \"Salesforce/blip2-itm-vit-g\"\n+        processor = Blip2Processor.from_pretrained(model_name)\n+        model = Blip2ForImageTextRetrieval.from_pretrained(model_name).to(torch_device)\n+\n+        image = prepare_img()\n+        text = \"A woman and her dog sitting in a beach\"\n+        inputs = processor(images=image, text=text, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        out_itm = model(**inputs, use_image_text_matching_head=True)\n+        out = model(**inputs)\n+\n+        # verify\n+        expected_scores = torch.Tensor([[0.0238, 0.9762]])\n+        self.assertTrue(torch.allclose(torch.nn.Softmax()(out_itm[0].cpu()), expected_scores, rtol=1e-3, atol=1e-3))\n+        self.assertTrue(torch.allclose(out[0].cpu(), torch.Tensor([[0.4406]]), rtol=1e-3, atol=1e-3))\n+\n+    @require_torch_gpu\n+    @require_torch_fp16\n+    def test_inference_itm_fp16(self):\n+        model_name = \"Salesforce/blip2-itm-vit-g\"\n+        processor = Blip2Processor.from_pretrained(model_name)\n+        model = Blip2ForImageTextRetrieval.from_pretrained(model_name, torch_dtype=torch.float16).to(torch_device)\n+\n+        image = prepare_img()\n+        text = \"A woman and her dog sitting in a beach\"\n+        inputs = processor(images=image, text=text, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n+\n+        # forward pass\n+        out_itm = model(**inputs, use_image_text_matching_head=True)\n+        out = model(**inputs)\n+\n+        # verify\n+        expected_scores = torch.Tensor([[0.0239, 0.9761]])\n+        self.assertTrue(\n+            torch.allclose(torch.nn.Softmax()(out_itm[0].cpu().float()), expected_scores, rtol=1e-3, atol=1e-3)\n+        )\n+        self.assertTrue(torch.allclose(out[0].cpu().float(), torch.Tensor([[0.4406]]), rtol=1e-3, atol=1e-3))\n+\n+    @require_torch_gpu\n+    @require_torch_fp16\n+    def test_inference_vision_with_projection_fp16(self):\n+        model_name = \"Salesforce/blip2-itm-vit-g\"\n+        processor = Blip2Processor.from_pretrained(model_name)\n+        model = Blip2VisionModelWithProjection.from_pretrained(model_name, torch_dtype=torch.float16).to(torch_device)\n+\n+        image = prepare_img()\n+        inputs = processor(images=image, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n+\n+        # forward pass\n+        out = model(**inputs)\n+\n+        # verify\n+        expected_image_embeds = [\n+            -0.093994140625,\n+            -0.075927734375,\n+            0.031890869140625,\n+            0.053009033203125,\n+            0.0352783203125,\n+            -0.01190185546875,\n+        ]\n+        self.assertTrue(np.allclose(out.image_embeds[0][0][:6].tolist(), expected_image_embeds, atol=1e-3))\n+\n+    @require_torch_gpu\n+    @require_torch_fp16\n+    def test_inference_text_with_projection_fp16(self):\n+        model_name = \"Salesforce/blip2-itm-vit-g\"\n+        processor = Blip2Processor.from_pretrained(model_name)\n+        model = Blip2TextModelWithProjection.from_pretrained(model_name, torch_dtype=torch.float16).to(torch_device)\n+\n+        inputs = processor(text=\"a woman sitting on the beach with a dog\", padding=True, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        # forward pass\n+        out = model(**inputs)\n+\n+        # verify\n+        expected_text_embeds = [\n+            -0.1082763671875,\n+            0.053192138671875,\n+            -0.02825927734375,\n+            0.0169830322265625,\n+            0.08648681640625,\n+            -0.04656982421875,\n+        ]\n+        self.assertTrue(np.allclose(out.text_embeds[0][0][:6].tolist(), expected_text_embeds, atol=1e-3))"
        },
        {
            "sha": "b57adf609d1e0909ebcb27b789417165da6960f1",
            "filename": "tests/pipelines/test_pipelines_zero_shot_image_classification.py",
            "status": "modified",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/tests%2Fpipelines%2Ftest_pipelines_zero_shot_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/tests%2Fpipelines%2Ftest_pipelines_zero_shot_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot_image_classification.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -279,3 +279,46 @@ def test_siglip_model_pt(self):\n             ]\n             * 5,\n         )\n+\n+    @slow\n+    @require_torch\n+    def test_blip2_model_pt(self):\n+        image_classifier = pipeline(\n+            task=\"zero-shot-image-classification\",\n+            model=\"Salesforce/blip2-itm-vit-g\",\n+        )\n+        # This is an image of 2 cats with remotes and no planes\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        output = image_classifier(\n+            image,\n+            candidate_labels=[\"2 cats\", \"a plane\", \"a remote\"],\n+            tokenizer_kwargs={\"return_token_type_ids\": False},\n+        )\n+\n+        self.assertEqual(\n+            nested_simplify(output),\n+            [\n+                {\"score\": 0.369, \"label\": \"2 cats\"},\n+                {\"score\": 0.333, \"label\": \"a remote\"},\n+                {\"score\": 0.297, \"label\": \"a plane\"},\n+            ],\n+        )\n+\n+        output = image_classifier(\n+            [image] * 5,\n+            candidate_labels=[\"2 cats\", \"a plane\", \"a remote\"],\n+            batch_size=2,\n+            tokenizer_kwargs={\"return_token_type_ids\": False},\n+        )\n+\n+        self.assertEqual(\n+            nested_simplify(output),\n+            [\n+                [\n+                    {\"score\": 0.369, \"label\": \"2 cats\"},\n+                    {\"score\": 0.333, \"label\": \"a remote\"},\n+                    {\"score\": 0.297, \"label\": \"a plane\"},\n+                ]\n+            ]\n+            * 5,\n+        )"
        },
        {
            "sha": "47e17c546b333e6e86c70ed477f79a1b06900d53",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7591ca5bc583aadbdfe6b2fb36a12c874233323c/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7591ca5bc583aadbdfe6b2fb36a12c874233323c/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=7591ca5bc583aadbdfe6b2fb36a12c874233323c",
            "patch": "@@ -169,6 +169,8 @@\n     \"ClapAudioModel\",\n     \"ClapAudioModelWithProjection\",\n     \"Blip2ForConditionalGeneration\",\n+    \"Blip2TextModelWithProjection\",\n+    \"Blip2VisionModelWithProjection\",\n     \"Blip2QFormerModel\",\n     \"Blip2VisionModel\",\n     \"ErnieMForInformationExtraction\","
        }
    ],
    "stats": {
        "total": 1679,
        "additions": 1573,
        "deletions": 106
    }
}