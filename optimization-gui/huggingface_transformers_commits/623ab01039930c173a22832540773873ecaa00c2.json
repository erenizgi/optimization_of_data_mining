{
    "author": "BlackSamorez",
    "message": "FP-Quant support (#38696)\n\n* quartet\n\n* quartet qat -> quartet\n\n* format\n\n* bf16 backward\n\n* interfaces\n\n* forward_method\n\n* quartet -> fp_quant\n\n* style\n\n* List -> list\n\n* list typing\n\n* fixed format and annotations\n\n* test_fp_quant\n\n* docstrings and default dtypes\n\n* better docstring and removed noop checks\n\n* docs\n\n* pseudoquantization support to test on non-blackwell\n\n* pseudoquant\n\n* Pseudoquant docs\n\n* Update docs/source/en/quantization/fp_quant.md\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Update docs/source/en/quantization/fp_quant.md\n\n* Update docs/source/en/quantization/fp_quant.md\n\n* Update src/transformers/utils/quantization_config.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update tests/quantization/fp_quant_integration/test_fp_quant.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update tests/quantization/fp_quant_integration/test_fp_quant.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* small test fixes\n\n* dockerfile update\n\n* spec link\n\n* removed `_process_model_after_weight_loading`\n\n* toctree\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "623ab01039930c173a22832540773873ecaa00c2",
    "files": [
        {
            "sha": "e930d6d5262af200d7833aae4eb0efab50f38185",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -78,6 +78,9 @@ RUN git clone https://github.com/NetEase-FuXi/EETQ.git && cd EETQ/ && git submod\n # RUN python3 -m pip install --no-cache-dir flute-kernel==0.4.1\n # RUN python3 -m pip install --no-cache-dir git+https://github.com/Dao-AILab/fast-hadamard-transform.git\n \n+# Add fp-quant for quantization testing\n+RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.1.6\"\n+\n # Add compressed-tensors for quantization testing\n RUN python3 -m pip install --no-cache-dir compressed-tensors\n "
        },
        {
            "sha": "c7fc6020655274ab53c28a4d5fc2540c139d4e40",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -179,6 +179,8 @@\n     title: FBGEMM\n   - local: quantization/finegrained_fp8\n     title: Fine-grained FP8\n+  - local: quantization/fp_quant\n+    title: FP-Quant\n   - local: gguf\n     title: GGUF\n   - local: quantization/gptq"
        },
        {
            "sha": "992f629e5a1bdc5e40f34efa1631e027c8220224",
            "filename": "docs/source/en/main_classes/quantization.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -93,6 +93,10 @@ Learn how to quantize models in the [Quantization](../quantization) guide.\n \n [[autodoc]] QuarkConfig\n \n+## FPQuantConfig\n+\n+[[autodoc]] FPQuantConfig\n+\n ## AutoRoundConfig\n \n [[autodoc]] AutoRoundConfig"
        },
        {
            "sha": "a89e35da5cccbc022e07cc3a1a537f183a52d20b",
            "filename": "docs/source/en/quantization/fp_quant.md",
            "status": "added",
            "additions": 66,
            "deletions": 0,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -0,0 +1,66 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+丘멆잺 Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# FP-Quant\n+\n+[FP-Quant](https://github.com/IST-DASLab/FP-Quant) is a family of quantization algorithms tailored for the Blackwell generation of Nvidia GPUs. The goal is to allow for efficient post-training quantization (PTQ) and quantization-aware trainin (QAT) of LLMs in the [MXFP4 and NVFP4 data-types](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf).\n+\n+Currently, only PTQ with MXFP4 is supported. Models can either be quantized on the fly with `quantization_config=FPQuantConfig()`:\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer, FPQuantConfig\n+import torch\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"qwen/Qwen3-8B\",\n+    quantization_config=FPQuantConfig(),\n+    device_map=\"cuda\",\n+    torch_dtype=torch.bfloat16,\n+)\n+```\n+\n+or pre-processed with GPTQ for better quality (see [FP Format Quantization Harness](https://github.com/IST-DASLab/FP-Quant)).\n+\n+A **Blackwell-generation GPU is required** to run the kernels. Runtime support for FP-Quant is implemented through the [QuTLASS](https://github.com/IST-DASLab/qutlass) library and a lightweight PyTorch interface lib [`fp_quant`](https://github.com/IST-DASLab/FP-Quant/tree/master/inference_lib). We recommend installing the former **from source** and the latter with  `pip install fp_quant`.\n+\n+Users **without a Blackwell-generation GPU** , can use the method with `quantization_config=FPQuantConfig(pseudoquant=True)` without having to install [QuTLASS](https://github.com/IST-DASLab/qutlass). This would provide no speedups but would fully emulate the effect of quantization.\n+\n+> [!TIP]\n+> Find models pre-quantized with FP-Quant in the official ISTA-DASLab [collection](https://huggingface.co/collections/ISTA-DASLab/fp-quant-6877c186103a21d3a02568ee).\n+\n+## torch.compile\n+\n+FP-Quant is fully compatible with [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).\n+\n+```python\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, FPQuantConfig\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"qwen/Qwen3-8B\",\n+    quantization_config=FPQuantConfig(),\n+    device_map=\"cuda\",\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+model.forward = torch.compile(model.forward, mode=\"max-autotune\", fullgraph=True)\n+```\n+\n+## Speedups\n+\n+FP-Quant currently performs best for very large batch size processing.\n+\n+See [QuTLASS README](https://github.com/IST-DASLab/qutlass/blob/main/README.md) for speedups.\n\\ No newline at end of file"
        },
        {
            "sha": "f551d0690d96acabedd69c3a7f85de4fd68dd9c3",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -30,6 +30,7 @@ Use the Space below to help you pick a quantization method depending on your har\n | [bitsandbytes](./bitsandbytes)            | 游릭                   | 游리 |     游릭     | 游리 | 游댮                    | 游리 | 游릭 | 4/8          | 游릭               | 游릭                          | 游릭                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n | [compressed-tensors](./compressed_tensors) | 游댮                   | 游릭              |     游릭     | 游릭        | 游댮                                 | 游댮              | 游댮              | 1/8          | 游릭               | 游릭                          | 游릭                      | https://github.com/neuralmagic/compressed-tensors |\n | [EETQ](./eetq)                            | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | ?               | 8            | 游릭               | 游릭                          | 游릭                      | https://github.com/NetEase-FuXi/EETQ        |\n+| [FP-Quant](./fp_quant)                          | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游릭              | 4           | 游댮               | 游릭                          | 游릭                      | https://github.com/IST-DASLab/FP-Quant      |\n | [GGUF / GGML (llama.cpp)](../gguf)        | 游릭                   | 游릭              | 游릭        | 游댮        | 游릭                                 | 游댮              | 游댮              | 1/8          | 游댮               | [See Notes](../gguf)     | [See Notes](../gguf) | https://github.com/ggerganov/llama.cpp      |\n | [GPTQModel](./gptq)                       | 游댮                   | 游릭 | 游릭        | 游릭        | 游릭                                 | 游릭 | 游댮              | 2/3/4/8      | 游릭               | 游릭                          | 游릭                      | https://github.com/ModelCloud/GPTQModel        |\n | [AutoGPTQ](./gptq)                        | 游댮                   | 游댮              | 游릭        | 游릭        | 游댮                                 | 游댮              | 游댮              | 2/3/4/8      | 游릭               | 游릭                          | 游릭                      | https://github.com/AutoGPTQ/AutoGPTQ        |"
        },
        {
            "sha": "9186277fdd2581885890d178530f24f676a25789",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -275,6 +275,7 @@\n         \"HqqConfig\",\n         \"QuantoConfig\",\n         \"QuarkConfig\",\n+        \"FPQuantConfig\",\n         \"SpQRConfig\",\n         \"TorchAoConfig\",\n         \"VptqConfig\",\n@@ -961,6 +962,7 @@\n         EetqConfig,\n         FbgemmFp8Config,\n         FineGrainedFP8Config,\n+        FPQuantConfig,\n         GPTQConfig,\n         HiggsConfig,\n         HqqConfig,"
        },
        {
            "sha": "89ebac7004ee32a284f9d2dc53783a715aa6ae4a",
            "filename": "src/transformers/integrations/fp_quant.py",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Fintegrations%2Ffp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Fintegrations%2Ffp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffp_quant.py?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -0,0 +1,47 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"FP-Quant integration file\"\n+\n+from ..utils import (\n+    is_fp_quant_available,\n+)\n+\n+\n+if is_fp_quant_available():\n+    from fp_quant import FPQuantConfig as FPQuantLinearConfig\n+    from fp_quant import FPQuantDtype\n+\n+from transformers.utils.quantization_config import FPQuantConfig\n+\n+\n+def adapt_fp_quant_config(config: FPQuantConfig):\n+    if config.forward_dtype == \"mxfp4\":\n+        forward_dtype = FPQuantDtype.MXFP4\n+    else:\n+        raise ValueError(f\"Unsupported forward dtype: {config.forward_dtype}\")\n+\n+    if config.backward_dtype == \"bf16\":\n+        backward_dtype = FPQuantDtype.BF16\n+    else:\n+        raise ValueError(f\"Unsupported backward dtype: {config.backward_dtype}\")\n+\n+    return FPQuantLinearConfig(\n+        forward_dtype=forward_dtype,\n+        forward_method=config.forward_method,\n+        backward_dtype=backward_dtype,\n+        store_master_weights=config.store_master_weights,\n+        hadamard_group_size=config.hadamard_group_size,\n+        pseudoquantization=config.pseudoquantization,\n+        modules_to_not_convert=config.modules_to_not_convert,\n+    )"
        },
        {
            "sha": "e4fbaadb5d767d6f66c80661dbc60465505bc289",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -27,6 +27,7 @@\n     EetqConfig,\n     FbgemmFp8Config,\n     FineGrainedFP8Config,\n+    FPQuantConfig,\n     GPTQConfig,\n     HiggsConfig,\n     HqqConfig,\n@@ -49,6 +50,7 @@\n from .quantizer_eetq import EetqHfQuantizer\n from .quantizer_fbgemm_fp8 import FbgemmFp8HfQuantizer\n from .quantizer_finegrained_fp8 import FineGrainedFP8HfQuantizer\n+from .quantizer_fp_quant import FPQuantHfQuantizer\n from .quantizer_gptq import GptqHfQuantizer\n from .quantizer_higgs import HiggsHfQuantizer\n from .quantizer_hqq import HqqHfQuantizer\n@@ -67,6 +69,7 @@\n     \"aqlm\": AqlmHfQuantizer,\n     \"quanto\": QuantoHfQuantizer,\n     \"quark\": QuarkHfQuantizer,\n+    \"fp_quant\": FPQuantHfQuantizer,\n     \"eetq\": EetqHfQuantizer,\n     \"higgs\": HiggsHfQuantizer,\n     \"hqq\": HqqHfQuantizer,\n@@ -89,6 +92,7 @@\n     \"aqlm\": AqlmConfig,\n     \"quanto\": QuantoConfig,\n     \"quark\": QuarkConfig,\n+    \"fp_quant\": FPQuantConfig,\n     \"hqq\": HqqConfig,\n     \"compressed-tensors\": CompressedTensorsConfig,\n     \"fbgemm_fp8\": FbgemmFp8Config,"
        },
        {
            "sha": "a94573a912c4fa658e2b06dee0c5783e7bb0a759",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "added",
            "additions": 183,
            "deletions": 0,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -0,0 +1,183 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING, Any, Optional\n+\n+from .base import HfQuantizer\n+from .quantizers_utils import get_module_from_name\n+\n+\n+if TYPE_CHECKING:\n+    from ..modeling_utils import PreTrainedModel\n+\n+from ..utils import is_fp_quant_available, is_qutlass_available, is_torch_available, logging\n+from ..utils.quantization_config import QuantizationConfigMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class FPQuantHfQuantizer(HfQuantizer):\n+    \"\"\"\n+    Quantizer for the FP-Quant method. Enables the loading of prequantized models and in-flight quantization of full-precision models.\n+    \"\"\"\n+\n+    requires_calibration = False\n+    requires_parameters_quantization = True\n+    is_qat_trainable = False\n+    required_packages = [\"fp_quant\"]\n+\n+    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n+        super().__init__(quantization_config, **kwargs)\n+        self.quantization_config = quantization_config\n+\n+    def validate_environment(self, device_map, **kwargs):\n+        if not torch.cuda.is_available():\n+            raise NotImplementedError(\n+                \"FPQuant quantization is only supported on GPU. Please use a different quantizer.\"\n+            )\n+\n+        if not is_qutlass_available() and not self.quantization_config.pseudoquantization:\n+            raise ImportError(\n+                \"Using `fp_quant` with real quantization requires a **Blackwell GPU** and qutlass: `git clone https://github.com/IST-DASLab/qutlass.git && cd qutlass && pip install --no-build-isolation .`. You can use `FPQuantConfig(pseudoquantization=True, ...)` to use Triton-based pseudo-quantization. It doesn't provide any speedups but emulates the quantization behavior of the real quantization.\"\n+            )\n+\n+        if self.quantization_config.pseudoquantization:\n+            logger.warning(\n+                \"Using pseudo-quantization for FP-Quant. This doesn't provide any speedups but emulates the quantization behavior of the real quantization.\"\n+            )\n+\n+        if not is_fp_quant_available():\n+            raise ImportError(\"Using `fp_quant` quantization requires fp_quant: `pip install fp_quant`\")\n+\n+        if device_map is None:\n+            raise ValueError(\n+                \"You are attempting to load a FPQuant model without setting device_map.\"\n+                \" Please set device_map comprised of 'cuda' devices.\"\n+            )\n+        elif isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n+            raise ValueError(\n+                \"You are attempting to load a FPQuant model with a device_map that contains a CPU or disk device.\"\n+                \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n+            )\n+\n+    def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n+        if torch_dtype is None:\n+            logger.info(\"`torch_dtype` is None. Setting `torch_dtype=torch.bfloat16` for qutlass compatibility.\")\n+            torch_dtype = torch.bfloat16\n+        elif torch_dtype != torch.bfloat16:\n+            raise ValueError(\n+                f\"Invalid `torch_dtype` {torch_dtype}. fp_quant quantization only supports `torch_dtype=torch.bfloat16`.\"\n+            )\n+\n+        return torch_dtype\n+\n+    def create_quantized_param(\n+        self,\n+        model: \"PreTrainedModel\",\n+        param_value: \"torch.Tensor\",\n+        param_name: str,\n+        target_device: \"torch.device\",\n+        state_dict: dict[str, Any],\n+        unexpected_keys: Optional[list[str]] = None,\n+    ):\n+        module, _ = get_module_from_name(model, param_name)\n+\n+        # The module holds either:\n+        #  * `weight` when `store_master_weights=True`\n+        #  * `qweight` and `scales` when `store_master_weights=False` and `pseudoquantization=False`\n+        #  * `dqweight` when `store_master_weights=False` and `pseudoquantization=True`\n+\n+        if param_name.endswith(\".qweight\"):\n+            # Loading a real quantized checkpoint without master weights\n+            module.qweight = torch.nn.Parameter(\n+                param_value.to(target_device),\n+                requires_grad=False,\n+            )\n+            module.weight = None\n+            module.dqweight = None\n+            return\n+\n+        if param_name.endswith(\".dqweight\"):\n+            # Loading a pseudo-quantized checkpoint without master weights\n+            module.dqweight = torch.nn.Parameter(param_value.to(target_device))\n+            module.weight = None\n+            module.qweight = None\n+            module.scales = None\n+            return\n+\n+        # Loading master weights or an unquantized checkpoint\n+        module.weight = torch.nn.Parameter(param_value.to(target_device))\n+        # Let pre-forward handle the quantization and set None where necessary\n+        module.pre_forward()\n+\n+        if unexpected_keys is not None and param_name in unexpected_keys:\n+            unexpected_keys.remove(param_name)\n+\n+    def _process_model_before_weight_loading(\n+        self,\n+        model: \"PreTrainedModel\",\n+        **kwargs,\n+    ):\n+        from fp_quant import replace_with_fp_quant_linear\n+\n+        from ..integrations.fp_quant import adapt_fp_quant_config\n+\n+        replace_with_fp_quant_linear(\n+            model,\n+            fp_quant_linear_config=adapt_fp_quant_config(self.quantization_config),\n+        )\n+        model.config.quantization_config = self.quantization_config\n+\n+    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n+        return model\n+\n+    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n+        from fp_quant import FPQuantLinear\n+\n+        fp_quant_names = {name for name, module in model.named_modules() if isinstance(module, FPQuantLinear)}\n+\n+        def should_exclude(key: str) -> bool:\n+            if key.endswith(\".weight\") or key.endswith(\".bias\"):\n+                return False\n+            full_key = f\"{prefix}.{key}\"\n+            return any(name in key or name in full_key for name in fp_quant_names)\n+\n+        return [key for key in missing_keys if not should_exclude(key)]\n+\n+    @property\n+    def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n+        return False\n+\n+    def is_serializable(self, safe_serialization=None):\n+        return True\n+\n+    def check_quantized_param(\n+        self,\n+        model: \"PreTrainedModel\",\n+        param_value: \"torch.Tensor\",\n+        param_name: str,\n+        state_dict: dict[str, Any],\n+        **kwargs,\n+    ) -> bool:\n+        from fp_quant import FPQuantLinear\n+\n+        module, tensor_name = get_module_from_name(model, param_name)\n+        if isinstance(module, FPQuantLinear) and tensor_name in [\"weight\", \"qweight\", \"dqweight\"]:\n+            # Only quantize weights of FPQuantLinear modules that are not already quantized\n+            return True\n+        else:\n+            return False"
        },
        {
            "sha": "fd5f62ec28c0b7c9a9d0f7244349b8edd45b7cae",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -90,6 +90,7 @@\n     is_flash_attn_3_available,\n     is_flax_available,\n     is_flute_available,\n+    is_fp_quant_available,\n     is_fsdp_available,\n     is_ftfy_available,\n     is_g2p_en_available,\n@@ -127,6 +128,7 @@\n     is_pytest_available,\n     is_pytorch_quantization_available,\n     is_quark_available,\n+    is_qutlass_available,\n     is_rjieba_available,\n     is_sacremoses_available,\n     is_safetensors_available,\n@@ -1467,6 +1469,20 @@ def require_flute_hadamard(test_case):\n     )(test_case)\n \n \n+def require_fp_quant(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires fp_quant and qutlass\n+    \"\"\"\n+    return unittest.skipUnless(is_fp_quant_available(), \"test requires fp_quant\")(test_case)\n+\n+\n+def require_qutlass(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires qutlass\n+    \"\"\"\n+    return unittest.skipUnless(is_qutlass_available(), \"test requires qutlass\")(test_case)\n+\n+\n def require_phonemizer(test_case):\n     \"\"\"\n     Decorator marking a test that requires phonemizer"
        },
        {
            "sha": "0bb3709a42ea18d83984eff4b5ca6c2e2eac378d",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -158,6 +158,7 @@\n     is_flash_attn_greater_or_equal_2_10,\n     is_flax_available,\n     is_flute_available,\n+    is_fp_quant_available,\n     is_fsdp_available,\n     is_ftfy_available,\n     is_g2p_en_available,\n@@ -204,6 +205,7 @@\n     is_pytest_available,\n     is_pytorch_quantization_available,\n     is_quark_available,\n+    is_qutlass_available,\n     is_rich_available,\n     is_rjieba_available,\n     is_rocm_platform,"
        },
        {
            "sha": "310b00eb730098eb59a2f4f588197d9ddde2c3ed",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -172,6 +172,8 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n # `importlib.metadata.version` doesn't work with `awq`\n _auto_awq_available = importlib.util.find_spec(\"awq\") is not None\n _quark_available = _is_package_available(\"quark\")\n+_fp_quant_available, _fp_quant_version = _is_package_available(\"fp_quant\", return_version=True)\n+_qutlass_available = _is_package_available(\"qutlass\")\n _is_optimum_quanto_available = False\n try:\n     importlib.metadata.version(\"optimum_quanto\")\n@@ -1314,6 +1316,14 @@ def is_quark_available():\n     return _quark_available\n \n \n+def is_fp_quant_available():\n+    return _fp_quant_available and version.parse(_fp_quant_version) >= version.parse(\"0.1.6\")\n+\n+\n+def is_qutlass_available():\n+    return _qutlass_available\n+\n+\n def is_compressed_tensors_available():\n     return _compressed_tensors_available\n "
        },
        {
            "sha": "0bc616c6ff9c3cad1aec3591828834751182acdc",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -63,6 +63,7 @@ class QuantizationMethod(str, Enum):\n     SPQR = \"spqr\"\n     FP8 = \"fp8\"\n     QUARK = \"quark\"\n+    FPQUANT = \"fp_quant\"\n     AUTOROUND = \"auto-round\"\n \n \n@@ -1549,6 +1550,67 @@ def post_init(self):\n             raise ValueError(\"hadamard_size must be divisible by group_size\")\n \n \n+@dataclass\n+class FPQuantConfig(QuantizationConfigMixin):\n+    \"\"\"\n+    FPQuantConfig is a configuration class for quantization using the FPQuant method.\n+\n+    Args:\n+        forward_dtype (`str`, *optional*, defaults to `\"mxfp4\"`):\n+            The dtype to use for the forward pass.\n+        forward_method (`str`, *optional*, defaults to `\"abs_max\"`):\n+            The scaling to use for the forward pass. Can be `\"abs_max\"` or `\"quest\"`. `\"abs_max\"` is better for PTQ, `\"quest\"` is better for QAT.\n+        backward_dtype (`str`, *optional*, defaults to `\"bf16\"`):\n+            The dtype to use for the backward pass.\n+        store_master_weights (`bool`, *optional*, defaults to `False`):\n+            Whether to store the master weights. Needed for QAT over layer weights.\n+        hadamard_group_size (`int`, *optional*, defaults to 32):\n+            The group size for the hadamard transform before quantization for `\"quest\"` it matches the MXFP4 group size (32).\n+        pseudoquantization (`bool`, *optional*, defaults to `False`):\n+            Whether to use Triton-based pseudo-quantization. Is mandatory for non-Blackwell GPUs. Doesn't provide any speedup. For debugging purposes.\n+        modules_to_not_convert (`list`, *optional*):\n+            The list of modules to not quantize, useful for quantizing models that explicitly require to have\n+            some modules left in their original precision.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        forward_dtype: str = \"mxfp4\",\n+        forward_method: str = \"abs_max\",\n+        backward_dtype: str = \"bf16\",\n+        store_master_weights: bool = False,\n+        hadamard_group_size: int = 32,\n+        pseudoquantization: bool = False,\n+        modules_to_not_convert: Optional[list[str]] = None,\n+        **kwargs,\n+    ):\n+        self.forward_dtype = forward_dtype\n+        self.forward_method = forward_method\n+        self.backward_dtype = backward_dtype\n+        self.store_master_weights = store_master_weights\n+        self.hadamard_group_size = hadamard_group_size\n+        self.pseudoquantization = pseudoquantization\n+        self.modules_to_not_convert = modules_to_not_convert\n+\n+        self.quant_method = QuantizationMethod.FPQUANT\n+        self.post_init()\n+\n+    def post_init(self):\n+        r\"\"\"\n+        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n+        \"\"\"\n+        if self.forward_dtype not in [\"mxfp4\"]:\n+            raise ValueError(\"Only 'mxfp4' is supported for forward_dtype for now.\")\n+        if self.forward_method not in [\"abs_max\", \"quest\"]:\n+            raise ValueError(\"Only 'abs_max' and 'quest' are supported for forward_method for now.\")\n+        if self.backward_dtype not in [\"bf16\"]:\n+            raise ValueError(\"Only 'bf16' is supported for backward_dtype for now.\")\n+        if self.hadamard_group_size not in [32]:\n+            raise ValueError(\"Only a hadamard_group_size of 32 is supported for now.\")\n+        if self.modules_to_not_convert is None:\n+            self.modules_to_not_convert = [\"lm_head\"]\n+\n+\n @dataclass\n class TorchAoConfig(QuantizationConfigMixin):\n     quant_method: QuantizationMethod"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/quantization/fp_quant_integration/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/tests%2Fquantization%2Ffp_quant_integration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/tests%2Fquantization%2Ffp_quant_integration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffp_quant_integration%2F__init__.py?ref=623ab01039930c173a22832540773873ecaa00c2"
        },
        {
            "sha": "2bb60f5a2dc3576b937012288e796cde4d1b60de",
            "filename": "tests/quantization/fp_quant_integration/test_fp_quant.py",
            "status": "added",
            "additions": 227,
            "deletions": 0,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/623ab01039930c173a22832540773873ecaa00c2/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/623ab01039930c173a22832540773873ecaa00c2/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py?ref=623ab01039930c173a22832540773873ecaa00c2",
            "patch": "@@ -0,0 +1,227 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import tempfile\n+import unittest\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, FPQuantConfig\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    require_accelerate,\n+    require_fp_quant,\n+    require_qutlass,\n+    require_torch_gpu,\n+    require_torch_multi_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+@require_torch_gpu\n+class FPQuantConfigTest(unittest.TestCase):\n+    def test_to_dict(self):\n+        \"\"\"\n+        Simple test that checks if one uses a config and converts it to a dict, the dict is the same as the config object\n+        \"\"\"\n+        quantization_config = FPQuantConfig()\n+        config_to_dict = quantization_config.to_dict()\n+\n+        for key in config_to_dict:\n+            self.assertEqual(getattr(quantization_config, key), config_to_dict[key])\n+\n+    def test_from_dict(self):\n+        \"\"\"\n+        Simple test that checks if one uses a dict and converts it to a config object, the config object is the same as the dict\n+        \"\"\"\n+        dict = {\"modules_to_not_convert\": [\"embed_tokens\", \"lm_head\"], \"quant_method\": \"fp_quant\"}\n+        quantization_config = FPQuantConfig.from_dict(dict)\n+\n+        self.assertEqual(dict[\"modules_to_not_convert\"], quantization_config.modules_to_not_convert)\n+        self.assertEqual(dict[\"quant_method\"], quantization_config.quant_method)\n+\n+\n+@slow\n+@require_torch_gpu\n+@require_fp_quant\n+@require_qutlass\n+@require_accelerate\n+class FPQuantTest(unittest.TestCase):\n+    model_name = \"unsloth/Llama-3.2-1B\"\n+\n+    input_text = \"1 2 3 4\"\n+    max_new_tokens = 4\n+\n+    EXPECTED_OUTPUT = \"1 2 3 4 5 6\"\n+\n+    device_map = \"cuda\"\n+\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"\n+        Setup quantized model\n+        \"\"\"\n+        quantization_config = FPQuantConfig(pseudoquantization=False)\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            cls.model_name, device_map=cls.device_map, quantization_config=quantization_config\n+        )\n+\n+    def tearDown(self):\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+\n+    def test_quantized_model(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    def test_save_pretrained(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly after being saved and loaded\n+        \"\"\"\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            self.quantized_model.save_pretrained(tmpdirname)\n+\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.device_map)\n+\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    @require_torch_multi_gpu\n+    def test_quantized_model_multi_gpu(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly with multiple GPUs\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        quantization_config = FPQuantConfig()\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, device_map=\"auto\", quantization_config=quantization_config\n+        )\n+        self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    @require_torch_multi_gpu\n+    def test_save_pretrained_multi_gpu(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly after being saved and loaded\n+        \"\"\"\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            self.quantized_model.save_pretrained(tmpdirname)\n+\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=\"auto\")\n+            self.assertTrue(set(model.hf_device_map.values()) == {0, 1})\n+\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+\n+@slow\n+@require_torch_gpu\n+@require_fp_quant\n+@require_accelerate\n+class FPQuantPseudoquantTest(unittest.TestCase):\n+    model_name = \"unsloth/Llama-3.2-1B\"\n+\n+    input_text = \"1 2 3 4\"\n+    max_new_tokens = 4\n+\n+    EXPECTED_OUTPUT = \"1 2 3 4 5 6\"\n+\n+    device_map = \"cuda\"\n+\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"\n+        Setup quantized model\n+        \"\"\"\n+        quantization_config = FPQuantConfig(pseudoquantization=True)\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            cls.model_name, device_map=cls.device_map, quantization_config=quantization_config\n+        )\n+\n+    def tearDown(self):\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+\n+    def test_quantized_model(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    def test_save_pretrained(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly after being saved and loaded\n+        \"\"\"\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            self.quantized_model.save_pretrained(tmpdirname)\n+\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.device_map)\n+\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    @require_torch_multi_gpu\n+    def test_quantized_model_multi_gpu(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly with multiple GPUs\n+        set CUDA_VISIBLE_DEVICES=0,1 if you have more than 2 GPUs\n+        \"\"\"\n+        input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        quantization_config = FPQuantConfig(pseudoquantization=True)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name, device_map=\"auto\", quantization_config=quantization_config\n+        )\n+        self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+\n+    @require_torch_multi_gpu\n+    def test_save_pretrained_multi_gpu(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly after being saved and loaded\n+        \"\"\"\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            self.quantized_model.save_pretrained(tmpdirname)\n+\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=\"auto\")\n+            self.assertTrue(set(model.hf_device_map.values()) == {0, 1})\n+\n+            input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)"
        }
    ],
    "stats": {
        "total": 629,
        "additions": 629,
        "deletions": 0
    }
}