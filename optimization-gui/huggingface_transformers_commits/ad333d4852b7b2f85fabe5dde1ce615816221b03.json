{
    "author": "dsnsabari",
    "message": "[Qwen2.5-VL] Fix torch.finfo() TypeError for integer attention_mask_tensor (#39333)\n\n* Update modeling_qwen2_5_vl.py\n\n### üêõ Bug Description\r\n\r\nWhen using Unsloth‚Äôs Qwen2.5-VL vision models (both 3B and 7B) with the latest HuggingFace Transformers (commit: 520b9dcb42cef21662c304583368ff6645116a45), the model crashes due to a type mismatch in the attention mask handling.\r\n\r\n---\r\n\r\n### üî• Error Traceback\n\n* Fix dtype compatibility in attention mask processing\n\nReplace hardcoded torch.finfo() usage with dtype-aware function selection to handle both integer and floating-point attention mask tensors.\r\nTechnical Details:\r\n\r\nProblem: Line 1292 assumes floating-point dtype for attention_mask_tensor\r\nSolution: Add dtype check to use torch.iinfo() for integer types and torch.finfo() for float types\r\nFiles Modified: transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\n\n* Update modeling_qwen2_5_vl.py\n\n* Update modeling_qwen2_5_vl.py\n\n* Fix: Cast to float before applying torch.finfo\n\n* # Fix: Use appropriate function based on dtype\n\n* Update modular_qwen2_5_vl.py\n\n* Fix: Cast to float before applying torch.finfo\n\n* Fix: Use appropriate function based on dtype\n\n* Fix: Use appropriate function based on dtype\n\n* Updatet modeling_glm4v.py\n\n* Only apply conversion for floating point tensors (inverted masks)\n\n* corrected the format issue\n\nreformatted modeling_glm4v.py\r\n\r\nAll done! ‚ú® üç∞ ‚ú®\r\n1 file reformatted\n\n* Fix: Cast to float before applying torch.finfo\n\nCorrected the format issue\n\n* Fix torch.finfo() for integer attention mask\n\n#39333\n\n* Run make fix-copies and make style for CI compliance\n\n- Updated dependency versions table\n- Fixed code formatting and style issues\n- Sorted auto mappings\n- Updated documentation TOC\n\n* Fix torch.finfo() TypeError for\n\nFix torch.finfo() TypeError for integer attention_mask_tensor #39333\n\n* Fix torch.finfo() TypeError for integer",
    "sha": "ad333d4852b7b2f85fabe5dde1ce615816221b03",
    "files": [
        {
            "sha": "3349d58402d444df4b8a65f53a53c50eca5e3930",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=ad333d4852b7b2f85fabe5dde1ce615816221b03",
            "patch": "@@ -1301,8 +1301,10 @@ def forward(\n             )\n             if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n                 attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n+                # Only apply conversion for floating point tensors (inverted masks)\n+                if attention_mask_tensor.dtype.is_floating_point:\n+                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n \n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length"
        },
        {
            "sha": "ebe02052cd11f5c87ed83bdd0d6427c15fd290ea",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=ad333d4852b7b2f85fabe5dde1ce615816221b03",
            "patch": "@@ -1297,8 +1297,10 @@ def forward(\n             )\n             if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n                 attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n+                # Only apply conversion for floating point tensors (inverted masks)\n+                if attention_mask_tensor.dtype.is_floating_point:\n+                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n \n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length"
        },
        {
            "sha": "47283174b96f72aee6a14c8dcd33b3ecb1bb77c8",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=ad333d4852b7b2f85fabe5dde1ce615816221b03",
            "patch": "@@ -1289,8 +1289,10 @@ def forward(\n             )\n             if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n                 attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n+                # Only apply conversion for floating point tensors (inverted masks)\n+                if attention_mask_tensor.dtype.is_floating_point:\n+                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n \n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length"
        },
        {
            "sha": "f072ba0641e8dd5d54f16bd0391944858c6515dc",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=ad333d4852b7b2f85fabe5dde1ce615816221b03",
            "patch": "@@ -639,8 +639,10 @@ def forward(\n             )\n             if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n                 attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n+                # Only apply conversion for floating point tensors (inverted masks)\n+                if attention_mask_tensor.dtype.is_floating_point:\n+                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n \n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length"
        },
        {
            "sha": "a5dd8099bdad20408346db59164f549e76e9f677",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad333d4852b7b2f85fabe5dde1ce615816221b03/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=ad333d4852b7b2f85fabe5dde1ce615816221b03",
            "patch": "@@ -1227,8 +1227,10 @@ def forward(\n             )\n             if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n                 attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n-                attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n-                attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n+                # Only apply conversion for floating point tensors (inverted masks)\n+                if attention_mask_tensor.dtype.is_floating_point:\n+                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n \n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 20,
        "deletions": 10
    }
}