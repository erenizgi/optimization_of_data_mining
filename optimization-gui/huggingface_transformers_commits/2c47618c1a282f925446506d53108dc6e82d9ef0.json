{
    "author": "ArthurZucker",
    "message": "ðŸš¨All attention refactorðŸš¨ (#35235)\n\n* refactor LlamaAttention\r\n\r\n* minimal changes\r\n\r\n* fix llama\r\n\r\n* update\r\n\r\n* modular gemmas\r\n\r\n* modular nits\r\n\r\n* modular updates\r\n\r\n* nits\r\n\r\n* simplify\r\n\r\n* gpt2\r\n\r\n* more modualr and fixes\r\n\r\n* granite\r\n\r\n* modular modular modular\r\n\r\n* nits\r\n\r\n* update\r\n\r\n* qwen2 + starcoder2\r\n\r\n* mostly gemma2\r\n\r\n* Update image_processing_auto.py\r\n\r\n* fix\r\n\r\n* Update modular_starcoder2.py\r\n\r\n* fix\r\n\r\n* remove all copied from attentions\r\n\r\n* remove gcv\r\n\r\n* make fix-copies\r\n\r\n* oups\r\n\r\n* oups2.0\r\n\r\n* fix some modulars + all copied from\r\n\r\n* should be good now\r\n\r\n* revert unwanted changes\r\n\r\n* Update modeling_decision_transformer.py\r\n\r\n* finish cleanup\r\n\r\n* Update modeling_olmo.py\r\n\r\n* consistency\r\n\r\n* re-add gradient checkpointing attribute\r\n\r\n* fix\r\n\r\n* style\r\n\r\n* make config necessary\r\n\r\n* bis\r\n\r\n* bis\r\n\r\n* Update modeling_my_new_model2.py\r\n\r\n* is_causal attr\r\n\r\n* fix\r\n\r\n* remove past kv return from decoder layer\r\n\r\n* fix\r\n\r\n* default rope config\r\n\r\n* correctly fix rope config\r\n\r\n* fix bias\r\n\r\n* fix gpt2 attention output\r\n\r\n* fix test\r\n\r\n* fix inits\r\n\r\n* fix default sdpa\r\n\r\n* fix default sdpa implementation\r\n\r\n* harmonize classes\r\n\r\n* fix mistral\r\n\r\n* fix sliding window models\r\n\r\n* mixtral\r\n\r\n* be more explicit\r\n\r\n* style\r\n\r\n* fix\r\n\r\n* several fixes\r\n\r\n* Update modeling_dbrx.py\r\n\r\n* fix test\r\n\r\n* olmo + phi\r\n\r\n* rotary\r\n\r\n* syle\r\n\r\n* phi\r\n\r\n* phi again\r\n\r\n* again\r\n\r\n* kwargs\r\n\r\n* Update test_modeling_common.py\r\n\r\n* skip fx tracing tests\r\n\r\n* Update modeling_utils.py\r\n\r\n* gemma 2\r\n\r\n* again\r\n\r\n* Update modeling_recurrent_gemma.py\r\n\r\n* gemma2\r\n\r\n* granite\r\n\r\n* style\r\n\r\n* starcoder\r\n\r\n* Update sdpa_attention.py\r\n\r\n* switch args\r\n\r\n* Update modeling_mllama.py\r\n\r\n* fix\r\n\r\n* cache type tests\r\n\r\n* gpt2\r\n\r\n* Update test_modeling_common.py\r\n\r\n* fix\r\n\r\n* consistency\r\n\r\n* fix shape with encoder\r\n\r\n* should be the last one\r\n\r\n* tests non model\r\n\r\n* most comments\r\n\r\n* small oupsi\r\n\r\n* be more explicit in modulars\r\n\r\n* more explicit modulars\r\n\r\n* CIs! it works locally\r\n\r\n* add kwargs to _flash_attention_forward\r\n\r\n---------\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "2c47618c1a282f925446506d53108dc6e82d9ef0",
    "files": [
        {
            "sha": "3e0aa6e9b2ad02e3c0849f4a62b0b132d29cc770",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 85,
            "deletions": 360,
            "changes": 445,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -4,26 +4,20 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_dummy.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n-    logging,\n-)\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n from .configuration_dummy import DummyConfig\n \n \n@@ -53,40 +47,18 @@ def extra_repr(self):\n class DummyRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: DummyConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[DummyConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`DummyRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -199,144 +171,73 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class DummyAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: DummyConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: DummyConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class DummyFlashAttention2(DummyAttention):\n-    \"\"\"\n-    Dummy flash attention module. This module inherits from `DummyAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n@@ -346,167 +247,38 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (DummyRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class DummySdpaAttention(DummyAttention):\n-    \"\"\"\n-    Dummy attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `DummyAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from DummyAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"DummyModel is using DummySdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-DUMMY_ATTENTION_CLASSES = {\n-    \"eager\": DummyAttention,\n-    \"flash_attention_2\": DummyFlashAttention2,\n-    \"sdpa\": DummySdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class DummyDecoderLayer(nn.Module):\n     def __init__(self, config: DummyConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = DUMMY_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = DummyAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = DummyMLP(config)\n         self.input_layernorm = DummyRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -522,36 +294,14 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -571,13 +321,9 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -724,10 +470,7 @@ def __init__(self, config: DummyConfig):\n         )\n         self.norm = DummyRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = DummyRotaryEmbedding(config=config)\n-\n         self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -744,7 +487,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -772,31 +515,22 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n+\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -805,7 +539,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -838,9 +571,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -850,18 +580,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,"
        },
        {
            "sha": "c4f90a5cbadab3ad77af79475f106b14f48f00ef",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 85,
            "deletions": 362,
            "changes": 447,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -4,26 +4,20 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_multimodal1.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n-    logging,\n-)\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n from .configuration_multimodal1 import Multimodal1TextConfig\n \n \n@@ -53,40 +47,18 @@ def extra_repr(self):\n class Multimodal1TextRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: Multimodal1TextConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[Multimodal1TextConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`Multimodal1TextRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -199,144 +171,73 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Multimodal1TextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Multimodal1TextConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: Multimodal1TextConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Multimodal1TextFlashAttention2(Multimodal1TextAttention):\n-    \"\"\"\n-    Multimodal1Text flash attention module. This module inherits from `Multimodal1TextAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n@@ -346,169 +247,38 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (Multimodal1TextRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Multimodal1TextSdpaAttention(Multimodal1TextAttention):\n-    \"\"\"\n-    Multimodal1Text attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Multimodal1TextAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from Multimodal1TextAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Multimodal1TextModel is using Multimodal1TextSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-MULTIMODAL1_TEXT_ATTENTION_CLASSES = {\n-    \"eager\": Multimodal1TextAttention,\n-    \"flash_attention_2\": Multimodal1TextFlashAttention2,\n-    \"sdpa\": Multimodal1TextSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class Multimodal1TextDecoderLayer(nn.Module):\n     def __init__(self, config: Multimodal1TextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = MULTIMODAL1_TEXT_ATTENTION_CLASSES[config._attn_implementation](\n-            config=config, layer_idx=layer_idx\n-        )\n+        self.self_attn = Multimodal1TextAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = Multimodal1TextMLP(config)\n         self.input_layernorm = Multimodal1TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -524,36 +294,14 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -573,13 +321,9 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -726,10 +470,7 @@ def __init__(self, config: Multimodal1TextConfig):\n         )\n         self.norm = Multimodal1TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = Multimodal1TextRotaryEmbedding(config=config)\n-\n         self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -746,7 +487,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -774,31 +515,22 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n+\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -807,7 +539,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -840,9 +571,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -852,18 +580,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,"
        },
        {
            "sha": "b8d5b5eb910095d3d6727714acfdb2a45973028e",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 152,
            "deletions": 367,
            "changes": 519,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -4,24 +4,20 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_my_new_model2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n-    logging,\n-)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n from .configuration_my_new_model2 import MyNewModel2Config\n \n \n@@ -48,56 +44,85 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n+class MyNewModel2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n class MyNewModel2RotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        config: MyNewModel2Config,\n+        device=None,\n+    ):\n         super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n     @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n \n-class MyNewModel2MLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        if config.hidden_activation is None:\n-            logger.warning_once(\n-                \"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\\n\"\n-                \"MyNewModel2's activation function will be set to `gelu_pytorch_tanh`. Please, use\\n\"\n-                \"`config.hidden_activation` if you want to override this behaviour.\\n\"\n-                \"See https://github.com/huggingface/transformers/pull/29402 for more details.\"\n-            )\n-            config.hidden_activation = \"gelu_pytorch_tanh\"\n-        hidden_activation = config.hidden_activation\n-        self.act_fn = ACT2FN[hidden_activation]\n-\n-    def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n def rotate_half(x):\n@@ -146,317 +171,115 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class MyNewModel2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: MyNewModel2Config, layer_idx: Optional[int] = None):\n+    def __init__(self, config: MyNewModel2Config, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = config.head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n-        self.scaling = 1 / math.sqrt(config.head_dim)\n-\n-        if self.hidden_size % self.num_heads != 0:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n \n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-        self.rotary_emb = MyNewModel2RotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n         )\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class MyNewModel2SdpaAttention(MyNewModel2Attention):\n-    \"\"\"\n-    MyNewModel2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `MyNewModel2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from MyNewModel2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"MyNewModel2Model is using MyNewModel2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-class MyNewModel2FlashAttention2(MyNewModel2Attention):\n-    \"\"\"\n-    MyNewModel2 flash attention module. This module inherits from `MyNewModel2Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (MyNewModel2RMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n-\n-\n-MY_NEW_MODEL2_ATTENTION_CLASSES = {\n-    \"eager\": MyNewModel2Attention,\n-    \"flash_attention_2\": MyNewModel2FlashAttention2,\n-    \"sdpa\": MyNewModel2SdpaAttention,\n-}\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n \n \n class MyNewModel2DecoderLayer(nn.Module):\n     def __init__(self, config: MyNewModel2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-        self.self_attn = MY_NEW_MODEL2_ATTENTION_CLASSES[config._attn_implementation](\n-            config=config, layer_idx=layer_idx\n-        )\n+\n+        self.self_attn = MyNewModel2Attention(config=config, layer_idx=layer_idx)\n+\n         self.mlp = MyNewModel2MLP(config)\n         self.input_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -470,40 +293,23 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -515,13 +321,9 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -667,10 +469,8 @@ def __init__(self, config: MyNewModel2Config):\n             [MyNewModel2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.rotary_emb = MyNewModel2RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -714,19 +514,8 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False  # noqa: F841\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True  # noqa: F841\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -744,6 +533,9 @@ def forward(\n         # embed positions\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # normalized\n         # MyNewModel2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n         # See https://github.com/huggingface/transformers/pull/29402\n@@ -753,7 +545,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -769,6 +560,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -779,13 +571,11 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -795,18 +585,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,"
        },
        {
            "sha": "477d084b1d93094b0c83c07ff6b772e532671641",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 24,
            "deletions": 10,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -10,7 +10,7 @@\n import torch\n from torch import nn\n \n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -253,19 +253,28 @@ def tie_weights(self):\n         return self.language_model.tie_weights()\n \n     def _update_causal_mask(\n-        self, attention_mask, token_type_ids, inputs_embeds, past_key_values, cache_position, is_training: bool = False\n+        self,\n+        attention_mask,\n+        token_type_ids,\n+        past_key_values,\n+        cache_position,\n+        input_ids=None,\n+        inputs_embeds=None,\n+        is_training: bool = False,\n     ):\n         if self.config.text_config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n \n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        dtype = inputs_embeds.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = inputs_embeds.shape[1]\n+        min_dtype = torch.finfo(self.dtype).min\n+        inputs_lead_dim = input_ids.shape[0] if input_ids is not None else inputs_embeds.shape[0]\n+        sequence_length = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        elif isinstance(past_key_values, HybridCache):\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -278,7 +287,7 @@ def _update_causal_mask(\n             return attention_mask\n \n         causal_mask = torch.full(\n-            (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            (sequence_length, target_length), fill_value=min_dtype, dtype=self.dtype, device=cache_position.device\n         )\n         # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n         if sequence_length != 1:\n@@ -288,7 +297,7 @@ def _update_causal_mask(\n                 causal_mask[:, :sequence_length] = 0.0\n \n         causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(inputs_embeds.shape[0], 1, -1, -1)\n+        causal_mask = causal_mask[None, None, :, :].expand(inputs_lead_dim, 1, -1, -1)\n         if attention_mask is not None:\n             causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n             mask_length = attention_mask.shape[-1]\n@@ -317,7 +326,7 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_outputs = self.vision_tower(pixel_values)\n         selected_image_feature = image_outputs.last_hidden_state\n         image_features = self.multi_modal_projector(selected_image_feature)\n-        image_features = image_features / (self.config.hidden_size**0.5)\n+        image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n     @add_start_docstrings_to_model_forward(NEW_TASK_MODEL_INPUTS_DOCSTRING)\n@@ -414,6 +423,7 @@ def prepare_inputs_for_generation(\n         token_type_ids=None,\n         use_cache=True,\n         num_logits_to_keep=None,\n+        labels=None,\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n@@ -433,12 +443,16 @@ def prepare_inputs_for_generation(\n         # position_ids in NewTaskModel are 1-indexed\n         if model_inputs.get(\"position_ids\") is not None:\n             model_inputs[\"position_ids\"] += 1\n-\n         # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n         # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n         if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n-\n+        is_training = token_type_ids is not None and labels is not None\n+        if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n+            causal_mask = self._update_causal_mask(\n+                attention_mask, token_type_ids, past_key_values, cache_position, input_ids, inputs_embeds, is_training\n+            )\n+            model_inputs[\"attention_mask\"] = causal_mask\n         return model_inputs\n \n     def resize_token_embeddings("
        },
        {
            "sha": "42d8108ee72a68a201327e7620df1513218ea092",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 77,
            "deletions": 334,
            "changes": 411,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -4,26 +4,20 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_super.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n-    logging,\n-)\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n from .configuration_super import SuperConfig\n \n \n@@ -53,40 +47,18 @@ def extra_repr(self):\n class SuperRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: SuperConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[SuperConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`SuperRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -199,144 +171,73 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class SuperAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: SuperConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: SuperConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class SuperFlashAttention2(SuperAttention):\n-    \"\"\"\n-    Super flash attention module. This module inherits from `SuperAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n@@ -346,167 +247,38 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (SuperRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class SuperSdpaAttention(SuperAttention):\n-    \"\"\"\n-    Super attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `SuperAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from SuperAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"SuperModel is using SuperSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-SUPER_ATTENTION_CLASSES = {\n-    \"eager\": SuperAttention,\n-    \"flash_attention_2\": SuperFlashAttention2,\n-    \"sdpa\": SuperSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class SuperDecoderLayer(nn.Module):\n     def __init__(self, config: SuperConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = SUPER_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = SuperAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = SuperMLP(config)\n         self.input_layernorm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -522,36 +294,14 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -571,13 +321,9 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -724,10 +470,7 @@ def __init__(self, config: SuperConfig):\n         )\n         self.norm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = SuperRotaryEmbedding(config=config)\n-\n         self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "648877c8dce962d0d0387924ced7320781d9f056",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -37,10 +37,10 @@\n     download_url,\n     extract_commit_hash,\n     is_remote_url,\n-    is_timm_config_dict,\n     is_torch_available,\n     logging,\n )\n+from .utils.generic import is_timm_config_dict\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "1be223f8b079ba12f7259d8e53fd294f79d54941",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "added",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -0,0 +1,52 @@\n+from typing import Optional, Tuple\n+\n+import torch\n+\n+from ..modeling_flash_attention_utils import _flash_attention_forward\n+from ..utils import is_flash_attn_greater_or_equal_2_10\n+\n+\n+_use_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+\n+def flash_attention_forward(\n+    module: torch.nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    dropout: float = 0.0,\n+    scaling: Optional[float] = None,\n+    sliding_window: Optional[int] = None,\n+    softcap: Optional[float] = None,\n+    **kwargs,\n+) -> Tuple[torch.Tensor, None]:\n+    # This is before the transpose\n+    seq_len = query.shape[2]\n+\n+    # FA2 uses non-transposed inputs\n+    query = query.transpose(1, 2)\n+    key = key.transpose(1, 2)\n+    value = value.transpose(1, 2)\n+\n+    if query.dtype == torch.float32:\n+        query = query.to(torch.float16)\n+        key = key.to(torch.float16)\n+        value = value.to(torch.float16)\n+\n+    attn_output = _flash_attention_forward(\n+        query,\n+        key,\n+        value,\n+        attention_mask,\n+        seq_len,\n+        module.is_causal,\n+        dropout=dropout,\n+        softmax_scale=scaling,\n+        sliding_window=sliding_window,\n+        softcap=softcap,\n+        use_top_left_mask=_use_top_left_mask,\n+        **kwargs,\n+    )\n+\n+    return attn_output, None"
        },
        {
            "sha": "eacfb2b568b55b833645f1cf4155cadf0908d174",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -0,0 +1,44 @@\n+from typing import Optional, Tuple\n+\n+import torch\n+\n+from ..utils import is_torch_greater_or_equal\n+\n+\n+if is_torch_greater_or_equal(\"2.5\"):\n+    from torch.nn.attention.flex_attention import flex_attention\n+\n+\n+def flex_attention_forward(\n+    module: torch.nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    softcap: Optional[float] = None,\n+    **kwargs,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    causal_mask = attention_mask\n+    if causal_mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    def causal_mod(score, b, h, q_idx, kv_idx):\n+        if softcap is not None:\n+            score = softcap * torch.tanh(score / softcap)\n+        if causal_mask is not None:\n+            score += causal_mask[b][0][q_idx][kv_idx]\n+        return score\n+\n+    attn_output, attention_weights = flex_attention(\n+        query,\n+        key,\n+        value,\n+        score_mod=causal_mod,\n+        enable_gqa=True,\n+        scale=scaling,\n+        return_lse=True,\n+    )\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attention_weights"
        },
        {
            "sha": "265260c9b79e4c5c821a00218515b22d2b9694cf",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "added",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -0,0 +1,55 @@\n+from typing import Optional, Tuple\n+\n+import torch\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def sdpa_attention_forward(\n+    module: torch.nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    dropout: float = 0.0,\n+    scaling: Optional[float] = None,\n+    is_causal: Optional[bool] = None,\n+    **kwargs,\n+) -> Tuple[torch.Tensor, None]:\n+    if hasattr(module, \"num_key_value_groups\"):\n+        key = repeat_kv(key, module.num_key_value_groups)\n+        value = repeat_kv(value, module.num_key_value_groups)\n+\n+    causal_mask = attention_mask\n+    if attention_mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    query = query.contiguous()\n+    key = key.contiguous()\n+    value = value.contiguous()\n+\n+    if is_causal is None:\n+        is_causal = causal_mask is None and query.shape[2] > 1\n+\n+    attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        query,\n+        key,\n+        value,\n+        attn_mask=causal_mask,\n+        dropout_p=dropout,\n+        scale=scaling,\n+        is_causal=is_causal,\n+    )\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, None"
        },
        {
            "sha": "6adda0036cc0963d1a25e5774050cd1476a8dcb1",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -247,6 +247,7 @@ def _flash_attention_forward(\n     max_length_q: Optional[int] = None,\n     max_length_k: Optional[int] = None,\n     target_dtype: Optional[torch.dtype] = None,\n+    **kwargs,\n ):\n     \"\"\"\n     Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n@@ -276,7 +277,7 @@ def _flash_attention_forward(\n     if not use_top_left_mask:\n         causal = is_causal\n     else:\n-        # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__.\n+        # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1.\n         causal = is_causal and query_length != 1\n \n     # Assuming 4D tensors, key_states.shape[1] is the key/value sequence length (source length)."
        },
        {
            "sha": "9dcd6d758ecbe78915422008a684b4c4109fc308",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -45,6 +45,9 @@\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig, GenerationMixin\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n+from .integrations.flash_attention import flash_attention_forward\n+from .integrations.flex_attention import flex_attention_forward\n+from .integrations.sdpa_attention import sdpa_attention_forward\n from .loss.loss_utils import LOSS_MAPPING\n from .pytorch_utils import (  # noqa: F401\n     Conv1D,\n@@ -171,10 +174,8 @@ def is_local_dist_rank_0():\n if is_peft_available():\n     from .utils import find_adapter_config_file\n \n-\n SpecificPreTrainedModelType = TypeVar(\"SpecificPreTrainedModelType\", bound=\"PreTrainedModel\")\n \n-\n TORCH_INIT_FUNCTIONS = {\n     \"uniform_\": nn.init.uniform_,\n     \"normal_\": nn.init.normal_,\n@@ -5634,3 +5635,14 @@ def get_disk_only_shard_files(device_map, sharded_metadata, start_prefix):\n         files_content[filename].append(device_map[weight_name])\n \n     return [fname for fname, devices in files_content.items() if set(devices) == {\"disk\"}]\n+\n+\n+ALL_ATTENTION_FUNCTIONS: Dict[str, Dict[str, Callable]] = {}\n+\n+ALL_ATTENTION_FUNCTIONS.update(\n+    {\n+        \"flash_attention_2\": flash_attention_forward,\n+        \"flex_attention\": flex_attention_forward,\n+        \"sdpa\": sdpa_attention_forward,\n+    }\n+)"
        },
        {
            "sha": "6481d6f3c434c7bbb3bdd2a2875e83e6eac42011",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 84,
            "deletions": 354,
            "changes": 438,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -18,24 +18,22 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -478,144 +476,73 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class AriaTextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: AriaTextConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: AriaTextConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class AriaTextFlashAttention2(AriaTextAttention):\n-    \"\"\"\n-    AriaText flash attention module. This module inherits from `AriaTextAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n@@ -625,159 +552,30 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (AriaTextRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class AriaTextSdpaAttention(AriaTextAttention):\n-    \"\"\"\n-    AriaText attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `AriaTextAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from AriaTextAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"AriaTextModel is using AriaTextSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-ARIA_TEXT_ATTENTION_CLASSES = {\n-    \"eager\": AriaTextAttention,\n-    \"flash_attention_2\": AriaTextFlashAttention2,\n-    \"sdpa\": AriaTextSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class AriaTextDecoderLayer(nn.Module):\n@@ -797,7 +595,7 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = ARIA_TEXT_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = AriaTextAttention(config=config, layer_idx=layer_idx)\n         self.mlp = AriaTextMoELayer(config)\n         self.input_layernorm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -812,36 +610,14 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -861,13 +637,9 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -953,40 +725,18 @@ def _init_weights(self, module):\n class AriaTextRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: AriaTextConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[AriaTextConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`AriaTextRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -1136,8 +886,6 @@ def __init__(self, config: AriaTextConfig):\n         self.norm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = AriaTextRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1154,7 +902,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1182,31 +930,22 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n+\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -1215,7 +954,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -1248,9 +986,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1260,18 +995,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,"
        },
        {
            "sha": "36a278263b558aed524cbf1779db2c478f2b3b22",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -197,7 +197,6 @@ class BarkSelfFlashAttention2(BarkSelfAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "4e1f0b389d42eae912049ecbf23857765cfe0e28",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -294,7 +294,6 @@ class BartFlashAttention2(BartAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "11bc411a00c0054065fbade052d2f1b09f8e46bd",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -362,7 +362,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with Llama->Chameleon\n+# NO LONGER EXIST copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with Llama->Chameleon\n # TODO(joao): add me back asap :)\n class ChameleonFlashAttention2(ChameleonAttention):\n     \"\"\""
        },
        {
            "sha": "0bd9c9c0abce2fb552c1e9496bdd8aa836cad3d5",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -401,7 +401,6 @@ class CLIPFlashAttention2(CLIPAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "7b8b9547ac1c334f580c956a463e843040d2c188",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 11,
            "deletions": 27,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -351,7 +351,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with Llama->Cohere\n+# NO LONGER EXIST Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with Llama->Cohere\n+# TODO cyril: modular\n class CohereFlashAttention2(CohereAttention):\n     \"\"\"\n     Cohere flash attention module. This module inherits from `CohereAttention` as the weights of the module stays\n@@ -760,7 +761,8 @@ def _init_weights(self, module):\n     \"The bare Cohere Model outputting raw hidden-states without any specific head on top.\",\n     COHERE_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Cohere, LLAMA->COHERE\n+# copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Cohere, LLAMA->COHERE\n+# TODO cyril: modular\n class CohereModel(CoherePreTrainedModel):\n     \"\"\"\n     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CohereDecoderLayer`]\n@@ -826,31 +828,22 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n+\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -859,7 +852,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -892,9 +884,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -904,18 +893,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,"
        },
        {
            "sha": "1ffa4bffddc3dfa007f85f17d3037973af73e8c1",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -659,11 +659,8 @@ def __init__(self, config: Cohere2Config):\n             [Cohere2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n-\n-        self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n         self.rotary_emb = Cohere2RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "03102d22ca0d773a660c79ce2b32f97f9df98d86",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -489,7 +489,6 @@ class Data2VecAudioFlashAttention2(Data2VecAudioAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "0d2c4297e0d4732ae49770dff2110c72b8a79b87",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -46,7 +46,6 @@\n _CONFIG_FOR_DOC = \"DbrxConfig\"\n \n \n-# Copied from transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding with Gemma->Dbrx\n class DbrxRotaryEmbedding(nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         super().__init__()\n@@ -318,7 +317,6 @@ class DbrxFlashAttention2(DbrxAttention):\n     calls the public API of flash attention.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "60fea55d87be5d9eedf770dffb52d0d7e5ae5a56",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 93,
            "deletions": 71,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -17,15 +17,15 @@\n import math\n import os\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n from ...utils import (\n     ModelOutput,\n@@ -100,6 +100,49 @@ def load_tf_weights_in_gpt2(model, config, gpt2_checkpoint_path):\n     return model\n \n \n+# Copied from transformers.models.gpt2.modeling_gpt2.eager_attention_forward\n+def eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2))\n+\n+    if module.scale_attn_weights:\n+        attn_weights = attn_weights / torch.full(\n+            [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n+        )\n+\n+    # Layer-wise attention scaling\n+    if module.scale_attn_by_inverse_layer_idx:\n+        attn_weights = attn_weights / float(module.layer_idx + 1)\n+\n+    if not module.is_cross_attention:\n+        # if only \"normal\" attention layer implements causal mask\n+        query_length, key_length = query.size(-2), key.size(-2)\n+        causal_mask = module.bias[:, :, key_length - query_length : key_length, :key_length]\n+        mask_value = torch.finfo(attn_weights.dtype).min\n+        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n+        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n+        mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n+        attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n+\n+    if attention_mask is not None:\n+        # Apply the attention mask\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n+    attn_weights = attn_weights.type(value.dtype)\n+    attn_weights = module.attn_dropout(attn_weights)\n+\n+    # Mask heads if we want to\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2)\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.gpt2.modeling_gpt2.GPT2Attention with GPT2->DecisionTransformerGPT2\n class DecisionTransformerGPT2Attention(nn.Module):\n     def __init__(self, config, is_cross_attention=False, layer_idx=None):\n@@ -161,46 +204,6 @@ def prune_heads(self, heads):\n         self.num_heads = self.num_heads - len(heads)\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n-        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n-\n-        if self.scale_attn_weights:\n-            attn_weights = attn_weights / torch.full(\n-                [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n-            )\n-\n-        # Layer-wise attention scaling\n-        if self.scale_attn_by_inverse_layer_idx:\n-            attn_weights = attn_weights / float(self.layer_idx + 1)\n-\n-        if not self.is_cross_attention:\n-            # if only \"normal\" attention layer implements causal mask\n-            query_length, key_length = query.size(-2), key.size(-2)\n-            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n-            mask_value = torch.finfo(attn_weights.dtype).min\n-            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n-            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n-            mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n-            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n-\n-        if attention_mask is not None:\n-            # Apply the attention mask\n-            attn_weights = attn_weights + attention_mask\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n-        attn_weights = attn_weights.type(value.dtype)\n-        attn_weights = self.attn_dropout(attn_weights)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n-        attn_output = torch.matmul(attn_weights, value)\n-\n-        return attn_output, attn_weights\n-\n     def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n         # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n         bsz, num_heads, q_seq_len, dk = query.size()\n@@ -250,25 +253,10 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n             attn_weights = attn_weights * head_mask\n \n         attn_output = torch.matmul(attn_weights, value)\n+        attn_output = attn_output.transpose(1, 2)\n \n         return attn_output, attn_weights\n \n-    def _split_heads(self, tensor, num_heads, attn_head_size):\n-        \"\"\"\n-        Splits hidden_size dim into attn_head_size and num_heads\n-        \"\"\"\n-        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n-        tensor = tensor.view(new_shape)\n-        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n-\n-    def _merge_heads(self, tensor, num_heads, attn_head_size):\n-        \"\"\"\n-        Merges attn_head_size dim and num_attn_heads dim into hidden_size\n-        \"\"\"\n-        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n-        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n-        return tensor.view(new_shape)\n-\n     def forward(\n         self,\n         hidden_states: Optional[Tuple[torch.FloatTensor]],\n@@ -279,6 +267,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n         if encoder_hidden_states is not None:\n             if not hasattr(self, \"q_attn\"):\n@@ -287,32 +276,65 @@ def forward(\n                     \"Please make sure to instantiate class with `DecisionTransformerGPT2Attention(..., is_cross_attention=True)`.\"\n                 )\n \n-            query = self.q_attn(hidden_states)\n-            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n+            query_states = self.q_attn(hidden_states)\n+            key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n             attention_mask = encoder_attention_mask\n         else:\n-            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n+            query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2)\n+\n+        shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n+        shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n \n-        query = self._split_heads(query, self.num_heads, self.head_dim)\n-        key = self._split_heads(key, self.num_heads, self.head_dim)\n-        value = self._split_heads(value, self.num_heads, self.head_dim)\n+        query_states = query_states.reshape(shape_q).transpose(1, 2)\n+        key_states = key_states.reshape(shape_kv).transpose(1, 2)\n+        value_states = value_states.reshape(shape_kv).transpose(1, 2)\n \n         if layer_past is not None:\n             past_key, past_value = layer_past\n-            key = torch.cat((past_key, key), dim=-2)\n-            value = torch.cat((past_value, value), dim=-2)\n+            key_states = torch.cat((past_key, key_states), dim=-2)\n+            value_states = torch.cat((past_value, value_states), dim=-2)\n \n         if use_cache is True:\n-            present = (key, value)\n+            present = (key_states, value_states)\n         else:\n             present = None\n \n-        if self.reorder_and_upcast_attn:\n-            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n+        is_cross_attention = encoder_hidden_states is not None\n+        is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n+\n+        using_eager = self.config._attn_implementation == \"eager\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and (output_attentions or head_mask is not None):\n+                using_eager = True\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                # Attention functions are consistent with previous equivalent attention classes, however they do not support some options\n+                # (e.g. layer scaling, head mask) that eager supports. These implementations are thus equivalent to previous code, but\n+                # not necessarily to eager (if mentionned options are provided).\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        if using_eager and self.reorder_and_upcast_attn:\n+            attn_output, attn_weights = self._upcast_and_reordered_attn(\n+                query_states, key_states, value_states, attention_mask, head_mask\n+            )\n         else:\n-            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n+            attn_output, attn_weights = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask,\n+                head_mask=head_mask,\n+                dropout=self.attn_dropout.p if self.training else 0.0,\n+                is_causal=is_causal,\n+                **kwargs,\n+            )\n \n-        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n+        attn_output = attn_output.reshape(*attn_output.shape[:-2], -1).contiguous()\n         attn_output = self.c_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n "
        },
        {
            "sha": "a826272956e503c664bb1c6524093ccfa342402d",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -245,7 +245,6 @@ class DistilBertFlashAttention2(MultiHeadSelfAttention):\n     API of flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "8d5a224f4f6654555410df1b30aea14a39925afb",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 30,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -113,40 +113,18 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n class FalconRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: FalconConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[FalconConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`FalconRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -492,7 +470,6 @@ class FalconFlashAttention2(FalconAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "e2ea12b03fe4346b782389702794c4a8bba37f9c",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 158,
            "deletions": 391,
            "changes": 549,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -19,8 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n@@ -29,19 +28,21 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -74,85 +75,85 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n+class GemmaMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n class GemmaRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        config: GemmaConfig,\n+        device=None,\n+    ):\n         super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n     @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-class GemmaLinearScalingRotaryEmbedding(GemmaRotaryEmbedding):\n-    \"\"\"GemmaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: a scaling factor is aplied to the position ids\n-        position_ids = position_ids.float() / self.scaling_factor\n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n-\n-\n-class GemmaDynamicNTKScalingRotaryEmbedding(GemmaRotaryEmbedding):\n-    \"\"\"GemmaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (\n-                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n \n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n \n-\n-class GemmaMLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        if config.hidden_activation is None:\n-            logger.warning_once(\n-                \"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\\n\"\n-                \"Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\\n\"\n-                \"`config.hidden_activation` if you want to override this behaviour.\\n\"\n-                \"See https://github.com/huggingface/transformers/pull/29402 for more details.\"\n-            )\n-            config.hidden_activation = \"gelu_pytorch_tanh\"\n-        hidden_activation = config.hidden_activation\n-        self.act_fn = ACT2FN[hidden_activation]\n-\n-    def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n def rotate_half(x):\n@@ -201,315 +202,115 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class GemmaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: GemmaConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = config.head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n-        self.scaling = 1 / math.sqrt(config.head_dim)\n \n-        if self.hidden_size % self.num_heads != 0:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-        self.rotary_emb = GemmaRotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n         )\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class GemmaSdpaAttention(GemmaAttention):\n-    \"\"\"\n-    Gemma attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `GemmaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from GemmaAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-class GemmaFlashAttention2(GemmaAttention):\n-    \"\"\"\n-    Gemma flash attention module. This module inherits from `GemmaAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (GemmaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n-\n-\n-GEMMA_ATTENTION_CLASSES = {\n-    \"eager\": GemmaAttention,\n-    \"flash_attention_2\": GemmaFlashAttention2,\n-    \"sdpa\": GemmaSdpaAttention,\n-}\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n \n \n class GemmaDecoderLayer(nn.Module):\n     def __init__(self, config: GemmaConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-        self.self_attn = GEMMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+\n+        self.self_attn = GemmaAttention(config=config, layer_idx=layer_idx)\n+\n         self.mlp = GemmaMLP(config)\n         self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -523,40 +324,23 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -568,13 +352,9 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -720,10 +500,8 @@ def __init__(self, config: GemmaConfig):\n             [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.rotary_emb = GemmaRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -767,19 +545,8 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False  # noqa: F841\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True  # noqa: F841\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -797,6 +564,9 @@ def forward(\n         # embed positions\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # normalized\n         # Gemma downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n         # See https://github.com/huggingface/transformers/pull/29402\n@@ -806,7 +576,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -822,6 +591,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -832,13 +602,11 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -848,18 +616,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -983,6 +746,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class GemmaForCausalLM(GemmaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -1030,7 +796,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1080,6 +846,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1088,7 +855,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "29b6f8a1946173f51f396757c61bb07c7477c12c",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 34,
            "deletions": 564,
            "changes": 598,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -13,32 +13,24 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n \n import sentencepiece as spm\n import torch\n import torch.utils.checkpoint\n from torch import nn\n \n-from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PretrainedConfig\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n-from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n+from ...modeling_outputs import BaseModelOutputWithPast\n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n from ..llama.modeling_llama import (\n-    LlamaDecoderLayer,\n-    LlamaFlashAttention2,\n     LlamaForCausalLM,\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n+    LlamaMLP,\n     LlamaModel,\n-    LlamaPreTrainedModel,\n-    apply_rotary_pos_emb,\n-    repeat_kv,\n )\n from ..llama.tokenization_llama import LlamaTokenizer\n \n@@ -352,472 +344,15 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(GemmaRMSNorm)\n-\n-\n-class GemmaRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n-        super().__init__()\n-\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-class GemmaLinearScalingRotaryEmbedding(GemmaRotaryEmbedding):\n-    \"\"\"GemmaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: a scaling factor is aplied to the position ids\n-        position_ids = position_ids.float() / self.scaling_factor\n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n-\n-\n-class GemmaDynamicNTKScalingRotaryEmbedding(GemmaRotaryEmbedding):\n-    \"\"\"GemmaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (\n-                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n-\n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n-\n-\n-class GemmaMLP(nn.Module):\n+class GemmaMLP(LlamaMLP):\n     def __init__(self, config):\n         super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        if config.hidden_activation is None:\n-            logger.warning_once(\n-                \"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\\n\"\n-                \"Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\\n\"\n-                \"`config.hidden_activation` if you want to override this behaviour.\\n\"\n-                \"See https://github.com/huggingface/transformers/pull/29402 for more details.\"\n-            )\n-            config.hidden_activation = \"gelu_pytorch_tanh\"\n-        hidden_activation = config.hidden_activation\n-        self.act_fn = ACT2FN[hidden_activation]\n-\n-    def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n-\n-class GemmaAttention(nn.Module):\n-    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n-\n-    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n-        super().__init__()\n-        self.config = config\n-        self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n-        self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = config.head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-        self.is_causal = True\n-        self.scaling = 1 / math.sqrt(config.head_dim)\n-\n-        if self.hidden_size % self.num_heads != 0:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-        self.rotary_emb = GemmaRotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class GemmaSdpaAttention(GemmaAttention):\n-    \"\"\"\n-    Gemma attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `GemmaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from GemmaAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-class GemmaFlashAttention2(LlamaFlashAttention2, GemmaAttention):\n-    \"\"\"\n-    Gemma flash attention module. This module inherits from `GemmaAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (GemmaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-GEMMA_ATTENTION_CLASSES = {\n-    \"eager\": GemmaAttention,\n-    \"flash_attention_2\": GemmaFlashAttention2,\n-    \"sdpa\": GemmaSdpaAttention,\n-}\n-\n-\n-class GemmaDecoderLayer(LlamaDecoderLayer):\n-    def __init__(self, config: GemmaConfig, layer_idx: int):\n-        super().__init__(config)\n-        self.self_attn = GEMMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n-        self.mlp = GemmaMLP(config)\n-        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-        residual = hidden_states\n-\n-        hidden_states = self.input_layernorm(hidden_states)\n-\n-        # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n-            hidden_states=hidden_states,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_value=past_key_value,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n-            cache_position=cache_position,\n-            **kwargs,\n-        )\n-        hidden_states = residual + hidden_states\n-\n-        # Fully Connected\n-        residual = hidden_states\n-        hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states = self.mlp(hidden_states)\n-        hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n-        return outputs\n-\n-\n-class GemmaPreTrainedModel(LlamaPreTrainedModel):\n-    pass\n \n \n class GemmaModel(LlamaModel):\n-    def __init__(self, config: GemmaConfig):\n-        super().__init__(config)\n-        self.layers = nn.ModuleList(\n-            [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n-        )\n-        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        del self.rotary_emb  # Gemma does not implement rotary emb at the modeling level yet!\n-        self.post_init()\n-\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -850,19 +385,8 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False  # noqa: F841\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True  # noqa: F841\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -880,6 +404,9 @@ def forward(\n         # embed positions\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # normalized\n         # Gemma downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n         # See https://github.com/huggingface/transformers/pull/29402\n@@ -889,7 +416,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -905,6 +431,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -915,13 +442,11 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -931,44 +456,33 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n \n-# Example where we ony modify the docstring and call super\n class GemmaForCausalLM(LlamaForCausalLM):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.model = GemmaModel(config)\n-        self.post_init()\n-\n-    def forward(\n-        self,\n-        input_ids: torch.LongTensor = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    def forward(**super_kwargs):\n         r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n         ```python\n         >>> from transformers import AutoTokenizer, GemmaForCausalLM\n \n@@ -983,59 +497,15 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            cache_position=cache_position,\n-        )\n-\n-        hidden_states = outputs[0]\n-        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n-\n-        return CausalLMOutputWithPast(\n-            loss=loss,\n-            logits=logits,\n-            past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+        return super().forward(**super_kwargs)\n \n \n class GemmaForSequenceClassification(LlamaForSequenceClassification):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.model = GemmaModel(config)\n-        self.post_init()\n+    pass\n \n \n class GemmaForTokenClassification(LlamaForTokenClassification):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.model = GemmaModel(config)\n-        self.post_init()\n+    pass\n \n \n __all__ = [\n@@ -1045,5 +515,5 @@ def __init__(self, config):\n     \"GemmaForCausalLM\",\n     \"GemmaForSequenceClassification\",\n     \"GemmaForTokenClassification\",\n-    \"GemmaPreTrainedModel\",\n+    \"GemmaPreTrainedModel\",  # noqa: F822\n ]"
        },
        {
            "sha": "67fc6c86a3bac64bf2ba95f28611ae0ac6462eae",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 153,
            "deletions": 284,
            "changes": 437,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -19,40 +19,34 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal,\n-    is_torch_greater_or_equal,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_gemma2 import Gemma2Config\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-if is_torch_greater_or_equal(\"2.5\"):\n-    from torch.nn.attention.flex_attention import flex_attention\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -92,35 +86,8 @@ def __init__(self, config):\n         self.act_fn = ACT2FN[config.hidden_activation]\n \n     def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n-\n-class Gemma2RotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n-        super().__init__()\n-\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n def rotate_half(x):\n@@ -170,266 +137,118 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n \n \n def eager_attention_forward(\n-    config: Gemma2Config,\n+    module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    **_kwargs,\n+    attention_mask: Optional[torch.Tensor],\n+    dropout: float = 0.0,\n+    scaling: Optional[float] = None,\n+    softcap: Optional[float] = None,\n+    **kwargs,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    key_states = repeat_kv(key, config.num_key_value_groups)\n-    value_states = repeat_kv(value, config.num_key_value_groups)\n+    if scaling is None:\n+        scaling = module.head_dim**-0.5\n \n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * config.scaling\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n \n-    if config.attn_logit_softcapping is not None:\n-        attn_weights = attn_weights / config.attn_logit_softcapping\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+\n+    if softcap is not None:\n+        attn_weights = attn_weights / softcap\n         attn_weights = torch.tanh(attn_weights)\n-        attn_weights = attn_weights * config.attn_logit_softcapping\n-    if mask is not None:  # no matter the length, we just slice it\n-        causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights * softcap\n+    if attention_mask is not None:  # no matter the length, we just slice it\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n         attn_weights = attn_weights + causal_mask\n \n     # upcast attention to fp32\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_weights = nn.functional.dropout(attn_weights, p=config.attention_dropout, training=config.training)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value_states)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, attn_weights\n \n \n-def flash_attention_forward(\n-    config: Gemma2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    target_dtype: torch.dtype = torch.float16,\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, None]:\n-    if mask is not None:\n-        seq_len = mask.shape[1]\n-        query = query[:, :, :seq_len]\n-        value = value[:, :, :seq_len]\n-\n-    # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n-    # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor rotary embedding\n-    query_states = query.transpose(1, 2)\n-    key_states = key.transpose(1, 2)\n-    value_states = value.transpose(1, 2)\n-\n-    dropout_rate = config.attention_dropout if config.training else 0.0\n-\n-    input_dtype = query_states.dtype\n-    if input_dtype == torch.float32:\n-        query_states = query_states.to(target_dtype)\n-        key_states = key_states.to(target_dtype)\n-        value_states = value_states.to(target_dtype)\n-\n-    attn_output = _flash_attention_forward(\n-        query_states,\n-        key_states,\n-        value_states,\n-        mask,\n-        seq_len,\n-        dropout=dropout_rate,\n-        softmax_scale=config.scaling,\n-        is_causal=config.is_causal,\n-        sliding_window=config.sliding_window,\n-        use_top_left_mask=config._flash_attn_uses_top_left_mask,\n-        softcap=config.attn_logit_softcapping if is_flash_attn_greater_or_equal(\"2.6.0\") else None,\n-    )\n-\n-    return attn_output, None\n-\n-\n-def flex_attention_forward(\n-    config: Gemma2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    output_attentions: bool = False,\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n-    def tanh_softcap(score, b, h, q_idx, kv_idx):\n-        soft_cap = config.attn_logit_softcapping\n-        score = soft_cap * torch.tanh(score / soft_cap)\n-        if mask is not None:\n-            return score + mask[b][0][q_idx][kv_idx]\n-        return score\n-\n-    attn_output = flex_attention(\n-        query,\n-        key,\n-        value,\n-        score_mod=tanh_softcap,\n-        enable_gqa=True,\n-        scale=config.scaling,\n-        return_lse=output_attentions,\n-    )\n-    if not output_attentions:\n-        attn_weights = None\n-    else:\n-        attn_output, attn_weights = attn_output\n-\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-    return attn_output, attn_weights\n-\n-\n-def sdpa_attention_forward(\n-    config: Gemma2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, None]:\n-    key = repeat_kv(key, config.num_key_value_groups)\n-    value = repeat_kv(value, config.num_key_value_groups)\n-\n-    causal_mask = mask\n-    if mask is not None:\n-        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n-\n-    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-    if query.device.type == \"cuda\" and causal_mask is not None:\n-        query = query.contiguous()\n-        key = key.contiguous()\n-        value = value.contiguous()\n-\n-    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-    is_causal = True if causal_mask is None and query.shape[1] > 1 else False\n-\n-    attn_output = torch.nn.functional.scaled_dot_product_attention(\n-        query,\n-        key,\n-        value,\n-        attn_mask=causal_mask,\n-        dropout_p=config.attention_dropout if config.training else 0.0,\n-        is_causal=is_causal,\n-        scale=config.scaling,\n-    )\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-    return attn_output, None\n-\n-\n-GEMMA2_ATTENTION_FUNCTION = {\n-    \"flash_attention_2\": flash_attention_forward,\n-    \"flex_attention\": flex_attention_forward,\n-    \"eager\": eager_attention_forward,\n-    \"sdpa\": sdpa_attention_forward,\n-}\n-\n-\n class Gemma2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n+    def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-\n-        self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = config.head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-        self.is_causal = True\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = config.query_pre_attn_scalar**-0.5\n-        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n-        self.attn_logit_softcapping = config.attn_logit_softcapping\n-        if self.hidden_size % self.num_heads != 0:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n+        self.attention_dropout = self.config.attention_dropout\n+        self.is_causal = True\n \n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-        self.rotary_emb = Gemma2RotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.attn_logit_softcapping = self.config.attn_logit_softcapping\n+        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"sliding_window\": self.sliding_window,\n-                \"cache_position\": cache_position,\n-            }\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n-            logger.warning_once(\"Setting `attention_type` to `flex_attention` because `output_attentions=True`\")\n-            attention_type = \"flex_attention\"\n-        else:\n-            attention_type = self.config._attn_implementation\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output, attn_weights = GEMMA2_ATTENTION_FUNCTION[attention_type](\n-            self, query_states, key_states, value_states, attention_mask, output_attentions=output_attentions\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=self.attention_dropout if self.training else 0.0,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            softcap=self.attn_logit_softcapping,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Gemma2FlashAttention2(Gemma2Attention):\n-    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n-        super().__init__(config, layer_idx)\n-        self.config._attn_implementation = \"flash_attention_2\"\n-        logger.warning_once(\n-            \"The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n-            \"attribute of the `GemmaAttention` class! It will be removed in v4.48\"\n-        )\n-\n-\n-class Gemma2SdpaAttention(Gemma2Attention):\n-    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n-        super().__init__(config, layer_idx)\n-        self.config._attn_implementation = \"sdpa\"\n-        logger.warning_once(\n-            \"The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n-            \"attribute of the `GemmaAttention` class! It will be removed in v4.48\"\n-        )\n+        return attn_output, attn_weights\n \n \n class Gemma2DecoderLayer(nn.Module):\n@@ -450,6 +269,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -476,8 +296,9 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n@@ -499,12 +320,74 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n+class Gemma2RotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: Gemma2Config,\n+        device=None,\n+    ):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n GEMMA2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -535,7 +418,7 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n-    _supports_quantized_cache = False\n+    _supports_quantized_cache = True\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -549,20 +432,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    @classmethod\n-    def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n-        \"\"\"\n-        Overloads `PreTrainedModel._check_and_enable_sdpa` so as to DISABLE torch SDPA by default on Gemma2 models.\n-        SDPA reduces the model performance on Gemma2 because of the logits softcapping.\n-        \"\"\"\n-        config = super()._check_and_enable_sdpa(config, hard_check_only=hard_check_only)\n-\n-        # if using the default path -> swap sdpa by eager\n-        if not hard_check_only and config._attn_implementation == \"sdpa\":\n-            config._attn_implementation = \"eager\"\n-\n-        return config\n-\n \n GEMMA2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n@@ -661,10 +530,8 @@ def __init__(self, config: Gemma2Config):\n             [Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.rotary_emb = Gemma2RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -734,6 +601,9 @@ def forward(\n         # embed positions\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # normalized\n         # Gemma2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n         # See https://github.com/huggingface/transformers/pull/29402\n@@ -752,6 +622,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n+                    position_embeddings,\n                     causal_mask,\n                     position_ids,\n                     past_key_values,\n@@ -762,6 +633,7 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n+                    position_embeddings=position_embeddings,\n                     attention_mask=causal_mask,\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n@@ -780,16 +652,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = past_key_values if use_cache else None\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     @torch.no_grad()\n     def _update_causal_mask("
        },
        {
            "sha": "48b12411361aff650f8c99e6451ba6b7390f9261",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 76,
            "deletions": 283,
            "changes": 359,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -13,7 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -22,36 +22,27 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache\n from ...configuration_utils import PretrainedConfig\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n )\n-from ...utils import (\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal,\n-    is_torch_greater_or_equal,\n-    logging,\n-)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import logging\n from ..gemma.modeling_gemma import (\n+    GemmaAttention,\n     GemmaForCausalLM,\n     GemmaForSequenceClassification,\n     GemmaForTokenClassification,\n+    GemmaMLP,\n     GemmaModel,\n-    GemmaPreTrainedModel,\n     GemmaRMSNorm,\n-    GemmaRotaryEmbedding,\n     apply_rotary_pos_emb,\n     repeat_kv,\n )\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-if is_torch_greater_or_equal(\"2.5\"):\n-    from torch.nn.attention.flex_attention import flex_attention\n-\n-\n _CHECKPOINT_FOR_DOC = \"google/gemma2-7b\"\n \n logger = logging.get_logger(__name__)\n@@ -194,286 +185,106 @@ class Gemma2RMSNorm(GemmaRMSNorm):\n     pass\n \n \n-class Gemma2MLP(nn.Module):\n+class Gemma2MLP(GemmaMLP):\n     def __init__(self, config):\n         super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n         self.act_fn = ACT2FN[config.hidden_activation]\n \n-    def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n-\n-class Gemma2RotaryEmbedding(GemmaRotaryEmbedding):\n-    pass\n-\n \n def eager_attention_forward(\n-    config: Gemma2Config,\n+    module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    **_kwargs,\n+    attention_mask: Optional[torch.Tensor],\n+    dropout: float = 0.0,\n+    scaling: Optional[float] = None,\n+    softcap: Optional[float] = None,\n+    **kwargs,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    key_states = repeat_kv(key, config.num_key_value_groups)\n-    value_states = repeat_kv(value, config.num_key_value_groups)\n+    if scaling is None:\n+        scaling = module.head_dim**-0.5\n+\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n \n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * config.scaling\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n \n-    if config.attn_logit_softcapping is not None:\n-        attn_weights = attn_weights / config.attn_logit_softcapping\n+    if softcap is not None:\n+        attn_weights = attn_weights / softcap\n         attn_weights = torch.tanh(attn_weights)\n-        attn_weights = attn_weights * config.attn_logit_softcapping\n-    if mask is not None:  # no matter the length, we just slice it\n-        causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights * softcap\n+    if attention_mask is not None:  # no matter the length, we just slice it\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n         attn_weights = attn_weights + causal_mask\n \n     # upcast attention to fp32\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_weights = nn.functional.dropout(attn_weights, p=config.attention_dropout, training=config.training)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value_states)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, attn_weights\n \n \n-def flash_attention_forward(\n-    config: Gemma2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    target_dtype: torch.dtype = torch.float16,\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, None]:\n-    if mask is not None:\n-        seq_len = mask.shape[1]\n-        query = query[:, :, :seq_len]\n-        value = value[:, :, :seq_len]\n-\n-    # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n-    # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor rotary embedding\n-    query_states = query.transpose(1, 2)\n-    key_states = key.transpose(1, 2)\n-    value_states = value.transpose(1, 2)\n-\n-    dropout_rate = config.attention_dropout if config.training else 0.0\n-\n-    input_dtype = query_states.dtype\n-    if input_dtype == torch.float32:\n-        query_states = query_states.to(target_dtype)\n-        key_states = key_states.to(target_dtype)\n-        value_states = value_states.to(target_dtype)\n-\n-    attn_output = _flash_attention_forward(\n-        query_states,\n-        key_states,\n-        value_states,\n-        mask,\n-        seq_len,\n-        dropout=dropout_rate,\n-        softmax_scale=config.scaling,\n-        is_causal=config.is_causal,\n-        sliding_window=config.sliding_window,\n-        use_top_left_mask=config._flash_attn_uses_top_left_mask,\n-        softcap=config.attn_logit_softcapping if is_flash_attn_greater_or_equal(\"2.6.0\") else None,\n-    )\n-\n-    return attn_output, None\n-\n-\n-def flex_attention_forward(\n-    config: Gemma2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    output_attentions: bool = False,\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n-    def tanh_softcap(score, b, h, q_idx, kv_idx):\n-        soft_cap = config.attn_logit_softcapping\n-        score = soft_cap * torch.tanh(score / soft_cap)\n-        if mask is not None:\n-            return score + mask[b][0][q_idx][kv_idx]\n-        return score\n-\n-    attn_output = flex_attention(\n-        query,\n-        key,\n-        value,\n-        score_mod=tanh_softcap,\n-        enable_gqa=True,\n-        scale=config.scaling,\n-        return_lse=output_attentions,\n-    )\n-    if not output_attentions:\n-        attn_weights = None\n-    else:\n-        attn_output, attn_weights = attn_output\n-\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-    return attn_output, attn_weights\n-\n-\n-def sdpa_attention_forward(\n-    config: Gemma2Config,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    mask: Optional[torch.Tensor],\n-    **_kwargs,\n-) -> Tuple[torch.Tensor, None]:\n-    key = repeat_kv(key, config.num_key_value_groups)\n-    value = repeat_kv(value, config.num_key_value_groups)\n-\n-    causal_mask = mask\n-    if mask is not None:\n-        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n-\n-    # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-    # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-    if query.device.type == \"cuda\" and causal_mask is not None:\n-        query = query.contiguous()\n-        key = key.contiguous()\n-        value = value.contiguous()\n-\n-    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-    is_causal = True if causal_mask is None and query.shape[1] > 1 else False\n-\n-    attn_output = torch.nn.functional.scaled_dot_product_attention(\n-        query,\n-        key,\n-        value,\n-        attn_mask=causal_mask,\n-        dropout_p=config.attention_dropout if config.training else 0.0,\n-        is_causal=is_causal,\n-        scale=config.scaling,\n-    )\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-    return attn_output, None\n-\n-\n-GEMMA2_ATTENTION_FUNCTION = {\n-    \"flash_attention_2\": flash_attention_forward,\n-    \"flex_attention\": flex_attention_forward,\n-    \"eager\": eager_attention_forward,\n-    \"sdpa\": sdpa_attention_forward,\n-}\n-\n-\n-class Gemma2Attention(nn.Module):\n-    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n-\n-    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n-        super().__init__()\n-        self.config = config\n-        self.layer_idx = layer_idx\n-\n-        self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = config.head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n+class Gemma2Attention(GemmaAttention):\n+    def __init__(self, config: Gemma2Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.attn_logit_softcapping = self.config.attn_logit_softcapping\n+        self.attention_dropout = self.config.attention_dropout\n         self.is_causal = True\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n-        self.attn_logit_softcapping = config.attn_logit_softcapping\n-        if self.hidden_size % self.num_heads != 0:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-        self.rotary_emb = Gemma2RotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"sliding_window\": self.sliding_window,\n-                \"cache_position\": cache_position,\n-            }\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n-            logger.warning_once(\"Setting `attention_type` to `flex_attention` because `output_attentions=True`\")\n-            attention_type = \"flex_attention\"\n-        else:\n-            attention_type = self.config._attn_implementation\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output, attn_weights = GEMMA2_ATTENTION_FUNCTION[attention_type](\n-            self, query_states, key_states, value_states, attention_mask, output_attentions=output_attentions\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=self.attention_dropout if self.training else 0.0,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            softcap=self.attn_logit_softcapping,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Gemma2FlashAttention2(Gemma2Attention):\n-    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n-        super().__init__(config, layer_idx)\n-        self.config._attn_implementation = \"flash_attention_2\"\n-        logger.warning_once(\n-            \"The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n-            \"attribute of the `GemmaAttention` class! It will be removed in v4.48\"\n-        )\n-\n-\n-class Gemma2SdpaAttention(Gemma2Attention):\n-    def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n-        super().__init__(config, layer_idx)\n-        self.config._attn_implementation = \"sdpa\"\n-        logger.warning_once(\n-            \"The `Gemma2FlashAttention2` class is deprecated in favor of simply modifying the `config._attn_implementation`\"\n-            \"attribute of the `GemmaAttention` class! It will be removed in v4.48\"\n-        )\n+        return attn_output, attn_weights\n \n \n class Gemma2DecoderLayer(nn.Module):\n@@ -494,6 +305,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[Cache] = None,\n@@ -520,8 +332,9 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n@@ -543,37 +356,15 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n-class Gemma2PreTrainedModel(GemmaPreTrainedModel):\n-    _supports_quantized_cache = False\n-\n-    @classmethod\n-    def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n-        \"\"\"\n-        Overloads `PreTrainedModel._check_and_enable_sdpa` so as to DISABLE torch SDPA by default on Gemma2 models.\n-        SDPA reduces the model performance on Gemma2 because of the logits softcapping.\n-        \"\"\"\n-        config = super()._check_and_enable_sdpa(config, hard_check_only=hard_check_only)\n-\n-        # if using the default path -> swap sdpa by eager\n-        if not hard_check_only and config._attn_implementation == \"sdpa\":\n-            config._attn_implementation = \"eager\"\n-\n-        return config\n-\n-\n-class Gemma2Model(GemmaModel, Gemma2PreTrainedModel):\n+class Gemma2Model(GemmaModel):\n     def __init__(self, config: Gemma2Config):\n         super().__init__(config)\n         self.layers = nn.ModuleList(\n             [Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.post_init()\n \n     def forward(\n         self,\n@@ -633,6 +424,9 @@ def forward(\n         # embed positions\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # normalized\n         # Gemma2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n         # See https://github.com/huggingface/transformers/pull/29402\n@@ -651,6 +445,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n+                    position_embeddings,\n                     causal_mask,\n                     position_ids,\n                     past_key_values,\n@@ -661,6 +456,7 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n+                    position_embeddings=position_embeddings,\n                     attention_mask=causal_mask,\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n@@ -679,16 +475,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = past_key_values if use_cache else None\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     @torch.no_grad()\n     def _update_causal_mask(\n@@ -909,7 +702,7 @@ def __init__(self, config):\n     \"Gemma2Config\",\n     \"Gemma2ForCausalLM\",\n     \"Gemma2Model\",\n-    \"Gemma2PreTrainedModel\",\n+    \"Gemma2PreTrainedModel\",  # noqa: F822\n     \"Gemma2ForSequenceClassification\",\n     \"Gemma2ForTokenClassification\",\n ]"
        },
        {
            "sha": "95ad0d9719951dfe953fb1a05b058e6f3ea0b15e",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 164,
            "deletions": 373,
            "changes": 537,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -19,8 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -29,20 +28,21 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -55,55 +55,6 @@\n _CONFIG_FOR_DOC = \"GlmConfig\"\n \n \n-class GlmRMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        GlmRMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n-class GlmRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n-        super().__init__()\n-\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n class GlmMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -135,6 +86,32 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., 0::2]\n@@ -191,134 +168,38 @@ def __init__(self, config: GlmConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.is_causal = True\n-        self.scaling = 1 / math.sqrt(self.head_dim)\n-\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class GlmFlashAttention2(GlmAttention):\n-    \"\"\"\n-    Glm flash attention module. This module inherits from `GlmAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n@@ -328,167 +209,123 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (GlmRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            softmax_scale=self.scaling,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class GlmSdpaAttention(GlmAttention):\n-    \"\"\"\n-    Glm attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `GlmAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from GlmAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"GlmModel is using GlmSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+class GlmRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        GlmRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n \n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n \n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n \n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+class GlmRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: GlmConfig,\n+        device=None,\n+    ):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-            scale=self.scaling,\n-        )\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n-        attn_output = self.o_proj(attn_output)\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n \n-        return attn_output, None, past_key_value\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n \n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n \n-GLM_ATTENTION_CLASSES = {\n-    \"eager\": GlmAttention,\n-    \"flash_attention_2\": GlmFlashAttention2,\n-    \"sdpa\": GlmSdpaAttention,\n-}\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n class GlmDecoderLayer(nn.Module):\n-    def __init__(self, config: GlmConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: GlmConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = GLM_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = GlmAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = GlmMLP(config)\n         self.input_layernorm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -504,36 +341,14 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -553,13 +368,9 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -705,14 +516,8 @@ def __init__(self, config: GlmConfig):\n             [GlmDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = GlmRotaryEmbedding(\n-            dim=int(config.head_dim * config.partial_rotary_factor),\n-            max_position_embeddings=config.max_position_embeddings,\n-            base=config.rope_theta,\n-        )\n+        self.rotary_emb = GlmRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -729,7 +534,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -757,31 +562,22 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n+\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -790,7 +586,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -823,9 +618,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -835,18 +627,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -970,11 +757,14 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class GlmForCausalLM(GlmPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n-    def __init__(self, config: GlmConfig):\n+    def __init__(self, config):\n         super().__init__(config)\n         self.model = GlmModel(config)\n         self.vocab_size = config.vocab_size\n@@ -1017,7 +807,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1038,16 +828,16 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, GlmForCausalLM\n \n-        >>> model = GlmForCausalLM.from_pretrained(\"google/glm-7b\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/glm-7b\")\n+        >>> model = GlmForCausalLM.from_pretrained(\"meta-glm/Glm-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-glm/Glm-2-7b-hf\")\n \n-        >>> prompt = \"What is your favorite condiment?\"\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"What is your favorite condiment?\"\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1067,6 +857,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1075,7 +866,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1106,7 +897,7 @@ def forward(\n     GLM_START_DOCSTRING,\n )\n class GlmForSequenceClassification(GlmPreTrainedModel):\n-    def __init__(self, config: GlmConfig):\n+    def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n         self.model = GlmModel(config)\n@@ -1202,7 +993,7 @@ def forward(\n     GLM_START_DOCSTRING,\n )\n class GlmForTokenClassification(GlmPreTrainedModel):\n-    def __init__(self, config: GlmConfig):\n+    def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n         self.model = GlmModel(config)"
        },
        {
            "sha": "ec07be10fb6a554ba2460e73054f891eaf59725d",
            "filename": "src/transformers/models/glm/modular_glm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 90,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -13,34 +13,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n from typing import Optional\n \n import torch\n import torch.nn as nn\n import torch.utils.checkpoint\n \n from ...utils import logging\n-from ..gemma.modeling_gemma import (\n-    GemmaForCausalLM,\n-    GemmaForSequenceClassification,\n-    GemmaForTokenClassification,\n-)\n-from ..granite.modeling_granite import (\n-    GraniteAttention,\n-    GraniteFlashAttention2,\n-    GraniteSdpaAttention,\n-)\n from ..llama.modeling_llama import (\n-    LlamaDecoderLayer,\n-    LlamaModel,\n-    LlamaPreTrainedModel,\n-)\n-from ..phi3.modeling_phi3 import (\n-    Phi3MLP,\n-    Phi3RMSNorm,\n-    Phi3RotaryEmbedding,\n+    LlamaAttention,\n+    LlamaForCausalLM,\n+    LlamaForSequenceClassification,\n+    LlamaForTokenClassification,\n )\n+from ..phi3.modeling_phi3 import Phi3MLP\n from .configuration_glm import GlmConfig\n \n \n@@ -49,14 +35,6 @@\n _CHECKPOINT_FOR_DOC = \"THUDM/glm-4-9b\"\n \n \n-class GlmRMSNorm(Phi3RMSNorm):\n-    pass\n-\n-\n-class GlmRotaryEmbedding(Phi3RotaryEmbedding):\n-    pass\n-\n-\n class GlmMLP(Phi3MLP):\n     pass\n \n@@ -110,83 +88,27 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-class GlmAttention(GraniteAttention):\n+class GlmAttention(LlamaAttention):\n     def __init__(self, config: GlmConfig, layer_idx: Optional[int] = None):\n         super().__init__(config, layer_idx)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n-        self.scaling = 1 / math.sqrt(self.head_dim)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n \n-class GlmFlashAttention2(GlmAttention, GraniteFlashAttention2):\n+class GlmForCausalLM(LlamaForCausalLM):\n     pass\n \n \n-class GlmSdpaAttention(GraniteSdpaAttention):\n+class GlmForSequenceClassification(LlamaForSequenceClassification):\n     pass\n \n \n-GLM_ATTENTION_CLASSES = {\n-    \"eager\": GlmAttention,\n-    \"flash_attention_2\": GlmFlashAttention2,\n-    \"sdpa\": GlmSdpaAttention,\n-}\n-\n-\n-class GlmDecoderLayer(LlamaDecoderLayer):\n-    def __init__(self, config: GlmConfig, layer_idx: Optional[int] = None):\n-        super().__init__()\n-\n-        self.mlp = GlmMLP(config)\n-        self.input_layernorm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_attention_layernorm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n-\n-class GlmPreTrainedModel(LlamaPreTrainedModel):\n+class GlmForTokenClassification(LlamaForTokenClassification):\n     pass\n \n \n-class GlmModel(GlmPreTrainedModel, LlamaModel):\n-    def __init__(self, config: GlmConfig):\n-        super().__init__(config)\n-        self.layers = nn.ModuleList(\n-            [GlmDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n-        )\n-        self.norm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = GlmRotaryEmbedding(\n-            dim=int(config.head_dim * config.partial_rotary_factor),\n-            max_position_embeddings=config.max_position_embeddings,\n-            base=config.rope_theta,\n-        )\n-        self.gradient_checkpointing = False\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-\n-class GlmForCausalLM(GemmaForCausalLM):\n-    def __init__(self, config: GlmConfig):\n-        super().__init__(config)\n-        self.model = GlmModel(config)\n-        self.post_init()\n-\n-\n-class GlmForSequenceClassification(GemmaForSequenceClassification):\n-    def __init__(self, config: GlmConfig):\n-        super().__init__(config)\n-        self.model = GlmModel(config)\n-        self.post_init()\n-\n-\n-class GlmForTokenClassification(GemmaForTokenClassification):\n-    def __init__(self, config: GlmConfig):\n-        super().__init__(config)\n-        self.model = GlmModel(config)\n-        self.post_init()\n-\n-\n __all__ = [\n-    \"GlmPreTrainedModel\",\n-    \"GlmModel\",\n+    \"GlmPreTrainedModel\",  # noqa: F822\n+    \"GlmModel\",  # noqa: F822\n     \"GlmForCausalLM\",\n     \"GlmForSequenceClassification\",\n     \"GlmForTokenClassification\","
        },
        {
            "sha": "ad53c7804ebeea61bc1878458123a5c7564b62af",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 94,
            "deletions": 305,
            "changes": 399,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -19,11 +19,10 @@\n import os\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n-from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -37,27 +36,20 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel, SequenceSummary\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel, SequenceSummary\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n from ...utils import (\n     ModelOutput,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    get_torch_version,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_gpt2 import GPT2Config\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"openai-community/gpt2\"\n@@ -120,6 +112,48 @@ def load_tf_weights_in_gpt2(model, config, gpt2_checkpoint_path):\n     return model\n \n \n+def eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2))\n+\n+    if module.scale_attn_weights:\n+        attn_weights = attn_weights / torch.full(\n+            [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n+        )\n+\n+    # Layer-wise attention scaling\n+    if module.scale_attn_by_inverse_layer_idx:\n+        attn_weights = attn_weights / float(module.layer_idx + 1)\n+\n+    if not module.is_cross_attention:\n+        # if only \"normal\" attention layer implements causal mask\n+        query_length, key_length = query.size(-2), key.size(-2)\n+        causal_mask = module.bias[:, :, key_length - query_length : key_length, :key_length]\n+        mask_value = torch.finfo(attn_weights.dtype).min\n+        # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n+        # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n+        mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n+        attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n+\n+    if attention_mask is not None:\n+        # Apply the attention mask\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n+    attn_weights = attn_weights.type(value.dtype)\n+    attn_weights = module.attn_dropout(attn_weights)\n+\n+    # Mask heads if we want to\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2)\n+\n+    return attn_output, attn_weights\n+\n+\n class GPT2Attention(nn.Module):\n     def __init__(self, config, is_cross_attention=False, layer_idx=None):\n         super().__init__()\n@@ -180,46 +214,6 @@ def prune_heads(self, heads):\n         self.num_heads = self.num_heads - len(heads)\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n-        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n-\n-        if self.scale_attn_weights:\n-            attn_weights = attn_weights / torch.full(\n-                [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n-            )\n-\n-        # Layer-wise attention scaling\n-        if self.scale_attn_by_inverse_layer_idx:\n-            attn_weights = attn_weights / float(self.layer_idx + 1)\n-\n-        if not self.is_cross_attention:\n-            # if only \"normal\" attention layer implements causal mask\n-            query_length, key_length = query.size(-2), key.size(-2)\n-            causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n-            mask_value = torch.finfo(attn_weights.dtype).min\n-            # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n-            # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n-            mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n-            attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n-\n-        if attention_mask is not None:\n-            # Apply the attention mask\n-            attn_weights = attn_weights + attention_mask\n-\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n-\n-        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n-        attn_weights = attn_weights.type(value.dtype)\n-        attn_weights = self.attn_dropout(attn_weights)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attn_weights = attn_weights * head_mask\n-\n-        attn_output = torch.matmul(attn_weights, value)\n-\n-        return attn_output, attn_weights\n-\n     def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, head_mask=None):\n         # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n         bsz, num_heads, q_seq_len, dk = query.size()\n@@ -269,25 +263,10 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n             attn_weights = attn_weights * head_mask\n \n         attn_output = torch.matmul(attn_weights, value)\n+        attn_output = attn_output.transpose(1, 2)\n \n         return attn_output, attn_weights\n \n-    def _split_heads(self, tensor, num_heads, attn_head_size):\n-        \"\"\"\n-        Splits hidden_size dim into attn_head_size and num_heads\n-        \"\"\"\n-        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)\n-        tensor = tensor.view(new_shape)\n-        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n-\n-    def _merge_heads(self, tensor, num_heads, attn_head_size):\n-        \"\"\"\n-        Merges attn_head_size dim and num_attn_heads dim into hidden_size\n-        \"\"\"\n-        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n-        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)\n-        return tensor.view(new_shape)\n-\n     def forward(\n         self,\n         hidden_states: Optional[Tuple[torch.FloatTensor]],\n@@ -298,6 +277,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n         if encoder_hidden_states is not None:\n             if not hasattr(self, \"q_attn\"):\n@@ -306,260 +286,73 @@ def forward(\n                     \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                 )\n \n-            query = self.q_attn(hidden_states)\n-            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n+            query_states = self.q_attn(hidden_states)\n+            key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n             attention_mask = encoder_attention_mask\n         else:\n-            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n+            query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2)\n+\n+        shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n+        shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n \n-        query = self._split_heads(query, self.num_heads, self.head_dim)\n-        key = self._split_heads(key, self.num_heads, self.head_dim)\n-        value = self._split_heads(value, self.num_heads, self.head_dim)\n+        query_states = query_states.reshape(shape_q).transpose(1, 2)\n+        key_states = key_states.reshape(shape_kv).transpose(1, 2)\n+        value_states = value_states.reshape(shape_kv).transpose(1, 2)\n \n         if layer_past is not None:\n             past_key, past_value = layer_past\n-            key = torch.cat((past_key, key), dim=-2)\n-            value = torch.cat((past_value, value), dim=-2)\n+            key_states = torch.cat((past_key, key_states), dim=-2)\n+            value_states = torch.cat((past_value, value_states), dim=-2)\n \n         if use_cache is True:\n-            present = (key, value)\n+            present = (key_states, value_states)\n         else:\n             present = None\n \n-        if self.reorder_and_upcast_attn:\n-            attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n-        else:\n-            attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n-\n-        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n-        attn_output = self.c_proj(attn_output)\n-        attn_output = self.resid_dropout(attn_output)\n-\n-        outputs = (attn_output, present)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs  # a, present, (attentions)\n-\n-\n-class GPT2FlashAttention2(GPT2Attention):\n-    \"\"\"\n-    GPT2 flash attention module. This module inherits from `GPT2Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        is_cross_attention = encoder_hidden_states is not None\n+        is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n \n-    def forward(\n-        self,\n-        hidden_states: Optional[Tuple[torch.FloatTensor]],\n-        layer_past: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n-        bsz, _, _ = hidden_states.size()\n-        if encoder_hidden_states is not None:\n-            if not hasattr(self, \"q_attn\"):\n-                raise ValueError(\n-                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n-                    \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n+        using_eager = self.config._attn_implementation == \"eager\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and (output_attentions or head_mask is not None):\n+                using_eager = True\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-\n-            query = self.q_attn(hidden_states)\n-            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n-            attention_mask = encoder_attention_mask\n-        else:\n-            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n-\n-        query = self._split_heads(query, self.num_heads, self.head_dim)\n-        key = self._split_heads(key, self.num_heads, self.head_dim)\n-        value = self._split_heads(value, self.num_heads, self.head_dim)\n-\n-        if layer_past is not None:\n-            past_key = layer_past[0]\n-            past_value = layer_past[1]\n-            key = torch.cat((past_key, key), dim=-2)\n-            value = torch.cat((past_value, value), dim=-2)\n-\n-        present = None\n-        if use_cache is True:\n-            present = (key, value)\n-\n-        query_length = query.shape[2]\n-        tgt_len = key.shape[2]\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        query = query.transpose(1, 2).view(bsz, query_length, self.num_heads, self.head_dim)\n-        key = key.transpose(1, 2).view(bsz, tgt_len, self.num_heads, self.head_dim)\n-        value = value.transpose(1, 2).view(bsz, tgt_len, self.num_heads, self.head_dim)\n-\n-        attn_dropout = self.attn_dropout.p if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        if query.dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.c_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query = query.to(target_dtype)\n-            key = key.to(target_dtype)\n-            value = value.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query,\n-            key,\n-            value,\n-            attention_mask,\n-            query_length,\n-            dropout=attn_dropout,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_weights_reshaped = attn_output.reshape(bsz, query_length, self.num_heads * self.head_dim)\n-        attn_output = self.c_proj(attn_weights_reshaped)\n-        attn_output = self.resid_dropout(attn_output)\n-\n-        outputs = (attn_output, present)\n-        if output_attentions:\n-            outputs += (attn_weights_reshaped,)\n-\n-        return outputs\n-\n-\n-class GPT2SdpaAttention(GPT2Attention):\n-    \"\"\"\n-    GPT2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `GPT2Attention` as the weights of the module stays untouched. The only changes are on the forward pass\n-    to adapt to the SDPA API.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # Idea adapted from transformers.models.bert.modeling_bert.BertSdpaSelfAttention.__init__\n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()`. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n-\n-    def forward(\n-        self,\n-        hidden_states: Optional[Tuple[torch.FloatTensor]],\n-        layer_past: Optional[Tuple[torch.Tensor]] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"`GPT2SdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                # Attention functions are consistent with previous equivalent attention classes, however they do not support some options\n+                # (e.g. layer scaling, head mask) that eager supports. These implementations are thus equivalent to previous code, but\n+                # not necessarily to eager (if mentionned options are provided).\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        if using_eager and self.reorder_and_upcast_attn:\n+            attn_output, attn_weights = self._upcast_and_reordered_attn(\n+                query_states, key_states, value_states, attention_mask, head_mask\n             )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                layer_past=layer_past,\n-                attention_mask=attention_mask,\n+        else:\n+            attn_output, attn_weights = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask,\n                 head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                encoder_attention_mask=encoder_attention_mask,\n-                use_cache=use_cache,\n-                output_attentions=output_attentions,\n+                dropout=self.attn_dropout.p if self.training else 0.0,\n+                is_causal=is_causal,\n+                **kwargs,\n             )\n \n-        bsz, q_len, _ = hidden_states.size()\n-\n-        # Initial attention projections\n-        is_cross_attention = encoder_hidden_states is not None\n-        if is_cross_attention:\n-            if not hasattr(self, \"q_attn\"):\n-                raise ValueError(\n-                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n-                    \"Please make sure to instantiate class with `GPT2SdpaAttention(..., is_cross_attention=True)`.\"\n-                )\n-\n-            query = self.q_attn(hidden_states)\n-            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n-            attention_mask = encoder_attention_mask\n-        else:\n-            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n-\n-        query = self._split_heads(query, self.num_heads, self.head_dim)\n-        key = self._split_heads(key, self.num_heads, self.head_dim)\n-        value = self._split_heads(value, self.num_heads, self.head_dim)\n-\n-        # Optional kv caching\n-        if layer_past is not None:\n-            past_key = layer_past[0]\n-            past_value = layer_past[1]\n-            key = torch.cat((past_key, key), dim=-2)\n-            value = torch.cat((past_value, value), dim=-2)\n-\n-        present = None\n-        if use_cache is True:\n-            present = (key, value)\n-\n-        # Avoid torch==2.1.2 specific bug for the memory-efficient backend in SDPA\n-        if self.require_contiguous_qkv and query.device.type == \"cuda\" and attention_mask is not None:\n-            query = query.contiguous()\n-            key = key.contiguous()\n-            value = value.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if attention_mask is None and q_len > 1 and not is_cross_attention else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query,\n-            key,\n-            value,\n-            attn_mask=attention_mask,\n-            dropout_p=self.attn_dropout.p if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        # Reshape outputs\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.embed_dim)\n-\n-        # Final projection\n+        attn_output = attn_output.reshape(*attn_output.shape[:-2], -1).contiguous()\n         attn_output = self.c_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n \n-        return attn_output, present, None\n+        outputs = (attn_output, present)\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs  # a, present, (attentions)\n \n \n class GPT2MLP(nn.Module):\n@@ -579,22 +372,18 @@ def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.Fl\n         return hidden_states\n \n \n-GPT2_ATTENTION_CLASSES = {\"eager\": GPT2Attention, \"flash_attention_2\": GPT2FlashAttention2, \"sdpa\": GPT2SdpaAttention}\n-\n-\n class GPT2Block(nn.Module):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n         inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n-        attention_class = GPT2_ATTENTION_CLASSES[config._attn_implementation]\n \n         self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n-        self.attn = attention_class(config=config, layer_idx=layer_idx)\n+        self.attn = GPT2Attention(config=config, layer_idx=layer_idx)\n         self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n \n         if config.add_cross_attention:\n-            self.crossattention = attention_class(config=config, is_cross_attention=True, layer_idx=layer_idx)\n+            self.crossattention = GPT2Attention(config=config, is_cross_attention=True, layer_idx=layer_idx)\n             self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n \n         self.mlp = GPT2MLP(inner_dim, config)"
        },
        {
            "sha": "403159cdf39c9a1a5d60678dab23441e2f33a073",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -278,7 +278,6 @@ class GPTBigCodeFlashAttention2(GPTBigCodeAttention):\n     API of flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "6763695bfba036f76751e938c3093bd2c3afa55b",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -278,7 +278,6 @@ class GPTNeoFlashAttention2(GPTNeoSelfAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "7152d72f5b7fc87f3e37e63c54681698b76e2db4",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 7,
            "deletions": 29,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -490,40 +490,18 @@ def __init__(self, config, layer_idx=None):\n class GPTNeoXRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: GPTNeoXConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[GPTNeoXConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`GPTNeoXRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]"
        },
        {
            "sha": "71602f01e7d6f8ce766afce375d7fc437eac9e29",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 7,
            "deletions": 29,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -227,40 +227,18 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n class GPTNeoXJapaneseRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: GPTNeoXJapaneseConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[GPTNeoXJapaneseConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`GPTNeoXJapaneseRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]"
        },
        {
            "sha": "4af8f73b5f5eeacc71eaa0b91ced746cbb9d928e",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -266,7 +266,6 @@ class GPTJFlashAttention2(GPTJAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "2e045e149d95de4aae511e486e088cc1956ac011",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 215,
            "deletions": 470,
            "changes": 685,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/granite/modular_granite.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_granite.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.\n #\n@@ -13,134 +19,41 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_granite import GraniteConfig\n \n \n logger = logging.get_logger(__name__)\n-\n _CONFIG_FOR_DOC = \"GraniteConfig\"\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Granite\n-class GraniteRMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        GraniteRMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n-ALL_LAYERNORM_LAYERS.append(GraniteRMSNorm)\n-\n-\n-class GraniteRotaryEmbedding(nn.Module):\n-    def __init__(self, config: GraniteConfig):\n-        super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n-        self.rope_kwargs = {}\n-        if config.rope_scaling is not None:\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device=None, **self.rope_kwargs)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.rotate_half with Llama->Granite\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb with Llama->Granite\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -168,24 +81,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-class GraniteMLP(nn.Module):\n-    # Copied from transformers.models.llama.modeling_llama.LlamaMLP.__init__ with Llama->Granite\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n-        self.act_fn = ACT2FN[config.hidden_act]\n-\n-    # Copied from transformers.models.gemma.modeling_gemma.GemmaMLP.forward with Gemma->Granite\n-    def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv with Llama->Granite\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -198,142 +93,73 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class GraniteAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: GraniteConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = config.attention_multiplier\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.is_causal = True\n \n-        self.scaling = config.attention_multiplier\n-\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class GraniteFlashAttention2(GraniteAttention):\n-    \"\"\"\n-    Granite flash attention module. This module inherits from `GraniteAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n@@ -343,172 +169,77 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (GraniteRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            softmax_scale=self.scaling,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n-class GraniteSdpaAttention(GraniteAttention):\n-    \"\"\"\n-    Granite attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `GraniteAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from GraniteAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"GraniteModel is using GraniteSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-            scale=self.scaling,\n-        )\n+class GraniteRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        GraniteRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n \n-        attn_output = self.o_proj(attn_output)\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n-        return attn_output, None, past_key_value\n \n+class GraniteMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n-GRANITE_ATTENTION_CLASSES = {\n-    \"eager\": GraniteAttention,\n-    \"flash_attention_2\": GraniteFlashAttention2,\n-    \"sdpa\": GraniteSdpaAttention,\n-}\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n class GraniteDecoderLayer(nn.Module):\n     def __init__(self, config: GraniteConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n-        self.self_attn = GRANITE_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = GraniteAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = GraniteMLP(config)\n         self.input_layernorm = GraniteRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n         self.residual_multiplier = config.residual_multiplier\n \n     def forward(\n@@ -550,7 +281,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -567,19 +298,81 @@ def forward(\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         hidden_states = self.mlp(hidden_states)\n-        hidden_states = residual + hidden_states * self.residual_multiplier\n+        hidden_states = residual + hidden_states * self.residual_multiplier  # main diff with Llama\n \n         outputs = (hidden_states,)\n \n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n+class GraniteRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: GraniteConfig,\n+        device=None,\n+    ):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n GRANITE_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -601,7 +394,6 @@ def forward(\n     \"The bare Granite Model outputting raw hidden-states without any specific head on top.\",\n     GRANITE_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel with Llama->Granite\n class GranitePreTrainedModel(PreTrainedModel):\n     config_class = GraniteConfig\n     base_model_prefix = \"model\"\n@@ -723,17 +515,9 @@ def __init__(self, config: GraniteConfig):\n             [GraniteDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = GraniteRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = GraniteRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-\n         self.embedding_multiplier = config.embedding_multiplier\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-\n-        # rope\n-        self.rotary_emb = GraniteRotaryEmbedding(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -750,13 +534,14 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -777,35 +562,24 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        inputs_embeds = inputs_embeds * self.embedding_multiplier\n+        inputs_embeds = inputs_embeds * self.embedding_multiplier  # main diff with Llama\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n \n-        # embed positions\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -814,9 +588,8 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n-        for decoder_layer in self.layers:\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n@@ -842,13 +615,11 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -858,18 +629,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -879,11 +645,6 @@ def _update_causal_mask(\n         past_key_values: Cache,\n         output_attentions: bool,\n     ):\n-        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n-        # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n-        # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n-        # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n-\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n@@ -906,7 +667,6 @@ def _update_causal_mask(\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n-        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -917,24 +677,17 @@ def _update_causal_mask(\n                 else past_seen_tokens + sequence_length + 1\n             )\n \n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            causal_mask = attention_mask\n-        else:\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n@@ -944,12 +697,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1006,10 +759,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class GraniteForCausalLM(GranitePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Granite\n     def __init__(self, config):\n         super().__init__(config)\n         self.model = GraniteModel(config)\n@@ -1052,6 +808,8 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1060,15 +818,20 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n \n         ```python\n         >>> from transformers import AutoTokenizer, GraniteForCausalLM\n \n-        >>> model = GraniteForCausalLM.from_pretrained(\"ibm/PowerLM-3b\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"ibm/PowerLM-3b\")\n+        >>> model = GraniteForCausalLM.from_pretrained(\"meta-granite/Granite-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-granite/Granite-2-7b-hf\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n@@ -1096,26 +859,17 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits / self.config.logits_scaling\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        logits = logits / self.config.logits_scaling  # main diff with Llama\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1128,12 +882,3 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n-\n-    @staticmethod\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past"
        },
        {
            "sha": "698280085f1852792785ec677901407faccb84e1",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "added",
            "additions": 291,
            "deletions": 0,
            "changes": 291,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -0,0 +1,291 @@\n+# coding=utf-8\n+# Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import LossKwargs, logging\n+from ..llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaForCausalLM, LlamaModel\n+from .configuration_granite import GraniteConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class GraniteAttention(LlamaAttention):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: GraniteConfig, layer_idx: Optional[int] = None):\n+        super().__init__(config, layer_idx)\n+        self.scaling = config.attention_multiplier\n+\n+\n+class GraniteDecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: GraniteConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.residual_multiplier = config.residual_multiplier\n+        self.self_attn = GraniteAttention(config=config, layer_idx=layer_idx)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states * self.residual_multiplier\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states * self.residual_multiplier  # main diff with Llama\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class GraniteModel(LlamaModel):\n+    def __init__(self, config: GraniteConfig):\n+        super().__init__(config)\n+        self.embedding_multiplier = config.embedding_multiplier\n+        self.layers = nn.ModuleList(\n+            [GraniteDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        inputs_embeds = inputs_embeds * self.embedding_multiplier  # main diff with Llama\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+class GraniteForCausalLM(LlamaForCausalLM):\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        logits = logits / self.config.logits_scaling  # main diff with Llama\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )"
        },
        {
            "sha": "1c4c06bbc8d71ef61113a941e517f23cd07d7420",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -158,11 +158,15 @@ def extra_repr(self):\n \n # Copied from transformers.models.granite.modeling_granite.GraniteRotaryEmbedding with Granite->GraniteMoe\n class GraniteMoeRotaryEmbedding(nn.Module):\n-    def __init__(self, config: GraniteMoeConfig):\n+    def __init__(\n+        self,\n+        config: GraniteMoeConfig,\n+        device=None,\n+    ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config.rope_scaling is not None:\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n             self.rope_type = \"default\"\n@@ -172,7 +176,7 @@ def __init__(self, config: GraniteMoeConfig):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device=None, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -413,7 +417,8 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-# Copied from transformers.models.granite.modeling_granite.GraniteAttention with Granite->GraniteMoe\n+# copied from transformers.models.granite.modeling_granite.GraniteAttention with Granite->GraniteMoe\n+# no longer copied after attention refactors\n class GraniteMoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -510,7 +515,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.granite.modeling_granite.GraniteFlashAttention2 with Granite->GraniteMoe\n+# NO LONGER EXIST Copied from transformers.models.granite.modeling_granite.GraniteFlashAttention2 with Granite->GraniteMoe\n+# TODO cyril: modular\n class GraniteMoeFlashAttention2(GraniteMoeAttention):\n     \"\"\"\n     GraniteMoe flash attention module. This module inherits from `GraniteMoeAttention` as the weights of the module stays\n@@ -617,7 +623,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.granite.modeling_granite.GraniteSdpaAttention with Granite->GraniteMoe\n+# NO LONGER EXIST Copied from transformers.models.granite.modeling_granite.GraniteSdpaAttention with Granite->GraniteMoe\n+# TODO cyril: modular\n class GraniteMoeSdpaAttention(GraniteMoeAttention):\n     \"\"\"\n     GraniteMoe attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from"
        },
        {
            "sha": "1629f7d4f3feae22a3548732c2aca0836bf2f6b0",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -563,7 +563,6 @@ class HubertFlashAttention2(HubertAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "b2ffbcbc695696dcd17d699530f6766ab43ce27f",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -444,7 +444,6 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n "
        },
        {
            "sha": "6d7295b5120d29d7c2fe4c961003eebf1007231d",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -272,7 +272,6 @@ class Idefics2VisionFlashAttention2(Idefics2VisionAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n@@ -859,15 +858,15 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.mistral.modeling_mistral.MistralFlashAttention2 with MistralAttention->Idefics2PerceiverAttention,MistralFlashAttention->Idefics2PerceiverFlashAttention,Mistral->Idefics2\n+# NO LONGER EXIST Copied from transformers.models.mistral.modeling_mistral.MistralFlashAttention2 with MistralAttention->Idefics2PerceiverAttention,MistralFlashAttention->Idefics2PerceiverFlashAttention,Mistral->Idefics2\n+# TODO cyril: modular\n class Idefics2PerceiverFlashAttention2(Idefics2PerceiverAttention):\n     \"\"\"\n     Idefics2 flash attention module. This module inherits from `Idefics2PerceiverAttention` as the weights of the module stays\n     untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "3a52b8b6d54d0e2d82721c438b7eec8a3a6f812e",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -273,7 +273,6 @@ class Idefics3VisionFlashAttention2(Idefics3VisionAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "ae7470d789b27e7df66bb6653218e00d40199823",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -384,7 +384,6 @@ class JambaFlashAttention2(JambaAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n@@ -835,15 +834,17 @@ def forward(\n class JambaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n+        self.config = config\n         self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n         self.act_fn = ACT2FN[config.hidden_act]\n \n-    def forward(self, hidden_state):\n-        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n # Adapted from transformers.models.mixtral.modeling_mixtral.MixtralSparseMoeBlock with Mistral->Jamba"
        },
        {
            "sha": "7b7fd5a90d69ed21f99b6ad993c44baa017b268a",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 49,
            "deletions": 17,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -32,6 +32,7 @@\n     MoeModelOutputWithPast,\n     SequenceClassifierOutputWithPast,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -385,31 +386,67 @@ def extra_repr(self):\n \n # Copied from transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding with Gemma->JetMoe\n class JetMoeRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        config: JetMoeConfig,\n+        device=None,\n+    ):\n         super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n     @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n@@ -486,11 +523,7 @@ def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n \n         self.kv_proj = torch.nn.Linear(config.hidden_size, self.kv_projection_size * 2, bias=False)\n \n-        self.rotary_emb = JetMoeRotaryEmbedding(\n-            config.kv_channels,\n-            max_position_embeddings=config.max_position_embeddings,\n-            base=config.rope_theta,\n-        )\n+        self.rotary_emb = JetMoeRotaryEmbedding(config)\n \n     def forward(\n         self,\n@@ -641,7 +674,6 @@ def forward(\n \n \n class JetMoeFlashAttention2(JetMoeAttention):\n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "5be33c26414cd7c7068839391c76552c33483849",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 84,
            "deletions": 355,
            "changes": 439,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -17,8 +17,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -28,7 +27,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -37,15 +36,14 @@\n     TokenClassifierOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n     LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -84,40 +82,18 @@ def extra_repr(self):\n class LlamaRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: LlamaConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[LlamaConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`LlamaRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -230,144 +206,73 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class LlamaAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: LlamaConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class LlamaFlashAttention2(LlamaAttention):\n-    \"\"\"\n-    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n@@ -377,167 +282,38 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class LlamaSdpaAttention(LlamaAttention):\n-    \"\"\"\n-    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from LlamaAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-LLAMA_ATTENTION_CLASSES = {\n-    \"eager\": LlamaAttention,\n-    \"flash_attention_2\": LlamaFlashAttention2,\n-    \"sdpa\": LlamaSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class LlamaDecoderLayer(nn.Module):\n     def __init__(self, config: LlamaConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = LlamaMLP(config)\n         self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -553,36 +329,14 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -602,13 +356,9 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -755,10 +505,7 @@ def __init__(self, config: LlamaConfig):\n         )\n         self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = LlamaRotaryEmbedding(config=config)\n-\n         self.gradient_checkpointing = False\n-        if getattr(config, \"pretraining_tp\", 1) != 1:\n-            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -775,7 +522,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -803,31 +550,22 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n+\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -836,7 +574,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -869,9 +606,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -881,18 +615,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,"
        },
        {
            "sha": "4e116e7e3db5858ad94187b76fa05c56363905b5",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -348,7 +348,6 @@ class M2M100FlashAttention2(M2M100Attention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "e272c98f06975a476d8613c5226508f8c81e074a",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -291,7 +291,6 @@ class MBartFlashAttention2(MBartAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "1440ce1e075c95eb81d890ecc9778decb809709b",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 54,
            "deletions": 18,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -26,6 +26,7 @@\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -364,31 +365,67 @@ def forward(self, x: torch.Tensor):\n \n # Copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Mimi\n class MimiRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        config: MimiConfig,\n+        device=None,\n+    ):\n         super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n     @torch.no_grad()\n-    # copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.forward\n-    # TODO(joao): add me back asap :)\n     def forward(self, x, position_ids):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n@@ -457,7 +494,8 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-# Copied from transformers.models.gemma.modeling_gemma.GemmaAttention with Gemma->Mimi\n+# copied from transformers.models.gemma.modeling_gemma.GemmaAttention with Gemma->Mimi\n+# no longer copied after attention refactors\n class MimiAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -493,11 +531,7 @@ def __init__(self, config: MimiConfig, layer_idx: Optional[int] = None):\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-        self.rotary_emb = MimiRotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n+        self.rotary_emb = MimiRotaryEmbedding(config)\n         self.sliding_window = config.sliding_window  # Ignore copy\n \n     def forward(\n@@ -559,7 +593,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->Mimi\n+# NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->Mimi\n+# TODO cyril: modular\n class MimiFlashAttention2(MimiAttention):\n     \"\"\"\n     Mimi flash attention module. This module inherits from `MimiAttention` as the weights of the module stays\n@@ -670,7 +705,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->Mimi\n+# NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->Mimi\n+# TODO cyril: modular\n class MimiSdpaAttention(MimiAttention):\n     \"\"\"\n     Mimi attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from"
        },
        {
            "sha": "90c38895b4280b986c69e7b3881f7a453e4069cf",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 273,
            "deletions": 515,
            "changes": 788,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -1,124 +1,69 @@\n-# coding=utf-8\n-# Copyright 2023 Mistral AI and the HuggingFace Inc. team. All rights reserved.\n-#\n-# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n-# and OPT implementations in this library. It has been modified from its\n-# original forms to accommodate minor architectural differences compared\n-# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Mistral model.\"\"\"\n-\n-import math\n-from typing import List, Optional, Tuple, Union\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/mistral/modular_mistral.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_mistral.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     QuestionAnsweringModelOutput,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_mistral import MistralConfig\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"mistralai/Mistral-7B-v0.1\"\n _CONFIG_FOR_DOC = \"MistralConfig\"\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Mistral\n-class MistralRMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        MistralRMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n-class MistralRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+class MistralMLP(nn.Module):\n+    def __init__(self, config):\n         super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-    @torch.no_grad()\n-    # copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.forward\n-    # TODO(joao): add me back asap :)\n-    def forward(self, x, position_ids):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -146,21 +91,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-class MistralMLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        self.act_fn = ACT2FN[config.hidden_act]\n-\n-    def forward(self, hidden_state):\n-        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -173,315 +103,125 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class MistralAttention(nn.Module):\n-    \"\"\"\n-    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n-    and \"Generating Long Sequences with Sparse Transformers\".\n-    \"\"\"\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: MistralConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: MistralConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = config.head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n-\n-        self.rotary_emb = MistralRotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class MistralFlashAttention2(MistralAttention):\n-    \"\"\"\n-    Mistral flash attention module. This module inherits from `MistralAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ):\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self.config, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),  # main diff with Llama\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.head_dim).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention with Llama->Mistral\n-# TODO(joao): add me back asap :)\n-class MistralSdpaAttention(MistralAttention):\n-    \"\"\"\n-    Mistral attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `MistralAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from MistralAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"MistralModel is using MistralSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n+class MistralRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        MistralRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n \n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n \n-MISTRAL_ATTENTION_CLASSES = {\n-    \"eager\": MistralAttention,\n-    \"flash_attention_2\": MistralFlashAttention2,\n-    \"sdpa\": MistralSdpaAttention,\n-}\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with Llama->Mistral, LLAMA->MISTRAL\n-# TODO(joao): add me back asap :)\n class MistralDecoderLayer(nn.Module):\n     def __init__(self, config: MistralConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n-        self.self_attn = MISTRAL_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n-\n+        self.self_attn = MistralAttention(config=config, layer_idx=layer_idx)\n         self.mlp = MistralMLP(config)\n         self.input_layernorm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -495,40 +235,23 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -540,16 +263,77 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n+class MistralRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: MistralConfig,\n+        device=None,\n+    ):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n MISTRAL_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -576,10 +360,11 @@ class MistralPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MistralDecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n+    _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_quantized_cache = True\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n@@ -663,7 +448,7 @@ def _init_weights(self, module):\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices indicating the position of the input sequence tokens in the sequence. Unlike `position_ids`,\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n             this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n             the complete sequence length.\n \"\"\"\n@@ -690,10 +475,10 @@ def __init__(self, config: MistralConfig):\n         self.layers = nn.ModuleList(\n             [MistralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self._attn_implementation = config._attn_implementation\n         self.norm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.rotary_emb = MistralRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -709,48 +494,36 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # retrieve input_ids and inputs_embeds\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if self.gradient_checkpointing and self.training and use_cache:\n             logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n             )\n             use_cache = False\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -762,17 +535,19 @@ def forward(\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, use_cache, output_attentions\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n \n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n-        for decoder_layer in self.layers:\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n@@ -786,6 +561,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -796,13 +572,12 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -812,30 +587,24 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        use_cache: bool,\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and use_cache:\n+            if attention_mask is not None and past_key_values is not None:\n                 is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n                 if is_padding_right:\n                     raise ValueError(\n@@ -977,6 +746,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class MistralForCausalLM(MistralPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -1024,6 +796,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1044,8 +817,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, MistralForCausalLM\n \n-        >>> model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n+        >>> model = MistralForCausalLM.from_pretrained(\"meta-mistral/Mistral-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-mistral/Mistral-2-7b-hf\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n@@ -1055,7 +828,6 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1074,6 +846,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1082,18 +855,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Ensure tensors are on the same device\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1110,26 +872,24 @@ def forward(\n \n @add_start_docstrings(\n     \"\"\"\n-    The Mistral Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`MistralForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n+    The Mistral Model transformer with a token classification head on top (a linear layer on top of the hidden-states\n+    output) e.g. for Named-Entity-Recognition (NER) tasks.\n     \"\"\",\n     MISTRAL_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Mistral, LLAMA->MISTRAL\n-class MistralForSequenceClassification(MistralPreTrainedModel):\n+class MistralForTokenClassification(MistralPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n         self.model = MistralModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+        if getattr(config, \"classifier_dropout\", None) is not None:\n+            classifier_dropout = config.classifier_dropout\n+        elif getattr(config, \"hidden_dropout\", None) is not None:\n+            classifier_dropout = config.hidden_dropout\n+        else:\n+            classifier_dropout = 0.1\n+        self.dropout = nn.Dropout(classifier_dropout)\n+        self.score = nn.Linear(config.hidden_size, config.num_labels)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1141,19 +901,24 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> Union[Tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1162,7 +927,7 @@ def forward(\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        transformer_outputs = self.model(\n+        outputs = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1173,67 +938,47 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n-        hidden_states = transformer_outputs[0]\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n-        else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        sequence_output = outputs[0]\n+        sequence_output = self.dropout(sequence_output)\n+        logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n-            output = (pooled_logits,) + transformer_outputs[1:]\n+            output = (logits,) + outputs[2:]\n             return ((loss,) + output) if loss is not None else output\n \n-        return SequenceClassifierOutputWithPast(\n+        return TokenClassifierOutput(\n             loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n         )\n \n \n @add_start_docstrings(\n     \"\"\"\n-    The Mistral Model transformer with a token classification head on top (a linear layer on top of the hidden-states\n-    output) e.g. for Named-Entity-Recognition (NER) tasks.\n+    The Mistral Model transformer with a sequence classification head on top (linear layer).\n+\n+    [`MistralForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n+    (e.g. GPT-2) do.\n+\n+    Since it does classification on the last token, it requires to know the position of the last token. If a\n+    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n+    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n+    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n+    each row of the batch).\n     \"\"\",\n     MISTRAL_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->Mistral, LLAMA->MISTRAL\n-class MistralForTokenClassification(MistralPreTrainedModel):\n+class MistralForSequenceClassification(MistralPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n         self.model = MistralModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n+        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1245,24 +990,19 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1271,7 +1011,7 @@ def forward(\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        transformer_outputs = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1282,23 +1022,43 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n-        sequence_output = outputs[0]\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n+        hidden_states = transformer_outputs[0]\n+        logits = self.score(hidden_states)\n+\n+        if input_ids is not None:\n+            batch_size = input_ids.shape[0]\n+        else:\n+            batch_size = inputs_embeds.shape[0]\n+\n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n+        if self.config.pad_token_id is None:\n+            sequence_lengths = -1\n+        else:\n+            if input_ids is not None:\n+                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n+                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n+                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n+                sequence_lengths = sequence_lengths.to(logits.device)\n+            else:\n+                sequence_lengths = -1\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n         if not return_dict:\n-            output = (logits,) + outputs[2:]\n+            output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n-        return TokenClassifierOutput(\n+        return SequenceClassifierOutputWithPast(\n             loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n+            logits=pooled_logits,\n+            past_key_values=transformer_outputs.past_key_values,\n+            hidden_states=transformer_outputs.hidden_states,\n+            attentions=transformer_outputs.attentions,\n         )\n \n \n@@ -1309,15 +1069,13 @@ def forward(\n     \"\"\",\n     MISTRAL_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForQuestionAnswering with Llama->Mistral,LLAMA->MISTRAL,transformer->model\n class MistralForQuestionAnswering(MistralPreTrainedModel):\n     base_model_prefix = \"model\"\n \n-    # Copied from models.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Mistral\n     def __init__(self, config):\n         super().__init__(config)\n-        self.model = MistralModel(config)\n         self.qa_outputs = nn.Linear(config.hidden_size, 2)\n+        self.model = MistralModel(config)  # diff with Llama: transformer->model\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "362233a21b70f490db325fa6c2c2a105981474c2",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "added",
            "additions": 350,
            "deletions": 0,
            "changes": 350,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -0,0 +1,350 @@\n+from typing import Callable, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...cache_utils import Cache, SlidingWindowCache, StaticCache\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import QuestionAnsweringModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import logging\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaForQuestionAnswering,\n+    LlamaForSequenceClassification,\n+    LlamaForTokenClassification,\n+    LlamaMLP,\n+    LlamaModel,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from .configuration_mistral import MistralConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"mistralai/Mistral-7B-v0.1\"\n+\n+\n+class MistralMLP(LlamaMLP):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+\n+class MistralAttention(LlamaAttention):\n+    def __init__(self, config: MistralConfig, layer_idx: int):\n+        super().__init__()\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),  # main diff with Llama\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class MistralDecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: MistralConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.self_attn = MistralAttention(config=config, layer_idx=layer_idx)\n+        self.mlp = MistralMLP(config)\n+\n+\n+class MistralModel(LlamaModel):\n+    def __init__(self, config: MistralConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [MistralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n+        sequence_length = input_tensor.shape[1]\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        config: MistralConfig,\n+        past_key_values: Cache,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+            config (`MistralConfig`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n+            causal_mask *= diagonal_attend_mask\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+        return causal_mask\n+\n+\n+class MistralForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+class MistralForTokenClassification(LlamaForTokenClassification):\n+    pass\n+\n+\n+class MistralForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+class MistralForQuestionAnswering(LlamaForQuestionAnswering):\n+    base_model_prefix = \"model\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = MistralModel(config)  # diff with Llama: transformer->model\n+        del self.transformer\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+        r\"\"\"\n+        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        loss = None\n+        if start_positions is not None and end_positions is not None:\n+            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n+\n+        if not return_dict:\n+            output = (start_logits, end_logits) + outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )"
        },
        {
            "sha": "84ed327d9be9209d79fd9e5164ee9f5f984437f2",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 360,
            "deletions": 582,
            "changes": 942,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/mixtral/modular_mixtral.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_mixtral.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2023 Mistral AI and the HuggingFace Inc. team. All rights reserved.\n #\n@@ -17,142 +23,133 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch Mixtral model.\"\"\"\n \n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    CausalLMOutputWithPast,\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n     QuestionAnsweringModelOutput,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import is_torch_greater_or_equal_than_1_13\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n     logging,\n     replace_return_docstrings,\n )\n-from ...utils.import_utils import is_torch_fx_available\n from .configuration_mixtral import MixtralConfig\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-# This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n-# It means that the function will not be traced through and simply appear as a node in the graph.\n-if is_torch_fx_available():\n-    if not is_torch_greater_or_equal_than_1_13:\n-        import torch.fx\n-\n-    _prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n-\n-\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"mistralai/Mixtral-8x7B-v0.1\"\n _CONFIG_FOR_DOC = \"MixtralConfig\"\n \n \n-def load_balancing_loss_func(\n-    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],\n-    num_experts: Optional[int] = None,\n-    top_k=2,\n-    attention_mask: Optional[torch.Tensor] = None,\n-) -> Union[torch.Tensor, int]:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+class MixtralBlockSparseTop2MLP(nn.Module):\n+    def __init__(self, config: MixtralConfig):\n+        super().__init__()\n+        self.ffn_dim = config.intermediate_size\n+        self.hidden_dim = config.hidden_size\n \n-    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n+        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n+        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n+        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n \n-    Args:\n-        gate_logits:\n-            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n-            shape [batch_size X sequence_length, num_experts].\n-        num_experts:\n-            Number of experts\n-        top_k:\n-            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n-            parameter.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            The attention_mask used in forward function\n-            shape [batch_size X sequence_length] if not None.\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n-    Returns:\n-        The auxiliary loss.\n+    def forward(self, hidden_states):\n+        current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)\n+        current_hidden_states = self.w2(current_hidden_states)\n+        return current_hidden_states\n+\n+\n+class MixtralSparseMoeBlock(nn.Module):\n+    \"\"\"\n+    This implementation is\n+    strictly equivalent to standard MoE with full capacity (no\n+    dropped tokens). It's faster since it formulates MoE operations\n+    in terms of block-sparse operations to accommodate imbalanced\n+    assignments of tokens to experts, whereas standard MoE either\n+    (1) drop tokens at the cost of reduced performance or (2) set\n+    capacity factor to number of experts and thus waste computation\n+    and memory on padding.\n     \"\"\"\n-    if gate_logits is None or not isinstance(gate_logits, tuple):\n-        return 0\n \n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_dim = config.hidden_size\n+        self.ffn_dim = config.intermediate_size\n+        self.num_experts = config.num_local_experts\n+        self.top_k = config.num_experts_per_tok\n \n-    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+        # gating\n+        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n \n-    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n \n-    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+        # Jitter parameters\n+        self.jitter_noise = config.router_jitter_noise\n \n-    if attention_mask is None:\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        \"\"\" \"\"\"\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        if self.training and self.jitter_noise > 0:\n+            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        # router_logits: (batch * sequence_length, n_experts)\n+        router_logits = self.gate(hidden_states)\n \n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n-    else:\n-        batch_size, sequence_length = attention_mask.shape\n-        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        # we cast back to the input dtype\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n \n-        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n-        expert_attention_mask = (\n-            attention_mask[None, :, :, None, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n-            .reshape(-1, top_k, num_experts)\n-            .to(compute_device)\n+        final_hidden_states = torch.zeros(\n+            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n         )\n \n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n-            expert_attention_mask, dim=0\n-        )\n+        # One hot encode the selected experts to create an expert mask\n+        # this will be used to easily index which expert is going to be sollicitated\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n-        router_per_expert_attention_mask = (\n-            attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n-            .reshape(-1, num_experts)\n-            .to(compute_device)\n-        )\n+        # Loop over all available experts in the model and perform the computation on each expert\n+        for expert_idx in range(self.num_experts):\n+            expert_layer = self.experts[expert_idx]\n+            idx, top_x = torch.where(expert_mask[expert_idx])\n \n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n-            router_per_expert_attention_mask, dim=0\n-        )\n+            # Index the correct hidden states and compute the expert hidden state for\n+            # the current expert. We need to make sure to multiply the output hidden\n+            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n+            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n \n-    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n-    return overall_loss * num_experts\n+            # However `index_add_` only support torch tensors for indexing so we'll use\n+            # the `top_x` tensor here.\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return final_hidden_states, router_logits\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Mixtral\n class MixtralRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -173,65 +170,23 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Mixtral\n-# TODO @longjie no longer copied from Mistral after static cache\n-class MixtralRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n-        super().__init__()\n-\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-\n-        # Build here to make `torch.jit.trace` work.\n-        self._set_cos_sin_cache(\n-            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n-        )\n-\n-    def _set_cos_sin_cache(self, seq_len, device, dtype):\n-        self.max_seq_len_cached = seq_len\n-        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n-\n-        freqs = torch.outer(t, self.inv_freq)\n-        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n-        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n-\n-    def forward(self, x, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        if seq_len > self.max_seq_len_cached:\n-            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n-\n-        return (\n-            self.cos_cached[:seq_len].to(dtype=x.dtype),\n-            self.sin_cached[:seq_len].to(dtype=x.dtype),\n-        )\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# copied from transformers.models.mistral.modeling_mistral.apply_rotary_pos_emb\n-# TODO @longjie no longer copied from Mistral after static cache\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -242,14 +197,13 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n \n \n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -262,420 +216,106 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-# copied from transformers.models.mistral.modeling_mistral.MistralAttention with Mistral->Mixtral\n-# TODO @longjie no longer copied from Mistral after static cache\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class MixtralAttention(nn.Module):\n-    \"\"\"\n-    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n-    and \"Generating Long Sequences with Sparse Transformers\".\n-    \"\"\"\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: MixtralConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: MixtralConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = config.head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-        self.is_causal = True\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n-\n-        self.rotary_emb = MixtralRotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n-\n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n+        self.is_causal = True\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        kv_seq_len = key_states.shape[-2]\n         if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n-            )\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, -1)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# copied from transformers.models.mistral.modeling_mistral.MistralFlashAttention2 with Mistral->Mixtral\n-# TODO @longjie no longer copied from Mistral after static cache\n-class MixtralFlashAttention2(MixtralAttention):\n-    \"\"\"\n-    Mixtral flash attention module. This module inherits from `MixtralAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ):\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n-        # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        rotary_seq_len = (\n-            max(kv_seq_len, position_ids[:, -1].max().item() + 1) if position_ids is not None else kv_seq_len\n-        )\n-\n-        cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self.config, \"sliding_window\", None),\n-            is_causal=self.is_causal,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# copied from transformers.models.mistral.modeling_mistral.MistralSdpaAttention with Mistral->Mixtral\n-# TODO @longjie no longer copied from Mistral after static cache\n-class MixtralSdpaAttention(MixtralAttention):\n-    \"\"\"\n-    Mixtral attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `MixtralAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from MixtralAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"MixtralModel is using MixtralSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),  # main diff with Llama\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-MIXTRAL_ATTENTION_CLASSES = {\n-    \"eager\": MixtralAttention,\n-    \"flash_attention_2\": MixtralFlashAttention2,\n-    \"sdpa\": MixtralSdpaAttention,\n-}\n-\n-\n-class MixtralBlockSparseTop2MLP(nn.Module):\n-    def __init__(self, config: MixtralConfig):\n-        super().__init__()\n-        self.ffn_dim = config.intermediate_size\n-        self.hidden_dim = config.hidden_size\n-\n-        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n-        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n-        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n-\n-        self.act_fn = ACT2FN[config.hidden_act]\n-\n-    def forward(self, hidden_states):\n-        current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)\n-        current_hidden_states = self.w2(current_hidden_states)\n-        return current_hidden_states\n-\n-\n-class MixtralSparseMoeBlock(nn.Module):\n-    \"\"\"\n-    This implementation is\n-    strictly equivalent to standard MoE with full capacity (no\n-    dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accommodate imbalanced\n-    assignments of tokens to experts, whereas standard MoE either\n-    (1) drop tokens at the cost of reduced performance or (2) set\n-    capacity factor to number of experts and thus waste computation\n-    and memory on padding.\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.hidden_dim = config.hidden_size\n-        self.ffn_dim = config.intermediate_size\n-        self.num_experts = config.num_local_experts\n-        self.top_k = config.num_experts_per_tok\n-\n-        # gating\n-        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n-\n-        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n-\n-        # Jitter parameters\n-        self.jitter_noise = config.router_jitter_noise\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        \"\"\" \"\"\"\n-        batch_size, sequence_length, hidden_dim = hidden_states.shape\n-        if self.training and self.jitter_noise > 0:\n-            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n-        router_logits = self.gate(hidden_states)\n-\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        # we cast back to the input dtype\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-\n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n-        )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n-\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        return attn_output, attn_weights\n \n \n class MixtralDecoderLayer(nn.Module):\n     def __init__(self, config: MixtralConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = MIXTRAL_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n+        self.self_attn = MixtralAttention(config, layer_idx)\n \n         self.block_sparse_moe = MixtralSparseMoeBlock(config)\n         self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -691,7 +331,8 @@ def forward(\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -720,14 +361,16 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -742,15 +385,77 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n \n         return outputs\n \n \n+class MixtralRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: MixtralConfig,\n+        device=None,\n+    ):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n MIXTRAL_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -772,17 +477,17 @@ def forward(\n     \"The bare Mixtral Model outputting raw hidden-states without any specific head on top.\",\n     MIXTRAL_START_DOCSTRING,\n )\n-# copied from transformers.models.qwen2.modeling_qwen2.Qwen2PreTrainedModel with Qwen2->Mixtral\n-# TODO (Raushan): bring back copied after compile compatibility\n class MixtralPreTrainedModel(PreTrainedModel):\n     config_class = MixtralConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MixtralDecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n+    _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -817,7 +522,7 @@ def _init_weights(self, module):\n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n             [`PreTrainedTokenizer.__call__`] for details.\n \n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n             `past_key_values`).\n \n             If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n@@ -831,17 +536,24 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n@@ -855,9 +567,6 @@ def _init_weights(self, module):\n         output_hidden_states (`bool`, *optional*):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more detail.\n-        output_router_logits (`bool`, *optional*):\n-            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-            should not be returned during inference.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n@@ -871,8 +580,6 @@ def _init_weights(self, module):\n     \"The bare Mixtral Model outputting raw hidden-states without any specific head on top.\",\n     MIXTRAL_START_DOCSTRING,\n )\n-# copied from transformers.models.mistral.modeling_mistral.MistralModel with MISTRAL->MIXTRAL,Mistral->Mixtral\n-# TODO @longjie no longer copied from Mistral after static cache\n class MixtralModel(MixtralPreTrainedModel):\n     \"\"\"\n     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`MixtralDecoderLayer`]\n@@ -890,10 +597,10 @@ def __init__(self, config: MixtralConfig):\n         self.layers = nn.ModuleList(\n             [MixtralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self._attn_implementation = config._attn_implementation\n         self.norm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.rotary_emb = MixtralRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -903,7 +610,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n-    # Ignore copy\n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -918,7 +624,8 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, MoeModelOutputWithPast]:\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n@@ -940,19 +647,8 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -971,11 +667,13 @@ def forward(\n \n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -992,6 +690,7 @@ def forward(\n                     output_router_logits,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -1003,13 +702,12 @@ def forward(\n                     output_router_logits=output_router_logits,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1022,25 +720,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_router_logits]\n-                if v is not None\n-            )\n-        return MoeModelOutputWithPast(\n+        output = MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n         )\n+        return output if return_dict else output.to_tuple()\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1050,6 +738,14 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Mixtral. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n@@ -1117,7 +813,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Mixtral\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1185,8 +880,94 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n+\n+\n class MixtralForCausalLM(MixtralPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1196,6 +977,7 @@ def __init__(self, config):\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_local_experts\n         self.num_experts_per_tok = config.num_experts_per_tok\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1218,8 +1000,7 @@ def get_decoder(self):\n         return self.model\n \n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n-    # Ignore copy\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1235,8 +1016,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n-    ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1291,6 +1072,7 @@ def forward(\n             output_router_logits=output_router_logits,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1299,7 +1081,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n \n         aux_loss = None\n         if output_router_logits:\n@@ -1344,7 +1126,6 @@ def forward(\n     \"\"\",\n     MIXTRAL_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Mixtral, LLAMA->MIXTRAL\n class MixtralForSequenceClassification(MixtralPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1441,7 +1222,6 @@ def forward(\n     \"\"\",\n     MIXTRAL_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->Mixtral, LLAMA->MIXTRAL\n class MixtralForTokenClassification(MixtralPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1530,15 +1310,13 @@ def forward(\n     \"\"\",\n     MIXTRAL_START_DOCSTRING,\n )\n-# Copied from transformers.models.mistral.modeling_mistral.MistralForQuestionAnswering with Mistral->Mixtral, MISTRAL->MIXTRAL\n class MixtralForQuestionAnswering(MixtralPreTrainedModel):\n     base_model_prefix = \"model\"\n \n-    # Copied from models.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Mixtral\n     def __init__(self, config):\n         super().__init__(config)\n-        self.model = MixtralModel(config)\n         self.qa_outputs = nn.Linear(config.hidden_size, 2)\n+        self.model = MixtralModel(config)  # diff with Llama: transformer->model\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "a6069f69b33421349237815d44fa3a69ede36697",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "added",
            "additions": 574,
            "deletions": 0,
            "changes": 574,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -0,0 +1,574 @@\n+# coding=utf-8\n+# Copyright 2023 Mistral AI and the HuggingFace Inc. team. All rights reserved.\n+#\n+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n+# and OPT implementations in this library. It has been modified from its\n+# original forms to accommodate minor architectural differences compared\n+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Mixtral model.\"\"\"\n+\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import DynamicCache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    MoeCausalLMOutputWithPast,\n+    MoeModelOutputWithPast,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    LossKwargs,\n+    logging,\n+)\n+from ..mistral.modeling_mistral import (\n+    MistralAttention,\n+    MistralForCausalLM,\n+    MistralForQuestionAnswering,\n+    MistralForSequenceClassification,\n+    MistralForTokenClassification,\n+    MistralModel,\n+    MistralRMSNorm,\n+)\n+from .configuration_mixtral import MixtralConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"mistralai/Mixtral-8x7B-v0.1\"\n+_CONFIG_FOR_DOC = \"MixtralConfig\"\n+\n+\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n+\n+\n+class MixtralBlockSparseTop2MLP(nn.Module):\n+    def __init__(self, config: MixtralConfig):\n+        super().__init__()\n+        self.ffn_dim = config.intermediate_size\n+        self.hidden_dim = config.hidden_size\n+\n+        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n+        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n+        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n+\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states):\n+        current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)\n+        current_hidden_states = self.w2(current_hidden_states)\n+        return current_hidden_states\n+\n+\n+class MixtralSparseMoeBlock(nn.Module):\n+    \"\"\"\n+    This implementation is\n+    strictly equivalent to standard MoE with full capacity (no\n+    dropped tokens). It's faster since it formulates MoE operations\n+    in terms of block-sparse operations to accommodate imbalanced\n+    assignments of tokens to experts, whereas standard MoE either\n+    (1) drop tokens at the cost of reduced performance or (2) set\n+    capacity factor to number of experts and thus waste computation\n+    and memory on padding.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_dim = config.hidden_size\n+        self.ffn_dim = config.intermediate_size\n+        self.num_experts = config.num_local_experts\n+        self.top_k = config.num_experts_per_tok\n+\n+        # gating\n+        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n+\n+        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n+\n+        # Jitter parameters\n+        self.jitter_noise = config.router_jitter_noise\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        \"\"\" \"\"\"\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        if self.training and self.jitter_noise > 0:\n+            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        # router_logits: (batch * sequence_length, n_experts)\n+        router_logits = self.gate(hidden_states)\n+\n+        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        # we cast back to the input dtype\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+\n+        final_hidden_states = torch.zeros(\n+            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        )\n+\n+        # One hot encode the selected experts to create an expert mask\n+        # this will be used to easily index which expert is going to be sollicitated\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        # Loop over all available experts in the model and perform the computation on each expert\n+        for expert_idx in range(self.num_experts):\n+            expert_layer = self.experts[expert_idx]\n+            idx, top_x = torch.where(expert_mask[expert_idx])\n+\n+            # Index the correct hidden states and compute the expert hidden state for\n+            # the current expert. We need to make sure to multiply the output hidden\n+            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n+            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+\n+            # However `index_add_` only support torch tensors for indexing so we'll use\n+            # the `top_x` tensor here.\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return final_hidden_states, router_logits\n+\n+\n+class MixtralRMSNorm(MistralRMSNorm):\n+    pass\n+\n+\n+class MixtralAttention(MistralAttention):\n+    pass\n+\n+\n+class MixtralDecoderLayer(nn.Module):\n+    def __init__(self, config: MixtralConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = MixtralAttention(config, layer_idx)\n+\n+        self.block_sparse_moe = MixtralSparseMoeBlock(config)\n+        self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        output_attentions: Optional[bool] = False,\n+        output_router_logits: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n+                `(batch, sequence_length)` where padding elements are indicated by 0.\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_router_logits (`bool`, *optional*):\n+                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n+                should not be returned during inference.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if output_router_logits:\n+            outputs += (router_logits,)\n+\n+        return outputs\n+\n+\n+class MixtralModel(MistralModel):\n+    def __init__(self, config: MixtralConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [MixtralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        output_router_logits: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, MoeModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_router_logits = (\n+            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n+        )\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_router_logits = () if output_router_logits else None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    output_router_logits,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    output_router_logits=output_router_logits,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+            if output_router_logits:\n+                all_router_logits += (layer_outputs[-1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            router_logits=all_router_logits,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+class MixtralForCausalLM(MistralForCausalLM):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = MixtralModel(config)\n+        self.router_aux_loss_coef = config.router_aux_loss_coef\n+        self.num_experts = config.num_local_experts\n+        self.num_experts_per_tok = config.num_experts_per_tok\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        output_router_logits: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, MixtralForCausalLM\n+\n+        >>> model = MixtralForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_router_logits = (\n+            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n+        )\n+\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            output_router_logits=output_router_logits,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        aux_loss = None\n+        if output_router_logits:\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits if return_dict else outputs[-1],\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            if output_router_logits:\n+                output = (aux_loss,) + output\n+            return (loss,) + output if loss is not None else output\n+\n+        return MoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            router_logits=outputs.router_logits,\n+        )\n+\n+\n+class MixtralForSequenceClassification(MistralForSequenceClassification):\n+    pass\n+\n+\n+class MixtralForTokenClassification(MistralForTokenClassification):\n+    pass\n+\n+\n+class MixtralForQuestionAnswering(MistralForQuestionAnswering):\n+    pass"
        },
        {
            "sha": "3e0c4d7a5123a77d55d58e3fca633a0d7b2ed0cf",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -829,7 +829,8 @@ def __init__(self, config):\n         self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n # Modified from transformers.models.llama.modeling_llama.LlamaDecoderLayer"
        },
        {
            "sha": "f0281f57cf1c75a87e50230f5514678b26a6c318",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 54,
            "deletions": 18,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -36,6 +36,7 @@\n     ModelOutput,\n     Seq2SeqLMOutput,\n )\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n@@ -307,31 +308,67 @@ def forward(self, x, layer_idx=None):\n \n # Copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Moshi\n class MoshiRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(\n+        self,\n+        config: MoshiConfig,\n+        device=None,\n+    ):\n         super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n     @torch.no_grad()\n-    # copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.forward\n-    # TODO(joao): add me back asap :)\n     def forward(self, x, position_ids):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n@@ -456,13 +493,10 @@ def __init__(self, config: MoshiConfig, layer_idx: Optional[int] = None, use_fle\n         self.rotary_emb = None\n         if use_rope:\n             self.rope_theta = config.rope_theta\n-            self.rotary_emb = MoshiRotaryEmbedding(\n-                self.head_dim,\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n+            self.rotary_emb = MoshiRotaryEmbedding(config)\n \n-    # Copied from transformers.models.gemma.modeling_gemma.GemmaAttention.forward\n+    # copied from transformers.models.gemma.modeling_gemma.GemmaAttention.forward\n+    # no longer copied after attention refactors\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -527,7 +561,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->Moshi\n+# NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->Moshi\n+# TODO cyril: modular\n class MoshiFlashAttention2(MoshiAttention):\n     \"\"\"\n     Moshi flash attention module. This module inherits from `MoshiAttention` as the weights of the module stays\n@@ -643,7 +678,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->Moshi\n+# NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->Moshi\n+# TODO cyril: modular\n class MoshiSdpaAttention(MoshiAttention):\n     \"\"\"\n     Moshi attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from"
        },
        {
            "sha": "f83bccb7e4f6f306f6644db557d7a326c8c63b50",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -324,7 +324,6 @@ class MusicgenFlashAttention2(MusicgenAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "dc0e9b882b20cfa85074d425ea7cbd0e0f023da2",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -340,7 +340,6 @@ class MusicgenMelodyFlashAttention2(MusicgenMelodyAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "a0a10bdc6f355092727a9d33f0866c3ee373a527",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -301,7 +301,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n+# NO LONGER EXIST Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n+# TODO cyril: modular\n class NemotronFlashAttention2(NemotronAttention):\n     \"\"\"\n     Nemotron flash attention module. This module inherits from `NemotronAttention` as the weights of the module stays\n@@ -415,7 +416,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n+# NO LONGER EXIST Copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n+# TODO cyril: modular\n class NemotronSdpaAttention(NemotronAttention):\n     \"\"\"\n     Nemotron attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n@@ -514,7 +516,8 @@ def forward(\n }\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n+# copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n+# no longer copied after attention refactors\n class NemotronDecoderLayer(nn.Module):\n     # Ignore copy\n     def __init__(self, config: NemotronConfig, layer_idx: int):"
        },
        {
            "sha": "11d3d99f4f72c9c71d468f33200e5dcd09429de2",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 189,
            "deletions": 482,
            "changes": 671,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -1,59 +1,35 @@\n-# coding=utf-8\n-# Copyright 2024 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n-#\n-# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n-# and OPT implementations in this library. It has been modified from its\n-# original forms to accommodate minor architectural differences compared\n-# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch OLMo model.\"\"\"\n-\n-import math\n-from typing import List, Optional, Tuple, Union\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/olmo/modular_olmo.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_olmo.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n+import torch.nn as nn\n import torch.nn.functional as F\n-import torch.utils.checkpoint\n-from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-)\n-from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_olmo import OlmoConfig\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n-\n _CONFIG_FOR_DOC = \"OlmoConfig\"\n \n \n@@ -71,78 +47,29 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         )\n \n \n-ALL_LAYERNORM_LAYERS.append(OlmoLayerNorm)\n-\n-\n-# copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Olmo\n-# TODO(joao): add me back asap :)\n-class OlmoRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n+class OlmoMLP(nn.Module):\n+    def __init__(self, config):\n         super().__init__()\n-        self.scaling_factor = scaling_factor\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        # For BC we register cos and sin cached\n-        self.max_seq_len_cached = max_position_embeddings\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-class OlmoLinearScalingRotaryEmbedding(OlmoRotaryEmbedding):\n-    \"\"\"OlmoRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: a scaling factor is aplied to the position ids\n-        position_ids = position_ids.float() / self.scaling_factor\n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n-\n-\n-class OlmoDynamicNTKScalingRotaryEmbedding(OlmoRotaryEmbedding):\n-    \"\"\"OlmoRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (\n-                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -170,22 +97,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-class OlmoMLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        self.act_fn = ACT2FN[config.hidden_act]\n-\n-    def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -198,167 +109,69 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class OlmoAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    # copied from transformers.models.llama.modeling_llama.LlamaAttention.__init__ with Llama->Olmo\n-    # TODO(joao): add me back asap :)\n-    def __init__(self, config: OlmoConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: OlmoConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n-        self._init_rope()\n-\n-    def _init_rope(self):\n-        if self.config.rope_scaling is None:\n-            self.rotary_emb = OlmoRotaryEmbedding(\n-                self.head_dim,\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n-        else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n-            if scaling_type == \"linear\":\n-                self.rotary_emb = OlmoLinearScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            elif scaling_type == \"dynamic\":\n-                self.rotary_emb = OlmoDynamicNTKScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        if self.config.clip_qkv is not None:\n-            query_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-            key_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-            value_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class OlmoFlashAttention2(OlmoAttention):\n-    \"\"\"\n-    OLMo flash attention module. This module inherits from `OlmoAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n         query_states = self.q_proj(hidden_states)\n         key_states = self.k_proj(hidden_states)\n@@ -369,189 +182,54 @@ def forward(\n             key_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n             value_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (OlmoRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class OlmoSdpaAttention(OlmoAttention):\n-    \"\"\"\n-    OLMo attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `OlmoAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from OlmoAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"OlmoModel is using OlmoSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        if self.config.clip_qkv is not None:\n-            query_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-            key_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-            value_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        # if attention_mask is not None and cache_position is not None:\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-OLMO_ATTENTION_CLASSES = {\n-    \"eager\": OlmoAttention,\n-    \"flash_attention_2\": OlmoFlashAttention2,\n-    \"sdpa\": OlmoSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class OlmoDecoderLayer(nn.Module):\n     def __init__(self, config: OlmoConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n-        self.self_attn = OLMO_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = OlmoAttention(config=config, layer_idx=layer_idx)\n \n         self.mlp = OlmoMLP(config)\n         self.input_layernorm = OlmoLayerNorm(config.hidden_size)\n         self.post_attention_layernorm = OlmoLayerNorm(config.hidden_size)\n \n-    # copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer.forward\n-    # TODO(joao): add me back asap :)\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -561,40 +239,23 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -606,16 +267,77 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n+class OlmoRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: OlmoConfig,\n+        device=None,\n+    ):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n OLMO_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -637,7 +359,6 @@ def forward(\n     \"The bare Olmo Model outputting raw hidden-states without any specific head on top.\",\n     OLMO_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel with Llama->Olmo\n class OlmoPreTrainedModel(PreTrainedModel):\n     config_class = OlmoConfig\n     base_model_prefix = \"model\"\n@@ -759,6 +480,7 @@ def __init__(self, config: OlmoConfig):\n             [OlmoDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = OlmoLayerNorm(config.hidden_size)\n+        self.rotary_emb = OlmoRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -771,20 +493,19 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(OLMO_INPUTS_DOCSTRING)\n-    # copied from transformers.models.llama.modeling_llama.LlamaModel.forward\n-    # TODO(joao): add me back asap :)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -805,41 +526,32 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n \n-        # embed positions\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n-        for decoder_layer in self.layers:\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n@@ -853,6 +565,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -863,13 +576,12 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -879,20 +591,14 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -959,7 +665,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1016,9 +721,12 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->OLMO,Llama->Olmo\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class OlmoForCausalLM(OlmoPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1049,13 +757,12 @@ def get_decoder(self):\n \n     @add_start_docstrings_to_model_forward(OLMO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n-    # Ignore copy\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1064,7 +771,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1085,18 +792,17 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, OlmoForCausalLM\n \n-        >>> model = OlmoForCausalLM.from_pretrained(\"allenai/OLMo-1B-hf\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")\n+        >>> model = OlmoForCausalLM.from_pretrained(\"meta-olmo/Olmo-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-olmo/Olmo-2-7b-hf\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        'Hey, are you conscious? Can you talk to me?\\nIâ€™m not sure if youâ€™re conscious of this, but Iâ€™m'\n-        ```\n-        \"\"\"\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1115,6 +821,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1123,7 +830,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "2a43e6f9c75d05a9e22fcf6ca59a32ea1bd61411",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "added",
            "additions": 126,
            "deletions": 0,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -0,0 +1,126 @@\n+from typing import Callable, Optional, Tuple\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+\n+from ...cache_utils import Cache\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...utils import logging\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaMLP,\n+    LlamaModel,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from .configuration_olmo import OlmoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class OlmoLayerNorm(nn.Module):\n+    \"\"\"LayerNorm but with no learnable weight or bias.\"\"\"\n+\n+    def __init__(self, hidden_size: int) -> None:\n+        super().__init__()\n+        self.normalized_shape = (hidden_size,)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        orig_dtype = hidden_states.dtype\n+        return F.layer_norm(hidden_states.to(dtype=torch.float32), self.normalized_shape, None, None, eps=1e-5).to(\n+            orig_dtype\n+        )\n+\n+\n+class OlmoMLP(LlamaMLP):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+\n+class OlmoAttention(LlamaAttention):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        if self.config.clip_qkv is not None:\n+            query_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n+            key_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n+            value_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n+\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class OlmoDecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: OlmoConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.input_layernorm = OlmoLayerNorm(config.hidden_size)\n+        self.post_attention_layernorm = OlmoLayerNorm(config.hidden_size)\n+        self.self_attn = OlmoAttention(config=config, layer_idx=layer_idx)\n+\n+\n+class OlmoModel(LlamaModel):\n+    def __init__(self, config: OlmoConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [OlmoDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = OlmoLayerNorm(config.hidden_size)\n+\n+\n+class OlmoForCausalLM(LlamaForCausalLM):\n+    pass"
        },
        {
            "sha": "83c3263de1f552042700609330429100aa2f2abc",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -5,6 +5,7 @@\n #                          modular_olmo2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n \n+\n from ...configuration_utils import PretrainedConfig\n \n "
        },
        {
            "sha": "49ae798e7f110115f7b181ae2f164e504bcc49c2",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 174,
            "deletions": 416,
            "changes": 590,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -4,35 +4,31 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_olmo2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n-from torch import nn\n+import torch.nn as nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_olmo2 import Olmo2Config\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n-\n _CONFIG_FOR_DOC = \"Olmo2Config\"\n \n \n@@ -56,66 +52,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Olmo2\n-# TODO(joao): add me back asap :)\n-class Olmo2RotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n-        super().__init__()\n-        self.scaling_factor = scaling_factor\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        # For BC we register cos and sin cached\n-        self.max_seq_len_cached = max_position_embeddings\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-class Olmo2LinearScalingRotaryEmbedding(Olmo2RotaryEmbedding):\n-    \"\"\"Olmo2RotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: a scaling factor is aplied to the position ids\n-        position_ids = position_ids.float() / self.scaling_factor\n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n-\n-\n-class Olmo2DynamicNTKScalingRotaryEmbedding(Olmo2RotaryEmbedding):\n-    \"\"\"Olmo2RotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def forward(self, x, position_ids):\n-        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_position_embeddings:\n-            base = self.base * (\n-                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n-            ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (\n-                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n-\n-        cos, sin = super().forward(x, position_ids)\n-        return cos, sin\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -162,316 +98,112 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Olmo2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    # copied from transformers.models.llama.modeling_llama.LlamaAttention.__init__ with Llama->Olmo2\n-    # TODO(joao): add me back asap :)\n     def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n-        self._init_rope()\n-        self.q_norm = Olmo2RMSNorm(self.num_heads * self.head_dim, config.rms_norm_eps)\n-        self.k_norm = Olmo2RMSNorm(self.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n-\n-    def _init_rope(self):\n-        if self.config.rope_scaling is None:\n-            self.rotary_emb = Olmo2RotaryEmbedding(\n-                self.head_dim,\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n-        else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            scaling_factor = self.config.rope_scaling[\"factor\"]\n-            if scaling_type == \"linear\":\n-                self.rotary_emb = Olmo2LinearScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            elif scaling_type == \"dynamic\":\n-                self.rotary_emb = Olmo2DynamicNTKScalingRotaryEmbedding(\n-                    self.head_dim,\n-                    max_position_embeddings=self.max_position_embeddings,\n-                    scaling_factor=scaling_factor,\n-                    base=self.rope_theta,\n-                )\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_norm(self.q_proj(hidden_states))\n-        key_states = self.k_norm(self.k_proj(hidden_states))\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Olmo2FlashAttention2(Olmo2Attention):\n-    \"\"\"\n-    Olmo2 flash attention module. This module inherits from `Olmo2Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-\n-    OLMo2 flash attention module. This module inherits from `Olmo2Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n+        self.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n         query_states = self.q_norm(self.q_proj(hidden_states))\n         key_states = self.k_norm(self.k_proj(hidden_states))\n         value_states = self.v_proj(hidden_states)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (OlmoRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Olmo2SdpaAttention(Olmo2Attention):\n-    \"\"\"\n-    Olmo2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Olmo2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from Olmo2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Olmo2Model is using Olmo2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-        bsz, q_len, _ = hidden_states.size()\n-        query_states = self.q_norm(self.q_proj(hidden_states))\n-        key_states = self.k_norm(self.k_proj(hidden_states))\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        causal_mask = attention_mask\n-        # if attention_mask is not None and cache_position is not None:\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-        return attn_output, None, past_key_value\n+        return attn_output, attn_weights\n \n \n class Olmo2MLP(nn.Module):\n@@ -486,29 +218,20 @@ def __init__(self, config):\n         self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n-\n-OLMO2_ATTENTION_CLASSES = {\n-    \"eager\": Olmo2Attention,\n-    \"flash_attention_2\": Olmo2FlashAttention2,\n-    \"sdpa\": Olmo2SdpaAttention,\n-}\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n class Olmo2DecoderLayer(nn.Module):\n     def __init__(self, config: Olmo2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n-        self.self_attn = OLMO2_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = Olmo2Attention(config=config, layer_idx=layer_idx)\n \n         self.mlp = Olmo2MLP(config)\n         self.post_attention_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    # copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer.forward\n-    # TODO(joao): add me back asap :)\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -518,38 +241,21 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = self.post_attention_layernorm(hidden_states)\n@@ -564,11 +270,75 @@ def forward(\n         outputs = (hidden_states,)\n         if output_attentions:\n             outputs += (self_attn_weights,)\n-        if use_cache:\n-            outputs += (present_key_value,)\n+\n         return outputs\n \n \n+class Olmo2RotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: Olmo2Config,\n+        device=None,\n+    ):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n OLMO2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -711,6 +481,7 @@ def __init__(self, config: Olmo2Config):\n             [Olmo2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Olmo2RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -723,20 +494,19 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(OLMO2_INPUTS_DOCSTRING)\n-    # copied from transformers.models.llama.modeling_llama.LlamaModel.forward\n-    # TODO(joao): add me back asap :)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -757,41 +527,32 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n \n-        # embed positions\n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n-        for decoder_layer in self.layers:\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n@@ -805,6 +566,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -815,13 +577,12 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -831,18 +592,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -966,11 +722,14 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->OLMO2,Llama->Olmo2\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class Olmo2ForCausalLM(Olmo2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n-    def __init__(self, config: Olmo2Config):\n+    def __init__(self, config):\n         super().__init__(config)\n         self.model = Olmo2Model(config)\n         self.vocab_size = config.vocab_size\n@@ -999,13 +758,12 @@ def get_decoder(self):\n \n     @add_start_docstrings_to_model_forward(OLMO2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n-    # Ignore copy\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1014,7 +772,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1035,18 +793,17 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, Olmo2ForCausalLM\n \n-        >>> model = Olmo2ForCausalLM.from_pretrained(\"allenai/Olmo2-1B-hf\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/Olmo2-1B-hf\")\n+        >>> model = Olmo2ForCausalLM.from_pretrained(\"meta-olmo2/Olmo2-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-olmo2/Olmo2-2-7b-hf\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        'Hey, are you conscious? Can you talk to me?\\nIâ€™m not sure if youâ€™re conscious of this, but Iâ€™m'\n-        ```\n-        \"\"\"\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1065,6 +822,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1073,7 +831,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "5f1191708044661819e3ed672a07f325d465d619",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 37,
            "deletions": 229,
            "changes": 266,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -1,30 +1,23 @@\n-import math\n-from typing import Optional, Tuple\n+from typing import Callable, Optional, Tuple\n \n import torch\n from torch import nn\n \n from ...cache_utils import Cache\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n-from ...utils import is_flash_attn_2_available, is_flash_attn_greater_or_equal_2_10, logging\n-from ..llama.modeling_llama import LlamaRMSNorm\n+from ...utils import logging\n+from ..llama.modeling_llama import LlamaRMSNorm, eager_attention_forward\n from ..olmo.configuration_olmo import OlmoConfig\n from ..olmo.modeling_olmo import (\n     OlmoAttention,\n     OlmoDecoderLayer,\n-    OlmoFlashAttention2,\n     OlmoForCausalLM,\n     OlmoModel,\n-    OlmoPreTrainedModel,\n-    OlmoSdpaAttention,\n     apply_rotary_pos_emb,\n-    repeat_kv,\n )\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -170,242 +163,61 @@ class Olmo2RMSNorm(LlamaRMSNorm):\n class Olmo2Attention(OlmoAttention):\n     def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         super().__init__(config, layer_idx=layer_idx)\n-        self.q_norm = Olmo2RMSNorm(self.num_heads * self.head_dim, config.rms_norm_eps)\n-        self.k_norm = Olmo2RMSNorm(self.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n+        self.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n+        self.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_norm(self.q_proj(hidden_states))\n-        key_states = self.k_norm(self.k_proj(hidden_states))\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Olmo2FlashAttention2(OlmoFlashAttention2, Olmo2Attention):\n-    \"\"\"\n-    OLMo2 flash attention module. This module inherits from `Olmo2Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        Olmo2Attention.__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n         query_states = self.q_norm(self.q_proj(hidden_states))\n         key_states = self.k_norm(self.k_proj(hidden_states))\n         value_states = self.v_proj(hidden_states)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (OlmoRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Olmo2SdpaAttention(OlmoSdpaAttention, Olmo2Attention):\n-    # Adapted from Olmo2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Olmo2Model is using Olmo2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-        bsz, q_len, _ = hidden_states.size()\n-        query_states = self.q_norm(self.q_proj(hidden_states))\n-        key_states = self.k_norm(self.k_proj(hidden_states))\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        causal_mask = attention_mask\n-        # if attention_mask is not None and cache_position is not None:\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-        attn_output = self.o_proj(attn_output)\n-        return attn_output, None, past_key_value\n+        return attn_output, attn_weights\n \n \n # The OLMo2 layers are identical to those of the OLMo model except:\n@@ -416,6 +228,7 @@ def __init__(self, config: Olmo2Config, layer_idx: int):\n         super().__init__(config, layer_idx=layer_idx)\n         self.post_attention_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.self_attn = Olmo2Attention(config=config, layer_idx=layer_idx)\n         del self.input_layernorm\n \n     def forward(\n@@ -427,19 +240,21 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = self.post_attention_layernorm(hidden_states)\n@@ -454,36 +269,29 @@ def forward(\n         outputs = (hidden_states,)\n         if output_attentions:\n             outputs += (self_attn_weights,)\n-        if use_cache:\n-            outputs += (present_key_value,)\n-        return outputs\n-\n \n-class Olmo2PreTrainedModel(OlmoPreTrainedModel):\n-    pass\n+        return outputs\n \n \n # The OLMo2 model is identical to the OLMo model, except RMSNorm is used instead of\n # standard layer norm for the output norm.\n class Olmo2Model(OlmoModel):\n     def __init__(self, config: Olmo2Config):\n         super().__init__(config)\n+        self.norm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.layers = nn.ModuleList(\n             [Olmo2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.norm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n \n # The heads now only need to redefine the model inside to the correct `RobertaModel`\n class Olmo2ForCausalLM(OlmoForCausalLM):\n-    def __init__(self, config: Olmo2Config):\n-        super().__init__(config)\n-        self.model = Olmo2Model(config)\n+    pass\n \n \n __all__ = [\n     \"Olmo2Config\",\n     \"Olmo2ForCausalLM\",\n     \"Olmo2Model\",\n-    \"Olmo2PreTrainedModel\",\n+    \"Olmo2PreTrainedModel\",  # noqa: F822\n ]"
        },
        {
            "sha": "fa3c2f3cd4d11b132cd3a60c04762ef3dd8993c0",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 31,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -160,40 +160,18 @@ def extra_repr(self):\n class OlmoeRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: OlmoeConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[OlmoeConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`OlmoeRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -293,7 +271,8 @@ def __init__(self, config):\n         self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n # Copied from transformers.models.llama.modeling_llama.repeat_kv\n@@ -422,7 +401,6 @@ class OlmoeFlashAttention2(OlmoeAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "3350ae1a23c2b77286436304751cd60c5ab9cd24",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -257,7 +257,6 @@ class OptFlashAttention2(OPTAttention):\n     attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "8d3c20b9ace717a71525a1ecf91cecefe6537371",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 29,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -59,40 +59,18 @@\n class PersimmonRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: PersimmonConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[PersimmonConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`PersimmonRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]"
        },
        {
            "sha": "477896decd5318b1ae600fee203900f3da82d080",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 226,
            "deletions": 600,
            "changes": 826,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -1,161 +1,52 @@\n-# coding=utf-8\n-# Copyright 2023 Microsoft and the HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"PyTorch Phi model.\"\"\"\n-\n-import math\n-from typing import List, Optional, Tuple, Union\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/phi/modular_phi.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_phi.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n-from packaging import version\n-from torch import nn\n-from torch.nn import CrossEntropyLoss\n+import torch.nn as nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    get_torch_version,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_phi import PhiConfig\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"microsoft/phi-1\"\n+_CHECKPOINT_FOR_DOC = \"meta-phi/Phi-2-7b-hf\"\n _CONFIG_FOR_DOC = \"PhiConfig\"\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Phi\n-class PhiRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n-        device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[PhiConfig] = None,\n-    ):\n-        super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n-        self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`PhiRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n-        else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -183,23 +74,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-# Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Phi\n-class PhiMLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.activation_fn = ACT2FN[config.hidden_act]\n-        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n-        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        hidden_states = self.fc1(hidden_states)\n-        hidden_states = self.activation_fn(hidden_states)\n-        hidden_states = self.fc2(hidden_states)\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv with llama->phi\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -212,190 +86,79 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class PhiAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: PhiConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: PhiConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.rope_theta = config.rope_theta\n-        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n         self.is_causal = True\n-\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n-        self.dense = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=True)\n-\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.dense = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=True)\n+        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n         self.qk_layernorm = config.qk_layernorm\n         if self.qk_layernorm:\n             self.q_layernorm = nn.LayerNorm(\n-                config.hidden_size // self.num_heads, eps=config.layer_norm_eps, elementwise_affine=True\n+                config.hidden_size // config.num_attention_heads, eps=config.layer_norm_eps, elementwise_affine=True\n             )\n             self.k_layernorm = nn.LayerNorm(\n-                config.hidden_size // self.num_heads, eps=config.layer_norm_eps, elementwise_affine=True\n-            )\n-\n-        self.rotary_emb = PhiRotaryEmbedding(config=self.config)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        if self.qk_layernorm:\n-            query_states = self.q_layernorm(query_states)\n-            key_states = self.k_layernorm(key_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-\n-        # Partial rotary embedding\n-        query_rot, query_pass = (\n-            query_states[..., : self.rotary_ndims],\n-            query_states[..., self.rotary_ndims :],\n-        )\n-        key_rot, key_pass = (\n-            key_states[..., : self.rotary_ndims],\n-            key_states[..., self.rotary_ndims :],\n-        )\n-        # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n-        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n-\n-        # [batch_size, seq_length, num_heads, head_dim]\n-        query_states = torch.cat((query_rot, query_pass), dim=-1)\n-        key_states = torch.cat((key_rot, key_pass), dim=-1)\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_ndims,\n-                \"cache_position\": cache_position,\n-            }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        # Queries and keys upcast to fp32 is required by Phi-2 to avoid overflow\n-        attn_weights = torch.matmul(\n-            query_states.to(torch.float32), key_states.to(torch.float32).transpose(2, 3)\n-        ) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights += causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n+                config.hidden_size // config.num_attention_heads, eps=config.layer_norm_eps, elementwise_affine=True\n             )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.dense(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class PhiFlashAttention2(PhiAttention):\n-    \"\"\"\n-    Phi flash attention module. This module inherits from `PhiAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # PhiFlashAttention2 attention does not support output_attentions\n-\n-        output_attentions = False\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         if self.qk_layernorm:\n             query_states = self.q_layernorm(query_states)\n             key_states = self.k_layernorm(key_states)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n         cos, sin = position_embeddings\n-\n         # Partial rotary embedding\n         query_rot, query_pass = (\n             query_states[..., : self.rotary_ndims],\n@@ -413,199 +176,55 @@ def forward(\n         key_states = torch.cat((key_rot, key_pass), dim=-1)\n \n         if past_key_value is not None:\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_ndims,\n-                \"cache_position\": cache_position,\n-            }\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        attn_dropout = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32.\n-\n-        if query_states.dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=attn_dropout,\n-            softmax_scale=None,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.dense(attn_output)\n+        return attn_output, attn_weights\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class PhiSdpaAttention(PhiAttention):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n-\n-    \"\"\"\n-    SDPA attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `PhiAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from PhiAttention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"PhiModel is using PhiSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not \"\n-                \"support `output_attentions=True`. Falling back to the manual attention implementation, but specifying \"\n-                \"the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can \"\n-                'be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        if self.qk_layernorm:\n-            query_states = self.q_layernorm(query_states)\n-            key_states = self.k_layernorm(key_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-\n-        # Partial rotary embedding\n-        query_rot, query_pass = (\n-            query_states[..., : self.rotary_ndims],\n-            query_states[..., self.rotary_ndims :],\n-        )\n-        key_rot, key_pass = (\n-            key_states[..., : self.rotary_ndims],\n-            key_states[..., self.rotary_ndims :],\n-        )\n-        # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n-        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n-\n-        # [batch_size, seq_length, num_heads, head_dim]\n-        query_states = torch.cat((query_rot, query_pass), dim=-1)\n-        key_states = torch.cat((key_rot, key_pass), dim=-1)\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"partial_rotation_size\": self.rotary_ndims,\n-                \"cache_position\": cache_position,\n-            }\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.dense(attn_output)\n-\n-        return attn_output, None, past_key_value\n \n+class PhiMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n \n-PHI_ATTENTION_CLASSES = {\n-    \"eager\": PhiAttention,\n-    \"flash_attention_2\": PhiFlashAttention2,\n-    \"sdpa\": PhiSdpaAttention,\n-}\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n \n \n class PhiDecoderLayer(nn.Module):\n     def __init__(self, config: PhiConfig, layer_idx: int):\n         super().__init__()\n-        self.self_attn = PHI_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n+        self.self_attn = PhiAttention(config, layer_idx=layer_idx)\n         self.mlp = PhiMLP(config)\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n@@ -615,45 +234,19 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-                Indices of positions of each input sequence tokens in the position embeddings. Selected in the range\n-                `[0, config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        attn_outputs, self_attn_weights, present_key_value = self.self_attn(\n+        attn_outputs, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -662,6 +255,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         attn_outputs = self.resid_dropout(attn_outputs)\n \n@@ -672,12 +266,74 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n+class PhiRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: PhiConfig,\n+        device=None,\n+    ):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n PHI_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -704,12 +360,12 @@ class PhiPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PhiDecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n+    _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n-    _supports_static_cache = True\n     _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -816,17 +472,14 @@ def __init__(self, config: PhiConfig):\n         self.vocab_size = config.vocab_size\n \n         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n-        self.embed_dropout = nn.Dropout(config.embd_pdrop)\n         self.layers = nn.ModuleList(\n             [PhiDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.final_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.rotary_emb = PhiRotaryEmbedding(config=config)\n-\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n-\n         self.gradient_checkpointing = False\n+        self.embed_dropout = nn.Dropout(config.embd_pdrop)\n+        self.final_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -842,62 +495,51 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n         causal_mask = self._update_causal_mask(\n             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n \n-        inputs_embeds = self.embed_dropout(inputs_embeds)\n+        inputs_embeds = self.embed_dropout(inputs_embeds)  # diff with Llama\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n@@ -906,9 +548,8 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n-        for decoder_layer in self.layers:\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n@@ -918,9 +559,9 @@ def forward(\n                     hidden_states,\n                     causal_mask,\n                     position_ids,\n+                    past_key_values,\n                     output_attentions,\n                     use_cache,\n-                    past_key_values,\n                     cache_position,\n                     position_embeddings,\n                 )\n@@ -934,36 +575,28 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n-        hidden_states = self.final_layernorm(hidden_states)\n+        hidden_states = self.final_layernorm(hidden_states)  # diff with Llama\n \n         # add hidden states from the last decoder layer\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1030,7 +663,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1087,40 +719,37 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class PhiForCausalLM(PhiPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Phi,bias=False->bias=True\n     def __init__(self, config):\n         super().__init__(config)\n         self.model = PhiModel(config)\n         self.vocab_size = config.vocab_size\n-        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.model.embed_tokens\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings\n     def get_output_embeddings(self):\n         return self.lm_head\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder\n     def get_decoder(self):\n         return self.model\n \n@@ -1131,7 +760,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1140,7 +769,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1161,18 +790,17 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, PhiForCausalLM\n \n-        >>> model = PhiForCausalLM.from_pretrained(\"microsoft/phi-1\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n+        >>> model = PhiForCausalLM.from_pretrained(\"meta-phi/Phi-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-phi/Phi-2-7b-hf\")\n \n-        >>> prompt = \"This is an example script .\"\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        'This is an example script .\\n\\n\\n\\nfrom typing import List\\n\\ndef find_most_common_letter(words: List[str'\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1191,6 +819,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1199,7 +828,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1216,7 +845,7 @@ def forward(\n \n @add_start_docstrings(\n     \"\"\"\n-    The PhiModel with a sequence classification head on top (linear layer).\n+    The Phi Model transformer with a sequence classification head on top (linear layer).\n \n     [`PhiForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n     (e.g. GPT-2) do.\n@@ -1229,7 +858,6 @@ def forward(\n     \"\"\",\n     PHI_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with LLAMA->PHI,Llama->Phi with self.transformer->self.model, transformer_outputs->model_outputs\n class PhiForSequenceClassification(PhiPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1268,7 +896,7 @@ def forward(\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        model_outputs = self.model(\n+        transformer_outputs = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1279,7 +907,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n-        hidden_states = model_outputs[0]\n+        hidden_states = transformer_outputs[0]\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1307,44 +935,48 @@ def forward(\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n         if not return_dict:\n-            output = (pooled_logits,) + model_outputs[1:]\n+            output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n-            past_key_values=model_outputs.past_key_values,\n-            hidden_states=model_outputs.hidden_states,\n-            attentions=model_outputs.attentions,\n+            past_key_values=transformer_outputs.past_key_values,\n+            hidden_states=transformer_outputs.hidden_states,\n+            attentions=transformer_outputs.attentions,\n         )\n \n \n @add_start_docstrings(\n     \"\"\"\n-    PhiModel with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n+    The Phi Model transformer with a token classification head on top (a linear layer on top of the hidden-states\n+    output) e.g. for Named-Entity-Recognition (NER) tasks.\n     \"\"\",\n     PHI_START_DOCSTRING,\n )\n-# Copied from transformers.models.mpt.modeling_mpt.MptForTokenClassification with MPT->PHI,Mpt->Phi,self.transformer->self.model,transformer_outputs->model_outputs\n class PhiForTokenClassification(PhiPreTrainedModel):\n-    def __init__(self, config: PhiConfig):\n+    def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n-\n         self.model = PhiModel(config)\n-        if hasattr(config, \"classifier_dropout\") and config.classifier_dropout is not None:\n+        if getattr(config, \"classifier_dropout\", None) is not None:\n             classifier_dropout = config.classifier_dropout\n-        elif hasattr(config, \"hidden_dropout\") and config.hidden_dropout is not None:\n+        elif getattr(config, \"hidden_dropout\", None) is not None:\n             classifier_dropout = config.hidden_dropout\n         else:\n             classifier_dropout = 0.1\n         self.dropout = nn.Dropout(classifier_dropout)\n-        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+        self.score = nn.Linear(config.hidden_size, config.num_labels)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n     @add_start_docstrings_to_model_forward(PHI_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1354,16 +986,16 @@ def __init__(self, config: PhiConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        **deprecated_arguments,\n-    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> Union[Tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1372,38 +1004,32 @@ def forward(\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        model_outputs = self.model(\n+        outputs = self.model(\n             input_ids,\n-            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n-\n-        hidden_states = model_outputs[0]\n-        hidden_states = self.dropout(hidden_states)\n-        logits = self.classifier(hidden_states)\n+        sequence_output = outputs[0]\n+        sequence_output = self.dropout(sequence_output)\n+        logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            batch_size, seq_length = labels.shape\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(\n-                logits.view(batch_size * seq_length, self.num_labels), labels.view(batch_size * seq_length)\n-            )\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n-            output = (logits,) + model_outputs[2:]\n+            output = (logits,) + outputs[2:]\n             return ((loss,) + output) if loss is not None else output\n \n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n-            hidden_states=model_outputs.hidden_states,\n-            attentions=model_outputs.attentions,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n         )"
        },
        {
            "sha": "0faa4629f1a768f360826aaa0cba6f0c4de8210e",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "added",
            "additions": 295,
            "deletions": 0,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -0,0 +1,295 @@\n+from typing import Callable, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import logging\n+from ..clip.modeling_clip import CLIPMLP\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaForCausalLM,\n+    LlamaForSequenceClassification,\n+    LlamaForTokenClassification,\n+    LlamaModel,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,  # copied from Llama\n+)\n+from .configuration_phi import PhiConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class PhiAttention(LlamaAttention):\n+    def __init__(self, config: PhiConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.dense = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=True)\n+        del self.o_proj\n+        self.rotary_ndims = int(self.head_dim * config.partial_rotary_factor)\n+        self.qk_layernorm = config.qk_layernorm\n+        if self.qk_layernorm:\n+            self.q_layernorm = nn.LayerNorm(\n+                config.hidden_size // config.num_attention_heads, eps=config.layer_norm_eps, elementwise_affine=True\n+            )\n+            self.k_layernorm = nn.LayerNorm(\n+                config.hidden_size // config.num_attention_heads, eps=config.layer_norm_eps, elementwise_affine=True\n+            )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        if self.qk_layernorm:\n+            query_states = self.q_layernorm(query_states)\n+            key_states = self.k_layernorm(key_states)\n+\n+        cos, sin = position_embeddings\n+        # Partial rotary embedding\n+        query_rot, query_pass = (\n+            query_states[..., : self.rotary_ndims],\n+            query_states[..., self.rotary_ndims :],\n+        )\n+        key_rot, key_pass = (\n+            key_states[..., : self.rotary_ndims],\n+            key_states[..., self.rotary_ndims :],\n+        )\n+        # [batch_size, seq_length, num_heads, head_dim // config.partial_rotary_factor]\n+        query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n+\n+        # [batch_size, seq_length, num_heads, head_dim]\n+        query_states = torch.cat((query_rot, query_pass), dim=-1)\n+        key_states = torch.cat((key_rot, key_pass), dim=-1)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.dense(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class PhiMLP(CLIPMLP):\n+    pass\n+\n+\n+class PhiDecoderLayer(nn.Module):\n+    def __init__(self, config: PhiConfig, layer_idx: int):\n+        super().__init__()\n+        self.self_attn = PhiAttention(config, layer_idx=layer_idx)\n+        self.mlp = PhiMLP(config)\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        attn_outputs, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        attn_outputs = self.resid_dropout(attn_outputs)\n+\n+        feed_forward_hidden_states = self.resid_dropout(self.mlp(hidden_states))\n+        hidden_states = attn_outputs + feed_forward_hidden_states + residual\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class PhiModel(LlamaModel):\n+    def __init__(self, config: PhiConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [PhiDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.embed_dropout = nn.Dropout(config.embd_pdrop)\n+        self.final_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        del self.norm\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        inputs_embeds = self.embed_dropout(inputs_embeds)  # diff with Llama\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.final_layernorm(hidden_states)  # diff with Llama\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class PhiForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+class PhiForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+class PhiForTokenClassification(LlamaForTokenClassification):\n+    pass"
        },
        {
            "sha": "908fd982b9c73cd8eb1932694b010d9e43ede49a",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -74,7 +74,8 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# Copied from transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding with gemma->phi3, Gemma->Phi3\n+# copied from transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding with gemma->phi3, Gemma->Phi3\n+# TODO cyril: modular\n class Phi3RotaryEmbedding(nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         super().__init__()\n@@ -431,7 +432,6 @@ class Phi3FlashAttention2(Phi3Attention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n@@ -550,8 +550,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention with Llama->Phi3\n-# TODO @Arthur no longer copied from LLama after static cache\n+# NO LONGER EXIST copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention with Llama->Phi3\n+# TODO cyril: modular\n class Phi3SdpaAttention(Phi3Attention):\n     \"\"\"\n     Phi3 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from"
        },
        {
            "sha": "cd54b226e1d85cbe9759072f17e357bff0f84f91",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -186,7 +186,6 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.mixtral.modeling_mixtral.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -912,10 +911,12 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PhimoeDecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n+    _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "03886d4a528478be2b5dc8a61b7d2d8413cf891c",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -216,15 +216,17 @@ def forward(\n class PixtralMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n+        self.config = config\n         self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n         self.act_fn = ACT2FN[config.hidden_act]\n \n-    def forward(self, hidden_state):\n-        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Pixtral"
        },
        {
            "sha": "36fb1ddf1390acc4ecaaf4da319544d302cee9e1",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 227,
            "deletions": 566,
            "changes": 793,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -1,36 +1,19 @@\n-# coding=utf-8\n-# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n-#\n-# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n-# and OPT implementations in this library. It has been modified from its\n-# original forms to accommodate minor architectural differences compared\n-# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Qwen2 model.\"\"\"\n-\n-import math\n-from typing import List, Optional, Tuple, Union\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/qwen2/modular_qwen2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_qwen2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -39,148 +22,48 @@\n     TokenClassifierOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_qwen2 import Qwen2Config\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n-\n-_CHECKPOINT_FOR_DOC = \"Qwen/Qwen2-7B\"\n+_CHECKPOINT_FOR_DOC = \"meta-qwen2/Qwen2-2-7b-hf\"\n _CONFIG_FOR_DOC = \"Qwen2Config\"\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Qwen2\n-class Qwen2RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        Qwen2RMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Qwen2\n-class Qwen2RotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n-        device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[Qwen2Config] = None,\n-    ):\n+class Qwen2MLP(nn.Module):\n+    def __init__(self, config):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n-        self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`Qwen2RotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n-        else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n-\n         self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    def _dynamic_frequency_update(self, position_ids, device):\n-        \"\"\"\n-        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n-        1 - growing beyond the cached sequence length (allow scaling)\n-        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n-        \"\"\"\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n-            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n-            self.max_seq_len_cached = seq_len\n-\n-        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n-            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n-            self.max_seq_len_cached = self.original_max_seq_len\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids):\n-        if \"dynamic\" in self.rope_type:\n-            self._dynamic_frequency_update(position_ids, device=x.device)\n-\n-        # Core RoPE block\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-\n-        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n-        cos = cos * self.attention_scaling\n-        sin = sin * self.attention_scaling\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n \n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -208,22 +91,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-# Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Qwen2\n-class Qwen2MLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        self.act_fn = ACT2FN[config.hidden_act]\n-\n-    def forward(self, hidden_state):\n-        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -236,366 +103,160 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Qwen2Attention(nn.Module):\n-    \"\"\"\n-    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n-    and \"Generating Long Sequences with Sparse Transformers\".\n-    \"\"\"\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Qwen2Config, layer_idx: Optional[int] = None):\n+    def __init__(self, config: Qwen2Config, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will \"\n-                \"to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-        self.is_causal = True\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n-\n-        self.rotary_emb = Qwen2RotaryEmbedding(config=self.config)\n+        self.is_causal = True\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Qwen2FlashAttention2(Qwen2Attention):\n-    \"\"\"\n-    Qwen2 flash attention module, following Qwen2 attention module. This module inherits from `Qwen2Attention`\n-    as the weights of the module stays untouched. The only required change would be on the forward pass\n-    where it needs to correctly call the public API of flash attention and deal with padding tokens\n-    in case the input contains any of them. Additionally, for sliding window attention, we apply SWA only to the bottom\n-    config.max_window_layers layers.\n-    \"\"\"\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ):\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n+        sliding_window = None\n         if (\n             self.config.use_sliding_window\n             and getattr(self.config, \"sliding_window\", None) is not None\n             and self.layer_idx >= self.config.max_window_layers\n         ):\n             sliding_window = self.config.sliding_window\n-        else:\n-            sliding_window = None\n \n-        attn_output = _flash_attention_forward(\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=sliding_window,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=sliding_window,  # main diff with Llama\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Qwen2SdpaAttention(Qwen2Attention):\n-    \"\"\"\n-    Qwen2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Qwen2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from Qwen2Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Qwen2Model is using Qwen2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n+class Qwen2RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Qwen2RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n \n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n \n-QWEN2_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2Attention,\n-    \"flash_attention_2\": Qwen2FlashAttention2,\n-    \"sdpa\": Qwen2SdpaAttention,\n-}\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n class Qwen2DecoderLayer(nn.Module):\n     def __init__(self, config: Qwen2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n+        self.self_attn = Qwen2Attention(config=config, layer_idx=layer_idx)\n+        self.mlp = Qwen2MLP(config)\n+        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         if config.sliding_window and config._attn_implementation != \"flash_attention_2\":\n             logger.warning_once(\n                 f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n                 \"unexpected results may be encountered.\"\n             )\n-        self.self_attn = QWEN2_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n-\n-        self.mlp = Qwen2MLP(config)\n-        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -604,6 +265,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -614,16 +276,77 @@ def forward(\n         hidden_states = residual + hidden_states\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n+class Qwen2RotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: Qwen2Config,\n+        device=None,\n+    ):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n QWEN2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -650,7 +373,7 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2DecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n+    _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n@@ -690,7 +413,7 @@ def _init_weights(self, module):\n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n             [`PreTrainedTokenizer.__call__`] for details.\n \n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n             `past_key_values`).\n \n             If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n@@ -765,11 +488,10 @@ def __init__(self, config: Qwen2Config):\n         self.layers = nn.ModuleList(\n             [Qwen2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self._attn_implementation = config._attn_implementation\n         self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = Qwen2RotaryEmbedding(config=config)\n-\n         self.gradient_checkpointing = False\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -785,54 +507,43 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n@@ -848,9 +559,8 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n-        for decoder_layer in self.layers:\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n@@ -876,13 +586,11 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -892,20 +600,14 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -924,30 +626,21 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n                 past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n                 is_training=self.training,\n             ):\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n-        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n+        if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -964,8 +657,6 @@ def _update_causal_mask(\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n         )\n \n         if (\n@@ -977,12 +668,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Qwen2\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -991,20 +682,21 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n-        config: Qwen2Config,\n-        past_key_values: Cache,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n             attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n             sequence_length (`int`):\n                 The sequence length being processed.\n             target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n@@ -1013,10 +705,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n-            config (`Qwen2Config`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n         \"\"\"\n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n@@ -1026,30 +714,25 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             causal_mask = torch.full(\n                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-            if config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n-                        cache_position.reshape(-1, 1) - config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n                 )\n+\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class Qwen2ForCausalLM(Qwen2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -1088,7 +771,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1097,7 +780,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1118,8 +801,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, Qwen2ForCausalLM\n \n-        >>> model = Qwen2ForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n-        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n+        >>> model = Qwen2ForCausalLM.from_pretrained(\"meta-qwen2/Qwen2-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-qwen2/Qwen2-2-7b-hf\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n@@ -1129,7 +812,6 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1148,6 +830,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1156,7 +839,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1205,10 +888,10 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1260,27 +943,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n@@ -1301,7 +965,6 @@ def forward(\n     \"\"\",\n     QWEN2_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->Qwen2, LLAMA->QWEN2\n class Qwen2ForTokenClassification(Qwen2PreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1390,24 +1053,22 @@ def forward(\n     \"\"\",\n     QWEN2_START_DOCSTRING,\n )\n-# Copied from transformers.models.mistral.modeling_mistral.MistralForQuestionAnswering with Mistral->Qwen2, MISTRAL->QWEN2\n class Qwen2ForQuestionAnswering(Qwen2PreTrainedModel):\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"transformer\"\n \n-    # Copied from models.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Qwen2\n     def __init__(self, config):\n         super().__init__(config)\n-        self.model = Qwen2Model(config)\n+        self.transformer = Qwen2Model(config)\n         self.qa_outputs = nn.Linear(config.hidden_size, 2)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n     def get_input_embeddings(self):\n-        return self.model.embed_tokens\n+        return self.transformer.embed_tokens\n \n     def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n+        self.transformer.embed_tokens = value\n \n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     def forward(\n@@ -1436,7 +1097,7 @@ def forward(\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        outputs = self.model(\n+        outputs = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,"
        },
        {
            "sha": "718abd01090c2b053883e5456242d27eeb8594c8",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "added",
            "additions": 134,
            "deletions": 0,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -0,0 +1,134 @@\n+from typing import Callable, Optional, Tuple\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...cache_utils import Cache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import logging\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaForQuestionAnswering,\n+    LlamaForSequenceClassification,\n+    LlamaForTokenClassification,\n+    LlamaMLP,\n+    LlamaModel,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from .configuration_qwen2 import Qwen2Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Qwen2MLP(LlamaMLP):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+\n+class Qwen2Attention(LlamaAttention):\n+    def __init__(self, config: Qwen2Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        sliding_window = None\n+        if (\n+            self.config.use_sliding_window\n+            and getattr(self.config, \"sliding_window\", None) is not None\n+            and self.layer_idx >= self.config.max_window_layers\n+        ):\n+            sliding_window = self.config.sliding_window\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=sliding_window,  # main diff with Llama\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Qwen2DecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: Qwen2Config, layer_idx: int):\n+        super().__init__()\n+        self.self_attn = Qwen2Attention(config=config, layer_idx=layer_idx)\n+        self.mlp = Qwen2MLP(config)\n+        if config.sliding_window and config._attn_implementation != \"flash_attention_2\":\n+            logger.warning_once(\n+                f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n+                \"unexpected results may be encountered.\"\n+            )\n+\n+\n+class Qwen2Model(LlamaModel):\n+    pass\n+\n+\n+class Qwen2ForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+class Qwen2ForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+class Qwen2ForTokenClassification(LlamaForTokenClassification):\n+    pass\n+\n+\n+class Qwen2ForQuestionAnswering(LlamaForQuestionAnswering):\n+    pass"
        },
        {
            "sha": "44a5b5ce315570e8c5dd922299b711e8a9083e1c",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -223,7 +223,6 @@ class Qwen2AudioFlashAttention2(Qwen2AudioAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "1ce41509a5c0d1bbb1854bd4ac850b0956a413a1",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 35,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -169,40 +169,18 @@ def extra_repr(self):\n class Qwen2MoeRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: Qwen2MoeConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[Qwen2MoeConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`Qwen2MoeRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -318,7 +296,8 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2Attention with Qwen2->Qwen2Moe\n+# copied from transformers.models.qwen2.modeling_qwen2.Qwen2Attention with Qwen2->Qwen2Moe\n+# no longer copied after attention refactors\n class Qwen2MoeAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n@@ -419,7 +398,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2FlashAttention2 with Qwen2->Qwen2Moe\n+# NO LONGER EXIST Copied from transformers.models.qwen2.modeling_qwen2.Qwen2FlashAttention2 with Qwen2->Qwen2Moe\n+# TODO cyril: modular\n class Qwen2MoeFlashAttention2(Qwen2MoeAttention):\n     \"\"\"\n     Qwen2Moe flash attention module, following Qwen2Moe attention module. This module inherits from `Qwen2MoeAttention`\n@@ -429,7 +409,6 @@ class Qwen2MoeFlashAttention2(Qwen2MoeAttention):\n     config.max_window_layers layers.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n@@ -530,7 +509,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-# Copied from transformers.models.qwen2.modeling_qwen2.Qwen2SdpaAttention with Qwen2->Qwen2Moe\n+# NO LONGER EXIST Copied from transformers.models.qwen2.modeling_qwen2.Qwen2SdpaAttention with Qwen2->Qwen2Moe\n+# TODO cyril: modular\n class Qwen2MoeSdpaAttention(Qwen2MoeAttention):\n     \"\"\"\n     Qwen2Moe attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n@@ -1578,11 +1558,10 @@ def forward(\n class Qwen2MoeForQuestionAnswering(Qwen2MoePreTrainedModel):\n     base_model_prefix = \"model\"\n \n-    # Copied from models.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Qwen2Moe\n     def __init__(self, config):\n         super().__init__(config)\n-        self.model = Qwen2MoeModel(config)\n         self.qa_outputs = nn.Linear(config.hidden_size, 2)\n+        self.model = Qwen2MoeModel(config)  # diff with Llama: transformer->model\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "10c9b1638548cede247da5f96743b815cdd9afc6",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -460,15 +460,17 @@ def extra_repr(self):\n class Qwen2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n+        self.config = config\n         self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n         self.act_fn = ACT2FN[config.hidden_act]\n \n-    def forward(self, hidden_state):\n-        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n # Copied from transformers.models.llama.modeling_llama.repeat_kv"
        },
        {
            "sha": "74fc2085c36519c607a3157db4dfe04768a68011",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -77,7 +77,6 @@ def __init__(self, dim, base=10000, device=None):\n         self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n \n     @torch.no_grad()\n-    # Copied from transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding.forward with Gemma->RecurrentGemma\n     def forward(self, x, position_ids, seq_len=None):\n         # x: [bs, num_attention_heads, seq_len, head_size]\n         self.inv_freq.to(x.device)\n@@ -185,7 +184,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n+        cos, sin = self.rotary_emb(value_states, position_ids)\n \n         # Partial rotary embedding\n         query_rot, query_pass = torch.chunk(query_states, int(1 / self.partial_rotary_factor), dim=-1)"
        },
        {
            "sha": "1959d21e1d5d94007a01016fe476053e6e449251",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -563,7 +563,6 @@ class SEWFlashAttention2(SEWAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "9a2dfe013716a77ac274e60bf3b4006c08cb5e89",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -438,7 +438,6 @@ class SiglipFlashAttention2(SiglipAttention):\n \n     is_causal = False\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "88dc437cdcb91dfe232d59d80f1ee542687295fe",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 32,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0",
            "patch": "@@ -65,40 +65,18 @@\n class StableLmRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n+        config: StableLmConfig,\n         device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[StableLmConfig] = None,\n     ):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`StableLmRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -189,15 +167,17 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n class StableLmMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n+        self.config = config\n         self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n         self.act_fn = ACT2FN[config.hidden_act]\n \n-    def forward(self, hidden_state):\n-        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n \n \n class StableLmLayerNormPerHead(nn.Module):\n@@ -472,7 +452,6 @@ class StableLmFlashAttention2(StableLmAttention):\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2.__init__\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n "
        },
        {
            "sha": "3b4fdbcb81ccc4c694fbd9ede347133dbeef351a",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "32d64cd167ba50378b682323785e7cc30e478ae3",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 88,
            "deletions": 369,
            "changes": 457,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "d149643227952779b9535623ca67fdff5fa8a36d",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "49551b73577ad728ebd62f0c4affe1e6f93cca2e",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "ca743e1eaef3af9b93b68fa2a954af24297220c0",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "fb01823a29c0176c108d691c539aa75577a253b0",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "3b7348eadd4785960423f9bed8532162352f5e91",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "1c4051f2e2645c2722018aacfe182d8e100a2616",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "3ad46a92bc0938db29105cc76f0f7632a0eb4da6",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 22,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "88ccdc8ee45a2d7bbbad3e39c68fd52723d150d4",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "6d5e081d50b15223e3c011309133dd9d03e861d1",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 5,
            "deletions": 23,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "83e125c07c15bca03635666e2e630b4a3a578491",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "78e42e6ba71f2f21dbd69eb8755545f7162c7531",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "d9e6b9d7bfe7c0bc59470578d3d09c0a722ab56b",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "9abbf444d0b0b42ef853ec567a7ba5f7910daac6",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "e783cea95a63b368e8bc544fe3ede0e8e71483ce",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 23,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "c7b59d278e4fe6faad18be6e388f9a8dac0c8fdb",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 23,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "ecfa9189d12e6257605dd78b0679a2e117dbcd53",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "4806ec2c72d339685d269859142af73782f35e2f",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "21d11047ff1be8f930cb3e3f220b9e3c4b16e0a8",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "897d4b056f1977818fc72f5a4e3b4f9b33e45c64",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "c8aa55399035d2aa2e508dd087578c457f8263be",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 23,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "2b517034bffb15a7ed5b36c010e7e12fc7ce8836",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "1d7e995f80756c53aa52f10aff8ef0f5f7010c59",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 17,
            "deletions": 21,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "bfe1648de049e1ce67cc9da5652e950560ecac83",
            "filename": "tests/test_modeling_flax_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Ftest_modeling_flax_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Ftest_modeling_flax_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_flax_common.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "9dc712ab67b682308d9a8dff1b0853b4c9da80e0",
            "filename": "tests/test_modeling_tf_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Ftest_modeling_tf_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Ftest_modeling_tf_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_tf_common.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "383f0cbe60e1c9ca0e535ea4dce55eae64ea1c65",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        },
        {
            "sha": "420d6e6a2475d1385afa58315e58d0fad38b12bf",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c47618c1a282f925446506d53108dc6e82d9ef0/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c47618c1a282f925446506d53108dc6e82d9ef0/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=2c47618c1a282f925446506d53108dc6e82d9ef0"
        }
    ],
    "stats": {
        "total": 15413,
        "additions": 5635,
        "deletions": 9778
    }
}