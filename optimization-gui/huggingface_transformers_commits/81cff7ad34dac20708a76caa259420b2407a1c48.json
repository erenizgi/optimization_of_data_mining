{
    "author": "ydshieh",
    "message": "Fix `Gemma3IntegrationTest` (#38471)\n\n* check\n\n* check\n\n* check\n\n* check\n\n* check\n\n* check\n\n* check\n\n* test style bot\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "81cff7ad34dac20708a76caa259420b2407a1c48",
    "files": [
        {
            "sha": "7c3b59e40a9faa5f77beffce5713671290e07c9c",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 76,
            "deletions": 24,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/81cff7ad34dac20708a76caa259420b2407a1c48/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/81cff7ad34dac20708a76caa259420b2407a1c48/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=81cff7ad34dac20708a76caa259420b2407a1c48",
            "patch": "@@ -28,11 +28,14 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    is_flash_attn_2_available,\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n+    require_torch_large_accelerator,\n     slow,\n     torch_device,\n )\n@@ -368,7 +371,7 @@ def test_automodelforcausallm(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_read_token\n class Gemma3IntegrationTest(unittest.TestCase):\n     def setUp(self):\n@@ -407,9 +410,16 @@ def test_model_4b_bf16(self):\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like']  # fmt: skip\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water in the background. It looks like a lovely,'],\n+                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and'],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n+    @require_torch_large_accelerator\n     def test_model_4b_batch(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n@@ -444,12 +454,20 @@ def test_model_4b_batch(self):\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        EXPECTED_TEXTS = [\n-            'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like',\n-            \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n*   **Image 1:** Shows a cow\"\n-        ]  # fmt: skip\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8):\n+                    [\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and',\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. They depict very different scenes:\\n\\n*   **Image 1** shows a cow standing on a beach.',\n+                    ]\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n+    @require_torch_large_accelerator\n     def test_model_4b_crops(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n@@ -479,10 +497,17 @@ def test_model_4b_crops(self):\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n-        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a beach with a turquoise ocean and blue sky in the background. It looks like the cow is enjoying the beach']  # fmt: skip\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.']\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n         self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n+    @require_torch_large_accelerator\n     def test_model_4b_batch_crops(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n@@ -525,13 +550,20 @@ def test_model_4b_batch_crops(self):\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n         EXPECTED_NUM_IMAGES = 9  # 3 * (one for the origin image and two crops of images) = 9\n-        EXPECTED_TEXTS = [\n-            \"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a beach with a turquoise ocean and blue sky in the background. It looks like the cow is enjoying the beach\",\n-            \"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nWhile they all feature a brown cow in the foreground and a similar background (including the stop signs and\",\n-        ]  # fmt: skip\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): [\n+                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.',\n+                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nThe first image shows a cow on a beach, while the second image shows a street scene with a',\n+                ]\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n         self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n+    @require_torch_large_accelerator\n     def test_model_4b_multiimage(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n@@ -561,9 +593,14 @@ def test_model_4b_multiimage(self):\n \n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n-\n-        EXPECTED_TEXTS = [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt looks like a street scene in a vibrant,\"]  # fmt: skip\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Main Features:**\\n\\n*   **Chinese Archway:** The most prominent\"]\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n     def test_model_1b_text_only(self):\n         model_id = \"google/gemma-3-1b-it\"\n@@ -577,12 +614,18 @@ def test_model_1b_text_only(self):\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n \n-        EXPECTED_TEXTS = ['Write a poem about Machine Learning.\\n\\n---\\n\\nThe data flows, a river deep,\\nWith patterns hidden, secrets sleep.\\nA neural net, a watchful eye,\\nLearning']  # fmt: skip\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): ['Write a poem about Machine Learning.\\n\\n---\\n\\nThe data flows, a silent stream,\\nInto the neural net, a waking dream.\\nAlgorithms hum, a coded grace,\\n'],\n+                (\"cuda\", 8): ['Write a poem about Machine Learning.\\n\\n---\\n\\nThe data flows, a silent stream,\\nInto the neural net, a waking dream.\\nAlgorithms hum, a coded grace,\\n'],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n     # TODO: raushan FA2 generates gibberish for no reason, check later\n     @require_flash_attn\n-    @require_torch_gpu\n+    @require_torch_large_accelerator\n     @pytest.mark.flash_attn_test\n     def test_model_4b_flash_attn(self):\n         model_id = \"google/gemma-3-4b-it\"\n@@ -602,8 +645,14 @@ def test_model_4b_flash_attn(self):\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and']  # fmt: skip\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant island in the background. It looks like a sunny day'],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n+        self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"eager\",)])\n     def test_generation_beyond_sliding_window(self, attn_implementation: str):\n@@ -613,6 +662,9 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         \"\"\"\n         model_id = \"google/gemma-3-1b-it\"\n \n+        if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n+            self.skipTest(\"FlashAttention2 is required for this test.\")\n+\n         input_text = [\n             \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n             \"A list of colors: red, blue\",  # This will almost all be padding tokens"
        }
    ],
    "stats": {
        "total": 100,
        "additions": 76,
        "deletions": 24
    }
}