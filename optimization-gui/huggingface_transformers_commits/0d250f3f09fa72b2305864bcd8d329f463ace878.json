{
    "author": "merveenoyan",
    "message": "Fixes and compilation warning in torchao docs (#42909)\n\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "0d250f3f09fa72b2305864bcd8d329f463ace878",
    "files": [
        {
            "sha": "1a825ee49eb1c82b7e1b82d9cc6c0fec183cd735",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 16,
            "deletions": 13,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d250f3f09fa72b2305864bcd8d329f463ace878/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d250f3f09fa72b2305864bcd8d329f463ace878/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=0d250f3f09fa72b2305864bcd8d329f463ace878",
            "patch": "@@ -86,6 +86,9 @@ Create a [`TorchAoConfig`] and specify the quantization type and `group_size` of\n \n We'll show examples for recommended quantization methods based on hardwares, e.g. A100 GPU, H100 GPU, CPU.\n \n+> [!WARNING]\n+> torchao automatically compiles the model during the first inference if we set `cache_implementation=\"static\"`. The model is recompiled every time batch size or `max_new_tokens` is modified. Pass `disable_compile=True` in [`~GenerationMixin.generate`] to quantize without compilation.\n+\n ### H100 GPU\n \n <hfoptions id=\"examples-H100-GPU\">\n@@ -111,7 +114,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -140,7 +143,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -207,7 +210,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -243,7 +246,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -275,7 +278,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"RedHatAI/Sparse-Llama-3.1-8B-2of4\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -310,7 +313,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -342,7 +345,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device)\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -376,7 +379,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -406,7 +409,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -441,7 +444,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n # Manual Testing\n prompt = \"Hey, are you conscious? Can you talk to me?\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").to(quantized_model.device.type)\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(quantized_model.device, quantized_model.dtype)\n generated_ids = quantized_model.generate(**inputs, max_new_tokens=128)\n output_text = tokenizer.batch_decode(\n     generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n@@ -480,7 +483,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n # Manual Testing\n prompt = \"Hey, are you conscious? Can you talk to me?\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\", quantized_model.dtype)\n generated_ids = quantized_model.generate(**inputs, max_new_tokens=128, cache_implementation=\"static\")\n output_text = tokenizer.batch_decode(\n     generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n@@ -575,7 +578,7 @@ print(\"Prompt:\", prompt)\n inputs = tokenizer(\n     prompt,\n     return_tensors=\"pt\",\n-).to(\"cuda\")\n+).to(quantized_model.device, quantized_model.dtype)\n # setting temperature to 0 to make sure result deterministic\n generated_ids = quantized_model.generate(**inputs, max_new_tokens=128, temperature=0)\n \n@@ -736,7 +739,7 @@ reloaded_model = AutoModelForCausalLM.from_pretrained(\n )\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(reloaded_model.device.type)\n \n output = reloaded_model.generate(**input_ids, max_new_tokens=10)\n print(tokenizer.decode(output[0], skip_special_tokens=True))"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 16,
        "deletions": 13
    }
}