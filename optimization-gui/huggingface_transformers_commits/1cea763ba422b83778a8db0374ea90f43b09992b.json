{
    "author": "drbh",
    "message": "feat: extract rev in attn_implementation kernels via @ (#40009)\n\n* feat: extract rev in attn_implementation kernels via @\n\n* fix: adjust for ruff\n\n* fix: update regex and add explanatory comment\n\n* fix: move attn_implementation kernel doc\n\n* fix: remove extra line",
    "sha": "1cea763ba422b83778a8db0374ea90f43b09992b",
    "files": [
        {
            "sha": "364437d64b0f2a4bbc4902ad832d73b8ee42b8d5",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 2,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cea763ba422b83778a8db0374ea90f43b09992b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cea763ba422b83778a8db0374ea90f43b09992b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1cea763ba422b83778a8db0374ea90f43b09992b",
            "patch": "@@ -2721,7 +2721,7 @@ def _check_and_adjust_attn_implementation(\n             None to sdpa (to potentially eager).\n         \"\"\"\n         applicable_attn_implementation = \"sdpa\" if attn_implementation is None else attn_implementation\n-        if re.match(r\"^[^/:]+/[^/:]+:?[^/:]+$\", applicable_attn_implementation):\n+        if re.match(r\"^[^/:]+/[^/:]+(?:@[^/:]+)?(?::[^/:]+)?$\", applicable_attn_implementation):\n             if not is_kernels_available():\n                 raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n             attention_wrapper = None\n@@ -2738,8 +2738,12 @@ def _check_and_adjust_attn_implementation(\n                 repo_id = applicable_attn_implementation\n                 kernel_name = None\n             repo_id = repo_id.strip()\n+            # extract the rev after the @ if it exists\n+            repo_id, _, rev = repo_id.partition(\"@\")\n+            repo_id = repo_id.strip()\n+            rev = rev.strip() if rev else None\n             try:\n-                kernel = get_kernel(repo_id)\n+                kernel = get_kernel(repo_id, revision=rev)\n                 if hasattr(kernel, \"flash_attn_varlen_func\"):\n                     if attention_wrapper is None:\n                         attention_wrapper = flash_attention_forward\n@@ -4494,6 +4498,22 @@ def from_pretrained(\n             attn_implementation (`str`, *optional*):\n                 The attention implementation to use in the model (if relevant). Can be any of `\"eager\"` (manual implementation of the attention), `\"sdpa\"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `\"flash_attention_2\"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `\"flash_attention_3\"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `\"eager\"` implementation.\n \n+                Accept HF kernel references in the form:\n+                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]\n+\n+                - <namespace> and <repo_name> are any non-\"/\" and non-\":\" sequences.\n+                - \"@<revision>\" is optional (branch, tag, or commit-ish), e.g. \"@main\", \"@v1.2.0\", \"@abc123\".\n+                - \":<kernel_name>\" is optional and selects a function inside the kernel repo.\n+                - Both options can appear together and in this order only: @revision first, then :kernel_name.\n+                - We intentionally allow a leading \"<wrapper>|\" prefix (e.g., \"flash|...\") because the code\n+                  strips it before loading; '|' is not excluded in the character classes here.\n+\n+                Examples that match:\n+                  \"org/model\"\n+                  \"org/model@main\"\n+                  \"org/model:custom_kernel\"\n+                  \"org/model@v1.2.3:custom_kernel\"\n+\n             > Parameters for big model inference\n \n             torch_dtype (`str` or `torch.dtype`, *optional*):"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 22,
        "deletions": 2
    }
}