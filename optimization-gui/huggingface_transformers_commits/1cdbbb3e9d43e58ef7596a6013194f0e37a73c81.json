{
    "author": "remi-or",
    "message": "Support sliding window in CB (#40688)\n\n* CB example: better compare feature\n\n* Cache managers, still issue w/ effective length\n\n* WIP -- fix for effective length\n\n* Renames\n\n* Wroking, need better parity checks, we mind be missing 1 token\n\n* Small fixes\n\n* Fixed wrong attn mask and broke cache into pieces\n\n* Warmup is slowing down things, disabling it\n\n* Cache was too big, fixed\n\n* Simplified index objects\n\n* Added a profile option to the example\n\n* Avoid calls to memory reporing tools\n\n* Restore full attention read indices for better latency\n\n* Adressed some TODOS and style\n\n* Docstrings for cache managers\n\n* Docstrings for Schedulers\n\n* Refactor scheudlers\n\n* [Important] Cache fix for sliding window, check with small sw size\n\n* Updated doc for cache memory compute and cache as a whole\n\n* Moved a todo\n\n* Nits and style\n\n* Fix for when sliding window is smaller than max batch per token\n\n* Paged interface update\n\n* Support for FLash in new API\n\n* Fix example CB\n\n* Fix bug in CB for paged\n\n* Revert example\n\n* Style\n\n* Review compliance\n\n* Style\n\n* Styleeeee\n\n* Removed NO_SLIDING_WINDOW\n\n* Review #2 compliance\n\n* Better art\n\n* Turn cum_seqlens_k in a dict\n\n* Attn mask is now a dict\n\n* Update examples/pytorch/continuous_batching.py\n\nCo-authored-by: Luc Georges <McPatate@users.noreply.github.com>\n\n* Adressed McPatate pro review\n\n* Style and fix\n\n---------\n\nCo-authored-by: Luc Georges <McPatate@users.noreply.github.com>",
    "sha": "1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
    "files": [
        {
            "sha": "9108339468a9578db71496bc17fd4e9bae3200bb",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 58,
            "deletions": 39,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -13,49 +13,50 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import argparse\n+import contextlib\n import json\n import os\n import time\n from typing import Optional\n \n import datasets\n import torch\n+from torch.profiler import ProfilerActivity, profile\n+from tqdm import tqdm\n \n from transformers import AutoModelForCausalLM, AutoTokenizer\n from transformers.generation import GenerationConfig\n \n \n-MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n+# MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n+SLIDING_WINDOW = 0\n+MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"Qwen/Qwen3-4B-Instruct-2507\"\n+FORCE_MAX_LENGTH = False  # should be False unless you are debugging sliding window features\n \n \n def generate_simple(\n-    attn_implementation: str, simple_batch_inputs: list[int], generation_config: GenerationConfig\n-) -> list[str]:\n-    attn_implementation = {\n+    attn_impl: str, simple_batch_inputs: list[int], generation_config: GenerationConfig\n+) -> dict[str, str]:\n+    attn_impl = {\n         \"sdpa_paged\": \"sdpa\",\n         \"eager_paged\": \"eager\",\n         \"flash_paged\": \"flash_attention_2\",\n-    }[attn_implementation]\n+    }[attn_impl]\n \n-    model = (\n-        AutoModelForCausalLM.from_pretrained(\n-            MODEL_ID,\n-            torch_dtype=torch.bfloat16,\n-            attn_implementation=attn_implementation,\n-        )\n-        .cuda()\n-        .eval()\n-    )\n+    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=torch.bfloat16, attn_implementation=attn_impl)\n+    model = model.cuda().eval()\n+    if getattr(model.config, \"sliding_window\", None) is not None:\n+        model.config.sliding_window = SLIDING_WINDOW\n \n-    decoded_outputs = []\n-    for input_ids in simple_batch_inputs:\n+    decoded_outputs = {}\n+    for input_ids in tqdm(simple_batch_inputs, desc=\"Generating outputs without CB\"):\n+        key = \" \".join(map(str, input_ids))  # This will be used to identify the output after batched generation\n         input_ids = torch.tensor([input_ids]).to(\"cuda\")\n-        attention_mask = torch.ones_like(input_ids)\n-        outputs = model.generate(input_ids, attention_mask=attention_mask, generation_config=generation_config)\n+        # attention_mask = torch.ones_like(input_ids)\n+        outputs = model.generate(input_ids, generation_config=generation_config, use_model_defaults=False)\n         generated_tokens = outputs[0][input_ids.shape[1] :]\n         decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n-        decoded_outputs.append(decoded_output)\n-\n+        decoded_outputs[key] = decoded_output\n     return decoded_outputs\n \n \n@@ -117,7 +118,9 @@ def batch_generate(\n     data = []\n     for i, request in enumerate(batch_outputs):\n         input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=True)\n-        data.append({\"input\": input_text})\n+        # The key is used to tie back to the output of unbatched generation\n+        key = \" \".join(map(str, batch_outputs[request].prompt_ids))\n+        data.append({\"input\": input_text, \"key\": key})\n \n         # Try to decode the output\n         try:\n@@ -142,9 +145,11 @@ def batch_generate(\n \n         # Compare with classic generate if asked\n         if expected_outputs is not None:\n-            matches = output_text == expected_outputs[i]\n-            data[-1][\"ref\"] = expected_outputs[i]\n+            expected_output = expected_outputs.pop(key)\n+            matches = output_text == expected_output  # TODO: rework this for a better distance metric\n+            data[-1][\"ref\"] = expected_output\n             data[-1][\"matches\"] = matches\n+            data[-1].pop(\"key\")\n             print(f\"Request {i} matches\" if matches else f\"Request {i} does NOT match!\")\n \n     # Compute stats and maybe print them\n@@ -191,6 +196,7 @@ def batch_generate(\n     parser.add_argument(\"--output-file\", type=str, default=None)\n     parser.add_argument(\"--compare\", action=\"store_true\", default=False)\n     parser.add_argument(\"--metrics\", action=\"store_true\", default=False)\n+    parser.add_argument(\"--profile\", type=str, default=None)\n     args = parser.parse_args()\n \n     # If turned on, we setup metrics\n@@ -208,6 +214,9 @@ def batch_generate(\n         dtype=torch.bfloat16,\n     )\n     model = model.cuda().eval()\n+    if getattr(model.config, \"sliding_window\", None) is not None:\n+        print(f\"Setting sliding window from {model.config.sliding_window} to {SLIDING_WINDOW}\")\n+        model.config.sliding_window = SLIDING_WINDOW\n \n     # If turned on, we compile the model\n     if args.compile:\n@@ -218,16 +227,17 @@ def batch_generate(\n \n     # Prepare tokenizer and dataset\n     tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n+\n     dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n-    dataset = dataset.select(range(args.samples))  # Use only 5 examples for the simple version\n-    tokenized_datasets = dataset.map(lambda x: tokenizer(x[\"question\"]), batched=True)\n-    simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n+    dataset = dataset.select(range(args.samples))\n+\n+    simple_batch_inputs = [tokenizer(item[\"question\"])[\"input_ids\"] for item in dataset]\n \n     # Prepare generation config\n     generation_config = GenerationConfig(\n         max_new_tokens=512,\n         use_cuda_graph=args.use_cuda_graph,\n-        eos_token_id=tokenizer.eos_token_id,\n+        eos_token_id=tokenizer.pad_token_id if FORCE_MAX_LENGTH else tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,\n         do_sample=True,\n         temperature=0.8,\n@@ -247,7 +257,7 @@ def batch_generate(\n             f\"runs/cb/{args.num_blocks}_{args.max_batch_tokens}_{attn}_{args.matmul_precision}_{args.samples}.json\"\n         )\n \n-    # Run warmup batch generation\n+    # Run warmup batch generation # TODO: understand why warmup incurs a large overhead during cache creation\n     batch_generate(\n         model,\n         simple_batch_inputs[: min(5, args.samples)],\n@@ -257,17 +267,26 @@ def batch_generate(\n         slice_inputs=args.slice_inputs,\n     )\n \n-    # Run batch generation\n-    gen_time, tok_per_sec = batch_generate(\n-        model,\n-        simple_batch_inputs,\n-        generation_config,\n-        tokenizer,\n-        displayed_samples=args.displayed,\n-        output_file=args.output_file,\n-        expected_outputs=expected_outputs,\n-        slice_inputs=args.slice_inputs,\n-    )\n+    if args.profile is not None:\n+        cm = profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True)\n+    else:\n+        cm = contextlib.nullcontext()\n+    with cm as prof:\n+        # Run batch generation\n+        gen_time, tok_per_sec = batch_generate(\n+            model,\n+            simple_batch_inputs,\n+            generation_config,\n+            tokenizer,\n+            displayed_samples=args.displayed,\n+            output_file=args.output_file,\n+            expected_outputs=expected_outputs,\n+            slice_inputs=args.slice_inputs,\n+        )\n+    if args.profile is not None:\n+        filename = args.profile if args.profile.endswith(\".json\") else args.profile + \".json\"\n+        prof.export_chrome_trace(filename)\n \n # Example usage:\n+# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --slice-inputs --samples 3 --compare\n # python examples/pytorch/continuous_batching.py --num-blocks 369 --max-batch-tokens 23 --attn sdpa_paged -mp none --samples 1 --displayed 0 --output-file sliced.json"
        },
        {
            "sha": "8d6800f7db3550fbadfafcac6e2ae0c81eae53d7",
            "filename": "src/transformers/generation/continuous_batching/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -13,8 +13,14 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from .cache import PagedAttentionCache\n-from .classes import RequestState, RequestStatus\n from .continuous_api import ContinuousBatchingManager, ContinuousMixin\n+from .requests import RequestState, RequestStatus\n \n \n-__all__ = [\"PagedAttentionCache\", \"RequestState\", \"RequestStatus\", \"ContinuousMixin\", \"ContinuousBatchingManager\"]\n+__all__ = [\n+    \"ContinuousBatchingManager\",\n+    \"ContinuousMixin\",\n+    \"PagedAttentionCache\",\n+    \"RequestState\",\n+    \"RequestStatus\",\n+]"
        },
        {
            "sha": "82d2a0d47aac2e054046d86cd54300d9996af90e",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 375,
            "deletions": 175,
            "changes": 550,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -13,26 +13,116 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections import deque\n-from math import floor, sqrt\n+from math import floor, gcd, sqrt\n from typing import Optional, Union\n \n import torch\n \n from ...configuration_utils import PretrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n from ...utils.metrics import attach_tracer, traced\n-from .classes import RequestState, get_device_and_memory_breakdown, logger\n+from .cache_manager import CacheAllocator, FullAttentionCacheAllocator, SlidingAttentionCacheAllocator\n+from .requests import get_device_and_memory_breakdown, logger\n+\n+\n+def group_layers_by_attn_type(config: PretrainedConfig) -> tuple[list[list[int]], list[str]]:\n+    \"\"\"\n+    Group layers depending on the attention mix, according to VLLM's hybrid allocator rules:\n+        - Layers in each group need to have the same type of attention\n+        - All groups have the same number of layers\n+\n+    For a model with the following layer types: [\"sliding\", \"full\", \"full\", \"sliding\", \"full\", \"full\", \"full\", \"full\"]\n+    We would get two groups: [0, 3] and [1, 2], [4,5], [6,7].\n+    \"\"\"\n+    # If the config has no layer_type attribute, it means all layers are the same attention type\n+    layer_types = getattr(config, \"layer_types\", None)\n+    if layer_types is None:\n+        attn_type = \"sliding_attention\" if getattr(config, \"sliding_window\", None) is not None else \"full_attention\"\n+        layer_types = [attn_type for _ in range(config.num_hidden_layers)]\n+\n+    # We then count the number of layers of each type\n+    layer_counts = {}\n+    for i, layer_type in enumerate(layer_types):\n+        layer_counts[layer_type] = layer_counts.get(layer_type, []) + [i]\n+\n+    # The size of all groups is the greatest common divisor of the number of layers of each type\n+    group_size = gcd(*[len(indices) for indices in layer_counts.values()])\n+\n+    # We then group the layers by type\n+    layer_groups = []\n+    for layer_type, indices in layer_counts.items():\n+        for i in range(0, len(indices), group_size):\n+            layer_groups.append(indices[i : i + group_size])\n+    # And note the layer types\n+    group_types = [layer_types[lg[0]] for lg in layer_groups]\n+    return layer_groups, group_types\n \n \n @attach_tracer()\n class PagedAttentionCache:\n+    \"\"\"\n+    Manages the cache for a paged attention mechanism, inspired by VLLM's hybrid allocator. The cache relies on making\n+    groups of layers to reduce the complexity of cache management and fragmentation.\n+\n+    The cache uses a three-level hierarchy:\n+    - Pages: The smallest unit of cache, a page has a size of [num_heads, head_size], which is the space needed to\n+        store the key or value states for one token and one layer. For a model with only full-attention layers, to store\n+        the KV cache of one token, we need `2 * num_layers` pages: key and values each take `num_layers` pages.\n+        Pages are grouped into blocks:\n+    - Blocks: A block is a collection of `block_size` pages, serving as the allocation unit to reduce management\n+        complexity and fragmentation. Cache is allocated and freed block by block, not page by page. One block is\n+        allocated to one layer group, which only has one attention type, like full-attention or sliding-attention.\n+        If all layers in the model have the same attention type, then all layers will be in the same group. There is\n+        more than one group if and only if the model has a mixed attention types, like layers with full-attention and\n+        layers with sliding-attention.\n+    - Cache tensors: The physical supports for the cache. There are as many cache tensors as there are layer in a\n+        layer group, and the shape of the cache tensor is `[num_blocks * block_size, num_heads, head_size]`.\n+\n+    Grouping layers into groups is useful because when we allocate one block to a group N, the block allocated is the\n+        same for all layers in group N, equivalently it is allocated accross all cache tensors. This allows us to\n+        efficiently allocate and free blocks, and to efficiently read and write key and value states.\n+\n+    For instance, imagine we have 8 blocks of cache and a model with two layer groups: a full-attention group with 3\n+    layers and a sliding-attention group with 3 layers. At creation time, the physical cache tensors look like this:\n+\n+    cache_tensor_0: □ □ □ □ □ □ □ □\n+    cache_tensor_1: □ □ □ □ □ □ □ □\n+    cache_tensor_2: □ □ □ □ □ □ □ □\n+\n+    where □ means the blocks is not allocated to any layer group yet. We have 3 cache tensors because there are\n+    3 layers per group.\n+    We allocate 1 block to each group, after allocation, the cache tensors look like this:\n+\n+    cache_tensor_0: ✖ ◉ □ □ □ □ □ □\n+    cache_tensor_1: ✖ ◉ □ □ □ □ □ □\n+    cache_tensor_2: ✖ ◉ □ □ □ □ □ □\n+\n+    where ✖ means the block is allocated to the full-attention group, and ◉ means the block is allocated to the\n+    sliding-attention group.\n+    Now, if we continue to generate, and the sliding window has been reached, we only need to allocate a new block\n+    for the full-attention group, and the cache tensors look like this:\n+\n+    cache_tensor_0: ✖ ◉ ✖ □ □ □ □ □\n+    cache_tensor_1: ✖ ◉ ✖ □ □ □ □ □\n+    cache_tensor_2: ✖ ◉ ✖ □ □ □ □ □\n+\n+    And after further generation, when we need a new block allocated:\n+\n+    cache_tensor_0: ✖ ◉ ✖ ✖ □ □ □ □\n+    cache_tensor_1: ✖ ◉ ✖ ✖ □ □ □ □\n+    cache_tensor_2: ✖ ◉ ✖ ✖ □ □ □ □\n+\n+    This would not have been possible if all layers were in the same group: we would have had to allocate a new block\n+    for the sliding-attention group, although it is not needed.\n+    \"\"\"\n+\n+    # TODO: this init is quite long, maybe a refactor is in order\n     def __init__(\n         self,\n         config: PretrainedConfig,\n         generation_config: GenerationConfig,\n         device: torch.device,\n         dtype: torch.dtype = torch.float16,\n-        num_requests: int = 100,\n         layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n         tp_size: Optional[int] = None,\n     ) -> None:\n@@ -42,10 +132,11 @@ def __init__(\n             config: Model configuration\n             generation_config: Generation configuration containing cache parameters\n             device: Device for the cache tensors\n-            dtype: Data type for the cache tensors\n+            dtype: Data type of the cache\n             layer_device_map: Optional mapping of layer indices to devices\n-            initial_prompt_shapes: Optional sample prompts to help calculate optimal cache size\n+            tp_size: Tensor parallelism size\n         \"\"\"\n+        self.config = config\n         self.dtype = dtype\n         self.device = device\n \n@@ -55,10 +146,23 @@ def __init__(\n         head_dim = getattr(config, \"head_dim\", None)\n         self.head_dim: int = head_dim if head_dim is not None else config.hidden_size // config.num_attention_heads\n \n-        self.num_hidden_layers = config.num_hidden_layers\n+        # Extract cache dimensions\n         self.block_size = getattr(generation_config, \"block_size\", 32)\n \n-        # Handle TP\n+        # Group layers depending on the attention mix\n+        layer_groups, group_types = group_layers_by_attn_type(config)\n+        group_size = len(layer_groups[0])\n+        self.num_groups = len(layer_groups)\n+\n+        self.sliding_windows = {}\n+        self.layer_index_to_group_indices = {}\n+        for i, group in enumerate(layer_groups):\n+            sliding_window = config.sliding_window if group_types[i] == \"sliding_attention\" else 1\n+            for j, layer in enumerate(group):\n+                self.layer_index_to_group_indices[layer] = (i, j)\n+                self.sliding_windows[layer] = sliding_window\n+\n+        # Handle TP (or dont)\n         if tp_size is not None and tp_size > 1:\n             if self.num_key_value_heads % tp_size != 0:\n                 raise ValueError(\n@@ -68,13 +172,21 @@ def __init__(\n             # self.num_key_value_heads //= tp_size # TODO: why is this commented out?\n \n         # Infer number of blocks and max batch tokens\n+        page_size = self.head_dim * self.num_key_value_heads\n+\n+        if getattr(config, \"attn_implementation\", None) == \"paged_attention\":\n+            num_attention_masks = 0\n+        else:\n+            # TODO: when we generalize to allow for block-attn, we can use `num_attention_masks=sum(set(group_types))`\n+            num_attention_masks = 2 if \"sliding_attention\" in group_types else 1\n+\n         memory_handler = PagedAttentionMemoryHandler(\n             block_size=self.block_size,\n-            head_dim=self.head_dim,\n-            num_heads=self.num_key_value_heads,\n-            num_layers=self.num_hidden_layers,\n-            hidden_size=config.hidden_size,\n-            vocab_size=config.vocab_size,\n+            page_size=page_size,\n+            num_groups=self.num_groups,\n+            group_size=group_size,\n+            peak_activation_per_token=(config.hidden_size + config.vocab_size),\n+            num_attention_masks=num_attention_masks,\n         )\n         num_blocks, max_batch_tokens = memory_handler.infer_num_blocks_and_max_batch_tokens(\n             num_blocks=getattr(generation_config, \"num_blocks\", None),\n@@ -86,143 +198,206 @@ def __init__(\n         # Add the inferred attributes to the class\n         self.num_blocks = num_blocks\n         self.max_batch_tokens = max_batch_tokens\n-        logger.warning(f\"PagedAttentionCache initialized with {self.num_blocks = } and {self.max_batch_tokens = } \")\n+        logger.warning(\n+            f\"PagedAttentionCache initialized with {self.num_blocks = }, {self.block_size = }, {page_size = }, \"\n+            f\"{self.max_batch_tokens = } {num_attention_masks = }\"\n+        )\n \n         # Initialize the cache\n-        self.cache_shape = (self.num_key_value_heads, num_blocks, self.block_size, self.head_dim)\n         self.key_cache: list[torch.Tensor] = []\n         self.value_cache: list[torch.Tensor] = []\n-        for idx in range(config.num_hidden_layers):\n-            layer_device = layer_device_map[idx] if layer_device_map is not None else device\n-            new_layer_key_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n-            new_layer_value_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n-            # Note: `mark_static_address` is used to tag the cache as a fixed data pointer,\n-            # preventing compiled graph breaks when updating the cache.\n+        # We add one extra token to the cache to handle padding and generally discard unwanted tokens\n+        self.cache_shape = (num_blocks * self.block_size + 1, self.num_key_value_heads, self.head_dim)\n+        for _ in range(group_size):\n+            new_layer_key_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n+            new_layer_value_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n             torch._dynamo.mark_static_address(new_layer_key_cache)\n             torch._dynamo.mark_static_address(new_layer_value_cache)\n             self.key_cache.append(new_layer_key_cache)\n             self.value_cache.append(new_layer_value_cache)\n+        logger.info(f\"{self.cache_shape = } {self.key_cache[0].shape = } {self.key_cache[0].numel() = }\")\n \n         # Block management data structures\n         self._free_blocks = deque(range(num_blocks))\n-        self._block_tables: dict[str, list[int]] = {}\n+        self.group_cache_managers: list[CacheAllocator] = []\n+        for i, group_type in enumerate(group_types):\n+            if group_type == \"full_attention\":\n+                cm = FullAttentionCacheAllocator(i, self.block_size)\n+            elif group_type == \"sliding_attention\":\n+                cm = SlidingAttentionCacheAllocator(i, self.block_size, config.sliding_window)\n+            else:\n+                raise ValueError(f\"Invalid group type: {group_type}\")\n+            self.group_cache_managers.append(cm)\n \n     @traced\n-    def allocate_blocks(self, n_blocks: int, request_id: str) -> list[int]:\n-        \"\"\"Allocates n_blocks for a given request_id.\"\"\"\n-        if len(self._free_blocks) < n_blocks:\n-            return False\n-\n-        allocated = []\n-        for _ in range(n_blocks):\n-            allocated.append(self._free_blocks.popleft())\n-\n-        if request_id not in self._block_tables:\n-            self._block_tables[request_id] = []\n-        self._block_tables[request_id].extend(allocated)\n-        return allocated\n+    def allocate_blocks(self, n_blocks: int, request_id: str) -> int:\n+        \"\"\"Allocate cache blocks across all layer groups for a given request. Actual allocation is done by the cache\n+        managers, and this method only returns the maximum number of blocks actually allocated across all managers.\"\"\"\n+        max_allocated = 0\n+        for cm in self.group_cache_managers:\n+            allocated = cm.allocate_blocks(n_blocks, request_id, self._free_blocks)\n+            if allocated is None:\n+                return None\n+            max_allocated = max(max_allocated, allocated)\n+        return max_allocated\n \n     @traced\n     def free_blocks(self, request_id: str) -> None:\n-        \"\"\"Frees all blocks associated with a request_id.\"\"\"\n-        if request_id in self._block_tables:\n-            blocks_to_free = self._block_tables.pop(request_id)\n-            self._free_blocks.extend(blocks_to_free)\n-        else:\n-            logger.info(f\"Attempted to free blocks for non-existent request_id: {request_id}\")\n+        \"\"\"Free all allocated cache blocks for a given request across all layer groups. Actual deallocation is done\n+        by the cache managers.\"\"\"\n+        for cm in self.group_cache_managers:\n+            cm.free_blocks(request_id, self._free_blocks)\n \n     def get_num_free_blocks(self) -> int:\n-        \"\"\"Returns the number of free blocks available.\"\"\"\n+        \"\"\"Get the current number of unallocated blocks available for new requests.\"\"\"\n         return len(self._free_blocks)\n \n-    def get_block_table(self, request_id: str) -> list[int]:\n-        \"\"\"Returns the block table for a request.\"\"\"\n-        return self._block_tables.get(request_id, [])\n-\n     @traced\n-    def _get_physical_indices(self, state: RequestState, logical_indices: list[int]) -> list[int]:\n-        \"\"\"\n-        Maps logical sequence indices to physical cache indices using the block table, using PyTorch.\n-\n-        Args:\n-            request_id: The request ID.\n-            logical_indices: A list of logical indices.\n-\n-        Returns:\n-            A list of physical indices.\n-\n-        Raises:\n-            ValueError: If no block table is found for the request ID.\n-            IndexError: If a logical index maps to a block index that is out of bounds.\n+    def get_read_indices(\n+        self, request_id: str, past_length: int, query_length: int, read_index: list[list[int]]\n+    ) -> None:\n+        \"\"\"Retrieve physical cache indices for reading KV states in the cache across all layer groups. This method\n+        coordinates with all cache managers to build the complete set of read indices needed for attention computation.\n         \"\"\"\n-        request_id = state.request_id\n-        block_table = self._block_tables.get(request_id)\n-        if not block_table:\n-            raise ValueError(f\"No block table found for request {request_id}\")\n-\n-        block_size = self.block_size\n-        physical_indices = []\n-\n-        for idx in logical_indices:\n-            block_idx = idx // block_size\n-            block_offset = idx % block_size\n-\n-            if block_idx >= len(block_table):\n-                raise IndexError(\n-                    f\"Logical index {idx} maps to block index {block_idx} which is out of bounds \"\n-                    f\"for request {request_id}\"\n-                )\n+        for cm, read_indices in zip(self.group_cache_managers, read_index):\n+            indices = cm.get_read_indices(request_id, past_length, query_length)\n+            read_indices.extend(indices)\n \n-            physical_block_num = block_table[block_idx]\n-            physical_index = physical_block_num * block_size + block_offset\n-            physical_indices.append(physical_index)\n-\n-        return physical_indices\n+    @traced\n+    def get_write_indices(\n+        self, request_id: str, past_length: int, query_length: int, write_index: list[list[int]]\n+    ) -> None:\n+        \"\"\"Retrieve physical cache indices for writing new KV states to the cache across all layer groups. This method\n+        coordinates with all cache managers to build the complete set of write indices needed to store computed KV\n+        states.\"\"\"\n+        for cm, write_indices in zip(self.group_cache_managers, write_index):\n+            indices = cm.get_write_indices(request_id, past_length, query_length)\n+            write_indices.extend(indices)\n \n     @traced\n     def update(\n         self,\n-        key_states: torch.Tensor,\n-        value_states: torch.Tensor,\n+        key_states: torch.Tensor,  # shape [1, num_kv_heads, seqlen_kv, head_dim]\n+        value_states: torch.Tensor,  # shape [1, num_kv_heads, seqlen_kv, head_dim]\n         layer_idx: int,\n-        read_index,\n-        write_index,\n+        read_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_kv + past_length]\n+        write_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_q]\n         **kwargs,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        # Reshape cache for easier indexing\n-        total_slots = self.num_blocks * self.block_size\n-        k_cache_flat = self.key_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n-        v_cache_flat = self.value_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n-        k_cache_flat[:, write_index, :] = key_states[0]\n-        v_cache_flat[:, write_index, :] = value_states[0]\n-        return k_cache_flat[None, :, read_index, :], v_cache_flat[None, :, read_index, :]\n+    ) -> tuple[torch.Tensor, torch.Tensor]:  # shape [seqlen_kv + past_length, num_kv_heads, head_dim]\n+        \"\"\"Update the cache with new key-value states for a specific layer. This method writes new KV states to the\n+        appropriate cache locations. The behavior differs based on the layer's attention type:\n+\n+        - Full attention: New KV states are written to cache, then complete sequence is read from cache\n+        - Sliding window: Old KV is read from cache along with extra spaces for the new KV, then new KV is written to\n+            cache. This is because new KV might overwrite the old KV, so we need to read the old KV first.\n+\n+        Returns the complete KV states (cached + new) for attention computation.\n+        \"\"\"\n+        # Retrieve the layer read and write indices, and if there is a sliding window\n+        group_idx, layer_idx_in_group = self.layer_index_to_group_indices[layer_idx]\n+        layer_read_index = read_index[group_idx]\n+        layer_write_index = write_index[group_idx]\n+        # Select the correct cache\n+        k_cache = self.key_cache[layer_idx_in_group]\n+        v_cache = self.value_cache[layer_idx_in_group]\n+        # Transpose the key and value states to match the cache shape, after which shape is [seqlen_kv, num_kv_heads, head_dim]\n+        key_states = key_states.transpose(1, 2).squeeze(0)\n+        value_states = value_states.transpose(1, 2).squeeze(0)\n+\n+        # Case: full attention\n+        sliding_window = self.sliding_windows[layer_idx]\n+        if sliding_window == 1:\n+            k_cache[layer_write_index, :, :] = key_states\n+            v_cache[layer_write_index, :, :] = value_states\n+            key_states_with_cache = k_cache[layer_read_index, :, :]\n+            value_states_with_cache = v_cache[layer_read_index, :, :]\n+\n+        # Case: sliding window -- we  need to be careful of read/write order because of chunked prefill, because it's\n+        # the only case where you may write over cache you need to use\n+        else:\n+            # Add the cache to the key and value states\n+            mask = layer_read_index == -1  # TODO: can this can be efficiently precomputed?\n+            key_states_with_cache = k_cache[layer_read_index, :, :]\n+            key_states_with_cache[mask] = key_states\n+            value_states_with_cache = v_cache[layer_read_index, :, :]\n+            value_states_with_cache[mask] = value_states\n+            # Write new KV values to the cache\n+            k_cache[layer_write_index, :, :] = key_states\n+            v_cache[layer_write_index, :, :] = value_states\n+\n+        # Return the new KV values\n+        return key_states_with_cache, value_states_with_cache\n \n \n+# TODO: rework computation with the groups and their sizes\n class PagedAttentionMemoryHandler:\n+    \"\"\"A helper class to determine the best number of pages and maximum number of tokens per batch for the paged\n+    attention cache, providing automatic sizing based on available GPU memory.\n+    The helper works using the number of pages, which is tied to the number of blocks by:\n+        num_blocks = num_pages // block_size\n+\n+    The memory footprint consists of three main components:\n+    - Cache memory: the space needed to store the cache tensors:\n+        2 * layer_group_size * [num_pages, page_size] * cache_dtype\n+    - Activation memory: the space temporarly taken by the largest activation during the model forward pass:\n+        peak_activation_per_token * max_tokens_per_batch * activation_dtype_size\n+    - Static tensors: the space taken by the input/output buffers and metadata tensors for batch processing, sum of:\n+        - inputs_ids + outputs_ids + position_ids + logits_indices: 4 * max_tokens_per_batch * int32_size\n+        - attention_mask: num_attention_masks * num_pages * max_tokens_per_batch * activation_dtype_size\n+        - cumulative_seqlens_q + cumulative_seqlens_k: (1 + 2) * max_tokens_per_batch * int32_size\n+        - write_index_tensor: num_groups * max_tokens_per_batch * int32_size\n+        - read_index_tensor: num_groups * (num_pages + max_tokens_per_batch) * int32_size\n+\n+    The handler can operate in three modes:\n+    1. Auto-sizing: Determines both number of pages and maximum number of tokens per batch using quadratic optimization\n+    2. Fixed cache: Calculates max batch tokens given a fixed number of pages\n+    3. Fixed batch: Calculates number of pages given a fixed maximum batch size\n+\n+    \"\"\"\n+\n     _activation_dtype = torch.bfloat16\n-    _activation_safety_factor = 2\n     _input_dtype = torch.int32\n     _upper_bound_max_batch_tokens = 256\n     _upper_bound_num_blocks = 4096\n \n     def __init__(\n         self,\n         block_size: int,\n-        head_dim: int,\n-        num_heads: int,\n-        num_layers: int,\n-        hidden_size: int,\n-        vocab_size: int,\n+        page_size: int,\n+        num_groups: int,\n+        group_size: int,\n+        peak_activation_per_token: int,\n+        num_attention_masks: int,\n     ) -> None:\n+        \"\"\"Initialize the memory handler with the parameters that cannot be automatically inferred.\n+\n+        Args:\n+            block_size: Size of the cache blocks\n+            page_size: Size of the cache pages\n+            num_groups: Number of layer groups\n+            group_size: Number of layers per layer group\n+            peak_activation_per_token: Maximum size of activation tensor per token, = hidden_size + vocab_size\n+            num_attention_masks: Number of attention masks, 0 if no attention mask is used, 2 if hybrid model, else 1\n+        \"\"\"\n         self.block_size = block_size\n-        self.head_dim = head_dim\n-        self.num_heads = num_heads\n-        self.num_layers = num_layers\n-        self.hidden_size = hidden_size\n-        self.vocab_size = vocab_size\n+        self.page_size = page_size\n+        self.num_groups = num_groups\n+        self.group_size = group_size\n+        self.peak_activation_per_token = peak_activation_per_token\n+        self.num_attention_masks = num_attention_masks\n \n     @staticmethod\n     def get_available_memory(max_memory_percent: float = 1.0) -> int:\n+        \"\"\"Calculate available GPU memory for cache allocation, accounting for already allocated tensors.\n+        This method queries the current memory state and applies the specified percentage limit to determine\n+        how much memory can be safely used for the paged attention cache.\n+\n+        Args:\n+            max_memory_percent: Fraction of available memory to use (0.0-1.0). 1.0 means use all available memory.\n+\n+        Returns:\n+            int: Available memory in bytes for cache allocation\n+        \"\"\"\n         _, total, reserved, allocated = get_device_and_memory_breakdown()\n         available_memory = total - max(allocated, reserved)\n         available_memory = int(available_memory * max_memory_percent)\n@@ -235,16 +410,17 @@ def infer_num_blocks_and_max_batch_tokens(\n         max_memory_percent: float = 0.9,\n         cache_dtype: torch.dtype = torch.float16,\n     ) -> tuple[int, int]:\n-        \"\"\"\n-        The memory footprint depends on the cache size C and the max batch tokens M in the following way:\n-            Mem = Mem(cache) + Mem(activation) + Mem(static_tensors)\n-        where:\n-            Mem(cache) = 2 * num_heads * head_dim * num_layers * cache_dtype.itemsize * C\n-            Mem(activation) = M * (hidden_size + vocab_size) * activation_dtype.itemsize\n-            Mem(static_tensors) ~= 8M * input_dtype.itemsize + M * C * activation_dtype.itemsize\n-\n-        Depending on if C or M is given, we use different methods to infer the values (C = num_blocks * block_size) and\n-        since block_size is fixed, num_blocks is the true variable to find.\n+        \"\"\"Determine optimal number of blocks and maximum number of tokens per batch based on available memory and\n+        constraints. Check the class docstring for more details. Naming the number of pages as N and the maximum number\n+        of tokens per batch as M, the equation solved is:\n+\n+        available_memory = sum([\n+            MN * num_attention_masks * activation_dtype_size,\n+            2N * (layer_group_size * page_size * cache_dtype + 2 * num_group),\n+            M * (peak_activation_per_token * activation_dtype + 28 + 4 * num_group),\n+        ])\n+\n+        where we already simplified int32_size = 4.\n         \"\"\"\n         # If neither num_blocks nor max_batch_tokens are provided, we use a second-order polynomial\n         if num_blocks is None and max_batch_tokens is None:\n@@ -265,7 +441,7 @@ def infer_num_blocks_and_max_batch_tokens(\n             num_blocks=num_blocks,\n             cache_dtype=cache_dtype,\n         )\n-        if sum(memory_footprint) > available_memory:\n+        if memory_footprint > available_memory:\n             raise MemoryError(f\"Memory footprint {memory_footprint} is more than available memory {available_memory}\")\n         return num_blocks, max_batch_tokens\n \n@@ -275,29 +451,27 @@ def compute_num_blocks_and_max_batch_tokens(\n         cache_dtype: torch.dtype = torch.float16,\n         m: float = 0.01,\n     ) -> tuple[int, int]:\n-        \"\"\"\n-        If neither M nor C is given, we assume M = m*C so we have to solve a second-order polynomial in C:\n-            Mem = C * 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n-                + C * m * (hidden_size + vocab_size) * activation_dtype.itemsize\n-                + C * m * 8 * input_dtype.itemsize + C^2 * m * activation_dtype.itemsize\n-\n-        We solve for C and then M = m*C.\n+        \"\"\"Calculate optimal number of blocks and maximum number of tokens per batch using quadratic optimization when\n+        neither is fixed. This method assumes a relationship M = m * N where m is a small ratio below 1 and solves the\n+        resulting quadratic equation to find the optimal N that maximizes utilization within memory constraints. m is\n+        the amount of cache we can fill with one batch: m=0.01 means a batch fills at most 1% of the cache. The equation\n+        to solve is:\n+\n+        available_memory = sum([\n+            m * N^2 * num_attention_masks * activation_dtype_size,\n+            2N * (layer_group_size * page_size * cache_dtype + 2 * num_group),\n+            m * N * (peak_activation_per_token * activation_dtype + 28 + 4 * num_group),\n+        ])\n         \"\"\"\n         cache_memory = self.get_available_memory(max_memory_percent)\n         logger.info(f\"Cache memory: {cache_memory}\")\n \n-        # Compute memory footprints\n-        mem_per_activation_token = m * self._activation_dtype.itemsize * (self.hidden_size + self.vocab_size)\n-        mem_per_cache_token = 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n-        mem_per_input_token = 8 * m * self._input_dtype.itemsize\n-        logger.info(f\"Memory per activation token: {mem_per_activation_token}\")\n-        logger.info(f\"Memory per cache token: {mem_per_cache_token}\")\n-        logger.info(f\"Memory per input token: {mem_per_input_token}\")\n-\n         # Compute second-degree polynomial coefficients\n-        a = m * self._activation_dtype.itemsize\n-        b = mem_per_input_token + mem_per_cache_token + mem_per_activation_token\n+        a = m * self.num_attention_masks * self._activation_dtype.itemsize\n+        b = 2 * (self.group_size * self.page_size * cache_dtype.itemsize + 2 * self.num_groups)\n+        b += m * (self.peak_activation_per_token * self._activation_dtype.itemsize + 28 + 4 * self.num_groups)\n         c = -cache_memory\n+        logger.info(f\"Coefficients of 2nd degree polynomial: {a = }, {b = }, {c = }\")\n \n         # Compute discriminant and greatest solution\n         discriminant = b**2 - 4 * a * c\n@@ -308,7 +482,8 @@ def compute_num_blocks_and_max_batch_tokens(\n             raise ValueError(f\"Greatest solution is negative: {greatest_solution = }\")\n \n         # Infer number of blocks and max batch tokens\n-        num_blocks = int(greatest_solution) // self.block_size\n+        num_pages = floor(greatest_solution)\n+        num_blocks = num_pages // self.block_size\n         if num_blocks > self._upper_bound_num_blocks:\n             logger.warning(f\"{num_blocks = } is too large, setting to {self._upper_bound_num_blocks = }\")\n             num_blocks = self._upper_bound_num_blocks\n@@ -324,73 +499,98 @@ def compute_max_batch_tokens(\n         max_memory_percent: float = 0.9,\n         cache_dtype: torch.dtype = torch.float16,\n     ) -> int:\n-        \"\"\"\n-        If C is given, we have a formula for M:\n-            num = (Mem - C * 2 * num_heads * head_dim * num_layers * cache_dtype.itemsize)\n-            denum = (8 * input_dtype.itemsize + C * activation_dtype.itemsize + (hidden_size + vocab_size) * activation_dtype.itemsize)\n-        M = num / denum\n+        \"\"\"Calculate maximum batch tokens M given a fixed number of cache blocks. The formula for M is given by:\n+\n+        M = (available_memory - 2N * (layer_group_size * page_size * cache_dtype + 2 * num_group))\n+            / (activation_dtype_size * (N * num_attention_masks + peak_activation_per_token) + 28 + 4 * num_group)\n         \"\"\"\n         cache_memory = self.get_available_memory(max_memory_percent)\n-        cache_size = num_blocks * self.block_size\n+        num_pages = num_blocks * self.block_size\n         # Compute numerator\n         num = cache_memory\n-        num -= cache_size * 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n+        num -= 2 * num_pages * (self.group_size * self.page_size * cache_dtype.itemsize + 2 * self.num_groups)\n         # Compute denominator\n-        denum = 8 * self._input_dtype.itemsize + cache_size * self._activation_dtype.itemsize\n-        denum += (self.hidden_size + self.vocab_size) * self._activation_dtype.itemsize\n+        denum = self._activation_dtype.itemsize * (\n+            num_pages * self.num_attention_masks + self.peak_activation_per_token\n+        )\n+        denum += 28 + 4 * self.num_groups\n         # Compute max batch tokens and return\n-        return int(num / denum)\n+        max_batch_tokens = floor(num / denum)\n+        if max_batch_tokens > self._upper_bound_max_batch_tokens:\n+            logger.warning(f\"{max_batch_tokens = } is too large, setting to {self._upper_bound_max_batch_tokens = }\")\n+            max_batch_tokens = self._upper_bound_max_batch_tokens\n+        return max_batch_tokens\n \n     def compute_num_blocks(\n         self,\n         max_batch_tokens: int,\n         max_memory_percent: float = 0.9,\n         cache_dtype: torch.dtype = torch.float16,\n     ) -> int:\n-        \"\"\"\n-        If M is given, we have a formula for C:\n-            num = Mem - M * (hidden_size + vocab_size) * activation_dtype.itemsize - 8 * M * input_dtype.itemsize\n-            denum = 2 * num_heads * head_dim * num_layers * cache_dtype.itemsize + M * activation_dtype.itemsize\n-        C = num / denum\n+        \"\"\"Calculate number of cache blocks N given a fixed maximum token per token M. The formula for N is given by:\n+\n+        N = (available_memory - M * (peak_activation_per_token * activation_dtype + 28 + 4 * num_group))\n+          / (2 * (layer_group_size * page_size * cache_dtype + 2 * num_group) + M * (num_attention_masks * activation_dtype_size))\n         \"\"\"\n         cache_memory = self.get_available_memory(max_memory_percent)\n         # Compute numerator\n         num = cache_memory\n-        num -= self._activation_dtype.itemsize * (self.hidden_size + self.vocab_size) * max_batch_tokens\n-        num -= 8 * max_batch_tokens * self._input_dtype.itemsize\n+        num -= max_batch_tokens * self.peak_activation_per_token * self._activation_dtype.itemsize\n+        num -= max_batch_tokens * (28 + 4 * self.num_groups)\n         # Compute denominator\n-        denum = 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n+        denum = 2 * (self.group_size * self.page_size * cache_dtype.itemsize + 2 * self.num_groups)\n+        denum += max_batch_tokens * (self.num_attention_masks * self._activation_dtype.itemsize)\n         denum += max_batch_tokens * self._activation_dtype.itemsize\n         # Compute cache size and return number of blocks\n-        cache_size = int(num / denum)\n-        return floor(cache_size / self.block_size)\n+        num_pages = floor(num / denum)\n+        num_blocks = num_pages // self.block_size\n+        if num_blocks > self._upper_bound_num_blocks:\n+            logger.warning(f\"{num_blocks = } is too large, setting to {self._upper_bound_num_blocks = }\")\n+            num_blocks = self._upper_bound_num_blocks\n+        return num_blocks\n \n     def compute_memory_footprint(\n         self,\n         num_blocks: Optional[int] = None,\n         max_batch_tokens: Optional[int] = None,\n         cache_dtype: torch.dtype = torch.float16,\n     ) -> tuple[int, int, int]:\n-        # Compute activation memory footprint\n-        activation_memory_footprint = self._activation_dtype.itemsize * (self.hidden_size + self.vocab_size)\n+        \"\"\"Calculate the memory footprint breakdown for a given number of blocks and maximum batch tokens. The memory\n+        footprint is given by:\n+\n+        available_memory = sum([\n+            MN * num_attention_masks * activation_dtype_size,\n+            2N * (layer_group_size * page_size * cache_dtype + 2 * num_group),\n+            M * (peak_activation_per_token * activation_dtype + 28 + 4 * num_group),\n+        ])\n+        but is broken down below.\n+        \"\"\"\n+        num_pages = num_blocks * self.block_size\n+\n+        cache_memory_footprint = 2 * self.group_size * num_pages * self.page_size * cache_dtype.itemsize\n+\n+        activation_memory_footprint = self.peak_activation_per_token * self._activation_dtype.itemsize\n         activation_memory_footprint *= max_batch_tokens\n-        # Compute cache memory footprint if num_blocks is provided\n-        if num_blocks is not None:\n-            cache_size = num_blocks * self.block_size\n-            bytes_per_token = 2 * self.num_heads * self.head_dim * self.num_layers * cache_dtype.itemsize\n-            cache_memory_footprint = cache_size * bytes_per_token\n-        else:\n-            cache_memory_footprint = -1\n-        # Compute static tensors memory footprint if num_blocks and max_batch_tokens is provided\n-        if num_blocks is not None and max_batch_tokens is not None:\n-            static_memory_footprint = sum(\n-                [\n-                    3 * max_batch_tokens * self._input_dtype.itemsize,  # input_ids, position_ids, output_ids\n-                    max_batch_tokens * cache_size * self._activation_dtype.itemsize,  # attention_mask\n-                    2 * max_batch_tokens * self._input_dtype.itemsize,  # cumulative_seqlens_qk (we remove the +1 to M)\n-                    3 * max_batch_tokens * self._input_dtype.itemsize,  # write_index, read_index, logits_indices\n-                ]\n-            )\n-        else:\n-            static_memory_footprint = -1\n-        return activation_memory_footprint, cache_memory_footprint, static_memory_footprint\n+\n+        inputs_outputs_positions_and_logits_memory_footprint = 4 * max_batch_tokens * 4  # second 4 is for int32 size\n+\n+        attention_memory_footprint = self.num_attention_masks * self._activation_dtype.itemsize\n+        attention_memory_footprint *= num_pages * max_batch_tokens\n+\n+        cumulative_seqlens_memory_footprint = 3 * max_batch_tokens * 4  # 4 is for int32 size\n+\n+        write_index_memory_footprint = self.num_groups * max_batch_tokens * 4  # 4 is for int32 size\n+        read_index_memory_footprint = self.num_groups * (num_pages + max_batch_tokens) * 4  # 4 is for int32 size\n+\n+        total_memory_footprint = sum(\n+            [\n+                cache_memory_footprint,\n+                activation_memory_footprint,\n+                inputs_outputs_positions_and_logits_memory_footprint,\n+                attention_memory_footprint,\n+                cumulative_seqlens_memory_footprint,\n+                write_index_memory_footprint,\n+                read_index_memory_footprint,\n+            ]\n+        )\n+        return total_memory_footprint"
        },
        {
            "sha": "74fbcd7c1084526f5baa91dea3550c8ad42b5c69",
            "filename": "src/transformers/generation/continuous_batching/cache_manager.py",
            "status": "added",
            "additions": 211,
            "deletions": 0,
            "changes": 211,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -0,0 +1,211 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from abc import ABC, abstractmethod\n+from collections import deque\n+from math import ceil\n+from typing import Optional\n+\n+from .requests import logger\n+\n+\n+class CacheAllocator(ABC):\n+    \"\"\"Abstract base class for cache managers. Cache managers keep track of per-request cache allocations, determine\n+    when a new physical block needs to be allocated and compute physical indices for reading or writing to the cache.\"\"\"\n+\n+    _index: int\n+    _block_table: dict[str, list[int]]  # request_id -> list of block_ids allocated to the request\n+\n+    @abstractmethod\n+    def allocate_blocks(self, n_blocks: int, request_id: str, free_blocks: deque[int]) -> Optional[int]:\n+        \"\"\"Allocates n_blocks for a given request_id. Returns the num of blocks allocated if successful and None\n+        otherwise.\"\"\"\n+        pass\n+\n+    def free_blocks(self, request_id: str, free_blocks: deque[int]) -> None:\n+        \"\"\"Frees all blocks associated with a request_id.\"\"\"\n+        if request_id in self._block_table:\n+            blocks_to_free = self._block_table.pop(request_id)\n+            free_blocks.extend(blocks_to_free)\n+        else:\n+            logger.warning(\n+                f\"CacheAllocator {self._index} attempted to free blocks for non-existent request_id: {request_id}\"\n+            )\n+\n+    @abstractmethod\n+    def get_read_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n+        \"\"\"Returns the physical indices of where to read request_id's cache in the cache tensor.\"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def get_write_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n+        \"\"\"Returns the physical indices of where to write request_id's cache in the cache tensor.\"\"\"\n+        pass\n+\n+\n+class FullAttentionCacheAllocator(CacheAllocator):\n+    \"\"\"Cache manager for a group of full attention layers.\"\"\"\n+\n+    def __init__(self, index: int, block_size: int) -> None:\n+        \"\"\"Initializes the cache manager for a group of full attention layers.\n+        Args:\n+            - index: the index of the associated layer group\n+            - block_size: the size of the blocks in the cache\n+        \"\"\"\n+        self._index = index\n+        self.block_size = block_size\n+        self._block_table = {}\n+\n+    def allocate_blocks(self, n_blocks: int, request_id: str, free_blocks: deque[int]) -> Optional[int]:\n+        \"\"\"Allocate blocks for a given request_id. Returns the number of blocks allocated if successful and None\n+        otherwise. For group of full attention layers, we always allocate the number of requested blocks.\"\"\"\n+        if len(free_blocks) < n_blocks:\n+            return None\n+        if request_id not in self._block_table:\n+            self._block_table[request_id] = []\n+        self._block_table[request_id].extend(free_blocks.popleft() for _ in range(n_blocks))\n+        return n_blocks\n+\n+    def get_read_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n+        \"\"\"Returns the physical indices of where to read request_id's cache. For a group of full attention layers, we\n+        first write the new cache to the cache tensor and then read the entire cache from the beginning to the end.\"\"\"\n+        # Retrieve the block table for the request and raise an error if it doesn't exist\n+        block_table = self._block_table.get(request_id)\n+        if block_table is None:\n+            raise ValueError(f\"No block table found for request {request_id}\")\n+        # Compute the physical indices\n+        physical_indices = []\n+        for i in range(past_length + query_length):\n+            block_idx = i // self.block_size\n+            block_offset = i % self.block_size\n+            physical_index = block_table[block_idx] * self.block_size + block_offset\n+            physical_indices.append(physical_index)\n+        return physical_indices\n+\n+    def get_write_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n+        \"\"\"Returns the physical indices for writing to the cache. For a group of full attention layers, we write the new\n+        cache as a continuation of the existing cache for the same request.\"\"\"\n+        block_table = self._block_table.get(request_id)\n+        if block_table is None:\n+            raise ValueError(f\"No block table found for request {request_id}\")\n+        # Compute the physical indices\n+        physical_indices = []\n+        for i in range(past_length, past_length + query_length):\n+            block_idx = i // self.block_size\n+            block_offset = i % self.block_size\n+            physical_index = block_table[block_idx] * self.block_size + block_offset\n+            physical_indices.append(physical_index)\n+        return physical_indices\n+\n+\n+class SlidingAttentionCacheAllocator(CacheAllocator):\n+    \"\"\"Cache manager for sliding window attention layers.\"\"\"\n+\n+    def __init__(self, index: int, block_size: int, sliding_window: int) -> None:\n+        \"\"\"Initializes the cache manager for a group of sliding window attention layers.\n+        Args:\n+            - index: the index of the associated layer group\n+            - block_size: the size of the blocks in the cache\n+            - sliding_window: the size of the sliding window\n+        \"\"\"\n+        self._index = index\n+        self.block_size = block_size\n+        self.sliding_window = sliding_window\n+        self._max_blocks_per_request = ceil(self.sliding_window / self.block_size)\n+        self._block_table = {}\n+\n+    def allocate_blocks(self, n_blocks: int, request_id: str, free_blocks: deque[int]) -> Optional[int]:\n+        \"\"\"Allocate blocks for a given request_id. Returns the number of blocks allocated if successful and None\n+        otherwise. For group of sliding window attention layers, we only allocate up to the point where we can fit an\n+        entire sliding window in the cache tensor.\"\"\"\n+        if request_id not in self._block_table:\n+            self._block_table[request_id] = []\n+        # Early return if we are already at the max number of blocks per request\n+        already_allocated = len(self._block_table[request_id])\n+        if already_allocated == self._max_blocks_per_request:\n+            return 0\n+        # Compute actual number of blocks to allocate\n+        after_allocation = min(already_allocated + n_blocks, self._max_blocks_per_request)\n+        actual_n_blocks = after_allocation - already_allocated\n+        # Classic allocation\n+        if len(free_blocks) < actual_n_blocks:\n+            return None\n+        self._block_table[request_id].extend(free_blocks.popleft() for _ in range(actual_n_blocks))\n+        return actual_n_blocks\n+\n+    def get_read_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n+        \"\"\"Returns the physical indices of where to read request_id's cache in the cache tensor.\n+        For a group of sliding window attention layers, we read from the cache tensor before writing on it, because the\n+        new cache can overwrite the old one. To form the cache + new key / values states, we read the at most\n+        sliding_window - 1 cache page and then manually add the new key / values states after. Hence the -1 indices\n+        which indicate where to store the new key or values indices.\"\"\"\n+        # Retrieve the block table for the request and raise an error if it doesn't exist\n+        block_table = self._block_table.get(request_id)\n+        if block_table is None:\n+            raise ValueError(f\"No block table found for request {request_id}\")\n+        # Apply sliding window\n+        start_index = 0 if past_length < self.sliding_window else past_length % self.sliding_window\n+        cache_length = min(past_length, self.sliding_window - 1)\n+        # Compute the physical indices\n+        physical_indices = []\n+        for i in range(start_index, start_index + cache_length):\n+            i %= self.sliding_window\n+            block_idx = i // self.block_size\n+            block_offset = i % self.block_size\n+            physical_index = block_table[block_idx] * self.block_size + block_offset\n+            physical_indices.append(physical_index)\n+        return physical_indices + [-1] * query_length\n+\n+    def get_write_indices(self, request_id: str, past_length: int, query_length: int) -> list[int]:\n+        \"\"\"Returns the physical indices of where to write request_id's cache in the cache tensor. For a group of\n+        sliding window attention layers, we write the new cache in rolling-buffer kind of way: if we reach the end of\n+        the allocated physical cache, we start writing from the beginning of the physical cache again.\"\"\"\n+        # Retrieve the block table for the request and raise an error if it doesn't exist\n+        block_table = self._block_table.get(request_id)\n+        if block_table is None:\n+            raise ValueError(f\"No block table found for request {request_id}\")\n+        # Apply sliding window\n+        start_index = past_length % self.sliding_window\n+        cache_length = min(query_length, self.sliding_window)\n+        padding_length = query_length - cache_length\n+        # Compute the physical indices\n+        physical_indices = []\n+        for i in range(start_index, start_index + cache_length):\n+            i %= self.sliding_window\n+            block_idx = i // self.block_size\n+            block_offset = i % self.block_size\n+            physical_index = block_table[block_idx] * self.block_size + block_offset\n+            physical_indices.append(physical_index)\n+        if padding_length > 0:\n+            physical_indices = [-1] * padding_length + physical_indices\n+        return physical_indices\n+\n+\n+# TODO: test the impact of this\n+# def get_read_indices(self, request_id: str, past_length: int) -> list[int]:\n+#     # Retrieve the block table for the request and raise an error if it doesn't exist\n+#     block_table = self._block_table.get(request_id)\n+#     if block_table is None:\n+#         raise ValueError(f\"No block table found for request {request_id}\")\n+#     # Compute the physical indices\n+#     physical_indices = []\n+#     n_left = past_length\n+#     for block_idx in block_table:\n+#         block_physical_index = block_idx * self.block_size\n+#         pages_used = min(self.block_size, n_left)\n+#         physical_indices.extend(block_physical_index + i for i in range(pages_used))\n+#         n_left -= pages_used\n+#         if n_left == 0:\n+#             return physical_indices\n+#     raise ValueError(f\"Request {request_id} required too many indices: {past_length = } and {len(block_table) = }\")"
        },
        {
            "sha": "e1dfd638cb3455707239ee5bc88ffe83b061e68f",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 165,
            "deletions": 93,
            "changes": 258,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -17,6 +17,8 @@\n import threading\n from dataclasses import dataclass\n from functools import partial\n+from itertools import count\n+from time import perf_counter\n from typing import Optional\n \n import torch\n@@ -28,10 +30,45 @@\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n from .cache import PagedAttentionCache\n-from .classes import GenerationOutput, RequestState, RequestStatus, get_device_and_memory_breakdown, logger\n+from .requests import GenerationOutput, RequestState, RequestStatus, get_device_and_memory_breakdown, logger\n from .scheduler import SCHEDULER_MAPPING, FIFOScheduler, Scheduler\n \n \n+def build_attention_mask(\n+    attention_mask: torch.Tensor,\n+    cumulative_seqlens_q: torch.Tensor,\n+    cumulative_seqlens_k: torch.Tensor,\n+    sliding_window: int = 1,\n+) -> None:\n+    \"\"\"Builds an attention mask inplace using the cumulative seqlens of the query and key. If given a sliding window, it\n+    will also apply a sliding window mask on top. The attention mask is not boolean, it uses zeroes and -inf (or its\n+    equivalent) so it's more of an attention score bias tensor.\"\"\"\n+    min_value = torch.finfo(attention_mask.dtype).min\n+    for i in range(len(cumulative_seqlens_q) - 1):\n+        seqlen_q = cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]\n+        seqlen_k = cumulative_seqlens_k[i + 1] - cumulative_seqlens_k[i]\n+        if seqlen_q < seqlen_k and seqlen_q >= 1:\n+            causal_diagonal = seqlen_k - seqlen_q + 1\n+        else:\n+            causal_diagonal = 1\n+        query_range = slice(cumulative_seqlens_q[i], cumulative_seqlens_q[i + 1])\n+        key_range = slice(cumulative_seqlens_k[i], cumulative_seqlens_k[i + 1])\n+        # Apply causal mask\n+        minus_inf = torch.full(\n+            attention_mask[..., query_range, key_range].shape,\n+            min_value,\n+            dtype=attention_mask.dtype,\n+            device=attention_mask.device,\n+        )\n+        masked = torch.triu(minus_inf, diagonal=causal_diagonal)\n+        # Apply sliding window mask if needed\n+        if sliding_window > 1:\n+            sliding_diagonal = seqlen_k - seqlen_q + sliding_window\n+            masked = torch.tril(masked, diagonal=sliding_diagonal)\n+        # Replace in attention mask\n+        attention_mask[..., query_range, key_range] = masked\n+\n+\n @dataclass\n class PagedAttentionArgs:\n     input_ids: torch.Tensor\n@@ -41,10 +78,9 @@ class PagedAttentionArgs:\n     cumulative_seqlens_k: torch.Tensor\n     max_seqlen_q: int\n     max_seqlen_k: int\n-    write_index: torch.Tensor\n-    read_index: torch.Tensor\n+    write_index: list[torch.Tensor]\n+    read_index: list[torch.Tensor]\n     logits_indices: torch.Tensor\n-    block_tables: dict[str, list[int]]\n     cache: PagedAttentionCache\n     use_cache: bool = False\n \n@@ -65,19 +101,23 @@ def __init__(\n         scheduler: Scheduler,\n         streaming: bool = False,\n         manual_eviction: bool = False,\n-        slice_inputs: bool = True,  # TODO: remove this once parity is ensured\n+        slice_inputs: bool = True,  # TODO: There should be an heuristic to decide on slicing, compile, cuda graphs...\n     ):\n         \"\"\"Initialize the continuous batch processor.\n \n         Args:\n             cache: The paged attention cache to use\n+            config: The model configuration\n             generation_config: The generation configuration\n             input_queue: Queue for incoming requests\n             output_queue: Queue for outgoing results\n             stop_event: Event to signal processing should stop\n             model_device: Device for model inputs/outputs\n             model_dtype: Data type for model inputs/outputs\n+            scheduler: The [`Scheduler`] to use\n             streaming: Whether to stream tokens as they're generated\n+            manual_eviction: Whether to manually evict blocks from the cache\n+            slice_inputs: Whether to slice the inputs to the model\n         \"\"\"\n         self.cache = cache\n         self.config = config\n@@ -92,99 +132,119 @@ def __init__(\n         self.manual_eviction = manual_eviction\n         self.slice_inputs = slice_inputs\n \n+        # Retrieve the size of the sliding window if there is one\n+        self.sliding_window = 1 if getattr(config, \"sliding_window\", None) is None else config.sliding_window\n+\n         self.requests_in_batch: list[RequestState] = []\n \n         # Set up metrics collector\n         self.max_batch_tokens = cache.max_batch_tokens\n         self.metrics = ContinuousBatchProcessorMetrics(cache.max_batch_tokens)\n \n-        self.setup_static_tensors()\n+        # Setup static tensors\n+        self.total_query_length = 0\n+        self.total_key_length = 0\n+        self.total_batch_size = 0\n+        self.setup_static_tensors(cache.num_groups)\n \n     def return_attention_mask(self) -> bool:\n         return self.config._attn_implementation != \"paged_attention\"  # we set `is_causal` to True in paged call\n \n     @traced(standalone=True)\n-    def setup_static_tensors(self):\n+    def setup_static_tensors(self, num_groups: int):\n         T = self.max_batch_tokens\n-        max_token_budget = self.cache.num_blocks * self.cache.block_size\n+        num_pages = self.cache.num_blocks * self.cache.block_size\n         tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n-        # Prepare empty tensors\n         self.tensor_metadata = tensor_metadata\n         self.input_ids = torch.empty((1, T), **tensor_metadata)\n         self.position_ids = torch.empty((1, T), **tensor_metadata)\n         self.cumulative_seqlens_q = torch.empty((T + 1,), **tensor_metadata)\n-        self.cumulative_seqlens_k = torch.empty((T + 1,), **tensor_metadata)\n-        self.write_index = torch.empty((T,), **tensor_metadata)\n-        self.read_index = torch.empty((max_token_budget,), **tensor_metadata)\n+        self.cumulative_seqlens_k = {\n+            \"full_attention\": torch.empty((T + 1), **tensor_metadata),\n+            \"sliding_attention\": torch.empty((T + 1), **tensor_metadata),\n+            # TODO: can be generalized using layer types, for block-attn for instance\n+        }\n+\n+        # There is one read and write index tensor per group\n+        self.write_index_tensors = [torch.empty((T,), **tensor_metadata) for _ in range(num_groups)]\n+        self.read_index_tensors = [torch.empty((num_pages + T), **tensor_metadata) for _ in range(num_groups)]\n+        # +T is because there are -1 for seqlen_q when model uses a sliding window\n+\n         self.logits_indices = torch.empty((T,), **tensor_metadata)\n         self.max_seqlen_q = 0\n-        self.max_seqlen_k = 0\n+        self.max_seqlen_k = {\"full_attention\": 0, \"sliding_attention\": 0}\n         self.output_ids = torch.empty((1, T), **tensor_metadata)\n-        # Since attenention_mask is not always needed, we only allocate it if it is needed\n+        # Since attenention_mask is not always needed, we only allocate it if it is\n         if self.return_attention_mask():\n+            # TODO: this could be 2 iff model is hybrid, and then we can also change memory handler to account for it\n+            size_0 = 1 if self.sliding_window == 1 else 2\n             self.attention_mask = torch.empty(\n-                (1, 1, T, max_token_budget), dtype=self.model_dtype, device=self.model_device\n+                (size_0, 1, T, num_pages), dtype=self.model_dtype, device=self.model_device\n             )\n         else:\n+            logger.warning(f\"Attention mask is not needed for {self.config._attn_implementation}\")\n             self.attention_mask = None\n-        # Initialize the tensors by pretending they are in full use\n-        self.actual_tokens = T\n-        self.cache_used = max_token_budget\n-        self.reset_static_tensors()\n-        # Reset stats to 0\n-        self.actual_tokens = 0\n-        self.cache_used = 0\n+        self.reset_static_tensors(full_reset=True)\n \n     @traced\n     @torch.no_grad()\n-    def reset_static_tensors(self):\n-        \"\"\"Reset static tensors for the next batch.\"\"\"\n+    def reset_static_tensors(self, full_reset: bool = False):\n+        \"\"\"Reset static tensors for the next batch. In between batches, reset only the parts that were used in the last\n+        batch, but for initialisation, we can reset everything using the (full_reset) flag.\"\"\"\n         # Compute the slice to reset\n-        t = self.actual_tokens if self.slice_inputs else self.write_index.size(0)\n-        c = self.cache_used if self.slice_inputs else self.read_index.size(0)\n+        t = self.total_query_length if self.slice_inputs and not full_reset else self.write_index_tensors[0].size(-1)\n+        c = self.total_key_length if self.slice_inputs and not full_reset else self.read_index_tensors[0].size(-1)\n+        b = self.total_batch_size if self.slice_inputs and not full_reset else self.write_index_tensors[0].size(0)\n         # Reset the tensors\n         self.input_ids[:, :t].zero_()\n         self.position_ids[:, :t].zero_()\n-        self.cumulative_seqlens_q[: t + 1].zero_()\n-        self.cumulative_seqlens_k[: t + 1].zero_()\n-        self.write_index[:t].fill_(-1)\n-        self.read_index[:c].fill_(-1)\n+        self.cumulative_seqlens_q[: b + 1].zero_()\n+        for layer_type in self.cumulative_seqlens_k:\n+            self.cumulative_seqlens_k[layer_type][: b + 1].zero_()\n+            self.max_seqlen_k[layer_type] = 0\n+        for i in range(self.cache.num_groups):\n+            self.write_index_tensors[i][:t].fill_(-1)\n+            self.read_index_tensors[i][: t + c].fill_(-1)\n         self.logits_indices[:t].fill_(-1)\n         self.max_seqlen_q = 0\n-        self.max_seqlen_k = 0\n         self.output_ids[:, :t].fill_(-1)\n         if self.attention_mask is not None:\n             self.attention_mask[:, :, :t, :c].fill_(torch.finfo(self.model_dtype).min)\n \n     def get_model_kwargs(self) -> PagedAttentionArgs:\n         \"\"\"Get model keyword arguments for the current batch.\"\"\"\n         # Compute the slice to return\n-        t = self.actual_tokens if self.slice_inputs else self.write_index.size(0)\n-        c = self.cache_used if self.slice_inputs else self.read_index.size(0)\n+        t = self.total_query_length if self.slice_inputs else self.write_index.size(-1)\n+        b = self.total_batch_size\n         # Prepare the kwargs\n         kwargs = {\n             \"input_ids\": self.input_ids[:, :t],\n-            \"attention_mask\": self.attention_mask,\n             \"position_ids\": self.position_ids[:, :t],\n-            \"cu_seq_lens_q\": self.cumulative_seqlens_q[: t + 1],\n-            \"cu_seq_lens_k\": self.cumulative_seqlens_k[: t + 1],\n-            \"write_index\": self.write_index[:t],\n-            \"read_index\": self.read_index[:c],\n+            \"cu_seq_lens_q\": self.cumulative_seqlens_q[: b + 1],\n+            \"cu_seq_lens_k\": {},\n+            \"read_index\": self.read_index,  # slicing is done during building\n+            \"write_index\": self.write_index,  # slicing is done during building\n             \"logits_indices\": self.logits_indices[:t],\n             \"max_seqlen_q\": self.max_seqlen_q,\n             \"max_seqlen_k\": self.max_seqlen_k,\n-            \"block_tables\": self.cache._block_tables,\n             \"cache\": self.cache,\n             \"use_cache\": False,\n         }\n+        for layer_type in self.cumulative_seqlens_k:\n+            kwargs[\"cu_seq_lens_k\"][layer_type] = self.cumulative_seqlens_k[layer_type][: b + 1]\n         # If the attention mask is not None, we slice it as the others\n         if self.attention_mask is not None:\n-            kwargs[\"attention_mask\"] = self.attention_mask[:, :, :t, :c]\n+            kwargs[\"attention_mask\"] = {}\n+            for layer_type, seqlens_k in kwargs[\"cu_seq_lens_k\"].items():\n+                kwargs[\"attention_mask\"][layer_type] = self.attention_mask[:1, :, :t, : seqlens_k[-1]]\n+        else:\n+            kwargs[\"attention_mask\"] = None\n         return kwargs\n \n     def __repr__(self):\n         return (\n-            f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n+            f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, \"\n+            f\"active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n             + self.get_model_kwargs().__repr__()\n         )\n \n@@ -237,43 +297,60 @@ def prepare_next_batch(self) -> bool:\n             return False\n \n         # Get the request objects for this batch\n-        self.reset_static_tensors()\n+        self.reset_static_tensors()  # TOOD: with slice_inputs, this might be unnecessary\n         position_ids = []\n         input_ids = []\n-        read_index = []\n-        write_index = []\n+        read_index = [[] for _ in range(self.cache.num_groups)]\n+        write_index = [[] for _ in range(self.cache.num_groups)]\n         cumulative_seqlens_q = [0]\n-        cumulative_seqlens_k = [0]\n+        cumulative_seqlens_k = {\"full_attention\": [0], \"sliding_attention\": [0]}\n         logits_indices = []\n         self.metrics.record_batch_metrics(self.requests_in_batch)\n \n+        self.total_query_length = 0\n+        self.total_key_length = 0\n+        self.total_batch_size = 0\n+\n         for state in self.requests_in_batch:\n             next_input_ids = state.prompt_ids\n             input_ids.extend(next_input_ids)\n             past_length = state.position_offset\n             query_length = len(next_input_ids)\n             key_length = query_length + past_length\n-            cache_index = list(range(key_length))\n \n-            positions_to_add = cache_index[past_length:]\n-            read_indices = self.cache._get_physical_indices(state, cache_index)\n-            write_indices = read_indices[-query_length:]\n+            self.total_query_length += query_length\n+            self.total_key_length += key_length\n+            self.total_batch_size += 1\n+\n+            positions_to_add = list(range(past_length, key_length))\n+            self.cache.get_read_indices(state.request_id, past_length, query_length, read_index)\n+            self.cache.get_write_indices(state.request_id, past_length, query_length, write_index)\n \n             position_ids.extend(positions_to_add)\n-            read_index.extend(read_indices)\n-            write_index.extend(write_indices)\n             cumulative_seqlens_q.append(cumulative_seqlens_q[-1] + query_length)\n-            cumulative_seqlens_k.append(cumulative_seqlens_k[-1] + key_length)\n+\n+            cumulative_seqlens_k[\"full_attention\"].append(\n+                cumulative_seqlens_k[\"full_attention\"][-1] + query_length + past_length\n+            )\n+            cumulative_seqlens_k[\"sliding_attention\"].append(\n+                cumulative_seqlens_k[\"sliding_attention\"][-1]\n+                + query_length\n+                + min(past_length, self.sliding_window - 1)\n+            )\n+\n             if len(state.remaining_prompt_ids) == 0:\n                 logits_indices.append(cumulative_seqlens_q[-1] - 1)\n             self.max_seqlen_q = max(self.max_seqlen_q, query_length)\n-            self.max_seqlen_k = max(self.max_seqlen_k, key_length)\n+            self.max_seqlen_k[\"full_attention\"] = max(self.max_seqlen_k[\"full_attention\"], query_length + past_length)\n+            self.max_seqlen_k[\"sliding_attention\"] = max(\n+                self.max_seqlen_k[\"sliding_attention\"], query_length + min(past_length, self.sliding_window - 1)\n+            )\n             state.position_offset += query_length\n \n         logger.debug(\n             f\"Scheduled: {len(self.requests_in_batch)}, Waiting: {len(self.scheduler.waiting_requests)}, \"\n             f\"Active: {len(self.scheduler.active_requests)}. cum Q: {cumulative_seqlens_q[-1]}. \"\n-            f\"cum KV: {cumulative_seqlens_k[-1]}, free blocks: {self.cache.get_num_free_blocks()}\"\n+            f\"cum KV: {max(ck[-1] for ck in cumulative_seqlens_k)}, free blocks: {self.cache.get_num_free_blocks()}\"\n         )\n         self._build_tensors(\n             input_ids,\n@@ -294,57 +371,50 @@ def _build_tensors(\n         self,\n         input_ids,\n         position_ids,\n-        read_index,\n-        write_index,\n+        read_index: list[list[int]],\n+        write_index: list[list[int]],\n         cumulative_seqlens_q,\n         cumulative_seqlens_k,\n         logits_indices,\n     ):\n         to_tensor = partial(torch.tensor, **self.tensor_metadata)\n         self.input_ids[:, : len(input_ids)] = to_tensor(input_ids)\n         self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n-        self.write_index[: len(write_index)] = to_tensor(write_index)\n-        self.read_index[: len(read_index)] = to_tensor(read_index)\n+\n+        self.read_index = []\n+        self.write_index = []\n+        for i, group_read_indices, group_write_indices in zip(count(), read_index, write_index):\n+            # Write in the actual tensors\n+            self.read_index_tensors[i][: len(group_read_indices)] = to_tensor(group_read_indices)\n+            self.write_index_tensors[i][: len(group_write_indices)] = to_tensor(group_write_indices)\n+            # Slice to the right size\n+            r = len(group_read_indices) if self.slice_inputs else self.read_index_tensors[i].size(-1)\n+            w = len(group_write_indices) if self.slice_inputs else self.write_index_tensors[i].size(-1)\n+            # Add to the index\n+            self.read_index.append(self.read_index_tensors[i][:r])\n+            self.write_index.append(self.write_index_tensors[i][:w])\n+\n         self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n-        self.cumulative_seqlens_k[: len(cumulative_seqlens_k)] = to_tensor(cumulative_seqlens_k)\n+        for layer_type in self.cumulative_seqlens_k:\n+            l = len(cumulative_seqlens_k[layer_type])\n+            self.cumulative_seqlens_k[layer_type][:l] = to_tensor(cumulative_seqlens_k[layer_type])\n         self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n \n-        self.actual_tokens = len(input_ids)\n-        self.cache_used = len(read_index)\n-\n-        min_value = torch.finfo(self.model_dtype).min\n         if self.attention_mask is not None:\n-            for i in range(len(cumulative_seqlens_q) - 1):\n-                if (\n-                    cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]\n-                    < cumulative_seqlens_k[i + 1] - cumulative_seqlens_k[i]\n-                    and cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i] >= 1\n-                ):\n-                    diagonal = (\n-                        cumulative_seqlens_k[i + 1] - (cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]) + 1\n-                    )\n-                    diagonal = diagonal - cumulative_seqlens_k[i]\n-                else:\n-                    diagonal = 1\n-                query_range = slice(cumulative_seqlens_q[i], cumulative_seqlens_q[i + 1])\n-                key_range = slice(cumulative_seqlens_k[i], cumulative_seqlens_k[i + 1])\n-\n-                mask = torch.triu(\n-                    torch.full(\n-                        self.attention_mask[..., query_range, key_range].shape,\n-                        min_value,\n-                        dtype=self.model_dtype,\n-                        device=self.model_device,\n-                    ),\n-                    diagonal=diagonal,\n+            build_attention_mask(self.attention_mask[0], cumulative_seqlens_q, cumulative_seqlens_k[\"full_attention\"])\n+            if self.sliding_window != 1:\n+                build_attention_mask(\n+                    self.attention_mask[1],\n+                    cumulative_seqlens_q,\n+                    cumulative_seqlens_k[\"sliding_attention\"],\n+                    self.sliding_window,\n                 )\n-                self.attention_mask[..., query_range, key_range] = mask\n \n     @traced\n     def _sync(self):\n         if self.output_ids is not None:\n             try:\n-                out = self.output_ids.tolist()[0]  # should be the only synch we do\n+                out = self.output_ids.tolist()[0]  # should be the only sync we do\n             except Exception:\n                 out = [0, 1]\n         else:\n@@ -472,7 +542,6 @@ def start(self):\n         self._result_queue = queue.Queue()\n         self._generation_thread = threading.Thread(target=self._run_generation_loop)\n         self._generation_thread.start()\n-        logger.info(\"Continuous batching manager started.\")\n \n     def is_running(self):\n         \"\"\"Check if the background generation thread is running.\"\"\"\n@@ -670,15 +739,15 @@ def _run_generation_loop(self):\n         \"\"\"Main processing loop running in the background thread.\"\"\"\n         batch_processor = None\n         try:\n+            ref_time = perf_counter()\n             paged_attention_cache = PagedAttentionCache(\n                 self.model.config,\n                 self.generation_config,\n                 self.model.device,\n                 self.model.dtype,\n-                # FIXME: this is unused, why was it added?\n-                num_requests=len(self.input_queue.queue),\n                 tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n             )\n+            logger.debug(f\"PagedAttentionCache created in {perf_counter() - ref_time} seconds\")\n \n             scheduler = None\n             if hasattr(self.generation_config, \"scheduler\"):\n@@ -690,6 +759,7 @@ def _run_generation_loop(self):\n                 # Default to fifo\n                 scheduler = FIFOScheduler\n \n+            ref_time = perf_counter()\n             batch_processor = ContinuousBatchProcessor(\n                 paged_attention_cache,\n                 self.model.config,\n@@ -706,6 +776,7 @@ def _run_generation_loop(self):\n             )\n             self.batch_processor = batch_processor\n             self.current_batch = 0\n+            logger.debug(f\"batch_processor created in {perf_counter() - ref_time} seconds\")\n             while (not self.stop_event.is_set()) or batch_processor.has_pending_requests():\n                 self._inner_generation_loop(batch_processor)\n                 self.current_batch += 1\n@@ -722,8 +793,9 @@ def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor):\n             torch.cuda.synchronize()\n         if not batch_processor.prepare_next_batch():\n             return\n-        device, total, reserved, allocated = get_device_and_memory_breakdown()\n-        logger.debug(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n+        if logger.level <= logging.DEBUG:\n+            device, total, reserved, allocated = get_device_and_memory_breakdown()\n+            logger.debug(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n         if torch.cuda.is_available() and self.use_cuda_graph:\n             if self.current_batch == 0:\n                 self.warmup(batch_processor)"
        },
        {
            "sha": "a4e6da837e7acd7063bac7f048b4306da45c6be9",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -96,7 +96,7 @@ class RequestState:\n         prompt_ids (list[int] | None): The tokens IDs currently being processed.\n         remaining_prompt_ids (list[int]): The tokens IDs remaining to be processed (for split requests).\n         static_outputs (list[int]): The generated tokens.\n-        allocated_blocks (list[int]): The identifiers of the allocated blocks to the request.\n+        allocated_blocks (int): The number of blocks allocated to the request.\n         position_offset (int): The current position in the sequence for position_ids.\n         status (RequestStatus): The status of the request: can be one of PENDING, PREFILLING, PREFILLING_SPLIT,\n                                 SPLIT_PENDING_REMAINDER, DECODING, FINISHED, FAILED\n@@ -112,7 +112,7 @@ class RequestState:\n     prompt_ids: Optional[list[int]] = None  # Tokens IDs currently being processed (initial + generated)\n     remaining_prompt_ids: list[int] = field(default_factory=list)  # For split requests, prefill left to process\n     static_outputs: list[int] = field(default_factory=list)  # Generated tokens\n-    allocated_blocks: list[int] = field(default_factory=list)  # Block IDs allocated to the request\n+    allocated_blocks: int = 0  # Number of blocks allocated to the request\n     position_offset: int = 0  # Current position in the sequence for position_ids\n     _status: RequestStatus = RequestStatus.PENDING  # Status of the request, hidden behind a property\n     max_new_tokens: int = 20  # Maximum number of new tokens to generate",
            "previous_filename": "src/transformers/generation/continuous_batching/classes.py"
        },
        {
            "sha": "67cddbf14190a65f367b87bf09cd3f7d323b6c80",
            "filename": "src/transformers/generation/continuous_batching/scheduler.py",
            "status": "modified",
            "additions": 61,
            "deletions": 102,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -18,13 +18,14 @@\n \n from ...utils.metrics import attach_tracer, traced\n from .cache import PagedAttentionCache\n-from .classes import RequestState, RequestStatus\n+from .requests import RequestState, RequestStatus\n \n \n class Scheduler(ABC):\n     \"\"\"\n-    Abstract base class for scheduling requests in the continuous batch processor.\n-    It is expected that cache allocation and scheduling logic will be implemented in subclasses.\n+    Abstract base class for scheduling requests in the continuous batch processor. Schedulers manage the lifecycle of\n+    requests from when they are added to the waiting queue to when they are scheduled for processing. Different\n+    schedulers implement different strategies for prioritizing and batching requests.\n     \"\"\"\n \n     def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = False):\n@@ -36,38 +37,55 @@ def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = Fa\n         self._cancellation_lock = threading.Lock()\n         self._requests_to_cancel: set[str] = set()\n \n-    @abstractmethod\n+    @traced\n     def add_waiting_request(self, state: RequestState):\n-        \"\"\"Add a request to the waiting list.\"\"\"\n-        pass\n+        \"\"\"Adds a request to the waiting list.\"\"\"\n+        if self.retain_cache_on_finish and state.request_id in self.active_requests:\n+            old_state = self.active_requests.pop(state.request_id)\n+            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]  # XXX: check for indexing error?\n+            state.allocated_blocks = old_state.allocated_blocks\n+            state.position_offset = old_state.position_offset\n+        self.waiting_requests[state.request_id] = state\n+        self.waiting_requests_order.append(state.request_id)\n \n     @abstractmethod\n     def schedule_batch(self, token_budget: int) -> list[RequestState]:\n+        \"\"\"Schedules requests for the next batch based on available token budget. This method selects which requests\n+        should be processed in the current batch, considering the token budget and the scheduler's prioritization rules.\n+        The token_budget is the maximum number of tokens that can be processed in this batch.\"\"\"\n         pass\n \n     @traced\n     def has_pending_requests(self) -> bool:\n-        \"\"\"Check if there are requests ready to be processed.\"\"\"\n+        \"\"\"Checks if there are requests ready to be processed.\"\"\"\n         return len(self.active_requests) or len(self.waiting_requests)\n \n-    @abstractmethod\n+    @traced\n     def finish_request(self, request_id: str, evict_from_cache: bool = True):\n-        \"\"\"Finish processing a request and free its allocated blocks.\"\"\"\n-        pass\n+        \"\"\"Completes processing of a request and optionally frees its allocated cache blocks. This method is called\n+        when a request has finished generation or encountered an error.\n+        \"\"\"\n+        if evict_from_cache:\n+            self.cache.free_blocks(request_id)\n+            if request_id in self.active_requests:\n+                del self.active_requests[request_id]\n \n     @traced\n     def get_active_request_static_outputs(self, request_id: str) -> list[int]:\n+        \"\"\"Gets generated tokens for an active request.\"\"\"\n         if request_id in self.active_requests:\n             return self.active_requests[request_id].static_outputs\n         return []\n \n     @traced\n     def set_request_cancellation(self, request_id: str):\n+        \"\"\"Marks a request for cancellation.\"\"\"\n         with self._cancellation_lock:\n             self._requests_to_cancel.add(request_id)\n \n     @traced\n     def clear_cancelled_requests(self):\n+        \"\"\"Remove all cancelled requests from active and waiting queues.\"\"\"\n         with self._cancellation_lock:\n             for request_id in self._requests_to_cancel:\n                 if request_id in self.active_requests:\n@@ -81,36 +99,35 @@ def clear_cancelled_requests(self):\n \n     @traced\n     def request_is_cancelled(self, request_id: str) -> bool:\n+        \"\"\"Checks if a request has been cancelled or removed.\"\"\"\n         return request_id in self._requests_to_cancel or (\n             request_id not in self.active_requests and request_id not in self.waiting_requests\n         )\n \n-\n-@attach_tracer()\n-class FIFOScheduler(Scheduler):\n-    def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = False, safety_margin: float = 0.0):\n-        super().__init__(cache, retain_cache_on_finish)\n-        self.safety_margin = safety_margin\n-\n     @traced\n-    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n+    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int) -> bool:\n+        \"\"\"Allocate additional cache blocks for a request if the currently allocated blocks are insufficient to\n+        accommodate the next tokens. It calculates how many blocks are needed based on the request's current\n+        cache occupancy and the number of tokens to be processed. The allocation itself is done by the CacheAllocator\n+        objects. Returns a boolean indicating if the allocation was successful or not.\n+        \"\"\"\n         # 1. we check that the occupancy is less than the requested length\n         # 2. we allocate enough blocks to cover the requested length\n         current_len = state.current_len()\n-        occupancy = len(state.allocated_blocks) * self.cache.block_size - current_len\n-        if occupancy < len_next_tokens or (len(state.allocated_blocks) == 0):\n+        occupancy = state.allocated_blocks * self.cache.block_size - current_len\n+        if occupancy < len_next_tokens or state.allocated_blocks == 0:\n             blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n             allocated = self.cache.allocate_blocks(blocks_needed, state.request_id)\n-            if not allocated:\n+            if allocated is None:\n                 return False\n-            state.allocated_blocks.extend(allocated)\n+            state.allocated_blocks += allocated\n         return True\n \n     @traced(span_name=\"prepare_request\")\n     def _prepare_request_for_processing(\n         self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: set[str]\n     ):\n-        \"\"\"Prepare a request for processing in the current batch.\"\"\"\n+        \"\"\"Prepares a request for processing in the current batch.\"\"\"\n         request_tokens = (\n             state.remaining_prompt_ids if state.status == RequestStatus.SPLIT_PENDING_REMAINDER else state.prompt_ids\n         )\n@@ -135,16 +152,20 @@ def _prepare_request_for_processing(\n             state.remaining_prompt_ids = request_tokens[token_budget:]\n             state.prompt_ids = request_tokens[:token_budget]\n \n-    @traced\n-    def add_waiting_request(self, state: RequestState):\n-        \"\"\"Add a request to the waiting list.\"\"\"\n-        if self.retain_cache_on_finish and state.request_id in self.active_requests:\n-            old_state = self.active_requests.pop(state.request_id)\n-            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]\n-            state.allocated_blocks = old_state.allocated_blocks\n-            state.position_offset = old_state.position_offset\n-        self.waiting_requests[state.request_id] = state\n-        self.waiting_requests_order.append(state.request_id)\n+\n+@attach_tracer()\n+class FIFOScheduler(Scheduler):\n+    \"\"\"This scheduler processes requests in the order they arrive, meaning decoding requests has priority over\n+    prefilling requests. Additionally, it includes a safety margin mechanism to prevent cache exhaustion. By default,\n+    when 80% of the cache is full, new requests will not be scheduled to prioritize decoding active requests.\"\"\"\n+\n+    def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = False, safety_margin: float = 0.2):\n+        \"\"\"Initializes the FIFO scheduler. The safety margin is the percentage of free blocks under which we stop\n+        scheduling new prefill requests, so safety_margin = 0.1 means that when there is less than 10% of free blocks,\n+        or equivalently when more than 90% of blocks are already allocated, we stop scheduling new prefill requests.\n+        \"\"\"\n+        super().__init__(cache, retain_cache_on_finish)\n+        self.safety_margin = safety_margin\n \n     @traced\n     def schedule_batch(self, token_budget: int) -> list[RequestState]:\n@@ -155,7 +176,7 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n         for state in self.active_requests.values():\n             if state.status == RequestStatus.DECODING:\n                 priority_states.append(state)\n-            if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+            if state.status in [RequestStatus.SPLIT_PENDING_REMAINDER, RequestStatus.PREFILLING_SPLIT]:\n                 second_priority_states.append(state)\n \n         # Add waiting requests to second priority\n@@ -208,69 +229,13 @@ def _remove_from_waiting_requests(state: RequestState):\n \n         return scheduled_requests\n \n-    @traced\n-    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n-        if evict_from_cache:\n-            self.cache.free_blocks(request_id)\n-            if request_id in self.active_requests:\n-                del self.active_requests[request_id]\n-\n \n+# FIXME: prioritize adding from waiting reqs before scheduling `RequestStatus.DECODING` when cache space allows it\n @attach_tracer()\n class PrefillFirstScheduler(Scheduler):\n-    @traced\n-    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n-        # 1. we check that the occupancy is less than the requested length\n-        # 2. we allocate enough blocks to cover the requested length\n-        current_len = state.current_len()\n-        occupancy = len(state.allocated_blocks) * self.cache.block_size - current_len\n-        if occupancy < len_next_tokens or (len(state.allocated_blocks) == 0):\n-            blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n-            allocated = self.cache.allocate_blocks(blocks_needed, state.request_id)\n-            if not allocated:\n-                return False\n-            state.allocated_blocks.extend(allocated)\n-        return True\n-\n-    @traced(span_name=\"prepare_request\")\n-    def _prepare_request_for_processing(\n-        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: set[str]\n-    ):\n-        \"\"\"Prepare a request for processing in the current batch.\"\"\"\n-        request_tokens = (\n-            state.remaining_prompt_ids if state.status == RequestStatus.SPLIT_PENDING_REMAINDER else state.prompt_ids\n-        )\n-        if len(request_tokens) < token_budget:\n-            # Can process the entire prompt/remainder\n-            if state.status == RequestStatus.PENDING:\n-                self.active_requests[state.request_id] = state\n-                state.status = RequestStatus.PREFILLING\n-                request_ids_to_remove_from_waiting.add(state.request_id)\n-            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n-                state.status = RequestStatus.PREFILLING\n-                state.prompt_ids = state.remaining_prompt_ids\n-                state.remaining_prompt_ids = []\n-        else:\n-            # Need to split the request\n-            if state.status == RequestStatus.PENDING:\n-                self.active_requests[state.request_id] = state\n-                state.status = RequestStatus.PREFILLING_SPLIT\n-                request_ids_to_remove_from_waiting.add(state.request_id)\n-            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n-                state.status = RequestStatus.PREFILLING_SPLIT\n-            state.remaining_prompt_ids = request_tokens[token_budget:]\n-            state.prompt_ids = request_tokens[:token_budget]\n-\n-    @traced\n-    def add_waiting_request(self, state: RequestState):\n-        \"\"\"Add a request to the waiting list.\"\"\"\n-        if self.retain_cache_on_finish and state.request_id in self.active_requests:\n-            old_state = self.active_requests.pop(state.request_id)\n-            state.prompt_ids = state.prompt_ids[len(old_state.full_prompt_ids) :]  # XXX: check for indexing error?\n-            state.allocated_blocks = old_state.allocated_blocks\n-            state.position_offset = old_state.position_offset\n-        self.waiting_requests[state.request_id] = state\n-        self.waiting_requests_order.append(state.request_id)\n+    \"\"\"Scheduler that prioritizes split prefill requests over decoding requests. This scheduler ensures that split\n+    prefill requests (which are continuations of partially processed prompts) are completed before processing new\n+    decoding requests.\"\"\"\n \n     @traced\n     def schedule_batch(self, token_budget: int) -> list[RequestState]:\n@@ -279,7 +244,8 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n         scheduled_requests = []\n \n         for state in self.active_requests.values():\n-            if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+            # XXX: when cache is full, state can stay on `PREFILLING_SPLIT` so we need to take those into account\n+            if state.status in [RequestStatus.PREFILLING_SPLIT, RequestStatus.SPLIT_PENDING_REMAINDER]:\n                 priority_states.append(state)\n             elif state.status == RequestStatus.DECODING:\n                 second_priority_states.append(state)\n@@ -327,13 +293,6 @@ def _remove_from_waiting_requests(state: RequestState):\n \n         return scheduled_requests\n \n-    @traced\n-    def finish_request(self, request_id: str, evict_from_cache: bool = True):\n-        if evict_from_cache:\n-            self.cache.free_blocks(request_id)\n-            if request_id in self.active_requests:\n-                del self.active_requests[request_id]\n-\n \n SCHEDULER_MAPPING = {\n     \"fifo\": FIFOScheduler,"
        },
        {
            "sha": "6c0b16852f78465d70fb869e8d3693003d9b5f32",
            "filename": "src/transformers/integrations/eager_paged.py",
            "status": "modified",
            "additions": 22,
            "deletions": 9,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feager_paged.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -21,25 +21,38 @@ def eager_paged_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: Optional[torch.Tensor],  # shape [seqlen_q, seqlen_k]\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs,\n ):\n+    # Add KV cache to the key and value tensors\n     cache = kwargs.pop(\"cache\", None)\n     if cache is not None:\n+        # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n         key, value = cache.update(key, value, module.layer_idx, **kwargs)\n-\n-    key_states = repeat_kv(key, module.num_key_value_groups)\n-    value_states = repeat_kv(value, module.num_key_value_groups)\n-\n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n-    if attention_mask is not None:\n-        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        key = key.transpose(0, 1).unsqueeze(0)\n+        value = value.transpose(0, 1).unsqueeze(0)\n+\n+    # Repeat the key and value tensors for each group of key-value heads\n+    if hasattr(module, \"num_key_value_groups\"):\n+        key = repeat_kv(key, module.num_key_value_groups)\n+        value = repeat_kv(value, module.num_key_value_groups)\n+\n+    # Get the right causal mask for the current layer\n+    if isinstance(attention_mask, dict):\n+        sliding_window = getattr(module, \"sliding_window\", 1)\n+        layer_type = \"full_attention\" if sliding_window == 1 else \"sliding_attention\"\n+        causal_mask = attention_mask[layer_type]\n+    else:\n+        causal_mask = attention_mask\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if causal_mask is not None:\n         attn_weights = attn_weights + causal_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n \n     return attn_output, attn_weights"
        },
        {
            "sha": "d9f0b4a3b702a08af6c01954b718185a5dac11e6",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 17,
            "deletions": 6,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -24,7 +24,6 @@ def paged_attention_forward(\n     cu_seq_lens_k=None,\n     max_seqlen_q=None,\n     max_seqlen_k=None,\n-    block_tables=None,\n     implementation=None,\n     **kwargs,\n ) -> torch.Tensor:\n@@ -50,24 +49,36 @@ def paged_attention_forward(\n         window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n         softcap: float. Anything > 0 activates softcapping attention.\n     \"\"\"\n-    k, v = cache.update(k, v, module.layer_idx, **kwargs)\n+    sliding_window = (-1, -1) if not getattr(module, \"sliding_window\", False) else (module.sliding_window - 1, 0)\n+    layer_type = \"full_attention\" if sliding_window == (-1, -1) else \"sliding_attention\"\n+\n+    # .update changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n+    if cache is not None:\n+        k, v = cache.update(k, v, module.layer_idx, **kwargs)\n+\n+        # Check if we are in a sliding window context\n+        cu_seq_lens_k = cu_seq_lens_k[layer_type].clone()\n+        max_seqlen_k = max_seqlen_k[layer_type]\n+\n+    # If there is no cache, we assume this is full attention, and we check if cu_seq_lens_k is a list of tensors\n+    elif isinstance(cu_seq_lens_k, list):\n+        cu_seq_lens_k = cu_seq_lens_k[layer_type].clone()\n+        max_seqlen_k = max_seqlen_k[layer_type]\n \n-    sliding_window = (-1, -1) if not getattr(module, \"sliding_window\", False) else (module.sliding_window, 0)\n     if implementation is not None and hasattr(implementation, \"flash_attn_varlen_func\"):\n         flash_attn_varlen_func = implementation.flash_attn_varlen_func\n     custom_kwargs = {\"s_aux\": kwargs.get(\"s_aux\")} if \"s_aux\" in kwargs else {}\n     attn_output = flash_attn_varlen_func(\n         q.transpose(1, 2).squeeze(0).contiguous(),\n-        k.transpose(1, 2).squeeze(0).contiguous(),\n-        v.transpose(1, 2).squeeze(0).contiguous(),\n+        k.contiguous(),\n+        v.contiguous(),\n         cu_seq_lens_q.to(torch.int32),\n         cu_seq_lens_k.to(torch.int32).clone(),\n         max_seqlen_q,\n         max_seqlen_k,\n         softmax_scale=module.scaling,\n         causal=True,  # kind of a must, it automatically aligns the mask for q < k\n         window_size=sliding_window,  # -1 means infinite context window\n-        # block_table=block_tables, -> torch.Tensor\n         **custom_kwargs,\n     )\n     if isinstance(attn_output, tuple):"
        },
        {
            "sha": "b9ad60fa3ec1bb25fa747d66958df295ad3a7c4f",
            "filename": "src/transformers/integrations/sdpa_paged.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -26,14 +26,28 @@ def sdpa_attention_paged_forward(\n     is_causal: Optional[bool] = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n+    # Add KV cache to the key and value tensors\n     cache = kwargs.pop(\"cache\", None)\n     if cache is not None:\n+        # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n         key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+        key = key.transpose(0, 1).unsqueeze(0)\n+        value = value.transpose(0, 1).unsqueeze(0)\n+\n+    # Repeat the key and value tensors for each group of key-value heads\n     if hasattr(module, \"num_key_value_groups\"):\n         key = repeat_kv(key, module.num_key_value_groups)\n         value = repeat_kv(value, module.num_key_value_groups)\n \n-    causal_mask = attention_mask\n+    # Get the right causal mask for the current layer\n+    if isinstance(attention_mask, dict):\n+        sliding_window = getattr(module, \"sliding_window\", 1)\n+        layer_type = \"full_attention\" if sliding_window == 1 else \"sliding_attention\"\n+        causal_mask = attention_mask[layer_type]\n+    else:\n+        causal_mask = attention_mask\n+\n+    # Run the actual attention\n     query = query.contiguous()\n     key = key.contiguous()\n     value = value.contiguous()"
        },
        {
            "sha": "44e2f3c22122a1c09c2b0aef641f39859c9b66b9",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cdbbb3e9d43e58ef7596a6013194f0e37a73c81/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=1cdbbb3e9d43e58ef7596a6013194f0e37a73c81",
            "patch": "@@ -0,0 +1,84 @@\n+# Copyright 2025 The HuggingFace Team Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a clone of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+from typing import Optional\n+\n+from parameterized import parameterized\n+\n+from transformers import AutoConfig\n+from transformers.generation.continuous_batching.cache import group_layers_by_attn_type\n+\n+\n+class ContinuousBatchingTest(unittest.TestCase):\n+    @parameterized.expand(\n+        [\n+            (None, None, \"0\"),\n+            (None, 4096, \"0\"),\n+            (\"f\", None, \"0\"),\n+            (\"ffff\", None, \"0000\"),\n+            (\"sssss\", 4096, \"00000\"),\n+            (\"fs\", 4096, \"01\"),\n+            (\"ssfssf\", 4096, \"001221\"),\n+            (\"ssssf\", 4096, \"01234\"),\n+            (\"fffsffs\", 4096, \"0123456\"),\n+        ]\n+    )\n+    def test_group_layers(\n+        self,\n+        layer_types_str: Optional[str],\n+        sliding_window: Optional[int],\n+        expected_groups: str,\n+    ) -> None:\n+        # Take a config and change the layer_types attribute to the mix we want\n+        config = AutoConfig.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n+\n+        if layer_types_str is not None:\n+            layer_types = [{\"f\": \"full_attention\", \"s\": \"sliding_window\"}[char] for char in layer_types_str]\n+        else:\n+            layer_types = None\n+            config.num_hidden_layers = len(expected_groups)\n+\n+        config.layer_types = layer_types\n+        config.sliding_window = sliding_window\n+\n+        expected_lg = {}\n+        for i, group in enumerate(expected_groups):\n+            group = int(group)\n+            expected_lg[group] = expected_lg.get(group, []) + [i]\n+        expected_layer_groups = [expected_lg[i] for i in sorted(expected_lg.keys())]\n+\n+        # Test layer groups formation\n+        layer_groups, group_types = group_layers_by_attn_type(config)\n+        self.assertEqual(\n+            sorted(expected_layer_groups),\n+            sorted(layer_groups),\n+            f\"Test failed for: {layer_types_str = }, {sliding_window = }, {expected_layer_groups = }, {layer_groups = }\",\n+        )\n+\n+        # If layer_types is provided, check that group_types matches the type of the all layers in each group\n+        if layer_types is not None:\n+            for layer_group, group_type in zip(layer_groups, group_types):\n+                layer_types = [config.layer_types[i] for i in layer_group]\n+                self.assertEqual(layer_types, [group_type] * len(layer_types))\n+        # If layer_types is None, all groups should be of the same type\n+        else:\n+            for group_type in group_types:\n+                sliding_window = getattr(config, \"sliding_window\", None)\n+                expected_group_type = \"sliding_attention\" if sliding_window is not None else \"full_attention\"\n+                self.assertEqual(\n+                    group_type,\n+                    expected_group_type,\n+                    f\"Test failed for: {layer_types_str = }, {sliding_window = }, {group_types = }\",\n+                )"
        }
    ],
    "stats": {
        "total": 1447,
        "additions": 1018,
        "deletions": 429
    }
}