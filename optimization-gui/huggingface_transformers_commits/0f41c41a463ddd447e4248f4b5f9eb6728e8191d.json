{
    "author": "SunMarc",
    "message": "Fix hqq issue (#38551)\n\n* bc\n\n* style",
    "sha": "0f41c41a463ddd447e4248f4b5f9eb6728e8191d",
    "files": [
        {
            "sha": "07dda2588cb5307c25c98adcd4c34e43d9bc545b",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f41c41a463ddd447e4248f4b5f9eb6728e8191d/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f41c41a463ddd447e4248f4b5f9eb6728e8191d/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=0f41c41a463ddd447e4248f4b5f9eb6728e8191d",
            "patch": "@@ -135,7 +135,11 @@ def _find_hqq_quantizable_layers(model, layers):\n \n             # Append new expected layers based on _ref_keys\n             _ref_keys = HQQLinear(\n-                linear_layer=None, quant_config=None, compute_dtype=torch.float16, device=\"cpu\"\n+                linear_layer=None,\n+                quant_config=None,\n+                compute_dtype=torch.float16,\n+                device=\"cpu\",\n+                del_orig=False,\n             ).state_dict_keys() - {\"bias\"}\n \n             # Clean-up\n@@ -224,6 +228,7 @@ def create_quantized_param(\n                     quant_config=None,\n                     compute_dtype=self.torch_dtype,\n                     device=target_device,\n+                    del_orig=False,\n                 )\n \n             hqq_layer.load_state_dict(module_state_dict)"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 6,
        "deletions": 1
    }
}