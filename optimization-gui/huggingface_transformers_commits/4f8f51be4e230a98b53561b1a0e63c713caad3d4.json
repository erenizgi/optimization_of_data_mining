{
    "author": "capnmav77",
    "message": "Add Fast Segformer Processor (#37024)\n\n* Add Fast Segformer Processor\n\n* Modified the params according to segformer model\n\n* modified test_image_processing_Segformer_fast args\n\n- removed redundant params like do_center_crop,center_crop which aren't present in the original segformer class\n\n* added segmentation_maps processing logic form the slow segformer processing module with references from beitimageprocessing fast\n\n* fixed code_quality\n\n* added recommended fixes and tests to make sure everything processess smoothly\n\n* Fixed SegmentationMapsLogic\n\n- modified the preprocessing of segmentation maps to use tensors\n- added batch support\n\n* fixed some mismatched files\n\n* modified the tolerance for tests\n\n* use modular\n\n* fix ci\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "4f8f51be4e230a98b53561b1a0e63c713caad3d4",
    "files": [
        {
            "sha": "730757aca5f1dfba72fabf060db71b697416d6bc",
            "filename": "docs/source/en/model_doc/segformer.md",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f8f51be4e230a98b53561b1a0e63c713caad3d4/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f8f51be4e230a98b53561b1a0e63c713caad3d4/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md?ref=4f8f51be4e230a98b53561b1a0e63c713caad3d4",
            "patch": "@@ -128,6 +128,12 @@ If you're interested in submitting a resource to be included here, please feel f\n     - preprocess\n     - post_process_semantic_segmentation\n \n+## SegformerImageProcessorFast\n+\n+[[autodoc]] SegformerImageProcessorFast\n+    - preprocess\n+    - post_process_semantic_segmentation\n+\n <frameworkcontent>\n <pt>\n \n@@ -175,4 +181,4 @@ If you're interested in submitting a resource to be included here, please feel f\n     - call\n \n </tf>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "ec55316484646eadf860241a55d44b5771f69174",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=4f8f51be4e230a98b53561b1a0e63c713caad3d4",
            "patch": "@@ -156,6 +156,7 @@\n             (\"sam\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"sam_hq\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"segformer\", (\"SegformerImageProcessor\",)),\n+            (\"segformer\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n             (\"seggpt\", (\"SegGptImageProcessor\",)),\n             (\"shieldgemma2\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n@@ -179,7 +180,8 @@\n             (\"tvlt\", (\"TvltImageProcessor\",)),\n             (\"tvp\", (\"TvpImageProcessor\",)),\n             (\"udop\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n-            (\"upernet\", (\"SegformerImageProcessor\",)),\n+            (\"udop\", (\"LayoutLMv3ImageProcessor\",)),\n+            (\"upernet\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n             (\"van\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"videomae\", (\"VideoMAEImageProcessor\",)),\n             (\"vilt\", (\"ViltImageProcessor\", \"ViltImageProcessorFast\")),"
        },
        {
            "sha": "43ed6dd1125d9c56ea2f51863687ee650256a5da",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 17,
            "deletions": 5,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=4f8f51be4e230a98b53561b1a0e63c713caad3d4",
            "patch": "@@ -16,9 +16,6 @@\n \n from typing import Optional, Union\n \n-import torch\n-from torchvision.transforms import functional as F\n-\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -36,11 +33,26 @@\n     is_torch_tensor,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n \n \n class BeitFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n+    r\"\"\"\n     do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n         Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n         is used for background, and background itself is not included in all classes of a dataset (e.g."
        },
        {
            "sha": "81655dfa7048edc5fd9c91c80c3a77be90a6010d",
            "filename": "src/transformers/models/segformer/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fsegformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fsegformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2F__init__.py?ref=4f8f51be4e230a98b53561b1a0e63c713caad3d4",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_segformer import *\n     from .feature_extraction_segformer import *\n     from .image_processing_segformer import *\n+    from .image_processing_segformer_fast import *\n     from .modeling_segformer import *\n     from .modeling_tf_segformer import *\n else:"
        },
        {
            "sha": "e919628f450a19eacd1312b988de213c8b47af1c",
            "filename": "src/transformers/models/segformer/image_processing_segformer_fast.py",
            "status": "added",
            "additions": 247,
            "deletions": 0,
            "changes": 247,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py?ref=4f8f51be4e230a98b53561b1a0e63c713caad3d4",
            "patch": "@@ -0,0 +1,247 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/segformer/modular_segformer.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_segformer.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    is_torch_tensor,\n+    pil_torch_interpolation_mapping,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+class SegformerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+        is used for background, and background itself is not included in all classes of a dataset (e.g.\n+        ADE20k). The background label will be replaced by 255.\n+    \"\"\"\n+\n+    do_reduce_labels: Optional[bool]\n+\n+\n+@auto_docstring\n+class SegformerImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"height\": 512, \"width\": 512}\n+    default_to_square = True\n+    crop_size = None\n+    do_resize = True\n+    do_center_crop = None\n+    do_rescale = True\n+    do_normalize = True\n+    do_reduce_labels = False\n+    valid_kwargs = SegformerFastImageProcessorKwargs\n+    rescale_factor = 1 / 255\n+\n+    def __init__(self, **kwargs: Unpack[SegformerFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def reduce_label(self, labels: list[\"torch.Tensor\"]):\n+        for idx in range(len(labels)):\n+            label = labels[idx]\n+            label = torch.where(label == 0, torch.tensor(255, dtype=label.dtype), label)\n+            label = label - 1\n+            label = torch.where(label == 254, torch.tensor(255, dtype=label.dtype), label)\n+            labels[idx] = label\n+\n+        return label\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[SegformerFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, segmentation_maps, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[SegformerFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        images_kwargs = kwargs.copy()\n+        images_kwargs[\"do_reduce_labels\"] = False\n+        batch_feature = self._preprocess(images, **images_kwargs)\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update(\n+                {\n+                    \"do_normalize\": False,\n+                    \"do_rescale\": False,\n+                    # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n+                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                }\n+            )\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            ).pixel_values\n+            batch_feature[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+\n+        return batch_feature\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_reduce_labels: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_resize: bool,\n+        do_rescale: bool,\n+        do_normalize: bool,\n+        size: SizeDict,\n+        rescale_factor: float,\n+        image_mean: Union[float, list[float]],\n+        image_std: Union[float, list[float]],\n+        disable_grouping: bool,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:  # Return type can be list if return_tensors=None\n+        if do_reduce_labels:\n+            images = self.reduce_label(images)  # Apply reduction if needed\n+\n+        # Group images by size for batched resizing\n+        resized_images = images\n+        if do_resize:\n+            grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+            resized_images_grouped = {}\n+            for shape, stacked_images in grouped_images.items():\n+                resized_stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+                resized_images_grouped[shape] = resized_stacked_images\n+            resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing (rescale/normalize)\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Stack images into a single tensor if return_tensors is set\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n+        \"\"\"\n+        Converts the output of [`SegformerForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`SegformerForSemanticSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`list[Tuple]` of length `batch_size`, *optional*):\n+                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n+                predictions will not be resized.\n+\n+        Returns:\n+            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n+            specified). Each entry of each `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        # TODO: add support for other frameworks\n+        logits = outputs.logits\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if len(logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            if is_torch_tensor(target_sizes):\n+                target_sizes = target_sizes.numpy()\n+\n+            semantic_segmentation = []\n+\n+            for idx in range(len(logits)):\n+                resized_logits = torch.nn.functional.interpolate(\n+                    logits[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = logits.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+\n+__all__ = [\"SegformerImageProcessorFast\"]"
        },
        {
            "sha": "68aa0b752df478bd21aaebf9d210367915119668",
            "filename": "src/transformers/models/segformer/modular_segformer.py",
            "status": "added",
            "additions": 161,
            "deletions": 0,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f8f51be4e230a98b53561b1a0e63c713caad3d4/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py?ref=4f8f51be4e230a98b53561b1a0e63c713caad3d4",
            "patch": "@@ -0,0 +1,161 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Segformer.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from transformers.models.beit.image_processing_beit_fast import BeitFastImageProcessorKwargs, BeitImageProcessorFast\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    pil_torch_interpolation_mapping,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+class SegformerFastImageProcessorKwargs(BeitFastImageProcessorKwargs):\n+    pass\n+\n+\n+class SegformerImageProcessorFast(BeitImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"height\": 512, \"width\": 512}\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = True\n+    do_reduce_labels = False\n+    do_center_crop = None\n+    crop_size = None\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[SegformerFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        images_kwargs = kwargs.copy()\n+        images_kwargs[\"do_reduce_labels\"] = False\n+        batch_feature = self._preprocess(images, **images_kwargs)\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update(\n+                {\n+                    \"do_normalize\": False,\n+                    \"do_rescale\": False,\n+                    # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n+                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                }\n+            )\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            ).pixel_values\n+            batch_feature[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+\n+        return batch_feature\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_reduce_labels: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_resize: bool,\n+        do_rescale: bool,\n+        do_normalize: bool,\n+        size: SizeDict,\n+        rescale_factor: float,\n+        image_mean: Union[float, list[float]],\n+        image_std: Union[float, list[float]],\n+        disable_grouping: bool,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:  # Return type can be list if return_tensors=None\n+        if do_reduce_labels:\n+            images = self.reduce_label(images)  # Apply reduction if needed\n+\n+        # Group images by size for batched resizing\n+        resized_images = images\n+        if do_resize:\n+            grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+            resized_images_grouped = {}\n+            for shape, stacked_images in grouped_images.items():\n+                resized_stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+                resized_images_grouped[shape] = resized_stacked_images\n+            resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing (rescale/normalize)\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Stack images into a single tensor if return_tensors is set\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"SegformerImageProcessorFast\"]"
        },
        {
            "sha": "53e63676575dee7232981b03aa143b6284b77d64",
            "filename": "tests/models/segformer/test_image_processing_segformer.py",
            "status": "modified",
            "additions": 185,
            "deletions": 132,
            "changes": 317,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f8f51be4e230a98b53561b1a0e63c713caad3d4/tests%2Fmodels%2Fsegformer%2Ftest_image_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f8f51be4e230a98b53561b1a0e63c713caad3d4/tests%2Fmodels%2Fsegformer%2Ftest_image_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsegformer%2Ftest_image_processing_segformer.py?ref=4f8f51be4e230a98b53561b1a0e63c713caad3d4",
            "patch": "@@ -18,7 +18,7 @@\n from datasets import load_dataset\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -29,6 +29,9 @@\n if is_vision_available():\n     from transformers import SegformerImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import SegformerImageProcessorFast\n+\n \n class SegformerImageProcessingTester:\n     def __init__(\n@@ -98,6 +101,7 @@ def prepare_semantic_batch_inputs():\n @require_vision\n class SegformerImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = SegformerImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = SegformerImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -108,142 +112,191 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 30, \"width\": 30})\n-        self.assertEqual(image_processor.do_reduce_labels, False)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 30, \"width\": 30})\n+            self.assertEqual(image_processor.do_reduce_labels, False)\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, do_reduce_labels=True\n-        )\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n-        self.assertEqual(image_processor.do_reduce_labels, True)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, do_reduce_labels=True\n+            )\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            self.assertEqual(image_processor.do_reduce_labels, True)\n \n     def test_call_segmentation_maps(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        maps = []\n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n-            maps.append(torch.zeros(image.shape[-2:]).long())\n-\n-        # Test not batched input\n-        encoding = image_processing(image_inputs[0], maps[0], return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test batched\n-        encoding = image_processing(image_inputs, maps, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                self.image_processor_tester.batch_size,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test not batched input (PIL images)\n-        image, segmentation_map = prepare_semantic_single_inputs()\n-\n-        encoding = image_processing(image, segmentation_map, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                1,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n-\n-        # Test batched input (PIL images)\n-        images, segmentation_maps = prepare_semantic_batch_inputs()\n-\n-        encoding = image_processing(images, segmentation_maps, return_tensors=\"pt\")\n-        self.assertEqual(\n-            encoding[\"pixel_values\"].shape,\n-            (\n-                2,\n-                self.image_processor_tester.num_channels,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(\n-            encoding[\"labels\"].shape,\n-            (\n-                2,\n-                self.image_processor_tester.size[\"height\"],\n-                self.image_processor_tester.size[\"width\"],\n-            ),\n-        )\n-        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            maps = []\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+                maps.append(torch.zeros(image.shape[-2:]).long())\n+\n+            # Test not batched input\n+            encoding = image_processing(image_inputs[0], maps[0], return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched\n+            encoding = image_processing(image_inputs, maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    self.image_processor_tester.batch_size,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test not batched input (PIL images)\n+            image, segmentation_map = prepare_semantic_single_inputs()\n+\n+            encoding = image_processing(image, segmentation_map, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    1,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+            # Test batched input (PIL images)\n+            images, segmentation_maps = prepare_semantic_batch_inputs()\n+\n+            encoding = image_processing(images, segmentation_maps, return_tensors=\"pt\")\n+            self.assertEqual(\n+                encoding[\"pixel_values\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.num_channels,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(\n+                encoding[\"labels\"].shape,\n+                (\n+                    2,\n+                    self.image_processor_tester.size[\"height\"],\n+                    self.image_processor_tester.size[\"width\"],\n+                ),\n+            )\n+            self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n \n     def test_reduce_labels(self):\n         # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-\n-        # ADE20k has 150 classes, and the background is included, so labels should be between 0 and 150\n-        image, map = prepare_semantic_single_inputs()\n-        encoding = image_processing(image, map, return_tensors=\"pt\")\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 150)\n-\n-        image_processing.do_reduce_labels = True\n-        encoding = image_processing(image, map, return_tensors=\"pt\")\n-        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n-        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+\n+            # ADE20k has 150 classes, and the background is included, so labels should be between 0 and 150\n+            image, map = prepare_semantic_single_inputs()\n+            encoding = image_processing(image, map, return_tensors=\"pt\")\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 150)\n+\n+            image_processing.do_reduce_labels = True\n+            encoding = image_processing(image, map, return_tensors=\"pt\")\n+            self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+            self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image, dummy_map = prepare_semantic_single_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        image_encoding_slow = image_processor_slow(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        image_encoding_fast = image_processor_fast(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(\n+            image_encoding_slow.labels.float(), image_encoding_fast.labels.float(), atol=5, mean_atol=0.01\n+        )\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images, dummy_maps = prepare_semantic_batch_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.labels.float(), encoding_fast.labels.float(), atol=5, mean_atol=0.01\n+        )"
        }
    ],
    "stats": {
        "total": 760,
        "additions": 621,
        "deletions": 139
    }
}