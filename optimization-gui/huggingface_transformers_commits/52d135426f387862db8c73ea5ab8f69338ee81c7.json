{
    "author": "henryhmko",
    "message": "Multiple typo fixes in NLP, Audio docs (#35181)\n\nFixed multiple typos in Tutorials, NLP, and Audio sections",
    "sha": "52d135426f387862db8c73ea5ab8f69338ee81c7",
    "files": [
        {
            "sha": "87b8f024420ce655da2f5aa73113c58d997a93c3",
            "filename": "docs/source/en/tasks/asr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fasr.md?ref=52d135426f387862db8c73ea5ab8f69338ee81c7",
            "patch": "@@ -112,7 +112,7 @@ The next step is to load a Wav2Vec2 processor to process the audio signal:\n >>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base\")\n ```\n \n-The MInDS-14 dataset has a sampling rate of 8000kHz (you can find this information in its [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which means you'll need to resample the dataset to 16000kHz to use the pretrained Wav2Vec2 model:\n+The MInDS-14 dataset has a sampling rate of 8000Hz (you can find this information in its [dataset card](https://huggingface.co/datasets/PolyAI/minds14)), which means you'll need to resample the dataset to 16000Hz to use the pretrained Wav2Vec2 model:\n \n ```py\n >>> minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))"
        },
        {
            "sha": "18b12f2166637e6ff707bbba238ca75493653837",
            "filename": "docs/source/en/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md?ref=52d135426f387862db8c73ea5ab8f69338ee81c7",
            "patch": "@@ -419,7 +419,7 @@ Get the class with the highest probability:\n ```py\n >>> predicted_class = logits.argmax().item()\n >>> predicted_class\n-'0'\n+0\n ```\n </pt>\n <tf>\n@@ -448,7 +448,7 @@ Get the class with the highest probability:\n ```py\n >>> predicted_class = int(tf.math.argmax(logits, axis=-1)[0])\n >>> predicted_class\n-'0'\n+0\n ```\n </tf>\n </frameworkcontent>"
        },
        {
            "sha": "41d7fd48cf816e099892c7bfa3037825721f97eb",
            "filename": "docs/source/en/tasks/question_answering.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fquestion_answering.md?ref=52d135426f387862db8c73ea5ab8f69338ee81c7",
            "patch": "@@ -325,7 +325,7 @@ or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/no\n \n Evaluation for question answering requires a significant amount of postprocessing. To avoid taking up too much of your time, this guide skips the evaluation step. The [`Trainer`] still calculates the evaluation loss during training so you're not completely in the dark about your model's performance.\n \n-If have more time and you're interested in how to evaluate your model for question answering, take a look at the [Question answering](https://huggingface.co/course/chapter7/7?fw=pt#post-processing) chapter from the ðŸ¤— Hugging Face Course!\n+If you have more time and you're interested in how to evaluate your model for question answering, take a look at the [Question answering](https://huggingface.co/course/chapter7/7?fw=pt#post-processing) chapter from the ðŸ¤— Hugging Face Course!\n \n ## Inference\n \n@@ -397,7 +397,7 @@ Tokenize the text and return TensorFlow tensors:\n >>> from transformers import AutoTokenizer\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n->>> inputs = tokenizer(question, text, return_tensors=\"tf\")\n+>>> inputs = tokenizer(question, context, return_tensors=\"tf\")\n ```\n \n Pass your inputs to the model and return the `logits`:"
        },
        {
            "sha": "e16dd17dfe1fc8186cd0f121f5f794d0f7f7c16e",
            "filename": "docs/source/en/tasks/summarization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md?ref=52d135426f387862db8c73ea5ab8f69338ee81c7",
            "patch": "@@ -283,7 +283,7 @@ Pass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n ```py\n >>> from transformers.keras_callbacks import KerasMetricCallback\n \n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n+>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n ```\n \n Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:"
        },
        {
            "sha": "922cdc7241176a22faf030e37ea21b5b7f467ba3",
            "filename": "docs/source/en/tasks/translation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52d135426f387862db8c73ea5ab8f69338ee81c7/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md?ref=52d135426f387862db8c73ea5ab8f69338ee81c7",
            "patch": "@@ -290,7 +290,7 @@ Pass your `compute_metrics` function to [`~transformers.KerasMetricCallback`]:\n ```py\n >>> from transformers.keras_callbacks import KerasMetricCallback\n \n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n+>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n ```\n \n Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:"
        },
        {
            "sha": "69e674a2160643fa3263e95a9d40175aa0de5613",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/52d135426f387862db8c73ea5ab8f69338ee81c7/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52d135426f387862db8c73ea5ab8f69338ee81c7/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=52d135426f387862db8c73ea5ab8f69338ee81c7",
            "patch": "@@ -108,7 +108,7 @@ def load_adapter(\n                 </Tip>\n \n             token (`str`, `optional`):\n-                Whether to use authentication token to load the remote folder. Userful to load private repositories\n+                Whether to use authentication token to load the remote folder. Useful to load private repositories\n                 that are on HuggingFace Hub. You might need to call `huggingface-cli login` and paste your tokens to\n                 cache it.\n             device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 8,
        "deletions": 8
    }
}