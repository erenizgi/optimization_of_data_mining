{
    "author": "SunMarc",
    "message": "Fix optuna ddp hp search (#34073)",
    "sha": "d00f1ca860f19f4c0962882e56044bb6ef7b5626",
    "files": [
        {
            "sha": "4f7cf3632fe54988e76fcb90d82f295e8de7a1e5",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d00f1ca860f19f4c0962882e56044bb6ef7b5626/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d00f1ca860f19f4c0962882e56044bb6ef7b5626/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=d00f1ca860f19f4c0962882e56044bb6ef7b5626",
            "patch": "@@ -241,7 +241,8 @@ def _objective(trial, checkpoint_dir=None):\n                 if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:\n                     raise RuntimeError(\"only support DDP optuna HPO for ParallelMode.DISTRIBUTED currently.\")\n                 trainer._hp_search_setup(trial)\n-                torch.distributed.broadcast_object_list(pickle.dumps(trainer.args), src=0)\n+                args_main_rank_list = [pickle.dumps(trainer.args)]\n+                torch.distributed.broadcast_object_list(args_main_rank_list, src=0)\n                 trainer.train(resume_from_checkpoint=checkpoint)\n             else:\n                 trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n@@ -267,11 +268,11 @@ def _objective(trial, checkpoint_dir=None):\n     else:\n         for i in range(n_trials):\n             trainer.objective = None\n-            args_main_rank = list(pickle.dumps(trainer.args))\n+            args_main_rank_list = [None]\n             if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:\n                 raise RuntimeError(\"only support DDP optuna HPO for ParallelMode.DISTRIBUTED currently.\")\n-            torch.distributed.broadcast_object_list(args_main_rank, src=0)\n-            args = pickle.loads(bytes(args_main_rank))\n+            torch.distributed.broadcast_object_list(args_main_rank_list, src=0)\n+            args = pickle.loads(bytes(args_main_rank_list[0]))\n             for key, value in asdict(args).items():\n                 if key != \"local_rank\":\n                     setattr(trainer.args, key, value)"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}