{
    "author": "yonigozlan",
    "message": "[Sam2Video] Fix video inference with batched boxes and add test (#40797)\n\nfix video inference with batched boxes and add test",
    "sha": "8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d",
    "files": [
        {
            "sha": "83483e9d724e770ba46aab70f3e39475fb1dee4a",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d",
            "patch": "@@ -836,8 +836,7 @@ def process_new_points_or_boxes_for_video_frame(\n                     \"(please use clear_old_points=True instead)\"\n                 )\n             box_coords = input_boxes.reshape(1, -1, 2, 2)\n-            box_labels = torch.tensor([2, 3], dtype=torch.int32)\n-            box_labels = box_labels.reshape(1, -1, 2)\n+            box_labels = torch.tensor([2, 3], dtype=torch.int32).repeat(1, box_coords.shape[1], 1)\n             input_points = torch.cat([box_coords, input_points], dim=2)\n             input_labels = torch.cat([box_labels, input_labels], dim=2)\n "
        },
        {
            "sha": "d5a3c94d7f87066167f692f4fa24fa1a77d2287a",
            "filename": "src/transformers/models/sam2_video/processing_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py?ref=8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d",
            "patch": "@@ -721,8 +721,7 @@ def process_new_points_or_boxes_for_video_frame(\n                     \"(please use clear_old_points=True instead)\"\n                 )\n             box_coords = input_boxes.reshape(1, -1, 2, 2)\n-            box_labels = torch.tensor([2, 3], dtype=torch.int32)\n-            box_labels = box_labels.reshape(1, -1, 2)\n+            box_labels = torch.tensor([2, 3], dtype=torch.int32).repeat(1, box_coords.shape[1], 1)\n             input_points = torch.cat([box_coords, input_points], dim=2)\n             input_labels = torch.cat([box_labels, input_labels], dim=2)\n "
        },
        {
            "sha": "d4e41b365d8b0f07c21abb505b2f66758d1342c6",
            "filename": "tests/models/sam2_video/test_modeling_sam2_video.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d/tests%2Fmodels%2Fsam2_video%2Ftest_modeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d/tests%2Fmodels%2Fsam2_video%2Ftest_modeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2_video%2Ftest_modeling_sam2_video.py?ref=8502b41bf1862eeea0f1bc7c325f1d44e7b15c8d",
            "patch": "@@ -393,6 +393,47 @@ def test_inference_mask_generation_video_multi_objects_multi_points(self):\n             rtol=1e-4,\n         )\n \n+    def test_inference_mask_generation_video_batched_bb(self):\n+        raw_video = prepare_video()\n+        inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)\n+        ann_frame_idx = 0  # the frame index we interact with\n+        ann_obj_ids = [2, 3]  # give a unique id to each object we interact with (it can be any integers)\n+\n+        self.processor.add_inputs_to_inference_session(\n+            inference_session=inference_session,\n+            frame_idx=ann_frame_idx,\n+            obj_ids=ann_obj_ids,\n+            input_boxes=[[[300, 0, 500, 400], [400, 0, 600, 400]]],\n+        )\n+\n+        frames = []\n+        for sam2_video_output in self.video_model.propagate_in_video_iterator(\n+            inference_session=inference_session,\n+            start_frame_idx=ann_frame_idx,\n+            max_frame_num_to_track=2,\n+        ):\n+            video_res_masks = self.processor.post_process_masks(\n+                [sam2_video_output.pred_masks], [raw_video.shape[-3:-1]], binarize=False\n+            )[0]\n+            print(video_res_masks.shape)\n+            frames.append(video_res_masks)\n+        frames = torch.stack(frames, dim=0)\n+        self.assertEqual(frames.shape, (3, 2, 1, raw_video.shape[-3], raw_video.shape[-2]))\n+        print(frames.shape)\n+        print(frames[:3, :, :, :2, :2])\n+        torch.testing.assert_close(\n+            frames[:3, :, :, :2, :2],\n+            torch.tensor(\n+                [\n+                    [[[[-13.1427, -13.1427], [-13.7753, -13.7753]]], [[[-8.4576, -8.4576], [-8.7329, -8.7329]]]],\n+                    [[[[-14.9998, -14.9998], [-15.7086, -15.7086]]], [[[-9.2998, -9.2998], [-9.8947, -9.8947]]]],\n+                    [[[[-15.4558, -15.4558], [-16.1649, -16.1649]]], [[[-10.4880, -10.4880], [-11.2098, -11.2098]]]],\n+                ]\n+            ).to(torch_device),\n+            atol=1e-4,\n+            rtol=1e-4,\n+        )\n+\n     def test_inference_propagate_video_from_mask_input(self):\n         raw_video = prepare_video()\n         inference_session = self.processor.init_video_session(video=raw_video, inference_device=torch_device)"
        }
    ],
    "stats": {
        "total": 47,
        "additions": 43,
        "deletions": 4
    }
}