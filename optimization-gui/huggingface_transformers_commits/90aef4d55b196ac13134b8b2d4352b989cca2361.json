{
    "author": "remi-or",
    "message": "Make benchmarking lighter: clean-up result files and remove non-needed arguments (#42357)\n\n* Duplicate deletion  in config check\n\n* More attn implem configs\n\n* Remodel and remove backend\n\n* Change useless message to debug\n\n* Remove extra generation config\n\n* Simplify inter-token latency\n\n* Update src/transformers/generation/continuous_batching/continuous_api.py\n\nCo-authored-by: Luc Georges <McPatate@users.noreply.github.com>\n\n* Style and review compliance\n\n---------\n\nCo-authored-by: Luc Georges <McPatate@users.noreply.github.com>",
    "sha": "90aef4d55b196ac13134b8b2d4352b989cca2361",
    "files": [
        {
            "sha": "a75886a82df25f2584acd66086b171be61948d56",
            "filename": "benchmark_v2/framework/benchmark_config.py",
            "status": "modified",
            "additions": 36,
            "deletions": 36,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/90aef4d55b196ac13134b8b2d4352b989cca2361/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/90aef4d55b196ac13134b8b2d4352b989cca2361/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_config.py?ref=90aef4d55b196ac13134b8b2d4352b989cca2361",
            "patch": "@@ -2,9 +2,10 @@\n import itertools\n import json\n import logging\n+from functools import lru_cache\n from typing import Any\n \n-from transformers.utils.import_utils import is_flash_attn_2_available\n+from transformers.utils.import_utils import is_flash_attn_2_available, is_kernels_available\n \n \n KERNELIZATION_AVAILABLE = False\n@@ -18,17 +19,36 @@\n logger = logging.getLogger(__name__)\n \n \n+@lru_cache\n+def is_fa2_or_kernel_available() -> bool:\n+    \"\"\"Returns True if the flash_attn_2 or a fallback kernel is available\"\"\"\n+    # Early return if flash_attn_2 is available\n+    if is_flash_attn_2_available():\n+        return True\n+    # Early return if kernels is not available\n+    if not is_kernels_available():\n+        logger.warning(\n+            \"flash_attention_2 is not available. kernels is not installed. Benchmarking flash_attention_2 will not \"\n+            \"be possible.\"\n+        )\n+        return False\n+    # If kernels is available, try to get the flash_attn_2 kernel\n+    try:\n+        from kernels import get_kernel\n+\n+        get_kernel(\"kernels-community/flash-attn\")\n+    except Exception as _:\n+        logger.warning(\n+            \"flash_attention_2 is not available. kernels is installed, but the flash_attn kernel is not available.\"\n+            \"Benchmarking flash_attention_2 will not be possible.\"\n+        )\n+        return False\n+\n+\n class BenchmarkConfig:\n     \"\"\"Configuration for a single benchmark scenario.\"\"\"\n \n-    all_attn_implementations = [\n-        (\"flash_attention_2\", None),\n-        (\"eager\", None),\n-        (\"sdpa\", \"math\"),\n-        (\"sdpa\", \"flash_attention\"),\n-        (\"flex_attention\", None),\n-    ]\n-\n+    all_attn_implementations = [\"flash_attention_2\", \"eager\", \"sdpa\", \"flex_attention\"]\n     all_compiled_modes = [None, \"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"]\n \n     def __init__(\n@@ -41,7 +61,6 @@ def __init__(\n         sequence_length: int = 128,\n         num_tokens_to_generate: int = 128,\n         attn_implementation: str = \"eager\",\n-        sdpa_backend: str | None = None,\n         compile_mode: str | None = None,\n         compile_options: dict[str, Any] | None = None,\n         kernelize: bool = False,\n@@ -59,7 +78,6 @@ def __init__(\n         self.num_tokens_to_generate = num_tokens_to_generate\n         # Generation parameters\n         self.attn_implementation = attn_implementation\n-        self.sdpa_backend = sdpa_backend\n         # Optimization parameters\n         self.compile_mode = compile_mode\n         self.compile_options = compile_options if compile_options is not None else {}\n@@ -75,34 +93,21 @@ def check_validity(self, skip_validity_check: bool = False) -> None:\n         if skip_validity_check:\n             return\n         # Check FA is installed\n-        if self.attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n-            logger.warning(\n-                \"Flash attention does not support compile mode. Defaulting to SDPA w/ flash attention backend.\"\n-            )\n+        is_fa = self.attn_implementation == \"flash_attention_2\"\n+        if is_fa and not is_fa2_or_kernel_available():\n+            logger.warning(\"Flash attention is not available. Defaulting to SDPA.\")\n             self.attn_implementation = \"sdpa\"\n-            self.sdpa_backend = \"flash_attention\"\n         # Flash attention does not support compile mode, so we turn it off # FIXME: it would be better to support it\n-        is_fa = self.attn_implementation == \"flash_attention_2\"\n-        is_fa |= self.attn_implementation == \"sdpa\" and self.sdpa_backend == \"flash_attention\"\n-        if is_fa:\n+        if is_fa and self.compile_mode is not None:\n             logger.warning(\"Flash attention does not support compile mode. Turning off compile mode.\")\n             self.compile_mode = None\n-        # Handle SDPA backend if not determined by the config (needs to be done before skipping duplicates)\n-        if self.attn_implementation == \"sdpa\" and self.sdpa_backend is None:\n-            default_backend = \"flash_attention\"  # FIXME: torch has a _cur_sdpa_kernel_backends but it fails\n-            logger.warning(f\"No SDPA backend provided, using {default_backend} instead.\")\n-            self.sdpa_backend = default_backend\n+        # Handle continuous batching cases\n         if self.continuous_batching:\n             if self.attn_implementation == \"flex_attention\":\n                 logger.error(\n-                    \"disabling continuous batching because of invalid configuration: flex attention is not supported\"\n+                    \"Disabling continuous batching because of invalid configuration: flex attention is not supported.\"\n                 )\n                 self.continuous_batching = False\n-            elif self.attn_implementation == \"sdpa\" and self.sdpa_backend is not None:\n-                logger.warning(\n-                    \"when continuous batching is enabled, sdpa_backend must be None because of the attention mask, setting it to None\"\n-                )\n-                self.sdpa_backend = None\n \n     @property\n     def hash(self) -> str:\n@@ -115,7 +120,6 @@ def infer_name(self, compact: bool = True) -> str:\n             gpu_monitor_str = \"monitored\" if self.gpu_monitoring else \"unmonitored\"\n             dimensions_str = f\"b{self.batch_size}_s{self.sequence_length}_n{self.num_tokens_to_generate}\"\n             attn_code = self.attn_implementation\n-            attn_code += f\"_{self.sdpa_backend}\" if self.attn_implementation == \"sdpa\" else \"\"\n             compile_str = f\"compiled_{self.compile_mode}\" if self.compile_mode is not None else \"uncompiled\"\n             kernelize_str = \"kernelized\" if self.kernelize else \"unkernelized\"\n             continuous_batching_str = \"cb\" if self.continuous_batching else \"generate\"\n@@ -125,7 +129,6 @@ def infer_name(self, compact: bool = True) -> str:\n             gpu_monitor_str = (\"with\" if self.gpu_monitoring else \"no\") + \" GPU monitoring\"\n             dimensions_str = f\"batch size {self.batch_size}, sequence length {self.sequence_length}, {self.num_tokens_to_generate} generated tokens\"\n             attn_code = f\"{self.attn_implementation} attention\"\n-            attn_code += f\" with {self.sdpa_backend} backend\" if self.attn_implementation == \"sdpa\" else \"\"\n             compile_str = \"compiled\" if self.compile_mode is not None else \"not compiled\"\n             kernelize_str = \"kernelized\" if self.kernelize else \"not kernelized\"\n             continuous_batching_str = \"continuous batching\" if self.continuous_batching else \"regular generate\"\n@@ -145,7 +148,6 @@ def to_dict(self) -> dict[str, Any]:\n             \"sequence_length\": self.sequence_length,\n             \"num_tokens_to_generate\": self.num_tokens_to_generate,\n             \"attn_implementation\": self.attn_implementation,\n-            \"sdpa_backend\": self.sdpa_backend,\n             \"compile_mode\": self.compile_mode,\n             \"compile_options\": self.compile_options | {},  # to avoid inplace modification of the original dict\n             \"kernelize\": self.kernelize,\n@@ -162,7 +164,6 @@ def from_dict(cls, data: dict[str, Any], skip_validity_check: bool = False) -> \"\n             sequence_length=data.get(\"sequence_length\", 128),\n             num_tokens_to_generate=data.get(\"num_tokens_to_generate\", 128),\n             attn_implementation=data.get(\"attn_implementation\", \"eager\"),\n-            sdpa_backend=data.get(\"sdpa_backend\"),\n             compile_mode=data.get(\"compile_mode\"),\n             compile_options=data.get(\"compile_options\"),\n             kernelize=data.get(\"kernelize\", False),\n@@ -213,7 +214,7 @@ def get_config_by_level(level: int) -> list[BenchmarkConfig]:\n     configs = []\n     # Early return if level is greater than 3: we generate all combinations of configs, maybe even w/ all compile modes\n     if level >= 3:\n-        for attn_implementation, sdpa_backend in BenchmarkConfig.all_attn_implementations:\n+        for attn_implementation in BenchmarkConfig.all_attn_implementations:\n             # Usually there is not much to gain by compiling with other modes, but we allow it for level 4\n             compile_modes = BenchmarkConfig.all_compiled_modes if level >= 4 else [None, \"default\"]\n             for cm in compile_modes:\n@@ -222,7 +223,6 @@ def get_config_by_level(level: int) -> list[BenchmarkConfig]:\n                         configs.append(\n                             BenchmarkConfig(\n                                 attn_implementation=attn_implementation,\n-                                sdpa_backend=sdpa_backend,\n                                 compile_mode=cm,\n                                 kernelize=kernelize_on,\n                                 continuous_batching=cb_on,"
        },
        {
            "sha": "10db956353cba07c4d6936a05abb5ec47fbad44f",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 25,
            "deletions": 66,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/90aef4d55b196ac13134b8b2d4352b989cca2361/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/90aef4d55b196ac13134b8b2d4352b989cca2361/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=90aef4d55b196ac13134b8b2d4352b989cca2361",
            "patch": "@@ -6,7 +6,6 @@\n import re\n import tempfile\n import time\n-from contextlib import nullcontext\n from datetime import datetime\n from queue import Queue\n from typing import Any\n@@ -79,24 +78,6 @@ def get_git_revision() -> str:\n         return git_hash.readline().strip()\n \n \n-def get_sdpa_backend(backend_name: str | None) -> torch.nn.attention.SDPBackend | None:\n-    \"\"\"Get the SDPA backend enum from string name.\"\"\"\n-    if backend_name is None:\n-        return None\n-\n-    try:\n-        backend_map = {\n-            \"math\": torch.nn.attention.SDPBackend.MATH,\n-            \"flash_attention\": torch.nn.attention.SDPBackend.FLASH_ATTENTION,\n-            \"efficient_attention\": torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,\n-            \"cudnn_attention\": torch.nn.attention.SDPBackend.CUDNN_ATTENTION,\n-        }\n-        return backend_map.get(backend_name.lower())\n-    except AttributeError:\n-        # torch.nn.attention.SDPBackend not available in older torch versions\n-        return None\n-\n-\n def flush_memory():\n     \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n     gc.collect()\n@@ -223,12 +204,7 @@ def run_benchmark(\n         self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0\n     ) -> dict[str, Any] | None:\n         \"\"\"Run a single benchmark with the given model ID and config.\"\"\"\n-        sdpa_ctx = nullcontext()\n-        if config.attn_implementation == \"sdpa\":\n-            sdpa_backend = get_sdpa_backend(config.sdpa_backend)\n-            sdpa_ctx = torch.nn.attention.sdpa_kernel(sdpa_backend)\n-\n-        with sdpa_ctx, torch.no_grad():\n+        with torch.no_grad():\n             self.logger.info(f\"Running benchmark scenario: {config.name}\")\n \n             # Quick validation: try one measurement first to see if this scenario works\n@@ -243,14 +219,14 @@ def run_benchmark(\n \n             # Warmup runs\n             self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n-            for _ in trange(config.warmup_iterations):\n+            for _ in trange(config.warmup_iterations, desc=\"Warmup\"):\n                 _ = generate_fn(max_new_tokens=config.num_tokens_to_generate)\n             self.logger.info(\"Warmup over.\")\n \n             # Measurement runs\n             result = BenchmarkResult()\n             self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n-            for _ in trange(config.measurement_iterations):\n+            for _ in trange(config.measurement_iterations, desc=\"Benchmarking\"):\n                 e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = generate_fn(\n                     max_new_tokens=config.num_tokens_to_generate,\n                     gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n@@ -281,49 +257,32 @@ def time_generate_batch(\n     ) -> tuple[float, list[float], str, GPURawMetrics | None]:\n         if gpu_monitor is not None:\n             gpu_monitor.start()\n-        config = GenerationConfig(\n-            max_new_tokens=max_new_tokens,\n-            eos_token_id=self.tokenizer.eos_token_id,\n-            pad_token_id=self.tokenizer.pad_token_id,\n-            do_sample=True,\n-        )\n-        manager = self.model.init_continuous_batching(config)\n-        manager.start()\n-        try:\n-            first_req_results = []\n-            timestamps = []\n-            wall_time_0 = time.perf_counter()\n-            inputs = self.inputs[\"input_ids\"].tolist()\n+        # Prepare inputs\n+        inputs = self.inputs[\"input_ids\"].tolist()\n+        timestamps = []\n+        last_result_generated_tokens = None\n+        wall_time_0 = time.perf_counter()\n+        # We disable prefix sharing because all prompts are the same\n+        with self.model.continuous_batching_context_manager(allow_prefix_sharing=False) as manager:\n             manager.add_requests(inputs, max_new_tokens=max_new_tokens, streaming=True)\n-            first_req_id = None\n-            num_requests = len(inputs)\n-            finished_requests = 0\n-            while finished_requests < num_requests:\n+            unfinished_requests = len(inputs)\n+            while unfinished_requests > 0:\n                 # NOTE: I don't like having the extra if stmt here, but hopefully won't degrade perf too much\n                 result = manager.get_result()\n-                if result:\n-                    timestamps.append(time.perf_counter() - wall_time_0)\n+                if result is not None:\n+                    timestamps.append(time.perf_counter() - wall_time_0)  # FIXME: the timestamps are wrong\n                     if result.is_finished():\n-                        finished_requests += 1\n-                    if first_req_id is None:\n-                        first_req_id = result.request_id\n-                    if result.request_id == first_req_id:\n-                        first_req_results.append(result)\n-                else:\n-                    if not manager.is_running():\n-                        raise RuntimeError(\"Generation thread exited unexpectedly\")\n-            wall_time_1 = time.perf_counter()\n-            gpu_metrics = gpu_monitor.stop_and_collect() if gpu_monitor is not None else None\n-            decoded_output = self.tokenizer.decode(\n-                [res.generated_tokens[0] for res in first_req_results], skip_special_tokens=True\n-            )\n-            shape_and_decoded_output = f\"{(1, len(first_req_results))} | {decoded_output}\"\n-            e2e_latency = wall_time_1 - wall_time_0\n-            return e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics\n-        except Exception as e:\n-            raise e\n-        finally:\n-            manager.stop()\n+                        last_result_generated_tokens = result.generated_tokens\n+                        unfinished_requests -= 1\n+                elif not manager.is_running():\n+                    raise RuntimeError(\"Generation thread exited unexpectedly\")\n+        # Post-processing\n+        wall_time_1 = time.perf_counter()\n+        e2e_latency = wall_time_1 - wall_time_0\n+        gpu_metrics = gpu_monitor.stop_and_collect() if gpu_monitor is not None else None\n+        decoded_output = self.tokenizer.decode(last_result_generated_tokens, skip_special_tokens=True)\n+        shape_and_decoded_output = f\"{(1, len(last_result_generated_tokens))} | {decoded_output}\"\n+        return e2e_latency, timestamps, shape_and_decoded_output, gpu_metrics\n \n     def time_generate(\n         self,"
        },
        {
            "sha": "439c87c82d61cb7aa487e58a0a55656c9ea13b2a",
            "filename": "benchmark_v2/framework/data_classes.py",
            "status": "modified",
            "additions": 17,
            "deletions": 14,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/90aef4d55b196ac13134b8b2d4352b989cca2361/benchmark_v2%2Fframework%2Fdata_classes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/90aef4d55b196ac13134b8b2d4352b989cca2361/benchmark_v2%2Fframework%2Fdata_classes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fdata_classes.py?ref=90aef4d55b196ac13134b8b2d4352b989cca2361",
            "patch": "@@ -89,10 +89,14 @@ class BenchmarkResult:\n \n     def __init__(self) -> None:\n         self.e2e_latency = []\n-        self.token_generation_times = []  # time at which each token was generated (relative to start of the generation)\n+        self.time_to_first_token = []\n+        self.inter_token_latency = []\n         self.shape_and_decoded_outputs = []\n         self.gpu_metrics = []\n \n+    def compute_itl(self, token_generation_times: list[float]) -> list[float]:\n+        return (token_generation_times[-1] - token_generation_times[0]) / len(token_generation_times)\n+\n     def accumulate(\n         self,\n         e2e_latency: float,\n@@ -101,7 +105,9 @@ def accumulate(\n         gpu_metrics: GPURawMetrics | None,\n     ) -> None:\n         self.e2e_latency.append(e2e_latency)\n-        self.token_generation_times.append(token_generation_times)\n+        self.time_to_first_token.append(token_generation_times[0])\n+        # inter-token latency is already an average in itself\n+        self.inter_token_latency.append(self.compute_itl(token_generation_times))\n         self.shape_and_decoded_outputs.append(shape_and_decoded_output)\n         self.gpu_metrics.append(gpu_metrics)\n \n@@ -113,7 +119,8 @@ def to_dict(self) -> dict[str, None | int | float]:\n             gpu_metrics = [gm.to_dict() for gm in self.gpu_metrics]\n         return {\n             \"e2e_latency\": self.e2e_latency,\n-            \"token_generation_times\": self.token_generation_times,\n+            \"time_to_first_token\": self.time_to_first_token,\n+            \"inter_token_latency\": self.inter_token_latency,\n             \"shape_and_decoded_outputs\": self.shape_and_decoded_outputs,\n             \"gpu_metrics\": gpu_metrics,\n         }\n@@ -130,29 +137,25 @@ def from_dict(cls, data: dict[str, None | int | float]) -> \"BenchmarkResult\":\n         for i in range(len(data[\"e2e_latency\"])):\n             new_instance.accumulate(\n                 e2e_latency=data[\"e2e_latency\"][i],\n-                token_generation_times=data[\"token_generation_times\"][i],\n+                time_to_first_token=data[\"time_to_first_token\"][i],\n+                inter_token_latency=data[\"inter_token_latency\"][i],\n                 shape_and_decoded_output=data[\"shape_and_decoded_outputs\"][i],\n                 gpu_metrics=gpu_metrics[i],\n             )\n         return new_instance\n \n-    def get_measured_ttft(self) -> list[float]:\n-        return [dt[0] for dt in self.token_generation_times if len(dt) > 0]\n-\n-    def get_measured_itl(self) -> list[float]:\n-        return [(dt[-1] - dt[0]) / (len(dt) - 1) for dt in self.token_generation_times if len(dt) > 1]\n-\n     def get_throughput(self, total_generated_tokens: int) -> list[float]:\n         return [total_generated_tokens / e2e_latency for e2e_latency in self.e2e_latency]\n \n     def pprint(self, batch_size: int = 0, num_generated_tokens: int = 0, tabs: int = 0) -> None:\n         measurements = {\n             \"E2E Latency\": add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n-            \"Time to First Token\": add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n+            \"Time to First Token\": add_unit_to_duration(compute_basic_statistics(self.time_to_first_token)),\n         }\n-        itl_values = self.get_measured_itl()\n-        if len(itl_values) > 0:\n-            measurements[\"Inter-Token Latency\"] = add_unit_to_duration(compute_basic_statistics(itl_values))\n+        if len(self.inter_token_latency) > 0:\n+            measurements[\"Inter-Token Latency\"] = add_unit_to_duration(\n+                compute_basic_statistics(self.inter_token_latency)\n+            )\n         if batch_size > 0:\n             throughput_stats = compute_basic_statistics(self.get_throughput(batch_size * num_generated_tokens))\n             measurements[\"Throughput\"] = {key: f\"{value:.2f}tok/s\" for key, value in throughput_stats.items()}"
        },
        {
            "sha": "4c18cc5364b516ba7e53b19c39f4e7c4ab26c680",
            "filename": "benchmark_v2/run_benchmarks.py",
            "status": "modified",
            "additions": 19,
            "deletions": 4,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/90aef4d55b196ac13134b8b2d4352b989cca2361/benchmark_v2%2Frun_benchmarks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/90aef4d55b196ac13134b8b2d4352b989cca2361/benchmark_v2%2Frun_benchmarks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frun_benchmarks.py?ref=90aef4d55b196ac13134b8b2d4352b989cca2361",
            "patch": "@@ -19,19 +19,20 @@\n \"\"\"\n \n import argparse\n+import json\n import logging\n import sys\n import uuid\n \n-from framework.benchmark_config import adapt_configs, get_config_by_level\n+from framework.benchmark_config import BenchmarkConfig, adapt_configs, get_config_by_level\n from framework.benchmark_runner import BenchmarkRunner\n \n \n if __name__ == \"__main__\":\n     # Parse arguments\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\"--output-dir\", type=str, default=None, help=\"Output dir for benchmark results\")\n-    parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n+    parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"WARNING\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n     parser.add_argument(\"--warmup\", \"-w\", type=int, default=3, help=\"Number of warmup iterations\")\n     parser.add_argument(\"--iterations\", \"-i\", type=int, default=10, help=\"Number of measurement iterations\")\n@@ -48,6 +49,7 @@\n         \" each attn implementation an option, 3: cross-generate all combinations of configs, 4: cross-generate all\"\n         \" combinations of configs w/ all compile modes\",\n     )\n+    parser.add_argument(\"--config-file\", type=str, help=\"Path to a config file stored as a json or jsonl format\")\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n     parser.add_argument(\"--branch-name\", type=str, help=\"Git branch name\")\n@@ -90,8 +92,21 @@\n     if any(n <= 1 for n in args.num_tokens_to_generate):\n         raise ValueError(\"--num_tokens_to_generate arguments should be larger than 1\")\n \n-    # Get the configs for the given coverage level\n-    configs = get_config_by_level(args.level)\n+    # If a config file is provided, read it and use the configs therein. They will still be adapted to the given arguments.\n+    if args.config_file is not None:\n+        if args.config_file.endswith(\".json\"):\n+            with open(args.config_file, \"r\") as f:\n+                config_as_dicts = [json.load(f)]\n+        elif args.config_file.endswith(\".jsonl\"):\n+            with open(args.config_file, \"r\") as f:\n+                config_as_dicts = [json.loads(line) for line in f if line.startswith(\"{\")]\n+        else:\n+            raise ValueError(f\"Unsupported config file format: {args.config_file}\")\n+        configs = [BenchmarkConfig.from_dict(config) for config in config_as_dicts]\n+    else:\n+        # Otherwise, get the configs for the given coverage level\n+        configs = get_config_by_level(args.level)\n+\n     # Adapt the configs to the given arguments\n     configs = adapt_configs(\n         configs,"
        },
        {
            "sha": "1ea70e7c03fbfa7af89e9d9f857822df4be0fe04",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/90aef4d55b196ac13134b8b2d4352b989cca2361/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/90aef4d55b196ac13134b8b2d4352b989cca2361/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=90aef4d55b196ac13134b8b2d4352b989cca2361",
            "patch": "@@ -826,8 +826,6 @@ def stop(self, block: bool = True, timeout: float | None = None) -> None:\n                 logger.warning(\n                     f\"\\nPrefix sharing was on. Total prefix length: {self.batch_processor.cache._total_prefix_length}\"\n                 )\n-            else:\n-                logger.warning(\"\\nPrefix sharing was off.\")\n \n         if self._generation_thread is None:\n             logger.warning(\"Manager not started.\")"
        }
    ],
    "stats": {
        "total": 219,
        "additions": 97,
        "deletions": 122
    }
}