{
    "author": "Isotr0py",
    "message": "[GGUF] Refactor and decouple gguf checkpoint loading logic (#34385)\n\n* draft load_gguf refactor\n\n* update\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* remove llama mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* remove qwen2 mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* remove unused function\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate stablelm mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate phi3 mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate t5 mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate bloom mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* fix bloom\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate starcoder2 mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate gpt2 mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate mistral mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate nemotron mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate mamba mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* deprecate mamba mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* code format\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* code format\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* fix mamba\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* fix qwen2moe\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* remove qwen2moe mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* clean up\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* remove falcon 7b map\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* remove all ggml tensors mapping\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* add comments\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* update messages\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* fix tensors in parsed parameters\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* add gguf check\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n---------\n\nSigned-off-by: Isotr0py <2037008807@qq.com>",
    "sha": "3951da1a6bfb694fc387081768f163c470b0392b",
    "files": [
        {
            "sha": "49dbc5e3ad90e1a758be5d0d981952df678d6d19",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951da1a6bfb694fc387081768f163c470b0392b/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951da1a6bfb694fc387081768f163c470b0392b/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=3951da1a6bfb694fc387081768f163c470b0392b",
            "patch": "@@ -57,7 +57,6 @@\n     \"fsdp\": [\"is_fsdp_managed_module\"],\n     \"ggml\": [\n         \"GGUF_CONFIG_MAPPING\",\n-        \"GGUF_TENSOR_MAPPING\",\n         \"GGUF_TOKENIZER_MAPPING\",\n         \"_gguf_parse_value\",\n         \"load_dequant_gguf_tensor\",\n@@ -161,7 +160,6 @@\n     from .fsdp import is_fsdp_managed_module\n     from .ggml import (\n         GGUF_CONFIG_MAPPING,\n-        GGUF_TENSOR_MAPPING,\n         GGUF_TOKENIZER_MAPPING,\n         _gguf_parse_value,\n         load_dequant_gguf_tensor,"
        },
        {
            "sha": "e88071b6a02ee357ed82511b85c2273c8bd76f60",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 0,
            "deletions": 248,
            "changes": 248,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951da1a6bfb694fc387081768f163c470b0392b/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951da1a6bfb694fc387081768f163c470b0392b/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=3951da1a6bfb694fc387081768f163c470b0392b",
            "patch": "@@ -33,254 +33,6 @@\n logger = logging.get_logger(__name__)\n \n \n-GGUF_TENSOR_MAPPING = {\n-    \"llama\": {\n-        \"token_embd\": \"model.embed_tokens\",\n-        \"blk\": \"model.layers\",\n-        \"ffn_up\": \"mlp.up_proj\",\n-        \"ffn_down\": \"mlp.down_proj\",\n-        \"ffn_gate\": \"mlp.gate_proj\",\n-        \"ffn_norm\": \"post_attention_layernorm\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_q\": \"self_attn.q_proj\",\n-        \"attn_v\": \"self_attn.v_proj\",\n-        \"attn_k\": \"self_attn.k_proj\",\n-        \"attn_output\": \"self_attn.o_proj\",\n-        \"output.weight\": \"lm_head.weight\",\n-        \"output_norm\": \"model.norm\",\n-    },\n-    \"mistral\": {\n-        \"token_embd\": \"model.embed_tokens\",\n-        \"blk\": \"model.layers\",\n-        \"ffn_up\": \"mlp.up_proj\",\n-        \"ffn_down\": \"mlp.down_proj\",\n-        \"ffn_gate\": \"mlp.gate_proj\",\n-        \"ffn_norm\": \"post_attention_layernorm\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_q\": \"self_attn.q_proj\",\n-        \"attn_v\": \"self_attn.v_proj\",\n-        \"attn_k\": \"self_attn.k_proj\",\n-        \"attn_output\": \"self_attn.o_proj\",\n-        \"output.weight\": \"lm_head.weight\",\n-        \"output_norm\": \"model.norm\",\n-    },\n-    \"qwen2\": {\n-        \"token_embd\": \"model.embed_tokens\",\n-        \"blk\": \"model.layers\",\n-        \"ffn_up\": \"mlp.up_proj\",\n-        \"ffn_down\": \"mlp.down_proj\",\n-        \"ffn_gate\": \"mlp.gate_proj\",\n-        \"ffn_norm\": \"post_attention_layernorm\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_q\": \"self_attn.q_proj\",\n-        \"attn_v\": \"self_attn.v_proj\",\n-        \"attn_k\": \"self_attn.k_proj\",\n-        \"attn_output\": \"self_attn.o_proj\",\n-        \"output.weight\": \"lm_head.weight\",\n-        \"output_norm\": \"model.norm\",\n-    },\n-    \"qwen2moe\": {\n-        \"token_embd\": \"model.embed_tokens\",\n-        \"blk\": \"model.layers\",\n-        \"ffn_up_exps\": \"mlp.experts\",\n-        \"ffn_up_shexp\": \"mlp.shared_expert.up_proj\",\n-        \"ffn_down_exps\": \"mlp.experts\",\n-        \"ffn_down_shexp\": \"mlp.shared_expert.down_proj\",\n-        \"ffn_norm\": \"post_attention_layernorm\",\n-        \"ffn_gate_inp.weight\": \"mlp.gate.weight\",\n-        \"ffn_gate_exps\": \"mlp.experts\",\n-        \"ffn_gate_shexp\": \"mlp.shared_expert.gate_proj\",\n-        \"ffn_gate_inp_shexp\": \"mlp.shared_expert_gate\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_q\": \"self_attn.q_proj\",\n-        \"attn_v\": \"self_attn.v_proj\",\n-        \"attn_k\": \"self_attn.k_proj\",\n-        \"attn_output\": \"self_attn.o_proj\",\n-        \"output.weight\": \"lm_head.weight\",\n-        \"output_norm\": \"model.norm\",\n-    },\n-    \"phi3\": {\n-        \"token_embd\": \"model.embed_tokens\",\n-        \"blk\": \"model.layers\",\n-        \"ffn_up\": \"mlp.gate_up_proj\",\n-        \"ffn_down\": \"mlp.down_proj\",\n-        \"ffn_gate\": \"mlp.gate_up_proj\",\n-        \"ffn_norm\": \"post_attention_layernorm\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_qkv\": \"self_attn.qkv_proj\",\n-        \"attn_output\": \"self_attn.o_proj\",\n-        \"output.weight\": \"lm_head.weight\",\n-        \"output_norm\": \"model.norm\",\n-    },\n-    \"bloom\": {\n-        \"token_embd.weight\": \"transformer.word_embeddings.weight\",\n-        \"token_embd_norm\": \"transformer.word_embeddings_layernorm\",\n-        \"blk\": \"transformer.h\",\n-        \"ffn_up\": \"mlp.dense_h_to_4h\",\n-        \"ffn_down\": \"mlp.dense_4h_to_h\",\n-        \"ffn_norm\": \"post_attention_layernorm\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_qkv\": \"self_attention.query_key_value\",\n-        \"attn_output\": \"self_attention.dense\",\n-        \"output.weight\": \"lm_head.weight\",\n-        \"output_norm\": \"transformer.ln_f\",\n-    },\n-    \"falcon7b\": {\n-        \"token_embd\": \"word_embeddings\",\n-        \"blk\": \"h\",\n-        \"ffn_up\": \"mlp.dense_h_to_4h\",\n-        \"ffn_down\": \"mlp.dense_4h_to_h\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_qkv\": \"self_attention.query_key_value\",\n-        \"attn_output\": \"self_attention.dense\",\n-        \".output.\": \".lm_head.\",\n-        \"output_norm\": \"ln_f\",\n-    },\n-    \"falcon40b\": {\n-        \"token_embd\": \"word_embeddings\",\n-        \"blk\": \"h\",\n-        \"ffn_up\": \"mlp.dense_h_to_4h\",\n-        \"ffn_down\": \"mlp.dense_4h_to_h\",\n-        \".attn_norm.\": \".ln_mlp.\",\n-        \"attn_norm_2\": \"ln_attn\",\n-        \"attn_qkv\": \"self_attention.query_key_value\",\n-        \"attn_output\": \"self_attention.dense\",\n-        \".output.\": \".lm_head.\",\n-        \"output_norm\": \"ln_f\",\n-    },\n-    \"t5\": {\n-        \"token_embd\": \"shared\",\n-        \"dec.blk.{bid}.attn_q\": \"decoder.block.{bid}.layer.0.SelfAttention.q\",\n-        \"dec.blk.{bid}.attn_k\": \"decoder.block.{bid}.layer.0.SelfAttention.k\",\n-        \"dec.blk.{bid}.attn_v\": \"decoder.block.{bid}.layer.0.SelfAttention.v\",\n-        \"dec.blk.{bid}.attn_o\": \"decoder.block.{bid}.layer.0.SelfAttention.o\",\n-        \"dec.blk.{bid}.attn_rel_b\": \"decoder.block.{bid}.layer.0.SelfAttention.relative_attention_bias\",\n-        \"dec.blk.{bid}.attn_norm\": \"decoder.block.{bid}.layer.0.layer_norm\",\n-        \"dec.blk.{bid}.cross_attn_q\": \"decoder.block.{bid}.layer.1.EncDecAttention.q\",\n-        \"dec.blk.{bid}.cross_attn_k\": \"decoder.block.{bid}.layer.1.EncDecAttention.k\",\n-        \"dec.blk.{bid}.cross_attn_v\": \"decoder.block.{bid}.layer.1.EncDecAttention.v\",\n-        \"dec.blk.{bid}.cross_attn_o\": \"decoder.block.{bid}.layer.1.EncDecAttention.o\",\n-        \"dec.blk.{bid}.cross_attn_norm\": \"decoder.block.{bid}.layer.1.layer_norm\",\n-        \"dec.blk.{bid}.ffn_gate\": \"decoder.block.{bid}.layer.2.DenseReluDense.wi_0\",\n-        \"dec.blk.{bid}.ffn_up\": \"decoder.block.{bid}.layer.2.DenseReluDense.wi_1\",\n-        \"dec.blk.{bid}.ffn_down\": \"decoder.block.{bid}.layer.2.DenseReluDense.wo\",\n-        \"dec.blk.{bid}.ffn_norm\": \"decoder.block.{bid}.layer.2.layer_norm\",\n-        \"dec.output_norm\": \"decoder.final_layer_norm\",\n-        \"enc.blk.{bid}.attn_q\": \"encoder.block.{bid}.layer.0.SelfAttention.q\",\n-        \"enc.blk.{bid}.attn_k\": \"encoder.block.{bid}.layer.0.SelfAttention.k\",\n-        \"enc.blk.{bid}.attn_v\": \"encoder.block.{bid}.layer.0.SelfAttention.v\",\n-        \"enc.blk.{bid}.attn_o\": \"encoder.block.{bid}.layer.0.SelfAttention.o\",\n-        \"enc.blk.{bid}.attn_rel_b\": \"encoder.block.{bid}.layer.0.SelfAttention.relative_attention_bias\",\n-        \"enc.blk.{bid}.attn_norm\": \"encoder.block.{bid}.layer.0.layer_norm\",\n-        \"enc.blk.{bid}.ffn_gate\": \"encoder.block.{bid}.layer.1.DenseReluDense.wi_0\",\n-        \"enc.blk.{bid}.ffn_up\": \"encoder.block.{bid}.layer.1.DenseReluDense.wi_1\",\n-        \"enc.blk.{bid}.ffn_down\": \"encoder.block.{bid}.layer.1.DenseReluDense.wo\",\n-        \"enc.blk.{bid}.ffn_norm\": \"encoder.block.{bid}.layer.1.layer_norm\",\n-        \"enc.output_norm\": \"encoder.final_layer_norm\",\n-        \"output.weight\": \"lm_head.weight\",\n-    },\n-    \"t5encoder\": {\n-        \"token_embd\": \"shared\",\n-        \"enc.blk.{bid}.attn_q\": \"encoder.block.{bid}.layer.0.SelfAttention.q\",\n-        \"enc.blk.{bid}.attn_k\": \"encoder.block.{bid}.layer.0.SelfAttention.k\",\n-        \"enc.blk.{bid}.attn_v\": \"encoder.block.{bid}.layer.0.SelfAttention.v\",\n-        \"enc.blk.{bid}.attn_o\": \"encoder.block.{bid}.layer.0.SelfAttention.o\",\n-        \"enc.blk.{bid}.attn_rel_b\": \"encoder.block.{bid}.layer.0.SelfAttention.relative_attention_bias\",\n-        \"enc.blk.{bid}.attn_norm\": \"encoder.block.{bid}.layer.0.layer_norm\",\n-        \"enc.blk.{bid}.ffn_gate\": \"encoder.block.{bid}.layer.1.DenseReluDense.wi_0\",\n-        \"enc.blk.{bid}.ffn_up\": \"encoder.block.{bid}.layer.1.DenseReluDense.wi_1\",\n-        \"enc.blk.{bid}.ffn_down\": \"encoder.block.{bid}.layer.1.DenseReluDense.wo\",\n-        \"enc.blk.{bid}.ffn_norm\": \"encoder.block.{bid}.layer.1.layer_norm\",\n-        \"enc.output_norm\": \"encoder.final_layer_norm\",\n-    },\n-    \"stablelm\": {\n-        \"token_embd\": \"model.embed_tokens\",\n-        \"blk\": \"model.layers\",\n-        \"ffn_up\": \"mlp.up_proj\",\n-        \"ffn_down\": \"mlp.down_proj\",\n-        \"ffn_gate\": \"mlp.gate_proj\",\n-        \"ffn_norm\": \"post_attention_layernorm\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_q\": \"self_attn.q_proj\",\n-        \"attn_v\": \"self_attn.v_proj\",\n-        \"attn_k\": \"self_attn.k_proj\",\n-        \"attn_output\": \"self_attn.o_proj\",\n-        \"output.weight\": \"lm_head.weight\",\n-        \"output_norm\": \"model.norm\",\n-    },\n-    \"gpt2\": {\n-        \"token_embd\": \"transformer.wte\",\n-        \"blk\": \"transformer.h\",\n-        \"position_embd\": \"transformer.wpe\",\n-        \"output_norm\": \"transformer.ln_f\",\n-        \"attn_norm\": \"ln_1\",\n-        \"attn_qkv\": \"attn.c_attn\",\n-        \"attn_output.weight\": \"attn.c_proj.weight\",\n-        \"attn_output.bias\": \"attn.c_proj.bias\",\n-        \"ffn_norm\": \"ln_2\",\n-        \"ffn_up\": \"mlp.c_fc\",\n-        \"ffn_down\": \"mlp.c_proj\",\n-    },\n-    \"starcoder2\": {\n-        \"token_embd\": \"model.embed_tokens\",\n-        \"blk\": \"model.layers\",\n-        \"ffn_up\": \"mlp.c_fc\",\n-        \"ffn_down\": \"mlp.c_proj\",\n-        \"ffn_norm\": \"post_attention_layernorm\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_q\": \"self_attn.q_proj\",\n-        \"attn_v\": \"self_attn.v_proj\",\n-        \"attn_k\": \"self_attn.k_proj\",\n-        \"attn_output\": \"self_attn.o_proj\",\n-        \"output.weight\": \"lm_head.weight\",\n-        \"output_norm\": \"model.norm\",\n-    },\n-    \"mamba\": {\n-        \"token_embd\": \"backbone.embeddings\",\n-        \"blk\": \"backbone.layers\",\n-        \"ssm_a\": \"mixer.A_log\",\n-        \"ssm_conv1d\": \"mixer.conv1d\",\n-        \"ssm_in\": \"mixer.in_proj\",\n-        \"ssm_out\": \"mixer.out_proj\",\n-        \"ssm_x\": \"mixer.x_proj\",\n-        \"ssm_dt\": \"mixer.dt_proj\",\n-        \"attn_norm\": \"norm\",\n-        \"output_norm\": \"backbone.norm_f\",\n-        \"output.weight\": \"lm_head.weight\",\n-    },\n-    \"nemotron\": {\n-        \"token_embd\": \"model.embed_tokens\",\n-        \"blk\": \"model.layers\",\n-        \"ffn_up\": \"mlp.up_proj\",\n-        \"ffn_down\": \"mlp.down_proj\",\n-        \"ffn_norm\": \"post_attention_layernorm\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_q\": \"self_attn.q_proj\",\n-        \"attn_v\": \"self_attn.v_proj\",\n-        \"attn_k\": \"self_attn.k_proj\",\n-        \"attn_output\": \"self_attn.o_proj\",\n-        \"output.weight\": \"lm_head.weight\",\n-        \"output_norm\": \"model.norm\",\n-    },\n-    \"gemma2\": {\n-        \"token_embd\": \"model.embed_tokens\",\n-        \"blk\": \"model.layers\",\n-        \"ffn_up\": \"mlp.up_proj\",\n-        \"ffn_down\": \"mlp.down_proj\",\n-        \"ffn_gate\": \"mlp.gate_proj\",\n-        \"ffn_norm\": \"pre_feedforward_layernorm\",\n-        \"post_attention_norm\": \"post_attention_layernorm\",\n-        \"post_ffw_norm\": \"post_feedforward_layernorm\",\n-        \"attn_norm\": \"input_layernorm\",\n-        \"attn_q\": \"self_attn.q_proj\",\n-        \"attn_v\": \"self_attn.v_proj\",\n-        \"attn_k\": \"self_attn.k_proj\",\n-        \"attn_output\": \"self_attn.o_proj\",\n-        \"output_norm\": \"model.norm\",\n-    },\n-}\n-\n-\n GGUF_CONFIG_MAPPING = {\n     \"general\": {\n         \"architecture\": \"model_type\","
        },
        {
            "sha": "9b20c1b61226a0fa94a5160d7ab6d64cd476617c",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 88,
            "deletions": 42,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951da1a6bfb694fc387081768f163c470b0392b/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951da1a6bfb694fc387081768f163c470b0392b/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=3951da1a6bfb694fc387081768f163c470b0392b",
            "patch": "@@ -22,7 +22,6 @@\n \n from .integrations import (\n     GGUF_CONFIG_MAPPING,\n-    GGUF_TENSOR_MAPPING,\n     GGUF_TOKENIZER_MAPPING,\n     _gguf_parse_value,\n )\n@@ -47,12 +46,11 @@\n         \"general\": {\"file_type\": \"file_type\", \"quantization_version\": \"quantization_version\"},\n     },\n     \"config\": GGUF_CONFIG_MAPPING,\n-    \"tensors\": GGUF_TENSOR_MAPPING,\n     \"tokenizer\": {\"tokenizer\": GGUF_TOKENIZER_MAPPING[\"tokenizer\"]},\n     \"tokenizer_config\": {\"tokenizer\": GGUF_TOKENIZER_MAPPING[\"tokenizer_config\"]},\n }\n \n-GGUF_SUPPORTED_ARCHITECTURES = list(GGUF_TO_TRANSFORMERS_MAPPING[\"tensors\"].keys())\n+GGUF_SUPPORTED_ARCHITECTURES = list(GGUF_TO_TRANSFORMERS_MAPPING[\"config\"].keys())\n \n \n class GGUFTensor(NamedTuple):\n@@ -121,21 +119,10 @@ def _split_moe_expert_tensor(\n     ):\n         # Original merge implementation\n         # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L1994-L2022\n-        exp_name = \"\"\n-        if \"ffn_gate_exps\" in name:\n-            exp_name = \"gate_proj\"\n-        elif \"ffn_down_exps\" in name:\n-            exp_name = \"down_proj\"\n-        elif \"ffn_up_exps\" in name:\n-            exp_name = \"up_proj\"\n-        else:\n-            raise ValueError(f\"Cannot map expert tensor {name} in Qwen2Moe architecture.\")\n-        for tensor_name in tensor_key_mapping:\n-            if tensor_name in name:\n-                name = name.replace(tensor_name, tensor_key_mapping[tensor_name])\n+        name = tensor_key_mapping[name]\n         w_counter = self.config.get(\"num_experts\", 60)\n         for i in range(0, w_counter):\n-            temp_name = name.replace(\".weight\", f\".{i}.{exp_name}.weight\")\n+            temp_name = name.replace(\"mlp.experts.\", f\"mlp.experts.{i}.\")\n             exp_weight = weights[i]\n             parsed_parameters[\"tensors\"][temp_name] = torch.from_numpy(np.copy(exp_weight))\n \n@@ -223,10 +210,6 @@ def __init__(self, config=None):\n         super().__init__(config=config)\n \n     def process(self, weights, name, **kwargs):\n-        if \"ssm_d\" in name and \"bias\" not in name and \"weight\" not in name:\n-            # ssm_d has conflicts with ssm_dt in name checking\n-            # we have to explicitly check that name is exactly ssm_d\n-            name = name.replace(\"ssm_d\", \"mixer.D\")\n         if \"ssm_conv1d.weight\" in name:\n             # for compatibility tensor ssm_conv1d must be (5120, 1, 4]) dim,\n             # quantized one is (5120, 4)\n@@ -267,7 +250,84 @@ def read_field(reader, field):\n     return [_gguf_parse_value(value.parts[_data_index], value.types) for _data_index in value.data]\n \n \n-def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n+# modified from https://github.com/vllm-project/vllm/blob/v0.6.4.post1/vllm/model_executor/model_loader/loader.py#L1115-L1147\n+def get_gguf_hf_weights_map(\n+    hf_model,\n+    model_type: Optional[str] = None,\n+    num_layers: Optional[int] = None,\n+    qual_name: str = \"\",\n+):\n+    \"\"\"\n+    GGUF uses this naming convention for their tensors from HF checkpoint:\n+    `blk.N.BB.weight` and `blk.N.BB.bias`\n+    where N signifies the block number of a layer, and BB signifies the\n+    attention/mlp layer components.\n+    See \"Standardized tensor names\" in\n+    https://github.com/ggerganov/ggml/blob/master/docs/gguf.md for details.\n+    \"\"\"\n+    if is_gguf_available() and is_torch_available():\n+        from gguf import MODEL_ARCH_NAMES, get_tensor_name_map\n+    else:\n+        logger.error(\n+            \"Loading a GGUF checkpoint in PyTorch, requires both PyTorch and GGUF>=0.10.0 to be installed. Please see \"\n+            \"https://pytorch.org/ and https://github.com/ggerganov/llama.cpp/tree/master/gguf-py for installation instructions.\"\n+        )\n+        raise ImportError(\"Please install torch and gguf>=0.10.0 to load a GGUF checkpoint in PyTorch.\")\n+\n+    model_type = hf_model.config.model_type if model_type is None else model_type\n+    num_layers = hf_model.config.num_hidden_layers if num_layers is None else num_layers\n+    # hack: ggufs have a different name for cohere\n+    if model_type == \"cohere\":\n+        model_type = \"command-r\"\n+    if model_type == \"qwen2_moe\":\n+        model_type = \"qwen2moe\"\n+    arch = None\n+    for key, value in MODEL_ARCH_NAMES.items():\n+        if value == model_type:\n+            arch = key\n+            break\n+    if arch is None:\n+        raise NotImplementedError(\n+            f\"Unknown gguf model_type: {model_type} in gguf-py. \"\n+            \"This might because you're using an outdated version of gguf-py package, \"\n+            \"you can install `gguf` package from source refer to \"\n+            \"https://github.com/ggerganov/llama.cpp/tree/master/gguf-py#development\"\n+        )\n+    name_map = get_tensor_name_map(arch, num_layers)\n+\n+    # Use a dummy conversion to get the mapping, because\n+    # hf => gguf and gguf => hf mappings are reversed\n+    gguf_to_hf_name_map = {}\n+    state_dict = hf_model.state_dict()\n+    for hf_name in state_dict.keys():\n+        # An exception for qwen2moe model, where the expert layers are packed\n+        if model_type == \"qwen2moe\" and \"mlp.experts.\" in hf_name:\n+            hf_name = re.sub(r\"mlp.experts.\\d+.\", \"mlp.experts.\", hf_name)\n+\n+        name, suffix = hf_name, \"\"\n+        if hf_name.endswith(\".weight\") or hf_name.endswith(\".bias\"):\n+            name, suffix = hf_name.rsplit(\".\", 1)\n+            suffix = \".\" + suffix\n+\n+        gguf_name = name_map.get_name(name)\n+        if gguf_name is None:\n+            continue\n+\n+        gguf_to_hf_name_map[gguf_name + suffix] = qual_name + hf_name\n+\n+    # Some model like Bloom converted from BloomModel instead of BloomForCausalLM\n+    # Therefore, we need to check submodule as well to get a correct mapping\n+    if named_children := hf_model.named_children():\n+        for name, child in named_children:\n+            sub_map = get_gguf_hf_weights_map(child, model_type, num_layers, qual_name=f\"{qual_name}{name}.\")\n+            # Ignore the keys that are already in the main map to avoid overwriting\n+            sub_map = {k: v for k, v in sub_map.items() if k not in gguf_to_hf_name_map}\n+            gguf_to_hf_name_map.update(sub_map)\n+\n+    return gguf_to_hf_name_map\n+\n+\n+def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_load=None):\n     \"\"\"\n     Load a GGUF file and return a dictionary of parsed parameters containing tensors, the parsed\n     tokenizer and config attributes.\n@@ -323,20 +383,8 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n         parsed_parameters[\"config\"][\"use_qkv_bias\"] = qkv_bias\n         parsed_parameters[\"config\"][\"use_parallel_residual\"] = not use_parallel_residual\n \n-    model_size = \"\"\n-    # extract the number of params from file name as architectures can differ ;\n-    # eg. for falcon : `...falcon-7b-...`\n-    if \"falcon\" in architecture:\n-        gguf_file_name = gguf_checkpoint_path.split(\"/\")[-1].lower()\n-        m = re.search(r\"-\\d+b-\", gguf_file_name)  # regex to catch `-7b-`\n-        if m is None:\n-            raise ValueError(\n-                f\"From file name, cannot determine the number of parameters for {architecture} architecture\"\n-            )\n-        model_size = m.group().strip(\"-\")  # only keeps `7b`\n-\n-    if architecture + model_size not in GGUF_SUPPORTED_ARCHITECTURES:\n-        raise ValueError(f\"Architecture {architecture + model_size} not supported\")\n+    if architecture not in GGUF_SUPPORTED_ARCHITECTURES:\n+        raise ValueError(f\"GGUF model with architecture {architecture} is not supported yet.\")\n \n     # Handle tie_word_embeddings, if lm_head.weight is not present in tensors,\n     # tie_word_embeddings is true otherwise false\n@@ -388,7 +436,9 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n             )\n \n     if return_tensors:\n-        tensor_key_mapping = GGUF_TO_TRANSFORMERS_MAPPING[\"tensors\"][architecture + model_size]\n+        parsed_parameters[\"tensors\"] = {}\n+\n+        tensor_key_mapping = get_gguf_hf_weights_map(model_to_load)\n         config = parsed_parameters.get(\"config\", {})\n \n         ProcessorClass = TENSOR_PROCESSORS.get(architecture, TensorProcessor)\n@@ -407,16 +457,12 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n \n             weights = result.weights\n             name = result.name\n-            bid = result.metadata.get(\"bid\")\n \n-            if name is None:\n+            if name not in tensor_key_mapping:\n                 continue\n \n-            for tensor_name in tensor_key_mapping:\n-                if tensor_name.format(bid=bid) in name:\n-                    name = name.replace(tensor_name.format(bid=bid), tensor_key_mapping[tensor_name].format(bid=bid))\n+            name = tensor_key_mapping[name]\n \n-            # Use copy to avoid errors with numpy and pytorch\n             parsed_parameters[\"tensors\"][name] = torch.from_numpy(np.copy(weights))\n \n     if len(reader_keys) > 0:"
        },
        {
            "sha": "33ddc2fbcc438b6a41016807be468b9083965a20",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951da1a6bfb694fc387081768f163c470b0392b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951da1a6bfb694fc387081768f163c470b0392b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=3951da1a6bfb694fc387081768f163c470b0392b",
            "patch": "@@ -3917,7 +3917,10 @@ def from_pretrained(\n \n                 gguf_path = cached_file(pretrained_model_name_or_path, gguf_file, **cached_file_kwargs)\n \n-            state_dict = load_gguf_checkpoint(gguf_path, return_tensors=True)[\"tensors\"]\n+            # we need a dummy model to help rename state_dict\n+            with torch.device(\"meta\"):\n+                dummy_model = cls(config)\n+            state_dict = load_gguf_checkpoint(gguf_path, return_tensors=True, model_to_load=dummy_model)[\"tensors\"]\n \n             resolved_archive_file = None\n             is_sharded = False"
        }
    ],
    "stats": {
        "total": 385,
        "additions": 92,
        "deletions": 293
    }
}