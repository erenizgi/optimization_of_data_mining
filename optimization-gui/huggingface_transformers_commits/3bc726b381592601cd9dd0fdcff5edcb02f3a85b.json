{
    "author": "zucchini-nlp",
    "message": "[gemma3] fix bidirectional image mask (#39396)\n\n* fix gemma3 mask\n\n* make compile happy, and use only torch ops\n\n* no full attention between images\n\n* update tests\n\n* fix tests\n\n* add a fast test",
    "sha": "3bc726b381592601cd9dd0fdcff5edcb02f3a85b",
    "files": [
        {
            "sha": "e360acdac3417e22838a3706866ae2983a39251e",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc726b381592601cd9dd0fdcff5edcb02f3a85b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc726b381592601cd9dd0fdcff5edcb02f3a85b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=3bc726b381592601cd9dd0fdcff5edcb02f3a85b",
            "patch": "@@ -646,8 +646,8 @@ def prepare_inputs_for_generation(\n \n             # If it's not defined, it means the model uses the new general mask API\n             if causal_mask_creation_function is None:  # can't be found\n-                token_type_ids = getattr(model_input, \"token_type_ids\", None)\n-                position_ids = getattr(model_input, position_ids_key, None)\n+                token_type_ids = model_inputs.get(\"token_type_ids\", None)\n+                position_ids = model_inputs.get(position_ids_key, None)\n                 # Some models may overwrite the general one\n                 causal_mask_creation_function = getattr(self, \"create_masks_for_generate\", create_masks_for_generate)\n                 attention_mask = causal_mask_creation_function("
        },
        {
            "sha": "394e3800217148895746d87f0ddf764008a1e269",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 32,
            "deletions": 6,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc726b381592601cd9dd0fdcff5edcb02f3a85b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc726b381592601cd9dd0fdcff5edcb02f3a85b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=3bc726b381592601cd9dd0fdcff5edcb02f3a85b",
            "patch": "@@ -737,7 +737,11 @@ def forward(self, vision_outputs: torch.Tensor):\n         return projected_vision_outputs.type_as(vision_outputs)\n \n \n-def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor], tokens_per_image: int) -> Optional[Callable]:\n+def token_type_ids_mask_function(\n+    token_type_ids: Optional[torch.Tensor],\n+    image_group_ids: Optional[torch.Tensor],\n+    tokens_per_image: int,\n+) -> Optional[Callable]:\n     \"\"\"\n     This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n     not start and end indices.\n@@ -747,10 +751,18 @@ def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor], tokens_\n         return None\n \n     def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n-        # If the difference is less than image size, both are part of the same image block\n-        same_image_block = torch.abs(kv_idx - q_idx) <= tokens_per_image\n         # If it's 1 for both query and key/value, we are in an image block\n-        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids[batch_idx, kv_idx] == 1)\n+        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length\n+        # Since vmap doesn't support `if statement` we workaround it with `torch.where`\n+        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n+        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]\n+        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)\n+\n+        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]\n+        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)\n+\n+        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)\n+        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx\n \n         # This is bidirectional attention whenever we are dealing with image tokens\n         return is_image_block & same_image_block\n@@ -915,8 +927,15 @@ def forward(\n             }\n             if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n+\n+                # First find where a new image block starts: 1 if image and previous not image\n+                # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+                is_image = (token_type_ids == 1).to(cache_position.device)\n+                new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+                image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+                image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n                 mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                    token_type_ids.to(cache_position.device), self.config.mm_tokens_per_image\n+                    token_type_ids.to(cache_position.device), image_group_ids, self.config.mm_tokens_per_image\n                 )\n \n             # Create the masks\n@@ -1181,8 +1200,15 @@ def create_masks_for_generate(\n         # Add the token type ids mask for generate as well\n         if token_type_ids is not None and input_embeds.shape[1] != 1:\n             # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n+\n+            # First find where a new image block starts: 1 if image and previous not image\n+            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+            is_image = (token_type_ids == 1).to(cache_position.device)\n+            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n             mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                token_type_ids.to(cache_position.device), config.mm_tokens_per_image\n+                token_type_ids.to(cache_position.device), image_group_ids, config.mm_tokens_per_image\n             )\n \n         return create_masks_for_generate(**mask_kwargs)"
        },
        {
            "sha": "57ecedca918b3f42eca35a5392d7f00e564e2ac9",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 32,
            "deletions": 6,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc726b381592601cd9dd0fdcff5edcb02f3a85b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc726b381592601cd9dd0fdcff5edcb02f3a85b/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=3bc726b381592601cd9dd0fdcff5edcb02f3a85b",
            "patch": "@@ -716,7 +716,11 @@ def forward(self, vision_outputs: torch.Tensor):\n         return projected_vision_outputs.type_as(vision_outputs)\n \n \n-def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor], tokens_per_image: int) -> Optional[Callable]:\n+def token_type_ids_mask_function(\n+    token_type_ids: Optional[torch.Tensor],\n+    image_group_ids: Optional[torch.Tensor],\n+    tokens_per_image: int,\n+) -> Optional[Callable]:\n     \"\"\"\n     This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n     not start and end indices.\n@@ -726,10 +730,18 @@ def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor], tokens_\n         return None\n \n     def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n-        # If the difference is less than image size, both are part of the same image block\n-        same_image_block = torch.abs(kv_idx - q_idx) <= tokens_per_image\n         # If it's 1 for both query and key/value, we are in an image block\n-        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids[batch_idx, kv_idx] == 1)\n+        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length\n+        # Since vmap doesn't support `if statement` we workaround it with `torch.where`\n+        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n+        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]\n+        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)\n+\n+        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]\n+        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)\n+\n+        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)\n+        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx\n \n         # This is bidirectional attention whenever we are dealing with image tokens\n         return is_image_block & same_image_block\n@@ -840,8 +852,15 @@ def forward(\n             }\n             if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n+\n+                # First find where a new image block starts: 1 if image and previous not image\n+                # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+                is_image = (token_type_ids == 1).to(cache_position.device)\n+                new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+                image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+                image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n                 mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                    token_type_ids.to(cache_position.device), self.config.mm_tokens_per_image\n+                    token_type_ids.to(cache_position.device), image_group_ids, self.config.mm_tokens_per_image\n                 )\n \n             # Create the masks\n@@ -1062,8 +1081,15 @@ def create_masks_for_generate(\n         # Add the token type ids mask for generate as well\n         if token_type_ids is not None and input_embeds.shape[1] != 1:\n             # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n+\n+            # First find where a new image block starts: 1 if image and previous not image\n+            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+            is_image = (token_type_ids == 1).to(cache_position.device)\n+            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n             mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n-                token_type_ids.to(cache_position.device), config.mm_tokens_per_image\n+                token_type_ids.to(cache_position.device), image_group_ids, config.mm_tokens_per_image\n             )\n \n         return create_masks_for_generate(**mask_kwargs)"
        },
        {
            "sha": "3817acfd506f8096f6ce317a07a095e0442a7cba",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 46,
            "deletions": 7,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bc726b381592601cd9dd0fdcff5edcb02f3a85b/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bc726b381592601cd9dd0fdcff5edcb02f3a85b/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=3bc726b381592601cd9dd0fdcff5edcb02f3a85b",
            "patch": "@@ -270,6 +270,45 @@ def setUp(self):\n         self.model_tester = Gemma3Vision2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Gemma3Config, hidden_size=37)\n \n+    def test_bidirectional_image_attention(self):\n+        \"\"\"\n+        Tests that each image can attend to itself bidirectionally. However an image\n+        cannot attend to future images, even within the same batch.\n+        \"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config._attn_implementation = \"eager\"\n+        model = Gemma3Model(config).to(torch_device)\n+\n+        # First let's pass inputs without change which is one image per text and manipulate\n+        # `token_type_ids` to make sure bidirectional mask is applied where it has to be\n+        inputs_dict[\"token_type_ids\"] = torch.zeros_like(inputs_dict[\"token_type_ids\"])\n+        inputs_dict[\"token_type_ids\"][:, :4] = 1  # unmask first 4 tokens\n+        with torch.no_grad():\n+            out = model(**inputs_dict, output_attentions=True)\n+            # We expect a non-causal mask on first 4 tokens, thus no zeros\n+            for attention in out.attentions:\n+                self.assertTrue((attention[..., :4, :4] != 0).all().item())\n+\n+        # Now when removing `token_type_ids`, we will get simple causal mask\n+        inputs_dict[\"token_type_ids\"][:, :4] = 0  # mask back first 4 tokens\n+        with torch.no_grad():\n+            out = model(**inputs_dict, output_attentions=True)\n+            # We expect a causal mask on first 4 tokens, thus no zeros\n+            for attention in out.attentions:\n+                self.assertFalse((attention[..., :4, :4] != 0).all().item())\n+\n+        # Let's add two \"images\" per text, first one spanning 4 tokens and last one 3 tokens\n+        inputs_dict[\"token_type_ids\"][:, :4] = 1\n+        inputs_dict[\"token_type_ids\"][:, 7:10] = 1\n+        with torch.no_grad():\n+            out = model(**inputs_dict, output_attentions=True)\n+            for attention in out.attentions:\n+                self.assertTrue((attention[..., :4, :4] != 0).all().item())\n+                self.assertTrue((attention[..., 7:10, 7:10] != 0).all().item())\n+\n+                # We expect a non-causal mask only within same image and no looking ahead to the future\n+                self.assertTrue((attention[..., :4, 7:10] == 0).all().item())\n+\n     @unittest.skip(reason=\"SiglipVisionModel (vision backbone) does not support standalone training\")\n     def test_training_gradient_checkpointing(self):\n         pass\n@@ -413,7 +452,7 @@ def test_model_4b_bf16(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water in the background. It looks like a lovely,'],\n-                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant coastline in the background. It looks'],\n+                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like'],\n                 (\"rocm\", (9, 5)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant coastline in the background. It looks'],\n             }\n         )  # fmt: skip\n@@ -463,8 +502,8 @@ def test_model_4b_batch(self):\n                     ],\n                 (\"cuda\", 8):\n                     [\n-                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant island in the background. It looks',\n-                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. They depict very different scenes. \\n\\n*   **Image 1** shows a cow standing on a beach'\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear blue water and a blue sky in the background. It looks like',\n+                        \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n*   **Image 1:** Shows a brown\"\n                     ],\n                 (\"rocm\", (9, 5)):\n                     [\n@@ -508,7 +547,7 @@ def test_model_4b_crops(self):\n             {\n                 (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.'],\n                 (\"cuda\", 7): [],\n-                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.'],\n+                (\"cuda\", 8): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\"],\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n@@ -565,8 +604,8 @@ def test_model_4b_batch_crops(self):\n                 ],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8): [\n-                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.',\n-                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nThe first image shows a cow on a beach, while the second image shows a street scene with a',\n+                    \"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\",\n+                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nThe first image shows a cow on a beach, while the second image shows a street scene with a'\n                 ],\n                 (\"rocm\", (9, 5)) : [\n                     'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.',\n@@ -610,7 +649,7 @@ def test_model_4b_multiimage(self):\n             {\n                 (\"xpu\", 3): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image!\\n\\nHere's a description of the scene:\\n\\n*   **Chinese Arch\"],\n                 (\"cuda\", 7): [],\n-                (\"cuda\", 8): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Main Features:**\\n\\n*   **Chinese Archway:** The most prominent\"],\n+                (\"cuda\", 8): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt looks like a street scene in a vibrant,\"],\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()"
        }
    ],
    "stats": {
        "total": 133,
        "additions": 112,
        "deletions": 21
    }
}