{
    "author": "vasqu",
    "message": "[`Mamba2`] Move dt calculations to kernel (#33520)\n\n* use kernel for dt calculations\r\n\r\n* add small test\r\n\r\n* [run-slow] mamba2",
    "sha": "b50ff5993a5d8b2a3d8c7558e81684f8803b044a",
    "files": [
        {
            "sha": "7b414ff9570d9a65caec6c71578a860530dceb42",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b50ff5993a5d8b2a3d8c7558e81684f8803b044a/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b50ff5993a5d8b2a3d8c7558e81684f8803b044a/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=b50ff5993a5d8b2a3d8c7558e81684f8803b044a",
            "patch": "@@ -358,7 +358,6 @@ def cuda_kernels_forward(\n                     dim=-1,\n                 )\n \n-                time_step = nn.functional.softplus(time_step + self.dt_bias)\n                 # 1D Convolution\n                 if causal_conv1d_fn is None or self.activation not in [\"silu\", \"swish\"]:\n                     hidden_states_B_C = self.act(\n@@ -391,6 +390,8 @@ def cuda_kernels_forward(\n                     z=None,\n                     seq_idx=None,\n                     return_final_states=True,\n+                    dt_bias=self.dt_bias,\n+                    dt_softplus=True,\n                     **dt_limit_kwargs,\n                 )\n                 if ssm_state is not None and cache_params is not None:"
        },
        {
            "sha": "55c18abe6b96af599e82d2b0a47ba6c73eb7d4e7",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 25,
            "deletions": 1,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/b50ff5993a5d8b2a3d8c7558e81684f8803b044a/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b50ff5993a5d8b2a3d8c7558e81684f8803b044a/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=b50ff5993a5d8b2a3d8c7558e81684f8803b044a",
            "patch": "@@ -35,7 +35,7 @@\n         Mamba2ForCausalLM,\n         Mamba2Model,\n     )\n-    from transformers.models.mamba2.modeling_mamba2 import Mamba2Cache\n+    from transformers.models.mamba2.modeling_mamba2 import Mamba2Cache, Mamba2Mixer\n     from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_0\n else:\n     is_torch_greater_or_equal_than_2_0 = False\n@@ -378,3 +378,27 @@ def test_batched_equivalence_without_cache(self):\n             individual_gen = model.generate(**inputs, max_new_tokens=30, use_cache=True)\n             individual_output = tokenizer.batch_decode(individual_gen, skip_special_tokens=True)[0]\n             self.assertEqual(individual_output[:100], batched_output[index_gen][:100])\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_mamba2_mixer_train_vs_eval_equivalence(self):\n+        # Based on https://github.com/sustcsonglin/flash-linear-attention/issues/63\n+        # Credit to zhixuan-lin\n+\n+        B, T, D = 4, 512, 768\n+        dtype = torch.bfloat16\n+        config = Mamba2Config(num_heads=24, head_dim=64, hidden_size=768, expand=2, n_groups=1)\n+\n+        torch.manual_seed(42)\n+        with torch.amp.autocast(device_type=\"cuda\", dtype=dtype):\n+            with torch.no_grad():\n+                mixer = Mamba2Mixer(config, layer_idx=0).to(\"cuda\")\n+                hidden_states = torch.rand(size=(B, T, D), dtype=dtype, device=\"cuda\")\n+\n+                mixer.train()\n+                out_train = mixer(hidden_states)\n+\n+                mixer.eval()\n+                out_eval = mixer(hidden_states)\n+\n+                self.assertTrue(torch.allclose(out_train, out_eval, atol=1e-3))"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 27,
        "deletions": 2
    }
}