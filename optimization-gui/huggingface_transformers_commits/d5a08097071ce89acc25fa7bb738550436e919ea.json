{
    "author": "Cyrilvallez",
    "message": "Fix consistency (#39995)\n\n* modular\n\n* fix",
    "sha": "d5a08097071ce89acc25fa7bb738550436e919ea",
    "files": [
        {
            "sha": "0c07800955c50c67d583623d65ed8ad04250b37b",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5a08097071ce89acc25fa7bb738550436e919ea/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5a08097071ce89acc25fa7bb738550436e919ea/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=d5a08097071ce89acc25fa7bb738550436e919ea",
            "patch": "@@ -160,6 +160,8 @@ def forward(self, hidden_states):\n \n \n class GptOssRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n     def __init__(self, config: GptOssConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n@@ -348,7 +350,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        },
        {
            "sha": "b49366362bb495ae2d613427760bd54d5b7868ba",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5a08097071ce89acc25fa7bb738550436e919ea/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5a08097071ce89acc25fa7bb738550436e919ea/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=d5a08097071ce89acc25fa7bb738550436e919ea",
            "patch": "@@ -307,7 +307,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor]:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}