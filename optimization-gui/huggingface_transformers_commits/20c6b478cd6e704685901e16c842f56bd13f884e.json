{
    "author": "yonigozlan",
    "message": "ðŸš¨ Use lru_cache for sine pos embeddings MaskFormer (#40007)\n\n* use lru_cache for sine pos embeddings maskformer\n\n* fix calls to pos embed\n\n* change maxsize to 1",
    "sha": "20c6b478cd6e704685901e16c842f56bd13f884e",
    "files": [
        {
            "sha": "9899a2d41ed10ef4e16d16cfadff12ec2bfe8a78",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 18,
            "deletions": 6,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/20c6b478cd6e704685901e16c842f56bd13f884e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20c6b478cd6e704685901e16c842f56bd13f884e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=20c6b478cd6e704685901e16c842f56bd13f884e",
            "patch": "@@ -28,6 +28,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import auto_docstring, is_accelerate_available, logging\n from ...utils.backbone_utils import load_backbone\n from .configuration_mask2former import Mask2FormerConfig\n@@ -855,18 +856,25 @@ def __init__(\n         self.normalize = normalize\n         self.scale = 2 * math.pi if scale is None else scale\n \n-    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n+    @compile_compatible_method_lru_cache(maxsize=1)\n+    def forward(\n+        self,\n+        shape: torch.Size,\n+        device: Union[torch.device, str],\n+        dtype: torch.dtype,\n+        mask: Optional[Tensor] = None,\n+    ) -> Tensor:\n         if mask is None:\n-            mask = torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool)\n-        not_mask = (~mask).to(x.dtype)\n+            mask = torch.zeros((shape[0], shape[2], shape[3]), device=device, dtype=torch.bool)\n+        not_mask = (~mask).to(dtype)\n         y_embed = not_mask.cumsum(1)\n         x_embed = not_mask.cumsum(2)\n         if self.normalize:\n             eps = 1e-6\n             y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n             x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n \n-        dim_t = torch.arange(self.num_pos_feats, dtype=torch.int64, device=x.device).type_as(x)\n+        dim_t = torch.arange(self.num_pos_feats, dtype=torch.int64, device=device).to(dtype)\n         dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / self.num_pos_feats)\n \n         pos_x = x_embed[:, :, :, None] / dim_t\n@@ -1295,7 +1303,7 @@ def forward(\n         position_embeddings = []\n         for level, x in enumerate(features[::-1][: self.num_feature_levels]):\n             input_embeds.append(self.input_projections[level](x))\n-            position_embeddings.append(self.position_embedding(x))\n+            position_embeddings.append(self.position_embedding(x.shape, x.device, x.dtype))\n \n         masks = [\n             torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool) for x in input_embeds\n@@ -2052,7 +2060,11 @@ def forward(\n \n         for i in range(self.num_feature_levels):\n             size_list.append(multi_scale_features[i].shape[-2:])\n-            multi_stage_positional_embeddings.append(self.position_embedder(multi_scale_features[i], None).flatten(2))\n+            multi_stage_positional_embeddings.append(\n+                self.position_embedder(\n+                    multi_scale_features[i].shape, multi_scale_features[i].device, multi_scale_features[i].dtype, None\n+                ).flatten(2)\n+            )\n             multi_stage_features.append(\n                 self.input_projections[i](multi_scale_features[i]).flatten(2)\n                 + self.level_embed.weight[i][None, :, None]"
        },
        {
            "sha": "16493bc58881ac17868071a29c83dc7f90e25a5d",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/20c6b478cd6e704685901e16c842f56bd13f884e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20c6b478cd6e704685901e16c842f56bd13f884e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=20c6b478cd6e704685901e16c842f56bd13f884e",
            "patch": "@@ -17,7 +17,7 @@\n import math\n from dataclasses import dataclass\n from numbers import Number\n-from typing import Optional\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -28,6 +28,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n@@ -1238,18 +1239,25 @@ def __init__(\n         self.normalize = normalize\n         self.scale = 2 * math.pi if scale is None else scale\n \n-    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n+    @compile_compatible_method_lru_cache(maxsize=1)\n+    def forward(\n+        self,\n+        shape: torch.Size,\n+        device: Union[torch.device, str],\n+        dtype: torch.dtype,\n+        mask: Optional[Tensor] = None,\n+    ) -> Tensor:\n         if mask is None:\n-            mask = torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool)\n-        not_mask = (~mask).to(x.dtype)\n+            mask = torch.zeros((shape[0], shape[2], shape[3]), device=device, dtype=torch.bool)\n+        not_mask = (~mask).to(dtype)\n         y_embed = not_mask.cumsum(1)\n         x_embed = not_mask.cumsum(2)\n         if self.normalize:\n             eps = 1e-6\n             y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n             x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n \n-        dim_t = torch.arange(self.num_pos_feats, dtype=torch.int64, device=x.device).type_as(x)\n+        dim_t = torch.arange(self.num_pos_feats, dtype=torch.int64, device=device).to(dtype)\n         dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / self.num_pos_feats)\n \n         pos_x = x_embed[:, :, :, None] / dim_t\n@@ -1388,7 +1396,7 @@ def forward(\n     ) -> DetrDecoderOutput:\n         if self.input_projection is not None:\n             image_features = self.input_projection(image_features)\n-        object_queries = self.position_embedder(image_features)\n+        object_queries = self.position_embedder(image_features.shape, image_features.device, image_features.dtype)\n         # repeat the queries \"q c -> b q c\"\n         batch_size = image_features.shape[0]\n         queries_embeddings = self.queries_embedder.weight.unsqueeze(0).repeat(batch_size, 1, 1)"
        },
        {
            "sha": "8c2e633e11bc052f46ec34053069e891da0df58b",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/20c6b478cd6e704685901e16c842f56bd13f884e/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20c6b478cd6e704685901e16c842f56bd13f884e/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=20c6b478cd6e704685901e16c842f56bd13f884e",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n@@ -1395,7 +1396,7 @@ def forward(\n         position_embeddings_list = []\n         for level, source in enumerate(features[::-1][: self.num_feature_levels]):\n             sources.append(self.input_projections[level](source))\n-            position_embeddings_list.append(self.position_embedding(source))\n+            position_embeddings_list.append(self.position_embedding(source.shape, source.device, source.dtype))\n \n         masks = [torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool) for x in sources]\n \n@@ -2354,7 +2355,11 @@ def forward(\n \n         for i in range(self.num_feature_levels):\n             size_list.append(multi_scale_features[i].shape[-2:])\n-            multi_stage_positional_embeddings.append(self.position_embedder(multi_scale_features[i], None).flatten(2))\n+            multi_stage_positional_embeddings.append(\n+                self.position_embedder(\n+                    multi_scale_features[i].shape, multi_scale_features[i].device, multi_scale_features[i].dtype, None\n+                ).flatten(2)\n+            )\n             multi_stage_features.append(\n                 self.input_projections[i](multi_scale_features[i]).flatten(2)\n                 + self.level_embed.weight[i][None, :, None]\n@@ -2370,7 +2375,7 @@ def forward(\n         query_embeddings = self.queries_embedder.weight.unsqueeze(1).repeat(1, batch_size, 1)\n         task_token = task_token.unsqueeze(0)\n \n-        query_features = self.position_embedder(mask_features, None)\n+        query_features = self.position_embedder(mask_features.shape, mask_features.device, mask_features.dtype, None)\n \n         return self.decoder(\n             task_token=task_token,\n@@ -2403,18 +2408,25 @@ def __init__(\n         self.normalize = normalize\n         self.scale = 2 * math.pi if scale is None else scale\n \n-    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n+    @compile_compatible_method_lru_cache(maxsize=1)\n+    def forward(\n+        self,\n+        shape: torch.Size,\n+        device: Union[torch.device, str],\n+        dtype: torch.dtype,\n+        mask: Optional[Tensor] = None,\n+    ) -> Tensor:\n         if mask is None:\n-            mask = torch.zeros((x.size(0), x.size(2), x.size(3)), device=x.device, dtype=torch.bool)\n-        not_mask = (~mask).to(x.dtype)\n+            mask = torch.zeros((shape[0], shape[2], shape[3]), device=device, dtype=torch.bool)\n+        not_mask = (~mask).to(dtype)\n         y_embed = not_mask.cumsum(1)\n         x_embed = not_mask.cumsum(2)\n         if self.normalize:\n             eps = 1e-6\n             y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n             x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n \n-        dim_t = torch.arange(self.num_pos_feats, dtype=torch.int64, device=x.device).type_as(x)\n+        dim_t = torch.arange(self.num_pos_feats, dtype=torch.int64, device=device).to(dtype)\n         dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / self.num_pos_feats)\n \n         pos_x = x_embed[:, :, :, None] / dim_t"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 51,
        "deletions": 19
    }
}