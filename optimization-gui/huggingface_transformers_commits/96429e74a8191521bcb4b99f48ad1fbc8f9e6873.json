{
    "author": "a8nova",
    "message": "Add support for GGUF Phi-3 (#31844)\n\n* Update docs for GGUF supported models\r\n\r\n* Add tensor mappings and define class GGUFPhi3Converter\r\n\r\n* Fix tokenizer\r\n\r\n* Working version\r\n\r\n* Attempt to fix some CI failures\r\n\r\n* Run ruff format\r\n\r\n* Add vocab, merges, decoder methods like LlamaConverter\r\n\r\n* Resolve conflicts since Qwen2Moe was added to gguf\r\n\r\n- I missed one place when resolving conflict\r\n- I also made a mistake with tests_ggml.py and now has been fixed to reflect\r\nits master version.",
    "sha": "96429e74a8191521bcb4b99f48ad1fbc8f9e6873",
    "files": [
        {
            "sha": "8e6741a306d898e31efe14c2ed7a02b3bdeb7857",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=96429e74a8191521bcb4b99f48ad1fbc8f9e6873",
            "patch": "@@ -79,6 +79,7 @@ For now the supported model architectures are the architectures that have been v\n - Mistral\n - Qwen2\n - Qwen2Moe\n+- Phi3\n \n ## Example usage\n "
        },
        {
            "sha": "f2064a131dad42b4e38f92865d850b2e7dbead90",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=96429e74a8191521bcb4b99f48ad1fbc8f9e6873",
            "patch": "@@ -1575,6 +1575,7 @@ def converted(self) -> Tokenizer:\n     \"LlamaTokenizer\": LlamaConverter,\n     \"CodeLlamaTokenizer\": LlamaConverter,\n     \"GemmaTokenizer\": GemmaConvert,\n+    \"Phi3Tokenizer\": LlamaConverter,\n }\n \n "
        },
        {
            "sha": "0b93e4c53ff891e70a5ce33a8868237c430b1b18",
            "filename": "src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/src%2Ftransformers%2Fconvert_slow_tokenizers_checkpoints_to_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/src%2Ftransformers%2Fconvert_slow_tokenizers_checkpoints_to_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizers_checkpoints_to_fast.py?ref=96429e74a8191521bcb4b99f48ad1fbc8f9e6873",
            "patch": "@@ -28,7 +28,11 @@\n logger = logging.get_logger(__name__)\n \n \n-TOKENIZER_CLASSES = {name: getattr(transformers, name + \"Fast\") for name in SLOW_TO_FAST_CONVERTERS}\n+TOKENIZER_CLASSES = {\n+    # Phi3 uses Llama tokenizer\n+    name: getattr(transformers, \"LlamaTokenizerFast\" if name == \"Phi3Tokenizer\" else name + \"Fast\")\n+    for name in SLOW_TO_FAST_CONVERTERS\n+}\n \n \n def convert_slow_checkpoint_to_fast(tokenizer_name, checkpoint_name, dump_path, force_download):"
        },
        {
            "sha": "b5471574a13d8b8b43a12d4ae417e41c58da70c2",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=96429e74a8191521bcb4b99f48ad1fbc8f9e6873",
            "patch": "@@ -94,6 +94,19 @@\n         \"output.weight\": \"lm_head.weight\",\n         \"output_norm\": \"model.norm\",\n     },\n+    \"phi3\": {\n+        \"token_embd\": \"model.embed_tokens\",\n+        \"blk\": \"model.layers\",\n+        \"ffn_up\": \"mlp.gate_up_proj\",\n+        \"ffn_down\": \"mlp.down_proj\",\n+        \"ffn_gate\": \"mlp.gate_up_proj\",\n+        \"ffn_norm\": \"post_attention_layernorm\",\n+        \"attn_norm\": \"input_layernorm\",\n+        \"attn_qkv\": \"self_attn.qkv_proj\",\n+        \"attn_output\": \"self_attn.o_proj\",\n+        \"output.weight\": \"lm_head.weight\",\n+        \"output_norm\": \"model.norm\",\n+    },\n }\n \n \n@@ -156,6 +169,18 @@\n         \"ggml.unknown_token_id\": \"unk_token_id\",\n         \"ggml.padding_token_id\": \"pad_token_id\",\n     },\n+    \"phi3\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"rope.freq_base\": \"rope_theta\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n }\n \n GGUF_TOKENIZER_MAPPING = {\n@@ -390,10 +415,86 @@ def converted(self) -> Tokenizer:\n         return tokenizer\n \n \n+class GGUFPhi3Converter(LlamaConverter):\n+    def __init__(self, tokenizer_dict):\n+        self.proto = GGUFTokenizerSkeleton(tokenizer_dict)\n+        self.original_tokenizer = self.proto\n+        self.additional_kwargs = {}\n+\n+    def vocab(self, proto):\n+        return list(zip(proto.tokens, proto.scores))\n+\n+    def merges(self, proto):\n+        return proto.merges\n+\n+    def tokenizer(self, proto):\n+        vocab_scores = self.vocab(self.proto)\n+        merges = self.merges(self.proto)\n+        bpe_vocab = {word: i for i, (word, _score) in enumerate(vocab_scores)}\n+\n+        tokenizer = Tokenizer(BPE(bpe_vocab, merges))\n+        # add the special tokens from phi3 tokenizer config\n+        tokenizer.add_special_tokens(\n+            [\n+                AddedToken(\"</s>\", rstrip=True, lstrip=False, normalized=False, special=True),\n+                AddedToken(\"<|endoftext|>\", normalized=False, special=True),\n+                AddedToken(\"<|assistant|>\", rstrip=True, normalized=False, special=True),\n+                AddedToken(\"<|placeholder1|>\", rstrip=True, normalized=False, special=True),\n+                AddedToken(\"<|placeholder2|>\", rstrip=True, normalized=False, special=True),\n+                AddedToken(\"<|placeholder3|>\", rstrip=True, normalized=False, special=True),\n+                AddedToken(\"<|placeholder4|>\", rstrip=True, normalized=False, special=True),\n+                AddedToken(\"<|system|>\", rstrip=True, normalized=False, special=True),\n+                AddedToken(\"<|end|>\", rstrip=True, normalized=False, special=True),\n+                AddedToken(\"<|placeholder5|>\", rstrip=True, normalized=False, special=True),\n+                AddedToken(\"<|placeholder6|>\", rstrip=True, normalized=False, special=True),\n+                AddedToken(\"<|user|>\", rstrip=True, normalized=False, special=True),\n+            ]\n+        )\n+\n+        self.additional_kwargs[\"unk_token\"] = (\n+            proto.tokens[proto.unk_token_id] if proto.unk_token_id is not None else None\n+        )\n+        self.additional_kwargs[\"eos_token\"] = (\n+            proto.tokens[proto.eos_token_id] if proto.eos_token_id is not None else None\n+        )\n+        self.additional_kwargs[\"bos_token\"] = (\n+            proto.tokens[proto.bos_token_id] if proto.bos_token_id is not None else None\n+        )\n+        self.additional_kwargs[\"pad_token\"] = (\n+            proto.tokens[proto.pad_token_id] if proto.pad_token_id is not None else None\n+        )\n+\n+        return tokenizer\n+\n+    def decoder(self, replacement, add_prefix_space):\n+        sequence = [\n+            decoders.ByteFallback(),\n+            decoders.Fuse(),\n+            decoders.Replace(replacement, \" \"),\n+        ]\n+\n+        if add_prefix_space:\n+            sequence += [decoders.Strip(content=\" \", left=1)]\n+        return decoders.Sequence(sequence)\n+\n+    def converted(self) -> Tokenizer:\n+        tokenizer = self.tokenizer(self.proto)\n+\n+        replacement = \"‚ñÅ\"\n+        add_prefix_space = True\n+        if hasattr(self.original_tokenizer, \"add_prefix_space\"):\n+            add_prefix_space = self.original_tokenizer.add_prefix_space\n+\n+        tokenizer.decoder = self.decoder(replacement, add_prefix_space)\n+\n+        return tokenizer\n+\n+\n GGUF_TO_FAST_CONVERTERS = {\n     \"llama\": GGUFLlamaConverter,\n     \"qwen2\": GGUFQwen2Converter,\n     \"qwen2_moe\": GGUFQwen2Converter,\n+    \"phi3\": GGUFPhi3Converter,\n }\n \n "
        },
        {
            "sha": "6d3bb3f53371858455887af5ff5b4ba7284eb0df",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96429e74a8191521bcb4b99f48ad1fbc8f9e6873/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=96429e74a8191521bcb4b99f48ad1fbc8f9e6873",
            "patch": "@@ -41,6 +41,7 @@ class GgufIntegrationTests(unittest.TestCase):\n     qwen2_moe_model_id = \"RichardErkhov/Qwen_-_Qwen1.5-MoE-A2.7B-Chat-gguf\"\n     llama3_model_id = \"NousResearch/Meta-Llama-3-8B-GGUF\"\n     tinyllama_model_id = \"PenutChen/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n+    phi3_model_id = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n \n     # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n@@ -63,6 +64,7 @@ class GgufIntegrationTests(unittest.TestCase):\n     iq4_xs_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ4_XS.gguf\"\n     iq4_nl_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ4_NL.gguf\"\n \n+    q4_0_phi3_model_id = \"Phi-3-mini-4k-instruct-q4.gguf\"\n     q4_0_mistral_model_id = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n     q4_0_qwen2_model_id = \"qwen1_5-0_5b-chat-q4_0.gguf\"\n     q4_0_qwen2_moe_model_id = \"Qwen1.5-MoE-A2.7B-Chat.Q4_0.gguf\"\n@@ -347,6 +349,18 @@ def test_qwen2_moe_q4_0(self):\n         EXPECTED_TEXT = \"Hello everyone, I'm a newbie here and would like\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    def test_phi3_q4_0(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.phi3_model_id, gguf_file=self.q4_0_phi3_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.phi3_model_id, gguf_file=self.q4_0_phi3_model_id, device_map=\"auto\", torch_dtype=torch.float16\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I've been reading about the impact of\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n     def test_llama3_q4_0_tokenizer(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.llama3_model_id, gguf_file=self.q4_llama3_model_id)\n         with tempfile.TemporaryDirectory() as tmpdirname:"
        }
    ],
    "stats": {
        "total": 123,
        "additions": 122,
        "deletions": 1
    }
}