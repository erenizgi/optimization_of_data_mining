{
    "author": "Manalelaidouni",
    "message": "Add include_loss_for_metrics (#33088)\n\n* Add include_loss_for_metrics\r\n\r\n* Fix styling\r\n\r\n* Initialize inputs and losses to avoid AttributeError\r\n\r\n* Ruff styling\r\n\r\n* Refactor compute_metrics and update EvalPrediction\r\n\r\n* Change Naming\r\n\r\n* Added include_for_metrics to group both args\r\n\r\n* Fix style\r\n\r\n* Change warnings to logger\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "0c4c2d7e07c299eb8781c0d525b297684297ce06",
    "files": [
        {
            "sha": "5b5afee24846f4b7b47c8dda91dbda6e40a640b5",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 30,
            "deletions": 34,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c4c2d7e07c299eb8781c0d525b297684297ce06/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c4c2d7e07c299eb8781c0d525b297684297ce06/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=0c4c2d7e07c299eb8781c0d525b297684297ce06",
            "patch": "@@ -4047,6 +4047,7 @@ def evaluation_loop(\n         all_inputs = EvalLoopContainer(self.args.eval_do_concat_batches, padding_index=-100)\n \n         metrics = None\n+        eval_set_kwargs = {}\n \n         # Will be useful when we have an iterable dataset so don't know its length.\n         observed_num_examples = 0\n@@ -4064,7 +4065,9 @@ def evaluation_loop(\n             # Prediction step\n             losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n             main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n-            inputs_decode = self._prepare_input(inputs[main_input_name]) if args.include_inputs_for_metrics else None\n+            inputs_decode = (\n+                self._prepare_input(inputs[main_input_name]) if \"inputs\" in args.include_for_metrics else None\n+            )\n \n             if is_torch_xla_available():\n                 xm.mark_step()\n@@ -4098,16 +4101,13 @@ def evaluation_loop(\n             if self.args.batch_eval_metrics:\n                 if self.compute_metrics is not None and logits is not None and labels is not None:\n                     is_last_step = self.accelerator.gradient_state.end_of_dataloader\n-                    if args.include_inputs_for_metrics:\n-                        metrics = self.compute_metrics(\n-                            EvalPrediction(predictions=logits, label_ids=labels, inputs=inputs),\n-                            compute_result=is_last_step,\n-                        )\n-                    else:\n-                        metrics = self.compute_metrics(\n-                            EvalPrediction(predictions=logits, label_ids=labels),\n-                            compute_result=is_last_step,\n-                        )\n+                    batch_kwargs = {}\n+                    batch_kwargs[\"losses\"] = losses if \"loss\" in args.include_for_metrics else None\n+                    batch_kwargs[\"inputs\"] = inputs if \"inputs\" in args.include_for_metrics else None\n+                    metrics = self.compute_metrics(\n+                        EvalPrediction(predictions=logits, label_ids=labels, **batch_kwargs),\n+                        compute_result=is_last_step,\n+                    )\n \n                 del losses, logits, labels, inputs\n                 torch.cuda.empty_cache()\n@@ -4156,12 +4156,11 @@ def evaluation_loop(\n             and all_labels is not None\n             and not self.args.batch_eval_metrics\n         ):\n-            if args.include_inputs_for_metrics:\n-                metrics = self.compute_metrics(\n-                    EvalPrediction(predictions=all_preds, label_ids=all_labels, inputs=all_inputs)\n-                )\n-            else:\n-                metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n+            eval_set_kwargs[\"losses\"] = all_losses if \"loss\" in args.include_for_metrics else None\n+            eval_set_kwargs[\"inputs\"] = all_inputs if \"inputs\" in args.include_for_metrics else None\n+            metrics = self.compute_metrics(\n+                EvalPrediction(predictions=all_preds, label_ids=all_labels, **eval_set_kwargs)\n+            )\n         elif metrics is None:\n             metrics = {}\n \n@@ -4634,6 +4633,7 @@ def prediction_loop(\n         labels_host: Union[torch.Tensor, List[torch.Tensor]] = None\n         inputs_host: Union[torch.Tensor, List[torch.Tensor]] = None\n         metrics: Optional[dict] = None\n+        eval_set_kwargs: dict = {}\n \n         world_size = max(1, args.world_size)\n \n@@ -4660,7 +4660,9 @@ def prediction_loop(\n         for step, inputs in enumerate(dataloader):\n             loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n             main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n-            inputs_decode = self._prepare_input(inputs[main_input_name]) if args.include_inputs_for_metrics else None\n+            inputs_decode = (\n+                self._prepare_input(inputs[main_input_name]) if \"inputs\" in args.include_for_metrics else None\n+            )\n \n             if loss is not None:\n                 losses = loss.repeat(batch_size)\n@@ -4680,16 +4682,13 @@ def prediction_loop(\n             if self.args.batch_eval_metrics:\n                 if self.compute_metrics is not None and preds_host is not None and labels_host is not None:\n                     is_last_step = self.accelerator.gradient_state.end_of_dataloader\n-                    if args.include_inputs_for_metrics:\n-                        metrics = self.compute_metrics(\n-                            EvalPrediction(predictions=preds_host, label_ids=labels_host, inputs=inputs_host),\n-                            compute_result=is_last_step,\n-                        )\n-                    else:\n-                        metrics = self.compute_metrics(\n-                            EvalPrediction(predictions=preds_host, label_ids=labels_host),\n-                            compute_result=is_last_step,\n-                        )\n+                    batch_kwargs = {}\n+                    batch_kwargs[\"losses\"] = losses_host if \"loss\" in args.include_for_metrics else None\n+                    batch_kwargs[\"inputs\"] = inputs_host if \"inputs\" in args.include_for_metrics else None\n+                    metrics = self.compute_metrics(\n+                        EvalPrediction(predictions=preds_host, label_ids=labels_host, **batch_kwargs),\n+                        compute_result=is_last_step,\n+                    )\n \n             if self.args.batch_eval_metrics or (\n                 args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0\n@@ -4728,12 +4727,9 @@ def prediction_loop(\n             and label_ids is not None\n             and not self.args.batch_eval_metrics\n         ):\n-            if args.include_inputs_for_metrics:\n-                metrics = self.compute_metrics(\n-                    EvalPrediction(predictions=preds, label_ids=label_ids, inputs=inputs_ids)\n-                )\n-            else:\n-                metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n+            eval_set_kwargs[\"losses\"] = eval_loss if \"loss\" in args.include_for_metrics else None\n+            eval_set_kwargs[\"inputs\"] = inputs_ids if \"inputs\" in args.include_for_metrics else None\n+            metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids, **eval_set_kwargs))\n         elif metrics is None:\n             metrics = {}\n "
        },
        {
            "sha": "02c298cf7d2e65abb9d48642a6e0b71666129afb",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c4c2d7e07c299eb8781c0d525b297684297ce06/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c4c2d7e07c299eb8781c0d525b297684297ce06/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=0c4c2d7e07c299eb8781c0d525b297684297ce06",
            "patch": "@@ -156,36 +156,34 @@ class EvalPrediction:\n     Parameters:\n         predictions (`np.ndarray`): Predictions of the model.\n         label_ids (`np.ndarray`): Targets to be matched.\n-        inputs (`np.ndarray`, *optional*):\n+        inputs (`np.ndarray`, *optional*): Input data passed to the model.\n+        losses (`np.ndarray`, *optional*): Loss values computed during evaluation.\n     \"\"\"\n \n     def __init__(\n         self,\n         predictions: Union[np.ndarray, Tuple[np.ndarray]],\n         label_ids: Union[np.ndarray, Tuple[np.ndarray]],\n         inputs: Optional[Union[np.ndarray, Tuple[np.ndarray]]] = None,\n+        losses: Optional[Union[np.ndarray, Tuple[np.ndarray]]] = None,\n     ):\n         self.predictions = predictions\n         self.label_ids = label_ids\n         self.inputs = inputs\n+        self.losses = losses\n+        self.elements = (self.predictions, self.label_ids)\n+        if self.inputs is not None:\n+            self.elements += (self.inputs,)\n+        if self.losses is not None:\n+            self.elements += (self.losses,)\n \n     def __iter__(self):\n-        if self.inputs is not None:\n-            return iter((self.predictions, self.label_ids, self.inputs))\n-        else:\n-            return iter((self.predictions, self.label_ids))\n+        return iter(self.elements)\n \n     def __getitem__(self, idx):\n-        if idx < 0 or idx > 2:\n-            raise IndexError(\"tuple index out of range\")\n-        if idx == 2 and self.inputs is None:\n+        if idx < 0 or idx >= len(self.elements):\n             raise IndexError(\"tuple index out of range\")\n-        if idx == 0:\n-            return self.predictions\n-        elif idx == 1:\n-            return self.label_ids\n-        elif idx == 2:\n-            return self.inputs\n+        return self.elements[idx]\n \n \n class EvalLoopOutput(NamedTuple):"
        },
        {
            "sha": "b314d7855dd0160ff8a7fde017e16d718b4c7f36",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 23,
            "deletions": 3,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c4c2d7e07c299eb8781c0d525b297684297ce06/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c4c2d7e07c299eb8781c0d525b297684297ce06/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=0c4c2d7e07c299eb8781c0d525b297684297ce06",
            "patch": "@@ -707,8 +707,12 @@ class TrainingArguments:\n         gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n             Key word arguments to be passed to the `gradient_checkpointing_enable` method.\n         include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n-            Whether or not the inputs will be passed to the `compute_metrics` function. This is intended for metrics\n-            that need inputs, predictions and references for scoring calculation in Metric class.\n+            This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [\"inputs\"]`.\n+        include_for_metrics (`List[str]`, *optional*, defaults to `[]`):\n+            Include additional data in the `compute_metrics` function if needed for metrics computation.\n+            Possible options to add to `include_for_metrics` list:\n+            - `\"inputs\"`: Input data passed to the model, intended for calculating input dependent metrics.\n+            - `\"loss\"`: Loss values computed during evaluation, intended for calculating loss dependent metrics.\n         eval_do_concat_batches (`bool`, *optional*, defaults to `True`):\n             Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`,\n             will instead store them as lists, with each batch kept separate.\n@@ -1362,7 +1366,17 @@ class TrainingArguments:\n         },\n     )\n     include_inputs_for_metrics: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not the inputs will be passed to the `compute_metrics` function.\"}\n+        default=False,\n+        metadata={\n+            \"help\": \"This argument is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `include_for_metrics` instead.\"\n+        },\n+    )\n+    include_for_metrics: List[str] = field(\n+        default_factory=list,\n+        metadata={\n+            \"help\": \"List of strings to specify additional data to include in the `compute_metrics` function.\"\n+            \"Options: 'inputs', 'loss'.\"\n+        },\n     )\n     eval_do_concat_batches: bool = field(\n         default=True,\n@@ -2064,6 +2078,12 @@ def __post_init__(self):\n                 \"This is not supported and we recommend you to update your version.\"\n             )\n \n+        if self.include_inputs_for_metrics:\n+            logger.warning(\n+                \"Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Please use `include_for_metrics` list argument instead.\"\n+            )\n+            self.include_for_metrics.append(\"inputs\")\n+\n     def __str__(self):\n         self_as_dict = asdict(self)\n "
        }
    ],
    "stats": {
        "total": 116,
        "additions": 65,
        "deletions": 51
    }
}