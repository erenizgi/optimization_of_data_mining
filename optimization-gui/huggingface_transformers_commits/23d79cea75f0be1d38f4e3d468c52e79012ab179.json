{
    "author": "LysandreJik",
    "message": "Support for version spec in requires & arbitrary mismatching depths across folders (#37854)\n\n* Support for version spec in requires & arbitrary mismatching depths\n\n* Quality\n\n* Testing",
    "sha": "23d79cea75f0be1d38f4e3d468c52e79012ab179",
    "files": [
        {
            "sha": "749ece15da686d8640120b79776593656b0d0d5a",
            "filename": "docs/source/en/internal/import_utils.md",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/23d79cea75f0be1d38f4e3d468c52e79012ab179/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/23d79cea75f0be1d38f4e3d468c52e79012ab179/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md?ref=23d79cea75f0be1d38f4e3d468c52e79012ab179",
            "patch": "@@ -84,6 +84,19 @@ class Trainer:\n \n Backends that can be added here are all the backends that are available in the `import_utils.py` module.\n \n+Additionally, specific versions can be specified in each backend. For example, this is how you would specify\n+a requirement on torch>=2.6 on the `Trainer` class:\n+\n+```python\n+from .utils.import_utils import requires\n+\n+@requires(backends=(\"torch>=2.6\", \"accelerate\"))\n+class Trainer:\n+    ...\n+```\n+\n+You can specify the following operators: `==`, `>`, `>=`, `<`, `<=`, `!=`.\n+\n ## Methods\n \n [[autodoc]] utils.import_utils.define_import_structure"
        },
        {
            "sha": "3c0079459b10fcfc7f2c1f12c200fcf32aa7a3d9",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 162,
            "deletions": 25,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/23d79cea75f0be1d38f4e3d468c52e79012ab179/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23d79cea75f0be1d38f4e3d468c52e79012ab179/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=23d79cea75f0be1d38f4e3d468c52e79012ab179",
            "patch": "@@ -19,16 +19,19 @@\n import importlib.metadata\n import importlib.util\n import json\n+import operator\n import os\n+import re\n import shutil\n import subprocess\n import sys\n import warnings\n from collections import OrderedDict\n+from enum import Enum\n from functools import lru_cache\n from itertools import chain\n from types import ModuleType\n-from typing import Any, Dict, FrozenSet, Optional, Set, Tuple, Union\n+from typing import Any, Dict, FrozenSet, List, Optional, Set, Tuple, Union\n \n from packaging import version\n \n@@ -1838,8 +1841,16 @@ def requires_backends(obj, backends):\n     if \"tf\" in backends and \"torch\" not in backends and is_torch_available() and not is_tf_available():\n         raise ImportError(TF_IMPORT_ERROR_WITH_PYTORCH.format(name))\n \n-    checks = (BACKENDS_MAPPING[backend] for backend in backends)\n-    failed = [msg.format(name) for available, msg in checks if not available()]\n+    failed = []\n+    for backend in backends:\n+        if isinstance(backend, Backend):\n+            available, msg = backend.is_satisfied, backend.error_message\n+        else:\n+            available, msg = BACKENDS_MAPPING[backend]\n+\n+        if not available():\n+            failed.append(msg.format(name))\n+\n     if failed:\n         raise ImportError(\"\".join(failed))\n \n@@ -1884,10 +1895,13 @@ def __init__(\n         import_structure: IMPORT_STRUCTURE_T,\n         module_spec: Optional[importlib.machinery.ModuleSpec] = None,\n         extra_objects: Optional[Dict[str, object]] = None,\n+        explicit_import_shortcut: Optional[Dict[str, List[str]]] = None,\n     ):\n         super().__init__(name)\n \n         self._object_missing_backend = {}\n+        self._explicit_import_shortcut = explicit_import_shortcut if explicit_import_shortcut else {}\n+\n         if any(isinstance(key, frozenset) for key in import_structure.keys()):\n             self._modules = set()\n             self._class_to_module = {}\n@@ -1916,14 +1930,25 @@ def __init__(\n                 module_keys = set(\n                     chain(*[[k.rsplit(\".\", i)[0] for i in range(k.count(\".\") + 1)] for k in list(module.keys())])\n                 )\n+\n                 for backend in backends:\n-                    if backend not in BACKENDS_MAPPING:\n-                        raise ValueError(\n-                            f\"Error: the following backend: '{backend}' was specified around object {module} but isn't specified in the backends mapping.\"\n-                        )\n-                    callable, error = BACKENDS_MAPPING[backend]\n-                    if not callable():\n+                    if backend in BACKENDS_MAPPING:\n+                        callable, _ = BACKENDS_MAPPING[backend]\n+                    else:\n+                        if any(key in backend for key in [\"=\", \"<\", \">\"]):\n+                            backend = Backend(backend)\n+                            callable = backend.is_satisfied\n+                        else:\n+                            raise ValueError(\n+                                f\"Backend should be defined in the BACKENDS_MAPPING. Offending backend: {backend}\"\n+                            )\n+\n+                    try:\n+                        if not callable():\n+                            missing_backends.append(backend)\n+                    except (importlib.metadata.PackageNotFoundError, ModuleNotFoundError, RuntimeError):\n                         missing_backends.append(backend)\n+\n                 self._modules = self._modules.union(module_keys)\n \n                 for key, values in module.items():\n@@ -2000,12 +2025,29 @@ def call(self, *args, **kwargs):\n \n             value = Placeholder\n         elif name in self._class_to_module.keys():\n-            module = self._get_module(self._class_to_module[name])\n-            value = getattr(module, name)\n+            try:\n+                module = self._get_module(self._class_to_module[name])\n+                value = getattr(module, name)\n+            except (ModuleNotFoundError, RuntimeError) as e:\n+                raise ModuleNotFoundError(\n+                    f\"Could not import module '{name}'. Are this object's requirements defined correctly?\"\n+                ) from e\n+\n         elif name in self._modules:\n-            value = self._get_module(name)\n+            try:\n+                value = self._get_module(name)\n+            except (ModuleNotFoundError, RuntimeError) as e:\n+                raise ModuleNotFoundError(\n+                    f\"Could not import module '{name}'. Are this object's requirements defined correctly?\"\n+                ) from e\n         else:\n-            raise AttributeError(f\"module {self.__name__} has no attribute {name}\")\n+            value = None\n+            for key, values in self._explicit_import_shortcut.items():\n+                if name in values:\n+                    value = self._get_module(key)\n+\n+            if value is None:\n+                raise AttributeError(f\"module {self.__name__} has no attribute {name}\")\n \n         setattr(self, name, value)\n         return value\n@@ -2046,22 +2088,87 @@ def direct_transformers_import(path: str, file=\"__init__.py\") -> ModuleType:\n     return module\n \n \n+class VersionComparison(Enum):\n+    EQUAL = operator.eq\n+    NOT_EQUAL = operator.ne\n+    GREATER_THAN = operator.gt\n+    LESS_THAN = operator.lt\n+    GREATER_THAN_OR_EQUAL = operator.ge\n+    LESS_THAN_OR_EQUAL = operator.le\n+\n+    @staticmethod\n+    def from_string(version_string: str) -> \"VersionComparison\":\n+        string_to_operator = {\n+            \"=\": VersionComparison.EQUAL.value,\n+            \"==\": VersionComparison.EQUAL.value,\n+            \"!=\": VersionComparison.NOT_EQUAL.value,\n+            \">\": VersionComparison.GREATER_THAN.value,\n+            \"<\": VersionComparison.LESS_THAN.value,\n+            \">=\": VersionComparison.GREATER_THAN_OR_EQUAL.value,\n+            \"<=\": VersionComparison.LESS_THAN_OR_EQUAL.value,\n+        }\n+\n+        return string_to_operator[version_string]\n+\n+\n+@lru_cache()\n+def split_package_version(package_version_str) -> Tuple[str, str, str]:\n+    pattern = r\"([a-zA-Z0-9_-]+)([!<>=~]+)([0-9.]+)\"\n+    match = re.match(pattern, package_version_str)\n+    if match:\n+        return (match.group(1), match.group(2), match.group(3))\n+    else:\n+        raise ValueError(f\"Invalid package version string: {package_version_str}\")\n+\n+\n+class Backend:\n+    def __init__(self, backend_requirement: str):\n+        self.package_name, self.version_comparison, self.version = split_package_version(backend_requirement)\n+\n+        if self.package_name not in BACKENDS_MAPPING:\n+            raise ValueError(\n+                f\"Backends should be defined in the BACKENDS_MAPPING. Offending backend: {self.package_name}\"\n+            )\n+\n+    def is_satisfied(self) -> bool:\n+        return VersionComparison.from_string(self.version_comparison)(\n+            version.parse(importlib.metadata.version(self.package_name)), version.parse(self.version)\n+        )\n+\n+    def __repr__(self) -> str:\n+        return f'Backend(\"{self.package_name}\", {VersionComparison[self.version_comparison]}, \"{self.version}\")'\n+\n+    @property\n+    def error_message(self):\n+        return (\n+            f\"{{0}} requires the {self.package_name} library version {self.version_comparison}{self.version}. That\"\n+            f\" library was not found with this version in your environment.\"\n+        )\n+\n+\n def requires(*, backends=()):\n     \"\"\"\n     This decorator enables two things:\n     - Attaching a `__backends` tuple to an object to see what are the necessary backends for it\n       to execute correctly without instantiating it\n     - The '@requires' string is used to dynamically import objects\n     \"\"\"\n-    for backend in backends:\n-        if backend not in BACKENDS_MAPPING:\n-            raise ValueError(f\"Backend should be defined in the BACKENDS_MAPPING. Offending backend: {backend}\")\n \n     if not isinstance(backends, tuple):\n         raise ValueError(\"Backends should be a tuple.\")\n \n+    applied_backends = []\n+    for backend in backends:\n+        if backend in BACKENDS_MAPPING:\n+            applied_backends.append(backend)\n+        else:\n+            if any(key in backend for key in [\"=\", \"<\", \">\"]):\n+                applied_backends.append(Backend(backend))\n+            else:\n+                raise ValueError(f\"Backend should be defined in the BACKENDS_MAPPING. Offending backend: {backend}\")\n+\n     def inner_fn(fun):\n-        fun.__backends = backends\n+        fun.__backends = applied_backends\n         return fun\n \n     return inner_fn\n@@ -2369,23 +2476,53 @@ def spread_import_structure(nested_import_structure):\n     \"\"\"\n \n     def propagate_frozenset(unordered_import_structure):\n-        tuple_first_import_structure = {}\n+        frozenset_first_import_structure = {}\n         for _key, _value in unordered_import_structure.items():\n+            # If the value is not a dict but a string, no need for custom manipulation\n             if not isinstance(_value, dict):\n-                tuple_first_import_structure[_key] = _value\n+                frozenset_first_import_structure[_key] = _value\n \n             elif any(isinstance(v, frozenset) for v in _value.keys()):\n-                # Here we want to switch around key and v\n                 for k, v in _value.items():\n                     if isinstance(k, frozenset):\n-                        if k not in tuple_first_import_structure:\n-                            tuple_first_import_structure[k] = {}\n-                        tuple_first_import_structure[k][_key] = v\n+                        # Here we want to switch around _key and k to propagate k upstream if it is a frozenset\n+                        if k not in frozenset_first_import_structure:\n+                            frozenset_first_import_structure[k] = {}\n+                        if _key not in frozenset_first_import_structure[k]:\n+                            frozenset_first_import_structure[k][_key] = {}\n+\n+                        frozenset_first_import_structure[k][_key].update(v)\n+\n+                    else:\n+                        # If k is not a frozenset, it means that the dictionary is not \"level\": some keys (top-level)\n+                        # are frozensets, whereas some are not -> frozenset keys are at an unkown depth-level of the\n+                        # dictionary.\n+                        #\n+                        # We recursively propagate the frozenset for this specific dictionary so that the frozensets\n+                        # are at the top-level when we handle them.\n+                        propagated_frozenset = propagate_frozenset({k: v})\n+                        for r_k, r_v in propagated_frozenset.items():\n+                            if isinstance(_key, frozenset):\n+                                if r_k not in frozenset_first_import_structure:\n+                                    frozenset_first_import_structure[r_k] = {}\n+                                if _key not in frozenset_first_import_structure[r_k]:\n+                                    frozenset_first_import_structure[r_k][_key] = {}\n+\n+                                # _key is a frozenset -> we switch around the r_k and _key\n+                                frozenset_first_import_structure[r_k][_key].update(r_v)\n+                            else:\n+                                if _key not in frozenset_first_import_structure:\n+                                    frozenset_first_import_structure[_key] = {}\n+                                if r_k not in frozenset_first_import_structure[_key]:\n+                                    frozenset_first_import_structure[_key][r_k] = {}\n+\n+                                # _key is not a frozenset -> we keep the order of r_k and _key\n+                                frozenset_first_import_structure[_key][r_k].update(r_v)\n \n             else:\n-                tuple_first_import_structure[_key] = propagate_frozenset(_value)\n+                frozenset_first_import_structure[_key] = propagate_frozenset(_value)\n \n-        return tuple_first_import_structure\n+        return frozenset_first_import_structure\n \n     def flatten_dict(_dict, previous_key=None):\n         items = []"
        },
        {
            "sha": "6d7c10e979395857566b405aea42433bccc95422",
            "filename": "tests/utils/import_structures/import_structure_raw_register_with_versions.py",
            "status": "added",
            "additions": 92,
            "deletions": 0,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/23d79cea75f0be1d38f4e3d468c52e79012ab179/tests%2Futils%2Fimport_structures%2Fimport_structure_raw_register_with_versions.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23d79cea75f0be1d38f4e3d468c52e79012ab179/tests%2Futils%2Fimport_structures%2Fimport_structure_raw_register_with_versions.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Fimport_structures%2Fimport_structure_raw_register_with_versions.py?ref=23d79cea75f0be1d38f4e3d468c52e79012ab179",
            "patch": "@@ -0,0 +1,92 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# fmt: off\n+\n+from transformers.utils.import_utils import requires\n+\n+\n+@requires(backends=(\"torch>=2.5\",))\n+class D0:\n+    def __init__(self):\n+        pass\n+\n+\n+@requires(backends=(\"torch>=2.5\",))\n+def d0():\n+    pass\n+\n+\n+@requires(backends=(\"torch>2.5\",))\n+class D1:\n+    def __init__(self):\n+        pass\n+\n+\n+@requires(backends=(\"torch>2.5\",))\n+def d1():\n+    pass\n+\n+\n+@requires(backends=(\"torch<=2.5\",))\n+class D2:\n+    def __init__(self):\n+        pass\n+\n+\n+@requires(backends=(\"torch<=2.5\",))\n+def d2():\n+    pass\n+\n+@requires(backends=(\"torch<2.5\",))\n+class D3:\n+    def __init__(self):\n+        pass\n+\n+\n+@requires(backends=(\"torch<2.5\",))\n+def d3():\n+    pass\n+\n+\n+@requires(backends=(\"torch==2.5\",))\n+class D4:\n+    def __init__(self):\n+        pass\n+\n+\n+@requires(backends=(\"torch==2.5\",))\n+def d4():\n+    pass\n+\n+\n+@requires(backends=(\"torch!=2.5\",))\n+class D5:\n+    def __init__(self):\n+        pass\n+\n+\n+@requires(backends=(\"torch!=2.5\",))\n+def d5():\n+    pass\n+\n+@requires(backends=(\"torch>=2.5\", \"accelerate<0.20\"))\n+class D6:\n+    def __init__(self):\n+        pass\n+\n+\n+@requires(backends=(\"torch>=2.5\", \"accelerate<0.20\"))\n+def d6():\n+    pass"
        },
        {
            "sha": "87a90cae4392fb454ff7c2d2cabf2c6e090d8ddc",
            "filename": "tests/utils/test_import_structure.py",
            "status": "modified",
            "additions": 124,
            "deletions": 20,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/23d79cea75f0be1d38f4e3d468c52e79012ab179/tests%2Futils%2Ftest_import_structure.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23d79cea75f0be1d38f4e3d468c52e79012ab179/tests%2Futils%2Ftest_import_structure.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_import_structure.py?ref=23d79cea75f0be1d38f4e3d468c52e79012ab179",
            "patch": "@@ -1,11 +1,19 @@\n import os\n import unittest\n from pathlib import Path\n+from typing import Callable\n \n-from transformers.utils.import_utils import define_import_structure, spread_import_structure\n+import pytest\n \n+from transformers.utils.import_utils import (\n+    Backend,\n+    VersionComparison,\n+    define_import_structure,\n+    spread_import_structure,\n+)\n \n-import_structures = Path(\"import_structures\")\n+\n+import_structures = Path(__file__).parent / \"import_structures\"\n \n \n def fetch__all__(file_content):\n@@ -36,26 +44,39 @@ class TestImportStructures(unittest.TestCase):\n     models_path = base_transformers_path / \"src\" / \"transformers\" / \"models\"\n     models_import_structure = spread_import_structure(define_import_structure(models_path))\n \n-    # TODO: Lysandre\n-    # See https://app.circleci.com/pipelines/github/huggingface/transformers/104762/workflows/7ba9c6f7-a3b2-44e6-8eaf-749c7b7261f7/jobs/1393260/tests\n-    @unittest.skip(reason=\"failing\")\n     def test_definition(self):\n         import_structure = define_import_structure(import_structures)\n-        import_structure_definition = {\n-            frozenset(()): {\n-                \"import_structure_raw_register\": {\"A0\", \"a0\", \"A4\"},\n+        valid_frozensets: dict[frozenset | frozenset[str], dict[str, set[str]]] = {\n+            frozenset(): {\n+                \"import_structure_raw_register\": {\"A0\", \"A4\", \"a0\"},\n                 \"import_structure_register_with_comments\": {\"B0\", \"b0\"},\n             },\n-            frozenset((\"tf\", \"torch\")): {\n-                \"import_structure_raw_register\": {\"A1\", \"a1\", \"A2\", \"a2\", \"A3\", \"a3\"},\n-                \"import_structure_register_with_comments\": {\"B1\", \"b1\", \"B2\", \"b2\", \"B3\", \"b3\"},\n+            frozenset({\"random_item_that_should_not_exist\"}): {\"failing_export\": {\"A0\"}},\n+            frozenset({\"torch\"}): {\n+                \"import_structure_register_with_duplicates\": {\"C0\", \"C1\", \"C2\", \"C3\", \"c0\", \"c1\", \"c2\", \"c3\"}\n+            },\n+            frozenset({\"tf\", \"torch\"}): {\n+                \"import_structure_raw_register\": {\"A1\", \"A2\", \"A3\", \"a1\", \"a2\", \"a3\"},\n+                \"import_structure_register_with_comments\": {\"B1\", \"B2\", \"B3\", \"b1\", \"b2\", \"b3\"},\n             },\n-            frozenset((\"torch\",)): {\n-                \"import_structure_register_with_duplicates\": {\"C0\", \"c0\", \"C1\", \"c1\", \"C2\", \"c2\", \"C3\", \"c3\"},\n+            frozenset({\"torch>=2.5\"}): {\"import_structure_raw_register_with_versions\": {\"D0\", \"d0\"}},\n+            frozenset({\"torch>2.5\"}): {\"import_structure_raw_register_with_versions\": {\"D1\", \"d1\"}},\n+            frozenset({\"torch<=2.5\"}): {\"import_structure_raw_register_with_versions\": {\"D2\", \"d2\"}},\n+            frozenset({\"torch<2.5\"}): {\"import_structure_raw_register_with_versions\": {\"D3\", \"d3\"}},\n+            frozenset({\"torch==2.5\"}): {\"import_structure_raw_register_with_versions\": {\"D4\", \"d4\"}},\n+            frozenset({\"torch!=2.5\"}): {\"import_structure_raw_register_with_versions\": {\"D5\", \"d5\"}},\n+            frozenset({\"torch>=2.5\", \"accelerate<0.20\"}): {\n+                \"import_structure_raw_register_with_versions\": {\"D6\", \"d6\"}\n             },\n         }\n \n-        self.assertDictEqual(import_structure, import_structure_definition)\n+        self.assertEqual(len(import_structure.keys()), len(valid_frozensets.keys()))\n+        for _frozenset in valid_frozensets.keys():\n+            self.assertTrue(_frozenset in import_structure)\n+            self.assertListEqual(list(import_structure[_frozenset].keys()), list(valid_frozensets[_frozenset].keys()))\n+            for module, objects in valid_frozensets[_frozenset].items():\n+                self.assertTrue(module in import_structure[_frozenset])\n+                self.assertSetEqual(objects, import_structure[_frozenset][module])\n \n     def test_transformers_specific_model_import(self):\n         \"\"\"\n@@ -96,9 +117,92 @@ def test_transformers_specific_model_import(self):\n                         )\n                         self.assertListEqual(sorted(objects), sorted(_all), msg=error_message)\n \n-    # TODO: Lysandre\n-    # See https://app.circleci.com/pipelines/github/huggingface/transformers/104762/workflows/7ba9c6f7-a3b2-44e6-8eaf-749c7b7261f7/jobs/1393260/tests\n-    @unittest.skip(reason=\"failing\")\n-    def test_export_backend_should_be_defined(self):\n-        with self.assertRaisesRegex(ValueError, \"Backend should be defined in the BACKENDS_MAPPING\"):\n-            pass\n+    def test_import_spread(self):\n+        \"\"\"\n+        This test is specifically designed to test that varying levels of depth across import structures are\n+        respected.\n+\n+        In this instance, frozensets are at respective depths of 1, 2 and 3, for example:\n+        - models.{frozensets}\n+        - models.albert.{frozensets}\n+        - models.deprecated.transfo_xl.{frozensets}\n+        \"\"\"\n+        initial_import_structure = {\n+            frozenset(): {\"dummy_non_model\": {\"DummyObject\"}},\n+            \"models\": {\n+                frozenset(): {\"dummy_config\": {\"DummyConfig\"}},\n+                \"albert\": {\n+                    frozenset(): {\"configuration_albert\": {\"AlbertConfig\", \"AlbertOnnxConfig\"}},\n+                    frozenset({\"torch\"}): {\n+                        \"modeling_albert\": {\n+                            \"AlbertForMaskedLM\",\n+                        }\n+                    },\n+                },\n+                \"llama\": {\n+                    frozenset(): {\"configuration_llama\": {\"LlamaConfig\"}},\n+                    frozenset({\"torch\"}): {\n+                        \"modeling_llama\": {\n+                            \"LlamaForCausalLM\",\n+                        }\n+                    },\n+                },\n+                \"deprecated\": {\n+                    \"transfo_xl\": {\n+                        frozenset({\"torch\"}): {\n+                            \"modeling_transfo_xl\": {\n+                                \"TransfoXLModel\",\n+                            }\n+                        },\n+                        frozenset(): {\n+                            \"configuration_transfo_xl\": {\"TransfoXLConfig\"},\n+                            \"tokenization_transfo_xl\": {\"TransfoXLCorpus\", \"TransfoXLTokenizer\"},\n+                        },\n+                    },\n+                    \"deta\": {\n+                        frozenset({\"torch\"}): {\n+                            \"modeling_deta\": {\"DetaForObjectDetection\", \"DetaModel\", \"DetaPreTrainedModel\"}\n+                        },\n+                        frozenset(): {\"configuration_deta\": {\"DetaConfig\"}},\n+                        frozenset({\"vision\"}): {\"image_processing_deta\": {\"DetaImageProcessor\"}},\n+                    },\n+                },\n+            },\n+        }\n+\n+        ground_truth_spread_import_structure = {\n+            frozenset(): {\n+                \"dummy_non_model\": {\"DummyObject\"},\n+                \"models.dummy_config\": {\"DummyConfig\"},\n+                \"models.albert.configuration_albert\": {\"AlbertConfig\", \"AlbertOnnxConfig\"},\n+                \"models.llama.configuration_llama\": {\"LlamaConfig\"},\n+                \"models.deprecated.transfo_xl.configuration_transfo_xl\": {\"TransfoXLConfig\"},\n+                \"models.deprecated.transfo_xl.tokenization_transfo_xl\": {\"TransfoXLCorpus\", \"TransfoXLTokenizer\"},\n+                \"models.deprecated.deta.configuration_deta\": {\"DetaConfig\"},\n+            },\n+            frozenset({\"torch\"}): {\n+                \"models.albert.modeling_albert\": {\"AlbertForMaskedLM\"},\n+                \"models.llama.modeling_llama\": {\"LlamaForCausalLM\"},\n+                \"models.deprecated.transfo_xl.modeling_transfo_xl\": {\"TransfoXLModel\"},\n+                \"models.deprecated.deta.modeling_deta\": {\"DetaForObjectDetection\", \"DetaModel\", \"DetaPreTrainedModel\"},\n+            },\n+            frozenset({\"vision\"}): {\"models.deprecated.deta.image_processing_deta\": {\"DetaImageProcessor\"}},\n+        }\n+\n+        newly_spread_import_structure = spread_import_structure(initial_import_structure)\n+\n+        self.assertEqual(ground_truth_spread_import_structure, newly_spread_import_structure)\n+\n+\n+@pytest.mark.parametrize(\n+    \"backend,package_name,version_comparison,version\",\n+    [\n+        pytest.param(Backend(\"torch>=2.5 \"), \"torch\", VersionComparison.GREATER_THAN_OR_EQUAL.value, \"2.5\"),\n+        pytest.param(Backend(\"tf<=1\"), \"tf\", VersionComparison.LESS_THAN_OR_EQUAL.value, \"1\"),\n+        pytest.param(Backend(\"torchvision==0.19.1\"), \"torchvision\", VersionComparison.EQUAL.value, \"0.19.1\"),\n+    ],\n+)\n+def test_backend_specification(backend: Backend, package_name: str, version_comparison: Callable, version: str):\n+    assert backend.package_name == package_name\n+    assert VersionComparison.from_string(backend.version_comparison) == version_comparison\n+    assert backend.version == version"
        }
    ],
    "stats": {
        "total": 436,
        "additions": 391,
        "deletions": 45
    }
}