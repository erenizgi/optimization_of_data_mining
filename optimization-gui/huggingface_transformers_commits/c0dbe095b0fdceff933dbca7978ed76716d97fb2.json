{
    "author": "JJJYmmm",
    "message": "Adding Support for Qwen3-VL Series (#40795)\n\n* add qwen3vl series\n\n* make fixup\n\n* fix import\n\n* re-protect import\n\n* fix it finally (need to merge main into the branch)\n\n* skip processor test (need the checkpoint)\n\n* oups typo\n\n* simplify modular\n\n* remove unecesary attr\n\n* fix layer\n\n* remove unused rope_deltas args\n\n* reuse image def\n\n* remove unnesesary imports\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "c0dbe095b0fdceff933dbca7978ed76716d97fb2",
    "files": [
        {
            "sha": "aa5b35aeb1984aa41919d8a1ef94c993cf0bc40c",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -1127,6 +1127,10 @@\n         title: Qwen2Audio\n       - local: model_doc/qwen2_vl\n         title: Qwen2VL\n+      - local: model_doc/qwen3_vl\n+        title: Qwen3VL\n+      - local: model_doc/qwen3_vl_moe\n+        title: Qwen3VLMoe\n       - local: model_doc/sam2\n         title: SAM2\n       - local: model_doc/sam2_video"
        },
        {
            "sha": "9e90363a1eba6301bd77a544bdc8203dc2ae405f",
            "filename": "docs/source/en/model_doc/qwen3_vl.md",
            "status": "added",
            "additions": 117,
            "deletions": 0,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,117 @@\n+<!--Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on None and added to Hugging Face Transformers on 2025-08-16.*\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">    </div>\n+</div>\n+\n+# Qwen3-VL\n+\n+[Qwen3-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model series, encompassing both dense and MoE variants, as well as Instruct and Thinking versions. Building upon its predecessors, Qwen3-VL delivers significant improvements in visual understanding while maintaining strong pure text capabilities. Key architectural advancements include: enhanced MRope with interleaved layout for better spatial-temporal modeling, DeepStack integration to effectively leverage multi-level features from the Vision Transformer (ViT), and improved video understanding through text-based time alignmentâ€”evolving from T-RoPE to text timestamp alignment for more precise temporal grounding. These innovations collectively enable Qwen3-VL to achieve superior performance in complex multimodal tasks.\n+\n+Model usage\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n+\n+model = Qwen3VLForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen3-VL\",\n+    dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL\")\n+messages = [\n+    {\n+        \"role\":\"user\",\n+        \"content\":[\n+            {\n+                \"type\":\"image\",\n+                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+            },\n+            {\n+                \"type\":\"text\",\n+                \"text\":\"Describe this image.\"\n+            }\n+        ]\n+    }\n+\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+)\n+inputs.pop(\"token_type_ids\", None)\n+\n+generated_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids_trimmed = [\n+            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n+]\n+output_text = processor.batch_decode(\n+       generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+print(output_text)\n+```\n+</hfoption>\n+</hfoptions>\n+\n+## Qwen3VLConfig\n+\n+[[autodoc]] Qwen3VLConfig\n+\n+## Qwen3VLTextConfig\n+\n+[[autodoc]] Qwen3VLTextConfig\n+\n+## Qwen3VLProcessor\n+\n+[[autodoc]] Qwen3VLProcessor\n+\n+## Qwen3VLVideoProcessor\n+\n+[[autodoc]] Qwen3VLVideoProcessor\n+\n+## Qwen3VLVisionModel\n+\n+[[autodoc]] Qwen3VLVisionModel\n+    - forward\n+\n+## Qwen3VLTextModel\n+\n+[[autodoc]] Qwen3VLTextModel\n+    - forward\n+\n+## Qwen3VLModel\n+\n+[[autodoc]] Qwen3VLModel\n+    - forward\n+\n+## Qwen3VLForConditionalGeneration\n+\n+[[autodoc]] Qwen3VLForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "76d046efff2d1855eabb90c178eb925558dbfe9b",
            "filename": "docs/source/en/model_doc/qwen3_vl_moe.md",
            "status": "added",
            "additions": 109,
            "deletions": 0,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl_moe.md?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,109 @@\n+<!--Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on None and added to Hugging Face Transformers on 2025-08-17.*\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">    </div>\n+</div>\n+\n+# Qwen3-VL-Moe\n+\n+[Qwen3-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model series, encompassing both dense and MoE variants, as well as Instruct and Thinking versions. Building upon its predecessors, Qwen3-VL delivers significant improvements in visual understanding while maintaining strong pure text capabilities. Key architectural advancements include: enhanced MRope with interleaved layout for better spatial-temporal modeling, DeepStack integration to effectively leverage multi-level features from the Vision Transformer (ViT), and improved video understanding through text-based time alignmentâ€”evolving from T-RoPE to text timestamp alignment for more precise temporal grounding. These innovations collectively enable Qwen3-VL to achieve superior performance in complex multimodal tasks.\n+\n+Model usage\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor\n+\n+model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+    \"Qwen/Qwen3-VL-Moe\",\n+    dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-Moe\")\n+messages = [\n+    {\n+        \"role\":\"user\",\n+        \"content\":[\n+            {\n+                \"type\":\"image\",\n+                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+            },\n+            {\n+                \"type\":\"text\",\n+                \"text\":\"Describe this image.\"\n+            }\n+        ]\n+    }\n+\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+)\n+inputs.pop(\"token_type_ids\", None)\n+\n+generated_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids_trimmed = [\n+            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n+]\n+output_text = processor.batch_decode(\n+       generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+print(output_text)\n+```\n+</hfoption>\n+</hfoptions>\n+\n+## Qwen3VLMoeConfig\n+\n+[[autodoc]] Qwen3VLMoeConfig\n+\n+## Qwen3VLMoeTextConfig\n+\n+[[autodoc]] Qwen3VLMoeTextConfig\n+\n+## Qwen3VLMoeVisionModel\n+\n+[[autodoc]] Qwen3VLMoeVisionModel\n+    - forward\n+\n+## Qwen3VLMoeTextModel\n+\n+[[autodoc]] Qwen3VLMoeTextModel\n+    - forward\n+\n+## Qwen3VLMoeModel\n+\n+[[autodoc]] Qwen3VLMoeModel\n+    - forward\n+\n+## Qwen3VLMoeForConditionalGeneration\n+\n+[[autodoc]] Qwen3VLMoeForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "c18cbd44f7ea464c9b72eb8ce227ec30ed8113fa",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -278,6 +278,8 @@\n     from .qwen3 import *\n     from .qwen3_moe import *\n     from .qwen3_next import *\n+    from .qwen3_vl import *\n+    from .qwen3_vl_moe import *\n     from .rag import *\n     from .recurrent_gemma import *\n     from .reformer import *"
        },
        {
            "sha": "a977c727c9e87dd819c644272c1ff7bf410a9422",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -325,6 +325,10 @@\n         (\"qwen3\", \"Qwen3Config\"),\n         (\"qwen3_moe\", \"Qwen3MoeConfig\"),\n         (\"qwen3_next\", \"Qwen3NextConfig\"),\n+        (\"qwen3_vl\", \"Qwen3VLConfig\"),\n+        (\"qwen3_vl_moe\", \"Qwen3VLMoeConfig\"),\n+        (\"qwen3_vl_moe_text\", \"Qwen3VLMoeTextConfig\"),\n+        (\"qwen3_vl_text\", \"Qwen3VLTextConfig\"),\n         (\"rag\", \"RagConfig\"),\n         (\"realm\", \"RealmConfig\"),\n         (\"recurrent_gemma\", \"RecurrentGemmaConfig\"),\n@@ -764,6 +768,10 @@\n         (\"qwen3\", \"Qwen3\"),\n         (\"qwen3_moe\", \"Qwen3MoE\"),\n         (\"qwen3_next\", \"Qwen3Next\"),\n+        (\"qwen3_vl\", \"Qwen3VL\"),\n+        (\"qwen3_vl_moe\", \"Qwen3VLMoe\"),\n+        (\"qwen3_vl_moe_text\", \"Qwen3VLMoe\"),\n+        (\"qwen3_vl_text\", \"Qwen3VL\"),\n         (\"rag\", \"RAG\"),\n         (\"realm\", \"REALM\"),\n         (\"recurrent_gemma\", \"RecurrentGemma\"),\n@@ -952,6 +960,8 @@\n         (\"internvl_vision\", \"internvl\"),\n         (\"qwen2_5_vl_text\", \"qwen2_5_vl\"),\n         (\"qwen2_vl_text\", \"qwen2_vl\"),\n+        (\"qwen3_vl_text\", \"qwen3_vl\"),\n+        (\"qwen3_vl_moe_text\", \"qwen3_vl_moe\"),\n         (\"sam_vision_model\", \"sam\"),\n         (\"sam2_vision_model\", \"sam2\"),\n         (\"sam2_hiera_det_model\", \"sam2\"),"
        },
        {
            "sha": "193e8f8fd9401bfcfe88a5bb3bbbe2127c8b734c",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -156,6 +156,7 @@\n             (\"pvt_v2\", (\"PvtImageProcessor\", \"PvtImageProcessorFast\")),\n             (\"qwen2_5_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"qwen2_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n+            (\"qwen3_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"regnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"resnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"rt_detr\", (\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\")),"
        },
        {
            "sha": "1e0388de23cb4f9acaa1a9b4e460326f5be9e28d",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -319,6 +319,10 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"qwen3\", \"Qwen3Model\"),\n         (\"qwen3_moe\", \"Qwen3MoeModel\"),\n         (\"qwen3_next\", \"Qwen3NextModel\"),\n+        (\"qwen3_vl\", \"Qwen3VLModel\"),\n+        (\"qwen3_vl_moe\", \"Qwen3VLMoeModel\"),\n+        (\"qwen3_vl_moe_text\", \"Qwen3VLMoeTextModel\"),\n+        (\"qwen3_vl_text\", \"Qwen3VLTextModel\"),\n         (\"recurrent_gemma\", \"RecurrentGemmaModel\"),\n         (\"reformer\", \"ReformerModel\"),\n         (\"regnet\", \"RegNetModel\"),\n@@ -974,6 +978,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"pix2struct\", \"Pix2StructForConditionalGeneration\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLForConditionalGeneration\"),\n         (\"qwen2_vl\", \"Qwen2VLForConditionalGeneration\"),\n+        (\"qwen3_vl\", \"Qwen3VLForConditionalGeneration\"),\n+        (\"qwen3_vl_moe\", \"Qwen3VLMoeForConditionalGeneration\"),\n         (\"video_llava\", \"VideoLlavaForConditionalGeneration\"),\n         (\"vipllava\", \"VipLlavaForConditionalGeneration\"),\n         (\"vision-encoder-decoder\", \"VisionEncoderDecoderModel\"),\n@@ -1028,6 +1034,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"pixtral\", \"LlavaForConditionalGeneration\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLForConditionalGeneration\"),\n         (\"qwen2_vl\", \"Qwen2VLForConditionalGeneration\"),\n+        (\"qwen3_vl\", \"Qwen3VLForConditionalGeneration\"),\n+        (\"qwen3_vl_moe\", \"Qwen3VLMoeForConditionalGeneration\"),\n         (\"shieldgemma2\", \"Gemma3ForConditionalGeneration\"),\n         (\"smolvlm\", \"SmolVLMForConditionalGeneration\"),\n         (\"udop\", \"UdopForConditionalGeneration\"),"
        },
        {
            "sha": "13583c55002f9249ac88a894f770ce5e52d7dc42",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -120,6 +120,8 @@\n         (\"qwen2_5_vl\", \"Qwen2_5_VLProcessor\"),\n         (\"qwen2_audio\", \"Qwen2AudioProcessor\"),\n         (\"qwen2_vl\", \"Qwen2VLProcessor\"),\n+        (\"qwen3_vl\", \"Qwen3VLProcessor\"),\n+        (\"qwen3_vl_moe\", \"Qwen3VLProcessor\"),\n         (\"sam\", \"SamProcessor\"),\n         (\"sam2\", \"Sam2Processor\"),\n         (\"sam_hq\", \"SamHQProcessor\"),"
        },
        {
            "sha": "0ef450f45cb93c8b67592cb94527def525c6262a",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -583,6 +583,8 @@\n                 \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n+        (\"qwen3_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"qwen3_vl_moe\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"rag\", (\"RagTokenizer\", None)),\n         (\"realm\", (\"RealmTokenizer\", \"RealmTokenizerFast\" if is_tokenizers_available() else None)),\n         ("
        },
        {
            "sha": "551de914626eb06df3201e6b03f2d1557949cc7a",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -56,6 +56,8 @@\n             (\"qwen2_5_omni\", \"Qwen2VLVideoProcessor\"),\n             (\"qwen2_5_vl\", \"Qwen2VLVideoProcessor\"),\n             (\"qwen2_vl\", \"Qwen2VLVideoProcessor\"),\n+            (\"qwen3_vl\", \"Qwen3VLVideoProcessor\"),\n+            (\"qwen3_vl_moe\", \"Qwen3VLVideoProcessor\"),\n             (\"sam2_video\", \"Sam2VideoVideoProcessor\"),\n             (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),"
        },
        {
            "sha": "e37161a2e4153b0192ca2d10d85292c8fa2ed3f2",
            "filename": "src/transformers/models/qwen3_vl/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2F__init__.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_qwen3_vl import *\n+    from .modeling_qwen3_vl import *\n+    from .processing_qwen3_vl import *\n+    from .video_processing_qwen3_vl import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "132ffa8be150dbab496810254d9a9757c70ccc1c",
            "filename": "src/transformers/models/qwen3_vl/configuration_qwen3_vl.py",
            "status": "added",
            "additions": 287,
            "deletions": 0,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,287 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/qwen3_vl/modular_qwen3_vl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_qwen3_vl.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class Qwen3VLVisionConfig(PretrainedConfig):\n+    model_type = \"qwen3_vl\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        depth=27,\n+        hidden_size=1152,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        intermediate_size=4304,\n+        num_heads=16,\n+        in_channels=3,\n+        patch_size=16,\n+        spatial_merge_size=2,\n+        temporal_patch_size=2,\n+        out_hidden_size=3584,\n+        num_position_embeddings=2304,\n+        deepstack_visual_indexes=[8, 16, 24],\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.depth = depth\n+        self.hidden_size = hidden_size\n+        self.hidden_act = hidden_act\n+        self.intermediate_size = intermediate_size\n+        self.num_heads = num_heads\n+        self.in_channels = in_channels\n+        self.patch_size = patch_size\n+        self.spatial_merge_size = spatial_merge_size\n+        self.temporal_patch_size = temporal_patch_size\n+        self.out_hidden_size = out_hidden_size\n+        self.num_position_embeddings = num_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.deepstack_visual_indexes = deepstack_visual_indexes\n+\n+\n+class Qwen3VLTextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3VLTextModel`]. It is used to instantiate a\n+    Qwen3-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen3-VL-4B-Instruct [Qwen/Qwen3-VL-4B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 151936):\n+            Vocabulary size of the Qwen3VL model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Qwen3VLModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 22016):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 32):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.\n+        head_dim (`int`, *optional*, defaults to 128):\n+            The dimension of the head. If not specified, will default to `hidden_size // num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 128000):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_theta (`float`, *optional*, defaults to 5000000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    ```python\n+    >>> from transformers import Qwen3VLTextModel, Qwen3VLTextConfig\n+\n+    >>> # Initializing a Qwen3VL style configuration\n+    >>> configuration = Qwen3VLTextConfig()\n+\n+    >>> # Initializing a model from the Qwen3-VL-7B style configuration\n+    >>> model = Qwen3VLTextModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen3_vl_text\"\n+    base_config_key = \"text_config\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=151936,\n+        hidden_size=4096,\n+        intermediate_size=22016,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=32,\n+        head_dim=128,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=128000,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n+        rope_theta=5000000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.head_dim = head_dim\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"mrope_interleaved\"})\n+\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+\n+\n+class Qwen3VLConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3VLModel`]. It is used to instantiate a\n+    Qwen3-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen3-VL-4B-Instruct [Qwen/Qwen3-VL-4B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen3VLTextConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen3VLVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 151655):\n+            The image token index to encode the image prompt.\n+        video_token_id (`int`, *optional*, defaults to 151656):\n+            The video token index to encode the image prompt.\n+        vision_start_token_id (`int`, *optional*, defaults to 151652):\n+            The start token index to encode the image prompt.\n+        vision_end_token_id (`int`, *optional*, defaults to 151653):\n+            The end token index to encode the image prompt.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie the word embeddings.\n+\n+    ```python\n+    >>> from transformers import Qwen3VLForConditionalGeneration, Qwen3VLConfig\n+\n+    >>> # Initializing a Qwen3-VL style configuration\n+    >>> configuration = Qwen3VLConfig()\n+\n+    >>> # Initializing a model from the Qwen3-VL-4B style configuration\n+    >>> model = Qwen3VLForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen3_vl\"\n+    sub_configs = {\"vision_config\": Qwen3VLVisionConfig, \"text_config\": Qwen3VLTextConfig}\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        image_token_id=151655,\n+        video_token_id=151656,\n+        vision_start_token_id=151652,\n+        vision_end_token_id=151653,\n+        tie_word_embeddings=False,\n+        **kwargs,\n+    ):\n+        if isinstance(vision_config, dict):\n+            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n+        elif vision_config is None:\n+            self.vision_config = self.sub_configs[\"vision_config\"]()\n+\n+        if isinstance(text_config, dict):\n+            self.text_config = self.sub_configs[\"text_config\"](**text_config)\n+        elif text_config is None:\n+            self.text_config = self.sub_configs[\"text_config\"]()\n+\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+        self.vision_start_token_id = vision_start_token_id\n+        self.vision_end_token_id = vision_end_token_id\n+        super().__init__(**kwargs, tie_word_embeddings=tie_word_embeddings)\n+\n+\n+__all__ = [\"Qwen3VLConfig\", \"Qwen3VLTextConfig\"]"
        },
        {
            "sha": "a18366a2a53499902a51f026d9cfe9b2c7904b13",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "added",
            "additions": 1568,
            "deletions": 0,
            "changes": 1568,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,1568 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/qwen3_vl/modular_qwen3_vl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_qwen3_vl.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from .configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLTextConfig, Qwen3VLVisionConfig\n+\n+\n+class Qwen3VLVisionMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.linear_fc1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=True)\n+        self.linear_fc2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=True)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_state):\n+        return self.linear_fc2(self.act_fn(self.linear_fc1(hidden_state)))\n+\n+\n+class Qwen3VLVisionPatchEmbed(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        self.patch_size = config.patch_size\n+        self.temporal_patch_size = config.temporal_patch_size\n+        self.in_channels = config.in_channels\n+        self.embed_dim = config.hidden_size\n+\n+        kernel_size = [self.temporal_patch_size, self.patch_size, self.patch_size]\n+        self.proj = nn.Conv3d(self.in_channels, self.embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=True)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        target_dtype = self.proj.weight.dtype\n+        hidden_states = hidden_states.view(\n+            -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size\n+        )\n+        hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)\n+        return hidden_states\n+\n+\n+class Qwen3VLVisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n+        super().__init__()\n+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, seqlen: int) -> torch.Tensor:\n+        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        freqs = torch.outer(seq, self.inv_freq)\n+        return freqs\n+\n+\n+class Qwen3VLVisionPatchMerger(nn.Module):\n+    def __init__(self, config: Qwen3VLVisionConfig, use_postshuffle_norm=False) -> None:\n+        super().__init__()\n+        self.hidden_size = config.hidden_size * (config.spatial_merge_size**2)\n+        self.use_postshuffle_norm = use_postshuffle_norm\n+        self.norm = nn.LayerNorm(self.hidden_size if use_postshuffle_norm else config.hidden_size, eps=1e-6)\n+        self.linear_fc1 = nn.Linear(self.hidden_size, self.hidden_size)\n+        self.act_fn = nn.GELU()\n+        self.linear_fc2 = nn.Linear(self.hidden_size, config.out_hidden_size)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        x = self.norm(x.view(-1, self.hidden_size) if self.use_postshuffle_norm else x).view(-1, self.hidden_size)\n+        x = self.linear_fc2(self.act_fn(self.linear_fc1(x)))\n+        return x\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb_vision(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n+) -> tuple[torch.Tensor, torch.Tensor]:\n+    orig_q_dtype = q.dtype\n+    orig_k_dtype = k.dtype\n+    q, k = q.float(), k.float()\n+    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    q_embed = q_embed.to(orig_q_dtype)\n+    k_embed = k_embed.to(orig_k_dtype)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Qwen3VLVisionAttention(nn.Module):\n+    def __init__(self, config: Qwen3VLVisionConfig) -> None:\n+        super().__init__()\n+        self.dim = config.hidden_size\n+        self.num_heads = config.num_heads\n+        self.head_dim = self.dim // self.num_heads\n+        self.num_key_value_groups = 1  # needed for eager attention\n+        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.scaling = self.head_dim**-0.5\n+        self.config = config\n+        self.attention_dropout = 0.0\n+        self.is_causal = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        seq_length = hidden_states.shape[0]\n+        query_states, key_states, value_states = (\n+            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n+        )\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n+\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # Flash Attention 2: Use cu_seqlens for variable length attention\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()\n+            attn_output, _ = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask=None,\n+                scaling=self.scaling,\n+                dropout=0.0 if not self.training else self.attention_dropout,\n+                cu_seq_lens_q=cu_seqlens,\n+                cu_seq_lens_k=cu_seqlens,\n+                max_length_q=max_seqlen,\n+                max_length_k=max_seqlen,\n+                is_causal=False,\n+                **kwargs,\n+            )\n+        else:\n+            # Other implementations: Process each chunk separately\n+            lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n+            splits = [\n+                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)\n+            ]\n+\n+            attn_outputs = [\n+                attention_interface(\n+                    self,\n+                    q,\n+                    k,\n+                    v,\n+                    attention_mask=None,\n+                    scaling=self.scaling,\n+                    dropout=0.0 if not self.training else self.attention_dropout,\n+                    is_causal=False,\n+                    **kwargs,\n+                )[0]\n+                for q, k, v in zip(*splits)\n+            ]\n+            attn_output = torch.cat(attn_outputs, dim=1)\n+\n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n+        attn_output = self.proj(attn_output)\n+        return attn_output\n+\n+\n+class Qwen3VLVisionBlock(GradientCheckpointingLayer):\n+    def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n+        super().__init__()\n+        self.norm1 = nn.LayerNorm(config.hidden_size, eps=1e-6)\n+        self.norm2 = nn.LayerNorm(config.hidden_size, eps=1e-6)\n+        self.attn = Qwen3VLVisionAttention(config=config)\n+        self.mlp = Qwen3VLVisionMLP(config=config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        hidden_states = hidden_states + self.attn(\n+            self.norm1(hidden_states),\n+            cu_seqlens=cu_seqlens,\n+            rotary_pos_emb=rotary_pos_emb,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n+        return hidden_states\n+\n+\n+class Qwen3VLTextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Qwen3VLTextConfig, device=None):\n+        super().__init__()\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", \"default\")\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+        self.mrope_section = config.rope_scaling.get(\"mrope_section\", [24, 20, 20])\n+\n+    def apply_interleaved_mrope(self, freqs, mrope_section):\n+        \"\"\"Apply interleaved MRoPE to 3D rotary embeddings.\n+        Reorganizes frequency layout from chunked [TTT...HHH...WWW] to\n+        interleaved [THTHWHTHW...TT], preserving frequency continuity.\n+        args:\n+            x: (3, bs, seq_len, head_dim // 2)\n+            mrope_section: (3,)\n+        returns:\n+            x_t: (bs, seq_len, head_dim // 2)\n+        \"\"\"\n+        freqs_t = freqs[0]  # just overwrite the first dimension T\n+        for dim, offset in enumerate((1, 2), start=1):  # H, W\n+            length = mrope_section[dim] * 3\n+            idx = slice(offset, length, 3)\n+            freqs_t[..., idx] = freqs[dim, ..., idx]\n+        return freqs_t\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        # In contrast to other models, Qwen3VL has different position ids for the grids\n+        # So we expand the inv_freq to shape (3, ...)\n+        if position_ids.ndim == 2:\n+            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n+        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n+        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n+            freqs = self.apply_interleaved_mrope(freqs, self.mrope_section)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Qwen3VLTextRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps: float = 1e-6) -> None:\n+        \"\"\"\n+        Qwen3VLTextRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+class Qwen3VLTextAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.q_norm = Qwen3VLTextRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n+        self.k_norm = Qwen3VLTextRMSNorm(\n+            self.head_dim, eps=config.rms_norm_eps\n+        )  # thus post q_norm does not need reshape\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Qwen3VLTextMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Qwen3VLTextDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = Qwen3VLTextAttention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = Qwen3VLTextMLP(config)\n+        self.input_layernorm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class Qwen3VLModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+\n+\n+@auto_docstring\n+class Qwen3VLPreTrainedModel(PreTrainedModel):\n+    config: Qwen3VLConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Qwen3VLTextDecoderLayer\", \"Qwen3VLVisionBlock\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Qwen3VLTextDecoderLayer,\n+        \"attentions\": Qwen3VLTextAttention,\n+    }\n+\n+\n+class Qwen3VLVisionModel(Qwen3VLPreTrainedModel):\n+    config: Qwen3VLVisionConfig\n+    _no_split_modules = [\"Qwen3VLVisionBlock\"]\n+\n+    def __init__(self, config, *inputs, **kwargs) -> None:\n+        super().__init__(config, *inputs, **kwargs)\n+        self.spatial_merge_size = config.spatial_merge_size\n+        self.patch_size = config.patch_size\n+        self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size\n+\n+        self.patch_embed = Qwen3VLVisionPatchEmbed(\n+            config=config,\n+        )\n+\n+        self.pos_embed = nn.Embedding(config.num_position_embeddings, config.hidden_size)\n+        self.num_grid_per_side = int(config.num_position_embeddings**0.5)\n+\n+        head_dim = config.hidden_size // config.num_heads\n+        self.rotary_pos_emb = Qwen3VLVisionRotaryEmbedding(head_dim // 2)\n+\n+        self.blocks = nn.ModuleList([Qwen3VLVisionBlock(config) for _ in range(config.depth)])\n+        self.merger = Qwen3VLVisionPatchMerger(\n+            config=config,\n+            use_postshuffle_norm=False,\n+        )\n+\n+        self.deepstack_visual_indexes = config.deepstack_visual_indexes\n+        self.deepstack_merger_list = nn.ModuleList(\n+            [\n+                Qwen3VLVisionPatchMerger(\n+                    config=config,\n+                    use_postshuffle_norm=True,\n+                )\n+                for _ in range(len(config.deepstack_visual_indexes))\n+            ]\n+        )\n+\n+        self.gradient_checkpointing = False\n+\n+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n+        merge_size = self.spatial_merge_size\n+\n+        max_hw = int(grid_thw[:, 1:].max().item())\n+        freq_table = self.rotary_pos_emb(max_hw)  # (max_hw, dim // 2)\n+        device = freq_table.device\n+\n+        total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())\n+        pos_ids = torch.empty((total_tokens, 2), dtype=torch.long, device=device)\n+\n+        offset = 0\n+        for num_frames, height, width in grid_thw:\n+            merged_h, merged_w = height // merge_size, width // merge_size\n+\n+            block_rows = torch.arange(merged_h, device=device)  # block row indices\n+            block_cols = torch.arange(merged_w, device=device)  # block col indices\n+            intra_row = torch.arange(merge_size, device=device)  # intra-block row offsets\n+            intra_col = torch.arange(merge_size, device=device)  # intra-block col offsets\n+\n+            # Compute full-resolution positions\n+            row_idx = block_rows[:, None, None, None] * merge_size + intra_row[None, None, :, None]\n+            col_idx = block_cols[None, :, None, None] * merge_size + intra_col[None, None, None, :]\n+\n+            row_idx = row_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)\n+            col_idx = col_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)\n+\n+            coords = torch.stack((row_idx, col_idx), dim=-1)\n+\n+            if num_frames > 1:\n+                coords = coords.repeat(num_frames, 1)\n+\n+            num_tokens = coords.shape[0]\n+            pos_ids[offset : offset + num_tokens] = coords\n+            offset += num_tokens\n+\n+        embeddings = freq_table[pos_ids]  # lookup rotary embeddings\n+        embeddings = embeddings.flatten(1)\n+        return embeddings\n+\n+    def fast_pos_embed_interpolate(self, grid_thw):\n+        grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+\n+        idx_list = [[] for _ in range(4)]\n+        weight_list = [[] for _ in range(4)]\n+\n+        for t, h, w in zip(grid_ts, grid_hs, grid_ws):\n+            h_idxs = torch.linspace(0, self.num_grid_per_side - 1, h)\n+            w_idxs = torch.linspace(0, self.num_grid_per_side - 1, w)\n+\n+            h_idxs_floor = h_idxs.int()\n+            w_idxs_floor = w_idxs.int()\n+            h_idxs_ceil = (h_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)\n+            w_idxs_ceil = (w_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)\n+\n+            dh = h_idxs - h_idxs_floor\n+            dw = w_idxs - w_idxs_floor\n+\n+            base_h = h_idxs_floor * self.num_grid_per_side\n+            base_h_ceil = h_idxs_ceil * self.num_grid_per_side\n+\n+            indices = [\n+                (base_h[None].T + w_idxs_floor[None]).flatten(),\n+                (base_h[None].T + w_idxs_ceil[None]).flatten(),\n+                (base_h_ceil[None].T + w_idxs_floor[None]).flatten(),\n+                (base_h_ceil[None].T + w_idxs_ceil[None]).flatten(),\n+            ]\n+\n+            weights = [\n+                ((1 - dh)[None].T * (1 - dw)[None]).flatten(),\n+                ((1 - dh)[None].T * dw[None]).flatten(),\n+                (dh[None].T * (1 - dw)[None]).flatten(),\n+                (dh[None].T * dw[None]).flatten(),\n+            ]\n+\n+            for i in range(4):\n+                idx_list[i].extend(indices[i].tolist())\n+                weight_list[i].extend(weights[i].tolist())\n+\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n+        weight_tensor = torch.tensor(\n+            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n+        )\n+        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n+\n+        patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])\n+\n+        patch_pos_embeds_permute = []\n+        merge_size = self.config.spatial_merge_size\n+        for pos_embed, t, h, w in zip(patch_pos_embeds, grid_ts, grid_hs, grid_ws):\n+            pos_embed = pos_embed.repeat(t, 1)\n+            pos_embed = (\n+                pos_embed.view(t, h // merge_size, merge_size, w // merge_size, merge_size, -1)\n+                .permute(0, 1, 3, 2, 4, 5)\n+                .flatten(0, 4)\n+            )\n+            patch_pos_embeds_permute.append(pos_embed)\n+        patch_pos_embeds = torch.cat(patch_pos_embeds_permute)\n+        return patch_pos_embeds\n+\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):\n+                The temporal, height and width of feature shape of each image in LLM.\n+\n+        Returns:\n+            `torch.Tensor`: hidden_states.\n+        \"\"\"\n+        hidden_states = self.patch_embed(hidden_states)\n+\n+        pos_embeds = self.fast_pos_embed_interpolate(grid_thw)\n+        hidden_states = hidden_states + pos_embeds\n+\n+        rotary_pos_emb = self.rot_pos_emb(grid_thw)\n+\n+        seq_len, _ = hidden_states.size()\n+        hidden_states = hidden_states.reshape(seq_len, -1)\n+        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n+\n+        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n+            dim=0,\n+            # Select dtype based on the following factors:\n+            #  - FA2 requires that cu_seqlens_q must have dtype int32\n+            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw\n+            # See https://github.com/huggingface/transformers/pull/34852 for more information\n+            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n+        )\n+        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n+\n+        deepstack_feature_lists = []\n+        for layer_num, blk in enumerate(self.blocks):\n+            hidden_states = blk(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+            if layer_num in self.deepstack_visual_indexes:\n+                deepstack_feature = self.deepstack_merger_list[self.deepstack_visual_indexes.index(layer_num)](\n+                    hidden_states\n+                )\n+                deepstack_feature_lists.append(deepstack_feature)\n+\n+        hidden_states = self.merger(hidden_states)\n+\n+        return hidden_states, deepstack_feature_lists\n+\n+\n+@auto_docstring(\n+    custom_intro=(\n+        \"Text part of Qwen3VL, \"\n+        \"not a pure text-only model, as DeepStack integrates visual features into the early hidden states.\"\n+    )\n+)\n+class Qwen3VLTextModel(Qwen3VLPreTrainedModel):\n+    config: Qwen3VLTextConfig\n+    _no_split_modules = [\"Qwen3VLTextDecoderLayer\"]\n+\n+    def __init__(self, config: Qwen3VLTextConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Qwen3VLTextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Qwen3VLTextRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        # args for deepstack\n+        visual_pos_masks: Optional[torch.Tensor] = None,\n+        deepstack_visual_embeds: Optional[list[torch.Tensor]] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        visual_pos_masks (`torch.Tensor` of shape `(batch_size, seqlen)`, *optional*):\n+            The mask of the visual positions.\n+        deepstack_visual_embeds (`list[torch.Tensor]`, *optional*):\n+            The deepstack visual embeddings. The shape is (num_layers, visual_seqlen, embed_dim).\n+            The feature is extracted from the different visual encoder layers, and fed to the decoder\n+            hidden states. It's from the paper DeepStack(https://arxiv.org/abs/2406.04334).\n+        \"\"\"\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        # torch.jit.trace() doesn't support cache objects in the output\n+        if use_cache and past_key_values is None and not torch.jit.is_tracing():\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        # the hard coded `3` is for temporal, height and width.\n+        if position_ids is None:\n+            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n+        elif position_ids.ndim == 2:\n+            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n+\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            text_position_ids = position_ids[0]\n+\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=text_position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for layer_idx, decoder_layer in enumerate(self.layers):\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=text_position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+            hidden_states = layer_outputs\n+\n+            # add visual features to the hidden states of first several layers\n+            if deepstack_visual_embeds is not None and layer_idx in range(len(deepstack_visual_embeds)):\n+                hidden_states = self._deepstack_process(\n+                    hidden_states,\n+                    visual_pos_masks,\n+                    deepstack_visual_embeds[layer_idx],\n+                )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+    def _deepstack_process(\n+        self, hidden_states: torch.Tensor, visual_pos_masks: torch.Tensor, visual_embeds: torch.Tensor\n+    ):\n+        visual_pos_masks = visual_pos_masks.to(hidden_states.device)\n+        visual_embeds = visual_embeds.to(hidden_states.device, hidden_states.dtype)\n+        local_this = hidden_states[visual_pos_masks, :].clone() + visual_embeds\n+        hidden_states[visual_pos_masks, :] = local_this\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Qwen3VLModel(Qwen3VLPreTrainedModel):\n+    base_model_prefix = \"\"\n+    _checkpoint_conversion_mapping = {}\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n+    config: Qwen3VLConfig\n+    _no_split_modules = [\"Qwen3VLTextDecoderLayer\", \"Qwen3VLVisionBlock\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.visual = Qwen3VLVisionModel._from_config(config.vision_config)\n+        self.language_model = Qwen3VLTextModel._from_config(config.text_config)\n+        self.rope_deltas = None  # cache rope_deltas here\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n+    def get_rope_index(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"Different from the original implementation, Qwen3VL use timestamps rather than absolute time position ids.\"\"\"\n+\n+        # Since we use timestamps to seperate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split\n+        if video_grid_thw is not None:\n+            video_grid_thw = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)\n+            video_grid_thw[:, 0] = 1\n+\n+        spatial_merge_size = self.config.vision_config.spatial_merge_size\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+        vision_start_token_id = self.config.vision_start_token_id\n+        mrope_position_deltas = []\n+        if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):\n+            total_input_ids = input_ids\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(total_input_ids)\n+            position_ids = torch.ones(\n+                3,\n+                input_ids.shape[0],\n+                input_ids.shape[1],\n+                dtype=input_ids.dtype,\n+                device=input_ids.device,\n+            )\n+            image_index, video_index = 0, 0\n+            attention_mask = attention_mask.to(total_input_ids.device)\n+            for i, input_ids in enumerate(total_input_ids):\n+                input_ids = input_ids[attention_mask[i] == 1]\n+                image_nums, video_nums = 0, 0\n+                vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n+                vision_tokens = input_ids[vision_start_indices + 1]\n+                image_nums = (vision_tokens == image_token_id).sum()\n+                video_nums = (vision_tokens == video_token_id).sum()\n+                input_tokens = input_ids.tolist()\n+                llm_pos_ids_list: list = []\n+                st = 0\n+                remain_images, remain_videos = image_nums, video_nums\n+                for _ in range(image_nums + video_nums):\n+                    if image_token_id in input_tokens and remain_images > 0:\n+                        ed_image = input_tokens.index(image_token_id, st)\n+                    else:\n+                        ed_image = len(input_tokens) + 1\n+                    if video_token_id in input_tokens and remain_videos > 0:\n+                        ed_video = input_tokens.index(video_token_id, st)\n+                    else:\n+                        ed_video = len(input_tokens) + 1\n+                    if ed_image < ed_video:\n+                        t, h, w = (\n+                            image_grid_thw[image_index][0],\n+                            image_grid_thw[image_index][1],\n+                            image_grid_thw[image_index][2],\n+                        )\n+                        image_index += 1\n+                        remain_images -= 1\n+                        ed = ed_image\n+\n+                    else:\n+                        t, h, w = (\n+                            video_grid_thw[video_index][0],\n+                            video_grid_thw[video_index][1],\n+                            video_grid_thw[video_index][2],\n+                        )\n+                        video_index += 1\n+                        remain_videos -= 1\n+                        ed = ed_video\n+                    llm_grid_t, llm_grid_h, llm_grid_w = (\n+                        t.item(),\n+                        h.item() // spatial_merge_size,\n+                        w.item() // spatial_merge_size,\n+                    )\n+                    text_len = ed - st\n+\n+                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0\n+                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n+\n+                    # t_index is always 0 because llm_grid_t is always 1 (we use timestamps to encode the temporal information for videos)\n+                    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(-1, llm_grid_h * llm_grid_w).flatten()\n+                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()\n+                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()\n+                    llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)\n+                    st = ed + llm_grid_t * llm_grid_h * llm_grid_w\n+\n+                if st < len(input_tokens):\n+                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0\n+                    text_len = len(input_tokens) - st\n+                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n+\n+                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)\n+                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)\n+                mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))\n+            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)\n+            return position_ids, mrope_position_deltas\n+        else:\n+            if attention_mask is not None:\n+                position_ids = attention_mask.long().cumsum(-1) - 1\n+                position_ids.masked_fill_(attention_mask == 0, 1)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(attention_mask.device)\n+                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]\n+                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]\n+            else:\n+                position_ids = (\n+                    torch.arange(input_ids.shape[1], device=input_ids.device)\n+                    .view(1, 1, -1)\n+                    .expand(3, input_ids.shape[0], -1)\n+                )\n+                mrope_position_deltas = torch.zeros(\n+                    [input_ids.shape[0], 1],\n+                    device=input_ids.device,\n+                    dtype=input_ids.dtype,\n+                )\n+\n+            return position_ids, mrope_position_deltas\n+\n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model. The deepstack visual features are also returned.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        # Same implementation as for images\n+        return self.get_image_features(pixel_values_videos, video_grid_thw)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model. The deepstack visual features are also returned.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each image in LLM.\n+        \"\"\"\n+        pixel_values = pixel_values.type(self.visual.dtype)\n+        image_embeds, deepstack_image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+        split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\n+        image_embeds = torch.split(image_embeds, split_sizes)\n+        return image_embeds, deepstack_image_embeds\n+\n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Qwen3VLModelOutputWithPast]:\n+        r\"\"\"\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        image_mask = None\n+        video_mask = None\n+\n+        if pixel_values is not None:\n+            image_embeds, deepstack_image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+        if pixel_values_videos is not None:\n+            video_embeds, deepstack_video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+\n+        visual_pos_masks = None\n+        deepstack_visual_embeds = None\n+        if image_mask is not None and video_mask is not None:\n+            # aggregate visual_pos_masks and deepstack_visual_embeds\n+            image_mask = image_mask[..., 0]\n+            video_mask = video_mask[..., 0]\n+            visual_pos_masks = image_mask | video_mask\n+            deepstack_visual_embeds = []\n+            image_mask_joint = image_mask[visual_pos_masks]\n+            video_mask_joint = video_mask[visual_pos_masks]\n+            for img_embed, vid_embed in zip(deepstack_image_embeds, deepstack_video_embeds):\n+                embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1]).to(img_embed.device)\n+                embed_joint[image_mask_joint, :] = img_embed\n+                embed_joint[video_mask_joint, :] = vid_embed\n+                deepstack_visual_embeds.append(embed_joint)\n+        elif image_mask is not None:\n+            image_mask = image_mask[..., 0]\n+            visual_pos_masks = image_mask\n+            deepstack_visual_embeds = deepstack_image_embeds\n+        elif video_mask is not None:\n+            video_mask = video_mask[..., 0]\n+            visual_pos_masks = video_mask\n+            deepstack_visual_embeds = deepstack_video_embeds\n+\n+        if position_ids is None:\n+            attention_mask_tensor = (\n+                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n+            )\n+            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n+                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n+                # Only apply conversion for floating point tensors (inverted masks)\n+                if attention_mask_tensor.dtype.is_floating_point:\n+                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n+\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            prefill_compiled_stage = is_torchdynamo_compiling() and (\n+                (input_ids is not None and input_ids.shape[1] != 1)\n+                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n+            )\n+            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n+                (cache_position is not None and cache_position[0] == 0)\n+                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n+            )\n+            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n+                position_ids, rope_deltas = self.get_rope_index(\n+                    input_ids,\n+                    image_grid_thw,\n+                    video_grid_thw,\n+                    attention_mask=attention_mask_tensor,\n+                )\n+                self.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            else:\n+                batch_size, seq_length, _ = inputs_embeds.shape\n+                delta = (\n+                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n+                    if cache_position is not None\n+                    else 0\n+                )\n+                position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n+                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n+                if cache_position is not None:  # otherwise `deltas` is an int `0`\n+                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n+                position_ids = position_ids.add(delta)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n+\n+        outputs = self.language_model(\n+            input_ids=None,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            visual_pos_masks=visual_pos_masks,\n+            deepstack_visual_embeds=deepstack_visual_embeds,\n+            **kwargs,\n+        )\n+\n+        return Qwen3VLModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=self.rope_deltas,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Qwen3VL causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class Qwen3VLCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+\n+\n+class Qwen3VLForConditionalGeneration(Qwen3VLPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n+    config: Qwen3VLConfig\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Qwen3VLModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        return self.model.get_video_features(pixel_values_videos, video_grid_thw)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        return self.model.get_image_features(pixel_values, image_grid_thw)\n+\n+    # Make modules available through conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def visual(self):\n+        return self.model.visual\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Qwen3VLCausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each video in LLM.\n+\n+        Example:\n+            TODO: Add example\n+        \"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        return Qwen3VLCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=outputs.rope_deltas,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        pixel_values=None,\n+        pixel_values_videos=None,\n+        image_grid_thw=None,\n+        video_grid_thw=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+\n+        # Qwen3VL position_ids are prepareed with rope_deltas in forward\n+        model_inputs[\"position_ids\"] = None\n+\n+        if cache_position[0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_values_videos\"] = None\n+\n+        return model_inputs\n+\n+    def _get_image_nums_and_video_nums(\n+        self,\n+        input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n+        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.\n+\n+        Args:\n+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+                Indices of input sequence tokens in the vocabulary.\n+\n+        Returns:\n+            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)\n+            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)\n+        \"\"\"\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+        vision_start_token_id = self.config.vision_start_token_id\n+\n+        if inputs_embeds is not None:\n+            vision_start_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(vision_start_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            image_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            video_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+        else:\n+            vision_start_mask = input_ids == vision_start_token_id\n+            image_mask = input_ids == image_token_id\n+            video_mask = input_ids == video_token_id\n+\n+        vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)\n+        image_nums = torch.sum(vision_first_mask & image_mask, dim=1)\n+        video_nums = torch.sum(vision_first_mask & video_mask, dim=1)\n+\n+        return image_nums, video_nums\n+\n+    def _expand_inputs_for_generation(\n+        self,\n+        expand_size: int = 1,\n+        is_encoder_decoder: bool = False,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        **model_kwargs,\n+    ) -> tuple[torch.LongTensor, dict[str, Any]]:\n+        # Overwritten -- Support for expanding tensors without a batch size dimension\n+        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t\n+        # pixel_values.shape[0] is sum(seqlen_images for samples)\n+        # image_grid_thw.shape[0] is sum(num_images for samples)\n+\n+        if expand_size == 1:\n+            return input_ids, model_kwargs\n+\n+        visual_keys = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\", \"second_per_grid_ts\"]\n+\n+        def _expand_dict_for_generation_visual(dict_to_expand):\n+            image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n+            video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(\n+                input_ids, inputs_embeds=model_kwargs.get(\"inputs_embeds\", None)\n+            )\n+\n+            def _repeat_interleave_samples(x, lengths, repeat_times):\n+                samples = torch.split(x, lengths)\n+                repeat_args = [repeat_times] + [1] * (x.dim() - 1)\n+                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)\n+                return result\n+\n+            for key in dict_to_expand:\n+                if key == \"pixel_values\":\n+                    # split images into samples\n+                    samples = torch.split(image_grid_thw, list(image_nums))\n+                    # compute the sequence length of images for each sample\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"image_grid_thw\":\n+                    # get the num of images for each sample\n+                    lengths = list(image_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"pixel_values_videos\":\n+                    samples = torch.split(video_grid_thw, list(video_nums))\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_grid_thw\":\n+                    lengths = list(video_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"second_per_grid_ts\":\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=list(video_nums), repeat_times=expand_size\n+                    )\n+            return dict_to_expand\n+\n+        def _expand_dict_for_generation(dict_to_expand):\n+            for key in dict_to_expand:\n+                if (\n+                    key != \"cache_position\"\n+                    and dict_to_expand[key] is not None\n+                    and isinstance(dict_to_expand[key], torch.Tensor)\n+                    and key not in visual_keys\n+                ):\n+                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n+            return dict_to_expand\n+\n+        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+\n+        if input_ids is not None:\n+            input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n+\n+        model_kwargs = _expand_dict_for_generation(model_kwargs)\n+\n+        if is_encoder_decoder:\n+            if model_kwargs.get(\"encoder_outputs\") is None:\n+                raise ValueError(\"If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.\")\n+            model_kwargs[\"encoder_outputs\"] = _expand_dict_for_generation(model_kwargs[\"encoder_outputs\"])\n+\n+        return input_ids, model_kwargs\n+\n+\n+__all__ = [\n+    \"Qwen3VLVisionModel\",\n+    \"Qwen3VLForConditionalGeneration\",\n+    \"Qwen3VLModel\",\n+    \"Qwen3VLPreTrainedModel\",\n+    \"Qwen3VLTextModel\",\n+]"
        },
        {
            "sha": "ae608e81a05d2c4e0d00b64344571584f96dd653",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "added",
            "additions": 1472,
            "deletions": 0,
            "changes": 1472,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,1472 @@\n+# coding=utf-8\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Qwen3-VL model.\"\"\"\n+\n+from typing import Callable, Optional, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import PretrainedConfig\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update, rope_config_validation\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils.generic import check_model_inputs\n+from ...video_utils import VideoInput\n+from ..qwen2_5_vl.modeling_qwen2_5_vl import (\n+    Qwen2_5_VLCausalLMOutputWithPast,\n+    Qwen2_5_VLForConditionalGeneration,\n+    Qwen2_5_VLModel,\n+    Qwen2_5_VLVisionBlock,\n+)\n+from ..qwen2_vl.modeling_qwen2_vl import (\n+    PatchEmbed,\n+    Qwen2VLModelOutputWithPast,\n+    Qwen2VLPreTrainedModel,\n+    TransformersKwargs,\n+    VisionAttention,\n+    VisionRotaryEmbedding,\n+)\n+from ..qwen2_vl.processing_qwen2_vl import Qwen2VLImagesKwargs, Qwen2VLProcessor\n+from ..qwen3.modeling_qwen3 import (\n+    Qwen3Attention,\n+    Qwen3DecoderLayer,\n+    Qwen3Model,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Qwen3VLVisionConfig(PretrainedConfig):\n+    model_type = \"qwen3_vl\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        depth=27,\n+        hidden_size=1152,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        intermediate_size=4304,\n+        num_heads=16,\n+        in_channels=3,\n+        patch_size=16,\n+        spatial_merge_size=2,\n+        temporal_patch_size=2,\n+        out_hidden_size=3584,\n+        num_position_embeddings=2304,\n+        deepstack_visual_indexes=[8, 16, 24],\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.depth = depth\n+        self.hidden_size = hidden_size\n+        self.hidden_act = hidden_act\n+        self.intermediate_size = intermediate_size\n+        self.num_heads = num_heads\n+        self.in_channels = in_channels\n+        self.patch_size = patch_size\n+        self.spatial_merge_size = spatial_merge_size\n+        self.temporal_patch_size = temporal_patch_size\n+        self.out_hidden_size = out_hidden_size\n+        self.num_position_embeddings = num_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.deepstack_visual_indexes = deepstack_visual_indexes\n+\n+\n+class Qwen3VLTextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3VLTextModel`]. It is used to instantiate a\n+    Qwen3-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen3-VL-4B-Instruct [Qwen/Qwen3-VL-4B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 151936):\n+            Vocabulary size of the Qwen3VL model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Qwen3VLModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 22016):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 32):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.\n+        head_dim (`int`, *optional*, defaults to 128):\n+            The dimension of the head. If not specified, will default to `hidden_size // num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 128000):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_theta (`float`, *optional*, defaults to 5000000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    ```python\n+    >>> from transformers import Qwen3VLTextModel, Qwen3VLTextConfig\n+\n+    >>> # Initializing a Qwen3VL style configuration\n+    >>> configuration = Qwen3VLTextConfig()\n+\n+    >>> # Initializing a model from the Qwen3-VL-7B style configuration\n+    >>> model = Qwen3VLTextModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen3_vl_text\"\n+    base_config_key = \"text_config\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=151936,\n+        hidden_size=4096,\n+        intermediate_size=22016,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=32,\n+        head_dim=128,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=128000,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n+        rope_theta=5000000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.head_dim = head_dim\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"mrope_interleaved\"})\n+\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+\n+\n+class Qwen3VLConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3VLModel`]. It is used to instantiate a\n+    Qwen3-VL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen3-VL-4B-Instruct [Qwen/Qwen3-VL-4B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen3VLTextConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen3VLVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 151655):\n+            The image token index to encode the image prompt.\n+        video_token_id (`int`, *optional*, defaults to 151656):\n+            The video token index to encode the image prompt.\n+        vision_start_token_id (`int`, *optional*, defaults to 151652):\n+            The start token index to encode the image prompt.\n+        vision_end_token_id (`int`, *optional*, defaults to 151653):\n+            The end token index to encode the image prompt.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie the word embeddings.\n+\n+    ```python\n+    >>> from transformers import Qwen3VLForConditionalGeneration, Qwen3VLConfig\n+\n+    >>> # Initializing a Qwen3-VL style configuration\n+    >>> configuration = Qwen3VLConfig()\n+\n+    >>> # Initializing a model from the Qwen3-VL-4B style configuration\n+    >>> model = Qwen3VLForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen3_vl\"\n+    sub_configs = {\"vision_config\": Qwen3VLVisionConfig, \"text_config\": Qwen3VLTextConfig}\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        image_token_id=151655,\n+        video_token_id=151656,\n+        vision_start_token_id=151652,\n+        vision_end_token_id=151653,\n+        tie_word_embeddings=False,\n+        **kwargs,\n+    ):\n+        if isinstance(vision_config, dict):\n+            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n+        elif vision_config is None:\n+            self.vision_config = self.sub_configs[\"vision_config\"]()\n+\n+        if isinstance(text_config, dict):\n+            self.text_config = self.sub_configs[\"text_config\"](**text_config)\n+        elif text_config is None:\n+            self.text_config = self.sub_configs[\"text_config\"]()\n+\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+        self.vision_start_token_id = vision_start_token_id\n+        self.vision_end_token_id = vision_end_token_id\n+        super().__init__(**kwargs, tie_word_embeddings=tie_word_embeddings)\n+\n+\n+class Qwen3VLVisionMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.linear_fc1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=True)\n+        self.linear_fc2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=True)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_state):\n+        return self.linear_fc2(self.act_fn(self.linear_fc1(hidden_state)))\n+\n+\n+class Qwen3VLVisionPatchEmbed(PatchEmbed):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        self.patch_size = config.patch_size\n+        self.temporal_patch_size = config.temporal_patch_size\n+        self.in_channels = config.in_channels\n+        self.embed_dim = config.hidden_size\n+\n+        kernel_size = [self.temporal_patch_size, self.patch_size, self.patch_size]\n+        self.proj = nn.Conv3d(self.in_channels, self.embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=True)\n+\n+\n+class Qwen3VLVisionRotaryEmbedding(VisionRotaryEmbedding):\n+    pass\n+\n+\n+class Qwen3VLVisionPatchMerger(nn.Module):\n+    def __init__(self, config: Qwen3VLVisionConfig, use_postshuffle_norm=False) -> None:\n+        super().__init__()\n+        self.hidden_size = config.hidden_size * (config.spatial_merge_size**2)\n+        self.use_postshuffle_norm = use_postshuffle_norm\n+        self.norm = nn.LayerNorm(self.hidden_size if use_postshuffle_norm else config.hidden_size, eps=1e-6)\n+        self.linear_fc1 = nn.Linear(self.hidden_size, self.hidden_size)\n+        self.act_fn = nn.GELU()\n+        self.linear_fc2 = nn.Linear(self.hidden_size, config.out_hidden_size)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        x = self.norm(x.view(-1, self.hidden_size) if self.use_postshuffle_norm else x).view(-1, self.hidden_size)\n+        x = self.linear_fc2(self.act_fn(self.linear_fc1(x)))\n+        return x\n+\n+\n+class Qwen3VLVisionAttention(VisionAttention):\n+    def __init__(self, config: Qwen3VLVisionConfig) -> None:\n+        super().__init__()\n+        self.dim = config.hidden_size\n+\n+\n+class Qwen3VLVisionBlock(Qwen2_5_VLVisionBlock):\n+    def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n+        super().__init__()\n+        self.norm1 = nn.LayerNorm(config.hidden_size, eps=1e-6)\n+        self.norm2 = nn.LayerNorm(config.hidden_size, eps=1e-6)\n+        self.attn = Qwen3VLVisionAttention(config=config)\n+        self.mlp = Qwen3VLVisionMLP(config=config)\n+\n+\n+class Qwen3VLTextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Qwen3VLTextConfig, device=None):\n+        super().__init__()\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", \"default\")\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+        self.mrope_section = config.rope_scaling.get(\"mrope_section\", [24, 20, 20])\n+\n+    def apply_interleaved_mrope(self, freqs, mrope_section):\n+        \"\"\"Apply interleaved MRoPE to 3D rotary embeddings.\n+        Reorganizes frequency layout from chunked [TTT...HHH...WWW] to\n+        interleaved [THTHWHTHW...TT], preserving frequency continuity.\n+        args:\n+            x: (3, bs, seq_len, head_dim // 2)\n+            mrope_section: (3,)\n+        returns:\n+            x_t: (bs, seq_len, head_dim // 2)\n+        \"\"\"\n+        freqs_t = freqs[0]  # just overwrite the first dimension T\n+        for dim, offset in enumerate((1, 2), start=1):  # H, W\n+            length = mrope_section[dim] * 3\n+            idx = slice(offset, length, 3)\n+            freqs_t[..., idx] = freqs[dim, ..., idx]\n+        return freqs_t\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        # In contrast to other models, Qwen3VL has different position ids for the grids\n+        # So we expand the inv_freq to shape (3, ...)\n+        if position_ids.ndim == 2:\n+            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n+        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n+        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n+            freqs = self.apply_interleaved_mrope(freqs, self.mrope_section)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class Qwen3VLTextAttention(Qwen3Attention):\n+    def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        del self.sliding_window\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Qwen3VLTextDecoderLayer(Qwen3DecoderLayer):\n+    def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        del self.attention_type\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        return super().forward(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+\n+class Qwen3VLModelOutputWithPast(Qwen2VLModelOutputWithPast):\n+    pass\n+\n+\n+class Qwen3VLPreTrainedModel(Qwen2VLPreTrainedModel):\n+    config: Qwen3VLConfig\n+    _no_split_modules = [\"Qwen3VLTextDecoderLayer\", \"Qwen3VLVisionBlock\"]\n+    _can_record_outputs = {\n+        \"hidden_states\": Qwen3VLTextDecoderLayer,\n+        \"attentions\": Qwen3VLTextAttention,\n+    }\n+\n+\n+class Qwen3VLVisionModel(Qwen3VLPreTrainedModel):\n+    config: Qwen3VLVisionConfig\n+    _no_split_modules = [\"Qwen3VLVisionBlock\"]\n+\n+    def __init__(self, config, *inputs, **kwargs) -> None:\n+        super().__init__(config, *inputs, **kwargs)\n+        self.spatial_merge_size = config.spatial_merge_size\n+        self.patch_size = config.patch_size\n+        self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size\n+\n+        self.patch_embed = Qwen3VLVisionPatchEmbed(\n+            config=config,\n+        )\n+\n+        self.pos_embed = nn.Embedding(config.num_position_embeddings, config.hidden_size)\n+        self.num_grid_per_side = int(config.num_position_embeddings**0.5)\n+\n+        head_dim = config.hidden_size // config.num_heads\n+        self.rotary_pos_emb = Qwen3VLVisionRotaryEmbedding(head_dim // 2)\n+\n+        self.blocks = nn.ModuleList([Qwen3VLVisionBlock(config) for _ in range(config.depth)])\n+        self.merger = Qwen3VLVisionPatchMerger(\n+            config=config,\n+            use_postshuffle_norm=False,\n+        )\n+\n+        self.deepstack_visual_indexes = config.deepstack_visual_indexes\n+        self.deepstack_merger_list = nn.ModuleList(\n+            [\n+                Qwen3VLVisionPatchMerger(\n+                    config=config,\n+                    use_postshuffle_norm=True,\n+                )\n+                for _ in range(len(config.deepstack_visual_indexes))\n+            ]\n+        )\n+\n+        self.gradient_checkpointing = False\n+\n+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n+        merge_size = self.spatial_merge_size\n+\n+        max_hw = int(grid_thw[:, 1:].max().item())\n+        freq_table = self.rotary_pos_emb(max_hw)  # (max_hw, dim // 2)\n+        device = freq_table.device\n+\n+        total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())\n+        pos_ids = torch.empty((total_tokens, 2), dtype=torch.long, device=device)\n+\n+        offset = 0\n+        for num_frames, height, width in grid_thw:\n+            merged_h, merged_w = height // merge_size, width // merge_size\n+\n+            block_rows = torch.arange(merged_h, device=device)  # block row indices\n+            block_cols = torch.arange(merged_w, device=device)  # block col indices\n+            intra_row = torch.arange(merge_size, device=device)  # intra-block row offsets\n+            intra_col = torch.arange(merge_size, device=device)  # intra-block col offsets\n+\n+            # Compute full-resolution positions\n+            row_idx = block_rows[:, None, None, None] * merge_size + intra_row[None, None, :, None]\n+            col_idx = block_cols[None, :, None, None] * merge_size + intra_col[None, None, None, :]\n+\n+            row_idx = row_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)\n+            col_idx = col_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)\n+\n+            coords = torch.stack((row_idx, col_idx), dim=-1)\n+\n+            if num_frames > 1:\n+                coords = coords.repeat(num_frames, 1)\n+\n+            num_tokens = coords.shape[0]\n+            pos_ids[offset : offset + num_tokens] = coords\n+            offset += num_tokens\n+\n+        embeddings = freq_table[pos_ids]  # lookup rotary embeddings\n+        embeddings = embeddings.flatten(1)\n+        return embeddings\n+\n+    def fast_pos_embed_interpolate(self, grid_thw):\n+        grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+\n+        idx_list = [[] for _ in range(4)]\n+        weight_list = [[] for _ in range(4)]\n+\n+        for t, h, w in zip(grid_ts, grid_hs, grid_ws):\n+            h_idxs = torch.linspace(0, self.num_grid_per_side - 1, h)\n+            w_idxs = torch.linspace(0, self.num_grid_per_side - 1, w)\n+\n+            h_idxs_floor = h_idxs.int()\n+            w_idxs_floor = w_idxs.int()\n+            h_idxs_ceil = (h_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)\n+            w_idxs_ceil = (w_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)\n+\n+            dh = h_idxs - h_idxs_floor\n+            dw = w_idxs - w_idxs_floor\n+\n+            base_h = h_idxs_floor * self.num_grid_per_side\n+            base_h_ceil = h_idxs_ceil * self.num_grid_per_side\n+\n+            indices = [\n+                (base_h[None].T + w_idxs_floor[None]).flatten(),\n+                (base_h[None].T + w_idxs_ceil[None]).flatten(),\n+                (base_h_ceil[None].T + w_idxs_floor[None]).flatten(),\n+                (base_h_ceil[None].T + w_idxs_ceil[None]).flatten(),\n+            ]\n+\n+            weights = [\n+                ((1 - dh)[None].T * (1 - dw)[None]).flatten(),\n+                ((1 - dh)[None].T * dw[None]).flatten(),\n+                (dh[None].T * (1 - dw)[None]).flatten(),\n+                (dh[None].T * dw[None]).flatten(),\n+            ]\n+\n+            for i in range(4):\n+                idx_list[i].extend(indices[i].tolist())\n+                weight_list[i].extend(weights[i].tolist())\n+\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n+        weight_tensor = torch.tensor(\n+            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n+        )\n+        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n+\n+        patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])\n+\n+        patch_pos_embeds_permute = []\n+        merge_size = self.config.spatial_merge_size\n+        for pos_embed, t, h, w in zip(patch_pos_embeds, grid_ts, grid_hs, grid_ws):\n+            pos_embed = pos_embed.repeat(t, 1)\n+            pos_embed = (\n+                pos_embed.view(t, h // merge_size, merge_size, w // merge_size, merge_size, -1)\n+                .permute(0, 1, 3, 2, 4, 5)\n+                .flatten(0, 4)\n+            )\n+            patch_pos_embeds_permute.append(pos_embed)\n+        patch_pos_embeds = torch.cat(patch_pos_embeds_permute)\n+        return patch_pos_embeds\n+\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):\n+                The temporal, height and width of feature shape of each image in LLM.\n+\n+        Returns:\n+            `torch.Tensor`: hidden_states.\n+        \"\"\"\n+        hidden_states = self.patch_embed(hidden_states)\n+\n+        pos_embeds = self.fast_pos_embed_interpolate(grid_thw)\n+        hidden_states = hidden_states + pos_embeds\n+\n+        rotary_pos_emb = self.rot_pos_emb(grid_thw)\n+\n+        seq_len, _ = hidden_states.size()\n+        hidden_states = hidden_states.reshape(seq_len, -1)\n+        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n+\n+        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n+            dim=0,\n+            # Select dtype based on the following factors:\n+            #  - FA2 requires that cu_seqlens_q must have dtype int32\n+            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw\n+            # See https://github.com/huggingface/transformers/pull/34852 for more information\n+            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n+        )\n+        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n+\n+        deepstack_feature_lists = []\n+        for layer_num, blk in enumerate(self.blocks):\n+            hidden_states = blk(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+            if layer_num in self.deepstack_visual_indexes:\n+                deepstack_feature = self.deepstack_merger_list[self.deepstack_visual_indexes.index(layer_num)](\n+                    hidden_states\n+                )\n+                deepstack_feature_lists.append(deepstack_feature)\n+\n+        hidden_states = self.merger(hidden_states)\n+\n+        return hidden_states, deepstack_feature_lists\n+\n+\n+@auto_docstring(\n+    custom_intro=(\n+        \"Text part of Qwen3VL, \"\n+        \"not a pure text-only model, as DeepStack integrates visual features into the early hidden states.\"\n+    )\n+)\n+class Qwen3VLTextModel(Qwen3VLPreTrainedModel, Qwen3Model):\n+    config: Qwen3VLTextConfig\n+    _no_split_modules = [\"Qwen3VLTextDecoderLayer\"]\n+\n+    def __init__(self, config: Qwen3VLTextConfig):\n+        super().__init__(config)\n+        del self.has_sliding_layers\n+\n+    def _deepstack_process(\n+        self, hidden_states: torch.Tensor, visual_pos_masks: torch.Tensor, visual_embeds: torch.Tensor\n+    ):\n+        visual_pos_masks = visual_pos_masks.to(hidden_states.device)\n+        visual_embeds = visual_embeds.to(hidden_states.device, hidden_states.dtype)\n+        local_this = hidden_states[visual_pos_masks, :].clone() + visual_embeds\n+        hidden_states[visual_pos_masks, :] = local_this\n+        return hidden_states\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        # args for deepstack\n+        visual_pos_masks: Optional[torch.Tensor] = None,\n+        deepstack_visual_embeds: Optional[list[torch.Tensor]] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        visual_pos_masks (`torch.Tensor` of shape `(batch_size, seqlen)`, *optional*):\n+            The mask of the visual positions.\n+        deepstack_visual_embeds (`list[torch.Tensor]`, *optional*):\n+            The deepstack visual embeddings. The shape is (num_layers, visual_seqlen, embed_dim).\n+            The feature is extracted from the different visual encoder layers, and fed to the decoder\n+            hidden states. It's from the paper DeepStack(https://arxiv.org/abs/2406.04334).\n+        \"\"\"\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        # torch.jit.trace() doesn't support cache objects in the output\n+        if use_cache and past_key_values is None and not torch.jit.is_tracing():\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        # the hard coded `3` is for temporal, height and width.\n+        if position_ids is None:\n+            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n+        elif position_ids.ndim == 2:\n+            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n+\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            text_position_ids = position_ids[0]\n+\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=text_position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for layer_idx, decoder_layer in enumerate(self.layers):\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=text_position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+            hidden_states = layer_outputs\n+\n+            # add visual features to the hidden states of first several layers\n+            if deepstack_visual_embeds is not None and layer_idx in range(len(deepstack_visual_embeds)):\n+                hidden_states = self._deepstack_process(\n+                    hidden_states,\n+                    visual_pos_masks,\n+                    deepstack_visual_embeds[layer_idx],\n+                )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class Qwen3VLModel(Qwen2_5_VLModel):\n+    config: Qwen3VLConfig\n+    _checkpoint_conversion_mapping = {}\n+    _no_split_modules = [\"Qwen3VLTextDecoderLayer\", \"Qwen3VLVisionBlock\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.visual = Qwen3VLVisionModel._from_config(config.vision_config)\n+        self.language_model = Qwen3VLTextModel._from_config(config.text_config)\n+\n+    def get_rope_index(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"Different from the original implementation, Qwen3VL use timestamps rather than absolute time position ids.\"\"\"\n+\n+        # Since we use timestamps to seperate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split\n+        if video_grid_thw is not None:\n+            video_grid_thw = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)\n+            video_grid_thw[:, 0] = 1\n+\n+        spatial_merge_size = self.config.vision_config.spatial_merge_size\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+        vision_start_token_id = self.config.vision_start_token_id\n+        mrope_position_deltas = []\n+        if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):\n+            total_input_ids = input_ids\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(total_input_ids)\n+            position_ids = torch.ones(\n+                3,\n+                input_ids.shape[0],\n+                input_ids.shape[1],\n+                dtype=input_ids.dtype,\n+                device=input_ids.device,\n+            )\n+            image_index, video_index = 0, 0\n+            attention_mask = attention_mask.to(total_input_ids.device)\n+            for i, input_ids in enumerate(total_input_ids):\n+                input_ids = input_ids[attention_mask[i] == 1]\n+                image_nums, video_nums = 0, 0\n+                vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n+                vision_tokens = input_ids[vision_start_indices + 1]\n+                image_nums = (vision_tokens == image_token_id).sum()\n+                video_nums = (vision_tokens == video_token_id).sum()\n+                input_tokens = input_ids.tolist()\n+                llm_pos_ids_list: list = []\n+                st = 0\n+                remain_images, remain_videos = image_nums, video_nums\n+                for _ in range(image_nums + video_nums):\n+                    if image_token_id in input_tokens and remain_images > 0:\n+                        ed_image = input_tokens.index(image_token_id, st)\n+                    else:\n+                        ed_image = len(input_tokens) + 1\n+                    if video_token_id in input_tokens and remain_videos > 0:\n+                        ed_video = input_tokens.index(video_token_id, st)\n+                    else:\n+                        ed_video = len(input_tokens) + 1\n+                    if ed_image < ed_video:\n+                        t, h, w = (\n+                            image_grid_thw[image_index][0],\n+                            image_grid_thw[image_index][1],\n+                            image_grid_thw[image_index][2],\n+                        )\n+                        image_index += 1\n+                        remain_images -= 1\n+                        ed = ed_image\n+\n+                    else:\n+                        t, h, w = (\n+                            video_grid_thw[video_index][0],\n+                            video_grid_thw[video_index][1],\n+                            video_grid_thw[video_index][2],\n+                        )\n+                        video_index += 1\n+                        remain_videos -= 1\n+                        ed = ed_video\n+                    llm_grid_t, llm_grid_h, llm_grid_w = (\n+                        t.item(),\n+                        h.item() // spatial_merge_size,\n+                        w.item() // spatial_merge_size,\n+                    )\n+                    text_len = ed - st\n+\n+                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0\n+                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n+\n+                    # t_index is always 0 because llm_grid_t is always 1 (we use timestamps to encode the temporal information for videos)\n+                    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(-1, llm_grid_h * llm_grid_w).flatten()\n+                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()\n+                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()\n+                    llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)\n+                    st = ed + llm_grid_t * llm_grid_h * llm_grid_w\n+\n+                if st < len(input_tokens):\n+                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0\n+                    text_len = len(input_tokens) - st\n+                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n+\n+                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)\n+                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)\n+                mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))\n+            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)\n+            return position_ids, mrope_position_deltas\n+        else:\n+            if attention_mask is not None:\n+                position_ids = attention_mask.long().cumsum(-1) - 1\n+                position_ids.masked_fill_(attention_mask == 0, 1)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(attention_mask.device)\n+                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]\n+                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]\n+            else:\n+                position_ids = (\n+                    torch.arange(input_ids.shape[1], device=input_ids.device)\n+                    .view(1, 1, -1)\n+                    .expand(3, input_ids.shape[0], -1)\n+                )\n+                mrope_position_deltas = torch.zeros(\n+                    [input_ids.shape[0], 1],\n+                    device=input_ids.device,\n+                    dtype=input_ids.dtype,\n+                )\n+\n+            return position_ids, mrope_position_deltas\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model. The deepstack visual features are also returned.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each image in LLM.\n+        \"\"\"\n+        pixel_values = pixel_values.type(self.visual.dtype)\n+        image_embeds, deepstack_image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+        split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\n+        image_embeds = torch.split(image_embeds, split_sizes)\n+        return image_embeds, deepstack_image_embeds\n+\n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model. The deepstack visual features are also returned.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        # Same implementation as for images\n+        return self.get_image_features(pixel_values_videos, video_grid_thw)\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Qwen3VLModelOutputWithPast]:\n+        r\"\"\"\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        image_mask = None\n+        video_mask = None\n+\n+        if pixel_values is not None:\n+            image_embeds, deepstack_image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+        if pixel_values_videos is not None:\n+            video_embeds, deepstack_video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+\n+        visual_pos_masks = None\n+        deepstack_visual_embeds = None\n+        if image_mask is not None and video_mask is not None:\n+            # aggregate visual_pos_masks and deepstack_visual_embeds\n+            image_mask = image_mask[..., 0]\n+            video_mask = video_mask[..., 0]\n+            visual_pos_masks = image_mask | video_mask\n+            deepstack_visual_embeds = []\n+            image_mask_joint = image_mask[visual_pos_masks]\n+            video_mask_joint = video_mask[visual_pos_masks]\n+            for img_embed, vid_embed in zip(deepstack_image_embeds, deepstack_video_embeds):\n+                embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1]).to(img_embed.device)\n+                embed_joint[image_mask_joint, :] = img_embed\n+                embed_joint[video_mask_joint, :] = vid_embed\n+                deepstack_visual_embeds.append(embed_joint)\n+        elif image_mask is not None:\n+            image_mask = image_mask[..., 0]\n+            visual_pos_masks = image_mask\n+            deepstack_visual_embeds = deepstack_image_embeds\n+        elif video_mask is not None:\n+            video_mask = video_mask[..., 0]\n+            visual_pos_masks = video_mask\n+            deepstack_visual_embeds = deepstack_video_embeds\n+\n+        if position_ids is None:\n+            attention_mask_tensor = (\n+                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n+            )\n+            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n+                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n+                # Only apply conversion for floating point tensors (inverted masks)\n+                if attention_mask_tensor.dtype.is_floating_point:\n+                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n+\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            prefill_compiled_stage = is_torchdynamo_compiling() and (\n+                (input_ids is not None and input_ids.shape[1] != 1)\n+                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n+            )\n+            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n+                (cache_position is not None and cache_position[0] == 0)\n+                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n+            )\n+            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n+                position_ids, rope_deltas = self.get_rope_index(\n+                    input_ids,\n+                    image_grid_thw,\n+                    video_grid_thw,\n+                    attention_mask=attention_mask_tensor,\n+                )\n+                self.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            else:\n+                batch_size, seq_length, _ = inputs_embeds.shape\n+                delta = (\n+                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n+                    if cache_position is not None\n+                    else 0\n+                )\n+                position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n+                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n+                if cache_position is not None:  # otherwise `deltas` is an int `0`\n+                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n+                position_ids = position_ids.add(delta)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n+\n+        outputs = self.language_model(\n+            input_ids=None,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            visual_pos_masks=visual_pos_masks,\n+            deepstack_visual_embeds=deepstack_visual_embeds,\n+            **kwargs,\n+        )\n+\n+        return Qwen3VLModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=self.rope_deltas,\n+        )\n+\n+\n+class Qwen3VLCausalLMOutputWithPast(Qwen2_5_VLCausalLMOutputWithPast):\n+    pass\n+\n+\n+class Qwen3VLForConditionalGeneration(Qwen2_5_VLForConditionalGeneration):\n+    config: Qwen3VLConfig\n+    _checkpoint_conversion_mapping = {}\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Qwen3VLCausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each video in LLM.\n+\n+        Example:\n+            TODO: Add example\n+        \"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        return Qwen3VLCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=outputs.rope_deltas,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        pixel_values=None,\n+        pixel_values_videos=None,\n+        image_grid_thw=None,\n+        video_grid_thw=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+\n+        # Qwen3VL position_ids are prepareed with rope_deltas in forward\n+        model_inputs[\"position_ids\"] = None\n+\n+        if cache_position[0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_values_videos\"] = None\n+\n+        return model_inputs\n+\n+\n+class Qwen3VLVideosProcessorKwargs(VideosKwargs, total=False):\n+    pass\n+\n+\n+class Qwen3VLImagesKwargs(Qwen2VLImagesKwargs):\n+    pass\n+\n+\n+class Qwen3VLProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Qwen3VLImagesKwargs\n+    videos_kwargs: Qwen3VLVideosProcessorKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+            \"return_token_type_ids\": False,\n+            \"return_mm_token_type_ids\": False,\n+        },\n+        \"videos_kwargs\": {\"return_metadata\": True},\n+    }\n+\n+\n+class Qwen3VLProcessor(Qwen2VLProcessor):\n+    r\"\"\"\n+    Constructs a Qwen3VL processor which wraps a Qwen3VL image processor and a Qwen2 tokenizer into a single processor.\n+    [`Qwen3VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n+    [`~Qwen3VLProcessor.__call__`] and [`~Qwen3VLProcessor.decode`] for more information.\n+    Args:\n+        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        video_processor ([`Qwen3VLVideoProcessor`], *optional*):\n+            The video processor is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+    \"\"\"\n+\n+    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n+        super().__init__(image_processor, tokenizer, video_processor, chat_template, **kwargs)\n+        self.vision_start_token = (\n+            \"<|vision_start|>\" if not hasattr(tokenizer, \"vision_start_token\") else tokenizer.vision_start_token\n+        )\n+        self.vision_end_token = (\n+            \"<|vision_end|>\" if not hasattr(tokenizer, \"vision_end_token\") else tokenizer.vision_end_token\n+        )\n+        self.vision_start_token_id = (\n+            tokenizer.vision_start_token_id\n+            if getattr(tokenizer, \"vision_start_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.vision_start_token)\n+        )\n+        self.vision_end_token_id = (\n+            tokenizer.vision_end_token_id\n+            if getattr(tokenizer, \"vision_end_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.vision_end_token)\n+        )\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        videos: VideoInput = None,\n+        **kwargs: Unpack[Qwen3VLProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n+        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `list[str]`, `list[list[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n+                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **pixel_values_videos** -- Pixel values of videos to be fed to a model. Returned when `videos` is not `None`.\n+            - **image_grid_thw** -- List of image 3D grid in LLM. Returned when `images` is not `None`.\n+            - **video_grid_thw** -- List of video 3D grid in LLM. Returned when `videos` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            Qwen3VLProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        if images is not None:\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            image_grid_thw = image_inputs[\"image_grid_thw\"]\n+        else:\n+            image_inputs = {}\n+            image_grid_thw = None\n+\n+        if videos is not None:\n+            videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n+            video_grid_thw = videos_inputs[\"video_grid_thw\"]\n+            # If user has not requested video metadata, pop it\n+            if \"return_metadata\" not in kwargs:\n+                video_metadata = videos_inputs.pop(\"video_metadata\")\n+            else:\n+                video_metadata = videos_inputs[\"video_metadata\"]\n+            video_grid_thw = videos_inputs[\"video_grid_thw\"]\n+        else:\n+            videos_inputs = {}\n+            video_grid_thw = None\n+\n+        if not isinstance(text, list):\n+            text = [text]\n+\n+        text = text.copy()  # below lines change text in-place\n+        if image_grid_thw is not None:\n+            merge_length = self.image_processor.merge_size**2\n+            index = 0\n+            for i in range(len(text)):\n+                while self.image_token in text[i]:\n+                    num_image_tokens = image_grid_thw[index].prod() // merge_length\n+                    text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n+                    index += 1\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n+\n+        if video_grid_thw is not None:\n+            merge_length = self.video_processor.merge_size**2\n+            index = 0\n+            for i in range(len(text)):\n+                while self.video_token in text[i]:\n+                    metadata = video_metadata[i]\n+                    if metadata.fps is None:\n+                        logger.warning_once(\n+                            \"Qwen3VL requires frame timestamps to construct prompts, but the `fps` of the input video could not be inferred. \"\n+                            \"Probably `video_metadata` was missing from inputs and you passed pre-sampled frames. \"\n+                            \"Defaulting to `fps=24`. Please provide `video_metadata` for more accurate results.\"\n+                        )\n+                        metadata.fps = 24 if metadata.fps is None else metadata.fps\n+\n+                    # if timestamps are not provided, calculate them\n+                    curr_timestamp = self._calculate_timestamps(\n+                        metadata.frames_indices,\n+                        metadata.fps,\n+                        self.video_processor.merge_size,\n+                    )\n+\n+                    video_placeholder = \"\"\n+                    frame_seqlen = video_grid_thw[index][1:].prod() // merge_length\n+                    for frame_idx in range(video_grid_thw[index][0]):\n+                        curr_time = curr_timestamp[frame_idx]\n+                        video_placeholder += f\"<{curr_time:.1f} seconds>\"\n+                        video_placeholder += (\n+                            self.vision_start_token + \"<|placeholder|>\" * frame_seqlen + self.vision_end_token\n+                        )\n+                    if f\"{self.vision_start_token}{self.video_token}{self.vision_end_token}\" in text[i]:\n+                        text[i] = text[i].replace(\n+                            f\"{self.vision_start_token}{self.video_token}{self.vision_end_token}\", video_placeholder, 1\n+                        )\n+                    else:\n+                        # vllm may input video token directly\n+                        text[i] = text[i].replace(self.video_token, video_placeholder, 1)\n+                    index += 1\n+\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n+\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", None)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n+\n+    def _calculate_timestamps(self, indices: Union[list[int], np.ndarray], video_fps: float, merge_size: int = 2):\n+        if not isinstance(indices, list):\n+            indices = indices.tolist()\n+        if len(indices) % merge_size != 0:\n+            indices.extend(indices[-1] for _ in range(merge_size - len(indices) % merge_size))\n+        timestamps = [idx / video_fps for idx in indices]\n+        # @JJJYmmm frames are merged by self.merge_size, \\\n+        # so we need to average the timestamps between the first/last frame within the temporal patch\n+        timestamps = [\n+            (timestamps[i] + timestamps[i + merge_size - 1]) / 2 for i in range(0, len(timestamps), merge_size)\n+        ]\n+        return timestamps\n+\n+\n+__all__ = [\n+    \"Qwen3VLConfig\",\n+    \"Qwen3VLTextConfig\",\n+    \"Qwen3VLVisionModel\",\n+    \"Qwen3VLForConditionalGeneration\",\n+    \"Qwen3VLModel\",\n+    \"Qwen3VLPreTrainedModel\",\n+    \"Qwen3VLProcessor\",\n+    \"Qwen3VLTextModel\",\n+]"
        },
        {
            "sha": "cac82e738f397b4359c6ab1747dbc2f3edebd47e",
            "filename": "src/transformers/models/qwen3_vl/processing_qwen3_vl.py",
            "status": "added",
            "additions": 328,
            "deletions": 0,
            "changes": 328,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,328 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/qwen3_vl/modular_qwen3_vl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_qwen3_vl.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+from ...video_utils import VideoInput\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Qwen3VLVideosProcessorKwargs(VideosKwargs, total=False):\n+    pass\n+\n+\n+class Qwen3VLImagesKwargs(ImagesKwargs):\n+    min_pixels: Optional[int]\n+    max_pixels: Optional[int]\n+    patch_size: Optional[int]\n+    temporal_patch_size: Optional[int]\n+    merge_size: Optional[int]\n+\n+\n+class Qwen3VLProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Qwen3VLImagesKwargs\n+    videos_kwargs: Qwen3VLVideosProcessorKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+            \"return_token_type_ids\": False,\n+            \"return_mm_token_type_ids\": False,\n+        },\n+        \"videos_kwargs\": {\"return_metadata\": True},\n+    }\n+\n+\n+class Qwen3VLProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Qwen3VL processor which wraps a Qwen3VL image processor and a Qwen2 tokenizer into a single processor.\n+    [`Qwen3VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n+    [`~Qwen3VLProcessor.__call__`] and [`~Qwen3VLProcessor.decode`] for more information.\n+    Args:\n+        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        video_processor ([`Qwen3VLVideoProcessor`], *optional*):\n+            The video processor is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    video_processor_class = \"AutoVideoProcessor\"\n+    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n+\n+    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n+        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n+        self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n+        self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n+        self.image_token_id = (\n+            tokenizer.image_token_id\n+            if getattr(tokenizer, \"image_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.image_token)\n+        )\n+        self.video_token_id = (\n+            tokenizer.video_token_id\n+            if getattr(tokenizer, \"video_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.video_token)\n+        )\n+        self.vision_start_token = (\n+            \"<|vision_start|>\" if not hasattr(tokenizer, \"vision_start_token\") else tokenizer.vision_start_token\n+        )\n+        self.vision_end_token = (\n+            \"<|vision_end|>\" if not hasattr(tokenizer, \"vision_end_token\") else tokenizer.vision_end_token\n+        )\n+        self.vision_start_token_id = (\n+            tokenizer.vision_start_token_id\n+            if getattr(tokenizer, \"vision_start_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.vision_start_token)\n+        )\n+        self.vision_end_token_id = (\n+            tokenizer.vision_end_token_id\n+            if getattr(tokenizer, \"vision_end_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.vision_end_token)\n+        )\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        videos: VideoInput = None,\n+        **kwargs: Unpack[Qwen3VLProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n+        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `list[str]`, `list[list[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n+                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **pixel_values_videos** -- Pixel values of videos to be fed to a model. Returned when `videos` is not `None`.\n+            - **image_grid_thw** -- List of image 3D grid in LLM. Returned when `images` is not `None`.\n+            - **video_grid_thw** -- List of video 3D grid in LLM. Returned when `videos` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            Qwen3VLProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        if images is not None:\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            image_grid_thw = image_inputs[\"image_grid_thw\"]\n+        else:\n+            image_inputs = {}\n+            image_grid_thw = None\n+\n+        if videos is not None:\n+            videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n+            video_grid_thw = videos_inputs[\"video_grid_thw\"]\n+            # If user has not requested video metadata, pop it\n+            if \"return_metadata\" not in kwargs:\n+                video_metadata = videos_inputs.pop(\"video_metadata\")\n+            else:\n+                video_metadata = videos_inputs[\"video_metadata\"]\n+            video_grid_thw = videos_inputs[\"video_grid_thw\"]\n+        else:\n+            videos_inputs = {}\n+            video_grid_thw = None\n+\n+        if not isinstance(text, list):\n+            text = [text]\n+\n+        text = text.copy()  # below lines change text in-place\n+        if image_grid_thw is not None:\n+            merge_length = self.image_processor.merge_size**2\n+            index = 0\n+            for i in range(len(text)):\n+                while self.image_token in text[i]:\n+                    num_image_tokens = image_grid_thw[index].prod() // merge_length\n+                    text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n+                    index += 1\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n+\n+        if video_grid_thw is not None:\n+            merge_length = self.video_processor.merge_size**2\n+            index = 0\n+            for i in range(len(text)):\n+                while self.video_token in text[i]:\n+                    metadata = video_metadata[i]\n+                    if metadata.fps is None:\n+                        logger.warning_once(\n+                            \"Qwen3VL requires frame timestamps to construct prompts, but the `fps` of the input video could not be inferred. \"\n+                            \"Probably `video_metadata` was missing from inputs and you passed pre-sampled frames. \"\n+                            \"Defaulting to `fps=24`. Please provide `video_metadata` for more accurate results.\"\n+                        )\n+                        metadata.fps = 24 if metadata.fps is None else metadata.fps\n+\n+                    # if timestamps are not provided, calculate them\n+                    curr_timestamp = self._calculate_timestamps(\n+                        metadata.frames_indices,\n+                        metadata.fps,\n+                        self.video_processor.merge_size,\n+                    )\n+\n+                    video_placeholder = \"\"\n+                    frame_seqlen = video_grid_thw[index][1:].prod() // merge_length\n+                    for frame_idx in range(video_grid_thw[index][0]):\n+                        curr_time = curr_timestamp[frame_idx]\n+                        video_placeholder += f\"<{curr_time:.1f} seconds>\"\n+                        video_placeholder += (\n+                            self.vision_start_token + \"<|placeholder|>\" * frame_seqlen + self.vision_end_token\n+                        )\n+                    if f\"{self.vision_start_token}{self.video_token}{self.vision_end_token}\" in text[i]:\n+                        text[i] = text[i].replace(\n+                            f\"{self.vision_start_token}{self.video_token}{self.vision_end_token}\", video_placeholder, 1\n+                        )\n+                    else:\n+                        # vllm may input video token directly\n+                        text[i] = text[i].replace(self.video_token, video_placeholder, 1)\n+                    index += 1\n+\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n+\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", None)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+            video_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (num_frames, height, width) per each video.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = Qwen3VLProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+            merge_size = images_kwargs.get(\"merge_size\", None) or self.image_processor.merge_size\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        if video_sizes is not None:\n+            videos_kwargs = Qwen3VLProcessorKwargs._defaults.get(\"videos_kwargs\", {})\n+            videos_kwargs.update(kwargs)\n+            num_video_patches = [\n+                self.video_processor.get_number_of_video_patches(*video_size, videos_kwargs)\n+                for video_size in video_sizes\n+            ]\n+            num_video_tokens = [(num_patches // merge_size**2) for num_patches in num_video_patches]\n+            vision_data[\"num_video_tokens\"] = num_video_tokens\n+\n+        return MultiModalData(**vision_data)\n+\n+    def post_process_image_text_to_text(\n+        self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs\n+    ):\n+        \"\"\"\n+        Post-process the output of the model to decode the text.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n+                Whether or not to clean up the tokenization spaces. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[str]`: The decoded text.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(\n+            generated_outputs,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            **kwargs,\n+        )\n+\n+    def _calculate_timestamps(self, indices: Union[list[int], np.ndarray], video_fps: float, merge_size: int = 2):\n+        if not isinstance(indices, list):\n+            indices = indices.tolist()\n+        if len(indices) % merge_size != 0:\n+            indices.extend(indices[-1] for _ in range(merge_size - len(indices) % merge_size))\n+        timestamps = [idx / video_fps for idx in indices]\n+        # @JJJYmmm frames are merged by self.merge_size, \\\n+        # so we need to average the timestamps between the first/last frame within the temporal patch\n+        timestamps = [\n+            (timestamps[i] + timestamps[i + merge_size - 1]) / 2 for i in range(0, len(timestamps), merge_size)\n+        ]\n+        return timestamps\n+\n+\n+__all__ = [\"Qwen3VLProcessor\"]"
        },
        {
            "sha": "c4648788c9dc9c9660b31c09d652d27a5d3773cd",
            "filename": "src/transformers/models/qwen3_vl/video_processing_qwen3_vl.py",
            "status": "added",
            "additions": 276,
            "deletions": 0,
            "changes": 276,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fvideo_processing_qwen3_vl.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,276 @@\n+# coding=utf-8\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"video processor class for Qwen3-VL.\"\"\"\n+\n+import math\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ChannelDimension, PILImageResampling, SizeDict, get_image_size\n+from ...processing_utils import Unpack, VideosKwargs\n+from ...utils import TensorType, add_start_docstrings, logging\n+from ...video_processing_utils import BASE_VIDEO_PROCESSOR_DOCSTRING, BaseVideoProcessor\n+from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def smart_resize(\n+    num_frames: int,\n+    height: int,\n+    width: int,\n+    temporal_factor: int = 2,\n+    factor: int = 32,\n+    min_pixels: int = 128 * 128,\n+    max_pixels: int = 16 * 16 * 2 * 2 * 2 * 6144,\n+):\n+    if num_frames < temporal_factor:\n+        raise ValueError(f\"t:{num_frames} must be larger than temporal_factor:{temporal_factor}\")\n+    if height < factor or width < factor:\n+        raise ValueError(f\"height:{height} or width:{width} must be larger than factor:{factor}\")\n+    elif max(height, width) / min(height, width) > 200:\n+        raise ValueError(\n+            f\"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}\"\n+        )\n+    h_bar = round(height / factor) * factor\n+    w_bar = round(width / factor) * factor\n+    t_bar = round(num_frames / temporal_factor) * temporal_factor\n+\n+    if t_bar * h_bar * w_bar > max_pixels:\n+        beta = math.sqrt((num_frames * height * width) / max_pixels)\n+        h_bar = max(factor, math.floor(height / beta / factor) * factor)\n+        w_bar = max(factor, math.floor(width / beta / factor) * factor)\n+    elif t_bar * h_bar * w_bar < min_pixels:\n+        beta = math.sqrt(min_pixels / (num_frames * height * width))\n+        h_bar = math.ceil(height * beta / factor) * factor\n+        w_bar = math.ceil(width * beta / factor) * factor\n+\n+    return h_bar, w_bar\n+\n+\n+class Qwen3VLVideoProcessorInitKwargs(VideosKwargs):\n+    patch_size: Optional[int]\n+    temporal_patch_size: Optional[int]\n+    merge_size: Optional[int]\n+    min_frames: Optional[int]\n+    max_frames: Optional[int]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Qwen3-VL image processor that dynamically resizes videos based on the original videos.\",\n+    BASE_VIDEO_PROCESSOR_DOCSTRING,\n+    \"\"\"\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The spacial patch size of the vision encoder.\n+        temporal_patch_size (`int`, *optional*, defaults to 2):\n+            The temporal patch size of the vision encoder.\n+        merge_size (`int`, *optional*, defaults to 2):\n+            The merge size of the vision encoder to llm encoder.\n+    \"\"\",\n+)\n+class Qwen3VLVideoProcessor(BaseVideoProcessor):\n+    resample = PILImageResampling.BICUBIC\n+    size = {\"shortest_edge\": 128 * 32 * 32, \"longest_edge\": 32 * 32 * 768}\n+    image_mean = [0.5, 0.5, 0.5]\n+    image_std = [0.5, 0.5, 0.5]\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    patch_size = 16\n+    temporal_patch_size = 2\n+    merge_size = 2\n+    fps = 2\n+    min_frames = 4\n+    max_frames = 768\n+    do_sample_frames = True\n+    valid_kwargs = Qwen3VLVideoProcessorInitKwargs\n+    model_input_names = [\"pixel_values_videos\", \"video_grid_thw\"]\n+\n+    def __init__(self, **kwargs: Unpack[Qwen3VLVideoProcessorInitKwargs]):\n+        super().__init__(**kwargs)\n+        if self.size is not None and (\n+            self.size.get(\"shortest_edge\", None) is None or self.size.get(\"longest_edge\", None) is None\n+        ):\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+\n+        return super()._further_process_kwargs(size=size, **kwargs)\n+\n+    def sample_frames(\n+        self,\n+        metadata: VideoMetadata,\n+        num_frames: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n+        If `fps` is passed along with metadata, `fps` frames per second are sampled uniformty. Arguments `num_frames`\n+        and `fps` are mutually exclusive.\n+\n+        Args:\n+            video (`torch.Tensor`):\n+                Video that need to be sampled.\n+            metadata (`VideoMetadata`):\n+                Metadata of the video containing information about total duration, fps and total number of frames.\n+            num_frames (`int`, *optional*):\n+                Maximum number of frames to sample. Defaults to `self.num_frames`.\n+            fps (`int` or `float`, *optional*):\n+                Target frames to sample per second. Defaults to `self.fps`.\n+        Returns:\n+            torch.Tensor:\n+                Sampled video frames.\n+        \"\"\"\n+        if fps is not None and num_frames is not None:\n+            raise ValueError(\"`num_frames` and `fps` are mutually exclusive arguments, please use only one!\")\n+\n+        total_num_frames = metadata.total_num_frames\n+        fps = fps if fps is not None else self.fps\n+\n+        # If num_frames is not given but fps is, calculate num_frames from fps\n+        if num_frames is None and fps is not None:\n+            if metadata.fps is None:\n+                metadata.fps = 24\n+                logger.warning_once(\n+                    \"Asked to sample `fps` frames per second but no video metadata was provided which is required when sampling with `fps`. \"\n+                    \"Defaulting to `fps=24`. Please provide `video_metadata` for more accurate results.\"\n+                )\n+            num_frames = int(total_num_frames / metadata.fps * fps)\n+            num_frames = min(min(max(num_frames, self.min_frames), self.max_frames), total_num_frames)\n+\n+        if num_frames is None:\n+            num_frames = min(max(total_num_frames, self.min_frames), self.max_frames)\n+\n+        indices = np.linspace(0, total_num_frames - 1, num_frames).round().astype(int)\n+\n+        return indices\n+\n+    def _preprocess(\n+        self,\n+        videos: list[torch.Tensor],\n+        do_convert_rgb: bool = True,\n+        do_resize: bool = True,\n+        size: Optional[SizeDict] = None,\n+        interpolation: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: float = 1 / 255.0,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        patch_size: Optional[int] = None,\n+        temporal_patch_size: Optional[int] = None,\n+        merge_size: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n+    ):\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n+        resized_videos_grouped = {}\n+\n+        for shape, stacked_videos in grouped_videos.items():\n+            B, T, C, H, W = stacked_videos.shape\n+            num_frames, height, width = T, H, W\n+            if do_resize:\n+                resized_height, resized_width = smart_resize(\n+                    num_frames=num_frames,\n+                    height=height,\n+                    width=width,\n+                    temporal_factor=temporal_patch_size,\n+                    factor=patch_size * merge_size,\n+                    min_pixels=size.shortest_edge,\n+                    max_pixels=size.longest_edge,\n+                )\n+                stacked_videos = stacked_videos.view(B * T, C, H, W)\n+                stacked_videos = self.resize(\n+                    stacked_videos,\n+                    size=SizeDict(height=resized_height, width=resized_width),\n+                    interpolation=interpolation,\n+                )\n+                stacked_videos = stacked_videos.view(B, T, C, resized_height, resized_width)\n+            resized_videos_grouped[shape] = stacked_videos\n+        resized_videos = reorder_videos(resized_videos_grouped, grouped_videos_index)\n+\n+        # Group videos by size for further processing\n+        # Needed in case do_resize is False, or resize returns videos with different sizes\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(resized_videos)\n+        processed_videos_grouped = {}\n+        processed_grids = {}\n+        for shape, stacked_videos in grouped_videos.items():\n+            resized_height, resized_width = get_image_size(stacked_videos[0], channel_dim=ChannelDimension.FIRST)\n+\n+            # Fused rescale and normalize\n+            stacked_videos = self.rescale_and_normalize(\n+                stacked_videos, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            patches = stacked_videos\n+\n+            # Check that videos have `num_frames` divisible by `temporal_patch_size`\n+            if patches.shape[1] % temporal_patch_size != 0:\n+                repeats = patches[:, -1:].repeat(1, temporal_patch_size - 1, 1, 1, 1)\n+                patches = torch.cat([patches, repeats], dim=1)\n+            batch_size, grid_t, channel = patches.shape[:3]\n+            grid_t = grid_t // temporal_patch_size\n+            grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+\n+            patches = patches.view(\n+                batch_size,\n+                grid_t,\n+                temporal_patch_size,\n+                channel,\n+                grid_h // merge_size,\n+                merge_size,\n+                patch_size,\n+                grid_w // merge_size,\n+                merge_size,\n+                patch_size,\n+            )\n+            patches = patches.permute(0, 1, 4, 7, 5, 8, 3, 2, 6, 9)\n+            flatten_patches = patches.reshape(\n+                batch_size,\n+                grid_t * grid_h * grid_w,\n+                channel * temporal_patch_size * patch_size * patch_size,\n+            )\n+\n+            processed_videos_grouped[shape] = flatten_patches\n+            processed_grids[shape] = [[grid_t, grid_h, grid_w]] * batch_size\n+\n+        processed_videos = reorder_videos(processed_videos_grouped, grouped_videos_index)\n+        processed_grids = reorder_videos(processed_grids, grouped_videos_index)\n+        pixel_values_videos = torch.cat(processed_videos, dim=0)\n+        video_grid_thw = torch.tensor(processed_grids)\n+        data = {\n+            \"pixel_values_videos\": pixel_values_videos,\n+            \"video_grid_thw\": video_grid_thw,\n+        }\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"Qwen3VLVideoProcessor\"]"
        },
        {
            "sha": "a4000cb272723d9920136a6c78465e8413a8b4d1",
            "filename": "src/transformers/models/qwen3_vl_moe/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2F__init__.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_qwen3_vl_moe import *\n+    from .modeling_qwen3_vl_moe import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "c4a31e8f9f92e33ae23eb6dd3cbd4eb4fb3e9d9c",
            "filename": "src/transformers/models/qwen3_vl_moe/configuration_qwen3_vl_moe.py",
            "status": "added",
            "additions": 331,
            "deletions": 0,
            "changes": 331,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,331 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_qwen3_vl_moe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class Qwen3VLMoeTextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3VLMoeTextModel`]. It is used to instantiate a\n+    Qwen3-VL-MOE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen3-VL-30B-A3B-Instruct [Qwen/Qwen3-VL-30B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 151936):\n+            Vocabulary size of the Qwen2MoE model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Qwen2MoeModel`]\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 5632):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 16):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 128000):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_theta (`float`, *optional*, defaults to 5000000.0):\n+            The base period of the RoPE embeddings.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        decoder_sparse_step (`int`, *optional*, defaults to 1):\n+            The frequency of the MoE layer.\n+        moe_intermediate_size (`int`, *optional*, defaults to 1408):\n+            Intermediate size of the routed expert.\n+        num_experts_per_tok (`int`, *optional*, defaults to 4):\n+            Number of selected experts.\n+        num_experts (`int`, *optional*, defaults to 60):\n+            Number of routed experts.\n+        norm_topk_prob (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the topk probabilities.\n+        mlp_only_layers (`List[int]`, *optional*, defaults to `[]`):\n+            Indicate which layers use Qwen3VLMoeMLP rather than Qwen3VLMoeSparseMoeBlock\n+            The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n+            If `mlp_only_layers` is empty, `decoder_sparse_step` is used to determine the sparsity.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        head_dim (`int`, *optional*):\n+            The dimension of the head. If not specified, will default to `hidden_size // num_attention_heads`.\n+\n+    ```python\n+    >>> from transformers import Qwen3VLMoeForConditionalGeneration, Qwen3VLMoeConfig\n+\n+    >>> # Initializing a Qwen3VLMoe style configuration\n+    >>> configuration = Qwen3VLMoeConfig()\n+\n+    >>> # Initializing a model from the Qwen3-VL-30B-A3B style configuration\n+    >>> model = Qwen3VLMoeForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen3_vl_moe_text\"\n+    base_config_key = \"text_config\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `Qwen3VLMoe`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=151936,\n+        hidden_size=2048,\n+        intermediate_size=5632,\n+        num_hidden_layers=24,\n+        num_attention_heads=16,\n+        num_key_value_heads=16,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=128000,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n+        rope_theta=5000000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        decoder_sparse_step=1,\n+        moe_intermediate_size=1408,\n+        num_experts_per_tok=4,\n+        num_experts=60,\n+        norm_topk_prob=True,\n+        mlp_only_layers=None,\n+        rope_scaling=None,\n+        head_dim=None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.rope_scaling = rope_scaling\n+        self.head_dim = head_dim or hidden_size // num_attention_heads\n+\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"mrope_interleaved\"})\n+\n+        # MoE arguments\n+        self.decoder_sparse_step = decoder_sparse_step\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.norm_topk_prob = norm_topk_prob\n+        self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n+\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+\n+\n+class Qwen3VLMoeVisionConfig(PretrainedConfig):\n+    model_type = \"qwen3_vl_moe\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        depth=27,\n+        hidden_size=1152,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        intermediate_size=4304,\n+        num_heads=16,\n+        in_channels=3,\n+        patch_size=16,\n+        spatial_merge_size=2,\n+        temporal_patch_size=2,\n+        out_hidden_size=3584,\n+        num_position_embeddings=2304,\n+        deepstack_visual_indexes=[8, 16, 24],\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.depth = depth\n+        self.hidden_size = hidden_size\n+        self.hidden_act = hidden_act\n+        self.intermediate_size = intermediate_size\n+        self.num_heads = num_heads\n+        self.in_channels = in_channels\n+        self.patch_size = patch_size\n+        self.spatial_merge_size = spatial_merge_size\n+        self.temporal_patch_size = temporal_patch_size\n+        self.out_hidden_size = out_hidden_size\n+        self.num_position_embeddings = num_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.deepstack_visual_indexes = deepstack_visual_indexes\n+\n+\n+class Qwen3VLMoeConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3VLMoeModel`]. It is used to instantiate a\n+    Qwen3-VL-MOE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen3-VL-30B-A3B-Instruct [Qwen/Qwen3-VL-30B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen3VLMoeTextConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen3VLMoeVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 151655):\n+            The image token index to encode the image prompt.\n+        video_token_id (`int`, *optional*, defaults to 151656):\n+            The video token index to encode the image prompt.\n+        vision_start_token_id (`int`, *optional*, defaults to 151652):\n+            The start token index to encode the image prompt.\n+        vision_end_token_id (`int`, *optional*, defaults to 151653):\n+            The end token index to encode the image prompt.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie the word embeddings.\n+\n+    ```python\n+    >>> from transformers import Qwen3VLMoeForConditionalGeneration, Qwen3VLMoeConfig\n+\n+    >>> # Initializing a Qwen3-VL-MOE style configuration\n+    >>> configuration = Qwen3VLMoeConfig()\n+\n+    >>> # Initializing a model from the Qwen3-VL-30B-A3B style configuration\n+    >>> model = Qwen3VLMoeForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen3_vl_moe\"\n+    sub_configs = {\"vision_config\": Qwen3VLMoeVisionConfig, \"text_config\": Qwen3VLMoeTextConfig}\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        image_token_id=151655,\n+        video_token_id=151656,\n+        vision_start_token_id=151652,\n+        vision_end_token_id=151653,\n+        tie_word_embeddings=False,\n+        **kwargs,\n+    ):\n+        if isinstance(vision_config, dict):\n+            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n+        elif vision_config is None:\n+            self.vision_config = self.sub_configs[\"vision_config\"]()\n+\n+        if isinstance(text_config, dict):\n+            self.text_config = self.sub_configs[\"text_config\"](**text_config)\n+        elif text_config is None:\n+            self.text_config = self.sub_configs[\"text_config\"]()\n+\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+        self.vision_start_token_id = vision_start_token_id\n+        self.vision_end_token_id = vision_end_token_id\n+        super().__init__(**kwargs, tie_word_embeddings=tie_word_embeddings)\n+\n+\n+__all__ = [\"Qwen3VLMoeConfig\", \"Qwen3VLMoeTextConfig\"]"
        },
        {
            "sha": "74b793f096f3b1c19fdac0d22a16e3201704e092",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "added",
            "additions": 1711,
            "deletions": 0,
            "changes": 1711,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,1711 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_qwen3_vl_moe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from .configuration_qwen3_vl_moe import Qwen3VLMoeConfig, Qwen3VLMoeTextConfig, Qwen3VLMoeVisionConfig\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Qwen3VLMoeTextRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Qwen3VLMoeTextRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Qwen3VLMoeTextRouter(nn.Linear):\n+    def __init__(self, config):\n+        super().__init__(config.hidden_size, config.num_experts, bias=False)\n+        self.hidden_size = config.hidden_size\n+        self.top_k = config.num_experts_per_tok\n+        # since all the models use norm_topk_prob, we don't need to have a extra check for it\n+        # self.norm_topk_prob = config.norm_topk_prob\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.reshape(-1, self.hidden_size)\n+        router_logits = super().forward(hidden_states)\n+        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n+        routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n+        return router_weights, router_logits, router_indices\n+\n+\n+class Qwen3VLMoeTextExperts(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        self.intermediate_size = config.moe_intermediate_size\n+        self.hidden_size = config.hidden_size\n+        self.expert_dim = self.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n+        self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, routing_weights: torch.Tensor, router_indices: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        When training it is more efficient to just loop over the experts and compute the output for each expert\n+        as otherwise the memory would explode.\n+\n+        For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs.\n+\n+        Args:\n+            hidden_states (torch.Tensor): (batch_size * token_num, hidden_size)\n+            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n+            router_indices (torch.Tensor): (batch_size * token_num, top_k)\n+        Returns:\n+            torch.Tensor\n+        \"\"\"\n+        batch_size = hidden_states.shape[0]\n+        hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n+        if self.training:\n+            next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n+            with torch.no_grad():\n+                expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=self.num_experts)\n+                expert_mask = expert_mask.permute(2, 1, 0)\n+                # we sum on the top_k and on the sequence length to get which experts\n+                # are hit this time around\n+                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+            for expert_idx in expert_hit[:]:\n+                with torch.no_grad():\n+                    _, token_idx = torch.where(expert_mask[expert_idx[0]])\n+                current_state = hidden_states[token_idx]\n+                gate_up = current_state @ self.gate_up_proj[expert_idx]\n+                gate, up = gate_up.chunk(2, dim=-1)\n+                gated_output = up * self.act_fn(gate)\n+                out = gated_output @ self.down_proj[expert_idx]\n+                weighted_output = out[0] * routing_weights[token_idx, expert_idx, None]\n+                next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n+            next_states = next_states.view(batch_size, -1, self.hidden_size)\n+        else:\n+            hidden_states = hidden_states.repeat(self.num_experts, 1)\n+            hidden_states = hidden_states.view(self.num_experts, -1, self.hidden_size)\n+            gate_up = torch.bmm(hidden_states, self.gate_up_proj)\n+            gate, up = gate_up.chunk(2, dim=-1)  # not supported for DTensors\n+            next_states = torch.bmm((up * self.act_fn(gate)), self.down_proj)\n+            next_states = next_states.reshape(self.num_experts, batch_size, -1, self.hidden_size)\n+            next_states = (\n+                next_states * routing_weights.transpose(0, 1).view(self.num_experts, batch_size, -1)[..., None]\n+            )\n+            next_states = next_states.sum(dim=0)\n+        return next_states\n+\n+\n+class Qwen3VLMoeTextSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.num_experts = config.num_experts\n+        self.gate = Qwen3VLMoeTextRouter(config)\n+        self.experts = Qwen3VLMoeTextExperts(config)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        router_weights, router_logits, router_indices = self.gate(hidden_states)\n+        routed_out = self.experts(hidden_states, router_weights, router_indices)\n+        return routed_out, router_logits\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+class Qwen3VLMoeTextAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Qwen3VLMoeTextConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.q_norm = Qwen3VLMoeTextRMSNorm(\n+            self.head_dim, eps=config.rms_norm_eps\n+        )  # unlike olmo, only on the head dim!\n+        self.k_norm = Qwen3VLMoeTextRMSNorm(\n+            self.head_dim, eps=config.rms_norm_eps\n+        )  # thus post q_norm does not need reshape\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Qwen3VLMoeTextMLP(nn.Module):\n+    def __init__(self, config, intermediate_size=None):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Qwen3VLMoeTextDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Qwen3VLMoeTextConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = Qwen3VLMoeTextAttention(config, layer_idx)\n+\n+        if (layer_idx not in config.mlp_only_layers) and (\n+            config.num_experts > 0 and (layer_idx + 1) % config.decoder_sparse_step == 0\n+        ):\n+            self.mlp = Qwen3VLMoeTextSparseMoeBlock(config)\n+        else:\n+            self.mlp = Qwen3VLMoeTextMLP(config, intermediate_size=config.intermediate_size)\n+\n+        self.input_layernorm = Qwen3VLMoeTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Qwen3VLMoeTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n+                `(batch, sequence_length)` where padding elements are indicated by 0.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_router_logits (`bool`, *optional*):\n+                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n+                and should not be returned during inference.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        # For the MoE layers, we need to unpack\n+        if isinstance(hidden_states, tuple):\n+            hidden_states, _ = hidden_states\n+        hidden_states = residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Qwen3VLMoePreTrainedModel(PreTrainedModel):\n+    config: Qwen3VLMoeConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Qwen3VLMoeTextDecoderLayer\", \"Qwen3VLMoeVisionBlock\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(Qwen3VLMoeTextSparseMoeBlock, index=1),\n+        \"hidden_states\": Qwen3VLMoeTextDecoderLayer,\n+        \"attentions\": Qwen3VLMoeTextAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights.\"\"\"\n+        super()._init_weights(module)\n+        if hasattr(self.config, \"initializer_range\"):\n+            std = self.config.initializer_range\n+        else:\n+            std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n+        if isinstance(module, Qwen3VLMoeTextExperts):\n+            module.gate_up_proj.data.normal_(mean=0.0, std=std)\n+            module.down_proj.data.normal_(mean=0.0, std=std)\n+\n+\n+class Qwen3VLMoeVisionMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.linear_fc1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=True)\n+        self.linear_fc2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=True)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_state):\n+        return self.linear_fc2(self.act_fn(self.linear_fc1(hidden_state)))\n+\n+\n+class Qwen3VLMoeVisionPatchEmbed(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        self.patch_size = config.patch_size\n+        self.temporal_patch_size = config.temporal_patch_size\n+        self.in_channels = config.in_channels\n+        self.embed_dim = config.hidden_size\n+\n+        kernel_size = [self.temporal_patch_size, self.patch_size, self.patch_size]\n+        self.proj = nn.Conv3d(self.in_channels, self.embed_dim, kernel_size=kernel_size, stride=kernel_size, bias=True)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        target_dtype = self.proj.weight.dtype\n+        hidden_states = hidden_states.view(\n+            -1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size\n+        )\n+        hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)\n+        return hidden_states\n+\n+\n+class Qwen3VLMoeVisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n+        super().__init__()\n+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, seqlen: int) -> torch.Tensor:\n+        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        freqs = torch.outer(seq, self.inv_freq)\n+        return freqs\n+\n+\n+class Qwen3VLMoeVisionPatchMerger(nn.Module):\n+    def __init__(self, config: Qwen3VLMoeVisionConfig, use_postshuffle_norm=False) -> None:\n+        super().__init__()\n+        self.hidden_size = config.hidden_size * (config.spatial_merge_size**2)\n+        self.use_postshuffle_norm = use_postshuffle_norm\n+        self.norm = nn.LayerNorm(self.hidden_size if use_postshuffle_norm else config.hidden_size, eps=1e-6)\n+        self.linear_fc1 = nn.Linear(self.hidden_size, self.hidden_size)\n+        self.act_fn = nn.GELU()\n+        self.linear_fc2 = nn.Linear(self.hidden_size, config.out_hidden_size)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        x = self.norm(x.view(-1, self.hidden_size) if self.use_postshuffle_norm else x).view(-1, self.hidden_size)\n+        x = self.linear_fc2(self.act_fn(self.linear_fc1(x)))\n+        return x\n+\n+\n+def apply_rotary_pos_emb_vision(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n+) -> tuple[torch.Tensor, torch.Tensor]:\n+    orig_q_dtype = q.dtype\n+    orig_k_dtype = k.dtype\n+    q, k = q.float(), k.float()\n+    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    q_embed = q_embed.to(orig_q_dtype)\n+    k_embed = k_embed.to(orig_k_dtype)\n+    return q_embed, k_embed\n+\n+\n+class Qwen3VLMoeVisionAttention(nn.Module):\n+    def __init__(self, config: Qwen3VLMoeVisionConfig) -> None:\n+        super().__init__()\n+        self.dim = config.hidden_size\n+        self.num_heads = config.num_heads\n+        self.head_dim = self.dim // self.num_heads\n+        self.num_key_value_groups = 1  # needed for eager attention\n+        self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)\n+        self.proj = nn.Linear(self.dim, self.dim)\n+        self.scaling = self.head_dim**-0.5\n+        self.config = config\n+        self.attention_dropout = 0.0\n+        self.is_causal = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        seq_length = hidden_states.shape[0]\n+        query_states, key_states, value_states = (\n+            self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n+        )\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n+\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # Flash Attention 2: Use cu_seqlens for variable length attention\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()\n+            attn_output, _ = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask=None,\n+                scaling=self.scaling,\n+                dropout=0.0 if not self.training else self.attention_dropout,\n+                cu_seq_lens_q=cu_seqlens,\n+                cu_seq_lens_k=cu_seqlens,\n+                max_length_q=max_seqlen,\n+                max_length_k=max_seqlen,\n+                is_causal=False,\n+                **kwargs,\n+            )\n+        else:\n+            # Other implementations: Process each chunk separately\n+            lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n+            splits = [\n+                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)\n+            ]\n+\n+            attn_outputs = [\n+                attention_interface(\n+                    self,\n+                    q,\n+                    k,\n+                    v,\n+                    attention_mask=None,\n+                    scaling=self.scaling,\n+                    dropout=0.0 if not self.training else self.attention_dropout,\n+                    is_causal=False,\n+                    **kwargs,\n+                )[0]\n+                for q, k, v in zip(*splits)\n+            ]\n+            attn_output = torch.cat(attn_outputs, dim=1)\n+\n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n+        attn_output = self.proj(attn_output)\n+        return attn_output\n+\n+\n+class Qwen3VLMoeVisionBlock(GradientCheckpointingLayer):\n+    def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n+        super().__init__()\n+        self.norm1 = nn.LayerNorm(config.hidden_size, eps=1e-6)\n+        self.norm2 = nn.LayerNorm(config.hidden_size, eps=1e-6)\n+        self.attn = Qwen3VLMoeVisionAttention(config=config)\n+        self.mlp = Qwen3VLMoeVisionMLP(config=config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        rotary_pos_emb: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        hidden_states = hidden_states + self.attn(\n+            self.norm1(hidden_states),\n+            cu_seqlens=cu_seqlens,\n+            rotary_pos_emb=rotary_pos_emb,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = hidden_states + self.mlp(self.norm2(hidden_states))\n+        return hidden_states\n+\n+\n+class Qwen3VLMoeVisionModel(Qwen3VLMoePreTrainedModel):\n+    config: Qwen3VLMoeVisionConfig\n+    _no_split_modules = [\"Qwen3VLMoeVisionBlock\"]\n+\n+    def __init__(self, config, *inputs, **kwargs) -> None:\n+        super().__init__(config, *inputs, **kwargs)\n+        self.spatial_merge_size = config.spatial_merge_size\n+        self.patch_size = config.patch_size\n+        self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size\n+\n+        self.patch_embed = Qwen3VLMoeVisionPatchEmbed(\n+            config=config,\n+        )\n+\n+        self.pos_embed = nn.Embedding(config.num_position_embeddings, config.hidden_size)\n+        self.num_grid_per_side = int(config.num_position_embeddings**0.5)\n+\n+        head_dim = config.hidden_size // config.num_heads\n+        self.rotary_pos_emb = Qwen3VLMoeVisionRotaryEmbedding(head_dim // 2)\n+\n+        self.blocks = nn.ModuleList([Qwen3VLMoeVisionBlock(config) for _ in range(config.depth)])\n+        self.merger = Qwen3VLMoeVisionPatchMerger(\n+            config=config,\n+            use_postshuffle_norm=False,\n+        )\n+\n+        self.deepstack_visual_indexes = config.deepstack_visual_indexes\n+        self.deepstack_merger_list = nn.ModuleList(\n+            [\n+                Qwen3VLMoeVisionPatchMerger(\n+                    config=config,\n+                    use_postshuffle_norm=True,\n+                )\n+                for _ in range(len(config.deepstack_visual_indexes))\n+            ]\n+        )\n+\n+        self.gradient_checkpointing = False\n+\n+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n+        merge_size = self.spatial_merge_size\n+\n+        max_hw = int(grid_thw[:, 1:].max().item())\n+        freq_table = self.rotary_pos_emb(max_hw)  # (max_hw, dim // 2)\n+        device = freq_table.device\n+\n+        total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())\n+        pos_ids = torch.empty((total_tokens, 2), dtype=torch.long, device=device)\n+\n+        offset = 0\n+        for num_frames, height, width in grid_thw:\n+            merged_h, merged_w = height // merge_size, width // merge_size\n+\n+            block_rows = torch.arange(merged_h, device=device)  # block row indices\n+            block_cols = torch.arange(merged_w, device=device)  # block col indices\n+            intra_row = torch.arange(merge_size, device=device)  # intra-block row offsets\n+            intra_col = torch.arange(merge_size, device=device)  # intra-block col offsets\n+\n+            # Compute full-resolution positions\n+            row_idx = block_rows[:, None, None, None] * merge_size + intra_row[None, None, :, None]\n+            col_idx = block_cols[None, :, None, None] * merge_size + intra_col[None, None, None, :]\n+\n+            row_idx = row_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)\n+            col_idx = col_idx.expand(merged_h, merged_w, merge_size, merge_size).reshape(-1)\n+\n+            coords = torch.stack((row_idx, col_idx), dim=-1)\n+\n+            if num_frames > 1:\n+                coords = coords.repeat(num_frames, 1)\n+\n+            num_tokens = coords.shape[0]\n+            pos_ids[offset : offset + num_tokens] = coords\n+            offset += num_tokens\n+\n+        embeddings = freq_table[pos_ids]  # lookup rotary embeddings\n+        embeddings = embeddings.flatten(1)\n+        return embeddings\n+\n+    def fast_pos_embed_interpolate(self, grid_thw):\n+        grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+\n+        idx_list = [[] for _ in range(4)]\n+        weight_list = [[] for _ in range(4)]\n+\n+        for t, h, w in zip(grid_ts, grid_hs, grid_ws):\n+            h_idxs = torch.linspace(0, self.num_grid_per_side - 1, h)\n+            w_idxs = torch.linspace(0, self.num_grid_per_side - 1, w)\n+\n+            h_idxs_floor = h_idxs.int()\n+            w_idxs_floor = w_idxs.int()\n+            h_idxs_ceil = (h_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)\n+            w_idxs_ceil = (w_idxs.int() + 1).clip(max=self.num_grid_per_side - 1)\n+\n+            dh = h_idxs - h_idxs_floor\n+            dw = w_idxs - w_idxs_floor\n+\n+            base_h = h_idxs_floor * self.num_grid_per_side\n+            base_h_ceil = h_idxs_ceil * self.num_grid_per_side\n+\n+            indices = [\n+                (base_h[None].T + w_idxs_floor[None]).flatten(),\n+                (base_h[None].T + w_idxs_ceil[None]).flatten(),\n+                (base_h_ceil[None].T + w_idxs_floor[None]).flatten(),\n+                (base_h_ceil[None].T + w_idxs_ceil[None]).flatten(),\n+            ]\n+\n+            weights = [\n+                ((1 - dh)[None].T * (1 - dw)[None]).flatten(),\n+                ((1 - dh)[None].T * dw[None]).flatten(),\n+                (dh[None].T * (1 - dw)[None]).flatten(),\n+                (dh[None].T * dw[None]).flatten(),\n+            ]\n+\n+            for i in range(4):\n+                idx_list[i].extend(indices[i].tolist())\n+                weight_list[i].extend(weights[i].tolist())\n+\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n+        weight_tensor = torch.tensor(\n+            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n+        )\n+        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n+\n+        patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])\n+\n+        patch_pos_embeds_permute = []\n+        merge_size = self.config.spatial_merge_size\n+        for pos_embed, t, h, w in zip(patch_pos_embeds, grid_ts, grid_hs, grid_ws):\n+            pos_embed = pos_embed.repeat(t, 1)\n+            pos_embed = (\n+                pos_embed.view(t, h // merge_size, merge_size, w // merge_size, merge_size, -1)\n+                .permute(0, 1, 3, 2, 4, 5)\n+                .flatten(0, 4)\n+            )\n+            patch_pos_embeds_permute.append(pos_embed)\n+        patch_pos_embeds = torch.cat(patch_pos_embeds_permute)\n+        return patch_pos_embeds\n+\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n+                The final hidden states of the model.\n+            grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):\n+                The temporal, height and width of feature shape of each image in LLM.\n+\n+        Returns:\n+            `torch.Tensor`: hidden_states.\n+        \"\"\"\n+        hidden_states = self.patch_embed(hidden_states)\n+\n+        pos_embeds = self.fast_pos_embed_interpolate(grid_thw)\n+        hidden_states = hidden_states + pos_embeds\n+\n+        rotary_pos_emb = self.rot_pos_emb(grid_thw)\n+\n+        seq_len, _ = hidden_states.size()\n+        hidden_states = hidden_states.reshape(seq_len, -1)\n+        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n+\n+        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n+            dim=0,\n+            # Select dtype based on the following factors:\n+            #  - FA2 requires that cu_seqlens_q must have dtype int32\n+            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw\n+            # See https://github.com/huggingface/transformers/pull/34852 for more information\n+            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n+        )\n+        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n+\n+        deepstack_feature_lists = []\n+        for layer_num, blk in enumerate(self.blocks):\n+            hidden_states = blk(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+            if layer_num in self.deepstack_visual_indexes:\n+                deepstack_feature = self.deepstack_merger_list[self.deepstack_visual_indexes.index(layer_num)](\n+                    hidden_states\n+                )\n+                deepstack_feature_lists.append(deepstack_feature)\n+\n+        hidden_states = self.merger(hidden_states)\n+\n+        return hidden_states, deepstack_feature_lists\n+\n+\n+class Qwen3VLMoeTextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Qwen3VLMoeTextConfig, device=None):\n+        super().__init__()\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", \"default\")\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+        self.mrope_section = config.rope_scaling.get(\"mrope_section\", [24, 20, 20])\n+\n+    def apply_interleaved_mrope(self, freqs, mrope_section):\n+        \"\"\"Apply interleaved MRoPE to 3D rotary embeddings.\n+        Reorganizes frequency layout from chunked [TTT...HHH...WWW] to\n+        interleaved [THTHWHTHW...TT], preserving frequency continuity.\n+        args:\n+            x: (3, bs, seq_len, head_dim // 2)\n+            mrope_section: (3,)\n+        returns:\n+            x_t: (bs, seq_len, head_dim // 2)\n+        \"\"\"\n+        freqs_t = freqs[0]  # just overwrite the first dimension T\n+        for dim, offset in enumerate((1, 2), start=1):  # H, W\n+            length = mrope_section[dim] * 3\n+            idx = slice(offset, length, 3)\n+            freqs_t[..., idx] = freqs[dim, ..., idx]\n+        return freqs_t\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        # In contrast to other models, Qwen3VLMoe has different position ids for the grids\n+        # So we expand the inv_freq to shape (3, ...)\n+        if position_ids.ndim == 2:\n+            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n+        inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n+        position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)\n+            freqs = self.apply_interleaved_mrope(freqs, self.mrope_section)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@auto_docstring(\n+    custom_intro=(\n+        \"Text part of Qwen3VLMoe, \"\n+        \"not a pure text-only model, as DeepStack integrates visual features into the early hidden states.\"\n+    )\n+)\n+class Qwen3VLMoeTextModel(Qwen3VLMoePreTrainedModel):\n+    config: Qwen3VLMoeTextConfig\n+    _no_split_modules = [\"Qwen3VLMoeTextDecoderLayer\"]\n+\n+    def __init__(self, config: Qwen3VLMoeTextConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Qwen3VLMoeTextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Qwen3VLMoeTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Qwen3VLMoeTextRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        # args for deepstack\n+        visual_pos_masks: Optional[torch.Tensor] = None,\n+        deepstack_visual_embeds: Optional[list[torch.Tensor]] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        visual_pos_masks (`torch.Tensor` of shape `(batch_size, seqlen)`, *optional*):\n+            The mask of the visual positions.\n+        deepstack_visual_embeds (`list[torch.Tensor]`, *optional*):\n+            The deepstack visual embeddings. The shape is (num_layers, visual_seqlen, embed_dim).\n+            The feature is extracted from the different visual encoder layers, and fed to the decoder\n+            hidden states. It's from the paper DeepStack(https://arxiv.org/abs/2406.04334).\n+        \"\"\"\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        # torch.jit.trace() doesn't support cache objects in the output\n+        if use_cache and past_key_values is None and not torch.jit.is_tracing():\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        # the hard coded `3` is for temporal, height and width.\n+        if position_ids is None:\n+            position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n+        elif position_ids.ndim == 2:\n+            position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n+\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            text_position_ids = position_ids[0]\n+\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=text_position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for layer_idx, decoder_layer in enumerate(self.layers):\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=text_position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+            hidden_states = layer_outputs\n+\n+            # add visual features to the hidden states of first several layers\n+            if deepstack_visual_embeds is not None and layer_idx in range(len(deepstack_visual_embeds)):\n+                hidden_states = self._deepstack_process(\n+                    hidden_states,\n+                    visual_pos_masks,\n+                    deepstack_visual_embeds[layer_idx],\n+                )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+    def _deepstack_process(\n+        self, hidden_states: torch.Tensor, visual_pos_masks: torch.Tensor, visual_embeds: torch.Tensor\n+    ):\n+        visual_pos_masks = visual_pos_masks.to(hidden_states.device)\n+        visual_embeds = visual_embeds.to(hidden_states.device, hidden_states.dtype)\n+        local_this = hidden_states[visual_pos_masks, :].clone() + visual_embeds\n+        hidden_states[visual_pos_masks, :] = local_this\n+        return hidden_states\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class Qwen3VLMoeModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+\n+\n+@auto_docstring\n+class Qwen3VLMoeModel(Qwen3VLMoePreTrainedModel):\n+    base_model_prefix = \"\"\n+    _checkpoint_conversion_mapping = {}\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n+    config: Qwen3VLMoeConfig\n+    _no_split_modules = [\"Qwen3VLMoeTextDecoderLayer\", \"Qwen3VLMoeVisionBlock\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.visual = Qwen3VLMoeVisionModel._from_config(config.vision_config)\n+        self.language_model = Qwen3VLMoeTextModel._from_config(config.text_config)\n+        self.rope_deltas = None  # cache rope_deltas here\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n+    def get_rope_index(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"Different from the original implementation, Qwen3VLMoe use timestamps rather than absolute time position ids.\"\"\"\n+\n+        # Since we use timestamps to seperate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split\n+        if video_grid_thw is not None:\n+            video_grid_thw = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)\n+            video_grid_thw[:, 0] = 1\n+\n+        spatial_merge_size = self.config.vision_config.spatial_merge_size\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+        vision_start_token_id = self.config.vision_start_token_id\n+        mrope_position_deltas = []\n+        if input_ids is not None and (image_grid_thw is not None or video_grid_thw is not None):\n+            total_input_ids = input_ids\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(total_input_ids)\n+            position_ids = torch.ones(\n+                3,\n+                input_ids.shape[0],\n+                input_ids.shape[1],\n+                dtype=input_ids.dtype,\n+                device=input_ids.device,\n+            )\n+            image_index, video_index = 0, 0\n+            attention_mask = attention_mask.to(total_input_ids.device)\n+            for i, input_ids in enumerate(total_input_ids):\n+                input_ids = input_ids[attention_mask[i] == 1]\n+                image_nums, video_nums = 0, 0\n+                vision_start_indices = torch.argwhere(input_ids == vision_start_token_id).squeeze(1)\n+                vision_tokens = input_ids[vision_start_indices + 1]\n+                image_nums = (vision_tokens == image_token_id).sum()\n+                video_nums = (vision_tokens == video_token_id).sum()\n+                input_tokens = input_ids.tolist()\n+                llm_pos_ids_list: list = []\n+                st = 0\n+                remain_images, remain_videos = image_nums, video_nums\n+                for _ in range(image_nums + video_nums):\n+                    if image_token_id in input_tokens and remain_images > 0:\n+                        ed_image = input_tokens.index(image_token_id, st)\n+                    else:\n+                        ed_image = len(input_tokens) + 1\n+                    if video_token_id in input_tokens and remain_videos > 0:\n+                        ed_video = input_tokens.index(video_token_id, st)\n+                    else:\n+                        ed_video = len(input_tokens) + 1\n+                    if ed_image < ed_video:\n+                        t, h, w = (\n+                            image_grid_thw[image_index][0],\n+                            image_grid_thw[image_index][1],\n+                            image_grid_thw[image_index][2],\n+                        )\n+                        image_index += 1\n+                        remain_images -= 1\n+                        ed = ed_image\n+\n+                    else:\n+                        t, h, w = (\n+                            video_grid_thw[video_index][0],\n+                            video_grid_thw[video_index][1],\n+                            video_grid_thw[video_index][2],\n+                        )\n+                        video_index += 1\n+                        remain_videos -= 1\n+                        ed = ed_video\n+                    llm_grid_t, llm_grid_h, llm_grid_w = (\n+                        t.item(),\n+                        h.item() // spatial_merge_size,\n+                        w.item() // spatial_merge_size,\n+                    )\n+                    text_len = ed - st\n+\n+                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0\n+                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n+\n+                    # t_index is always 0 because llm_grid_t is always 1 (we use timestamps to encode the temporal information for videos)\n+                    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(-1, llm_grid_h * llm_grid_w).flatten()\n+                    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()\n+                    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()\n+                    llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)\n+                    st = ed + llm_grid_t * llm_grid_h * llm_grid_w\n+\n+                if st < len(input_tokens):\n+                    st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0\n+                    text_len = len(input_tokens) - st\n+                    llm_pos_ids_list.append(torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx)\n+\n+                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)\n+                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(position_ids.device)\n+                mrope_position_deltas.append(llm_positions.max() + 1 - len(total_input_ids[i]))\n+            mrope_position_deltas = torch.tensor(mrope_position_deltas, device=input_ids.device).unsqueeze(1)\n+            return position_ids, mrope_position_deltas\n+        else:\n+            if attention_mask is not None:\n+                position_ids = attention_mask.long().cumsum(-1) - 1\n+                position_ids.masked_fill_(attention_mask == 0, 1)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(attention_mask.device)\n+                max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]\n+                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]\n+            else:\n+                position_ids = (\n+                    torch.arange(input_ids.shape[1], device=input_ids.device)\n+                    .view(1, 1, -1)\n+                    .expand(3, input_ids.shape[0], -1)\n+                )\n+                mrope_position_deltas = torch.zeros(\n+                    [input_ids.shape[0], 1],\n+                    device=input_ids.device,\n+                    dtype=input_ids.dtype,\n+                )\n+\n+            return position_ids, mrope_position_deltas\n+\n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model. The deepstack visual features are also returned.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        # Same implementation as for images\n+        return self.get_image_features(pixel_values_videos, video_grid_thw)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model. The deepstack visual features are also returned.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each image in LLM.\n+        \"\"\"\n+        pixel_values = pixel_values.type(self.visual.dtype)\n+        image_embeds, deepstack_image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n+        split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()\n+        image_embeds = torch.split(image_embeds, split_sizes)\n+        return image_embeds, deepstack_image_embeds\n+\n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Qwen3VLMoeModelOutputWithPast]:\n+        r\"\"\"\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each video in LLM.\n+        \"\"\"\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        image_mask = None\n+        video_mask = None\n+\n+        if pixel_values is not None:\n+            image_embeds, deepstack_image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+        if pixel_values_videos is not None:\n+            video_embeds, deepstack_video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            _, video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+\n+        visual_pos_masks = None\n+        deepstack_visual_embeds = None\n+        if image_mask is not None and video_mask is not None:\n+            # aggregate visual_pos_masks and deepstack_visual_embeds\n+            image_mask = image_mask[..., 0]\n+            video_mask = video_mask[..., 0]\n+            visual_pos_masks = image_mask | video_mask\n+            deepstack_visual_embeds = []\n+            image_mask_joint = image_mask[visual_pos_masks]\n+            video_mask_joint = video_mask[visual_pos_masks]\n+            for img_embed, vid_embed in zip(deepstack_image_embeds, deepstack_video_embeds):\n+                embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1]).to(img_embed.device)\n+                embed_joint[image_mask_joint, :] = img_embed\n+                embed_joint[video_mask_joint, :] = vid_embed\n+                deepstack_visual_embeds.append(embed_joint)\n+        elif image_mask is not None:\n+            image_mask = image_mask[..., 0]\n+            visual_pos_masks = image_mask\n+            deepstack_visual_embeds = deepstack_image_embeds\n+        elif video_mask is not None:\n+            video_mask = video_mask[..., 0]\n+            visual_pos_masks = video_mask\n+            deepstack_visual_embeds = deepstack_video_embeds\n+\n+        if position_ids is None:\n+            attention_mask_tensor = (\n+                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n+            )\n+            if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n+                attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n+                # Only apply conversion for floating point tensors (inverted masks)\n+                if attention_mask_tensor.dtype.is_floating_point:\n+                    attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n+                    attention_mask_tensor = (1.0 - attention_mask_tensor).int()\n+\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            prefill_compiled_stage = is_torchdynamo_compiling() and (\n+                (input_ids is not None and input_ids.shape[1] != 1)\n+                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n+            )\n+            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n+                (cache_position is not None and cache_position[0] == 0)\n+                or (past_key_values is None or past_key_values.get_seq_length() == 0)\n+            )\n+            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n+                position_ids, rope_deltas = self.get_rope_index(\n+                    input_ids,\n+                    image_grid_thw,\n+                    video_grid_thw,\n+                    attention_mask=attention_mask_tensor,\n+                )\n+                self.rope_deltas = rope_deltas\n+            # then use the prev pre-calculated rope-deltas to get the correct position ids\n+            else:\n+                batch_size, seq_length, _ = inputs_embeds.shape\n+                delta = (\n+                    (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)\n+                    if cache_position is not None\n+                    else 0\n+                )\n+                position_ids = torch.arange(seq_length, device=inputs_embeds.device)\n+                position_ids = position_ids.view(1, -1).expand(batch_size, -1)\n+                if cache_position is not None:  # otherwise `deltas` is an int `0`\n+                    delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)\n+                position_ids = position_ids.add(delta)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n+\n+        outputs = self.language_model(\n+            input_ids=None,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            visual_pos_masks=visual_pos_masks,\n+            deepstack_visual_embeds=deepstack_visual_embeds,\n+            **kwargs,\n+        )\n+\n+        return Qwen3VLMoeModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=self.rope_deltas,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Qwen3VLMoe causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class Qwen3VLMoeCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+\n+\n+class Qwen3VLMoeForConditionalGeneration(Qwen3VLMoePreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n+    config: Qwen3VLMoeConfig\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Qwen3VLMoeModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        return self.model.get_video_features(pixel_values_videos, video_grid_thw)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        return self.model.get_image_features(pixel_values, image_grid_thw)\n+\n+    # Make modules available through conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def visual(self):\n+        return self.model.visual\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Qwen3VLMoeCausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each video in LLM.\n+\n+        Example:\n+            TODO: Add example\n+        \"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        return Qwen3VLMoeCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=outputs.rope_deltas,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        pixel_values=None,\n+        pixel_values_videos=None,\n+        image_grid_thw=None,\n+        video_grid_thw=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+\n+        # Qwen3VLMoe position_ids are prepareed with rope_deltas in forward\n+        model_inputs[\"position_ids\"] = None\n+\n+        if cache_position[0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_values_videos\"] = None\n+\n+        return model_inputs\n+\n+    def _get_image_nums_and_video_nums(\n+        self,\n+        input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n+        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.\n+\n+        Args:\n+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+                Indices of input sequence tokens in the vocabulary.\n+\n+        Returns:\n+            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)\n+            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)\n+        \"\"\"\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+        vision_start_token_id = self.config.vision_start_token_id\n+\n+        if inputs_embeds is not None:\n+            vision_start_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(vision_start_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            image_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            video_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+        else:\n+            vision_start_mask = input_ids == vision_start_token_id\n+            image_mask = input_ids == image_token_id\n+            video_mask = input_ids == video_token_id\n+\n+        vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)\n+        image_nums = torch.sum(vision_first_mask & image_mask, dim=1)\n+        video_nums = torch.sum(vision_first_mask & video_mask, dim=1)\n+\n+        return image_nums, video_nums\n+\n+    def _expand_inputs_for_generation(\n+        self,\n+        expand_size: int = 1,\n+        is_encoder_decoder: bool = False,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        **model_kwargs,\n+    ) -> tuple[torch.LongTensor, dict[str, Any]]:\n+        # Overwritten -- Support for expanding tensors without a batch size dimension\n+        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t\n+        # pixel_values.shape[0] is sum(seqlen_images for samples)\n+        # image_grid_thw.shape[0] is sum(num_images for samples)\n+\n+        if expand_size == 1:\n+            return input_ids, model_kwargs\n+\n+        visual_keys = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\", \"second_per_grid_ts\"]\n+\n+        def _expand_dict_for_generation_visual(dict_to_expand):\n+            image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n+            video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(\n+                input_ids, inputs_embeds=model_kwargs.get(\"inputs_embeds\", None)\n+            )\n+\n+            def _repeat_interleave_samples(x, lengths, repeat_times):\n+                samples = torch.split(x, lengths)\n+                repeat_args = [repeat_times] + [1] * (x.dim() - 1)\n+                result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)\n+                return result\n+\n+            for key in dict_to_expand:\n+                if key == \"pixel_values\":\n+                    # split images into samples\n+                    samples = torch.split(image_grid_thw, list(image_nums))\n+                    # compute the sequence length of images for each sample\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"image_grid_thw\":\n+                    # get the num of images for each sample\n+                    lengths = list(image_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"pixel_values_videos\":\n+                    samples = torch.split(video_grid_thw, list(video_nums))\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_grid_thw\":\n+                    lengths = list(video_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"second_per_grid_ts\":\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=list(video_nums), repeat_times=expand_size\n+                    )\n+            return dict_to_expand\n+\n+        def _expand_dict_for_generation(dict_to_expand):\n+            for key in dict_to_expand:\n+                if (\n+                    key != \"cache_position\"\n+                    and dict_to_expand[key] is not None\n+                    and isinstance(dict_to_expand[key], torch.Tensor)\n+                    and key not in visual_keys\n+                ):\n+                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n+            return dict_to_expand\n+\n+        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+\n+        if input_ids is not None:\n+            input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n+\n+        model_kwargs = _expand_dict_for_generation(model_kwargs)\n+\n+        if is_encoder_decoder:\n+            if model_kwargs.get(\"encoder_outputs\") is None:\n+                raise ValueError(\"If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.\")\n+            model_kwargs[\"encoder_outputs\"] = _expand_dict_for_generation(model_kwargs[\"encoder_outputs\"])\n+\n+        return input_ids, model_kwargs\n+\n+\n+__all__ = [\n+    \"Qwen3VLMoeVisionModel\",\n+    \"Qwen3VLMoeForConditionalGeneration\",\n+    \"Qwen3VLMoeModel\",\n+    \"Qwen3VLMoePreTrainedModel\",\n+    \"Qwen3VLMoeTextModel\",\n+]"
        },
        {
            "sha": "456d7c60aa89537153cd2d6b5006e1808e26da4a",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "added",
            "additions": 434,
            "deletions": 0,
            "changes": 434,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,434 @@\n+# coding=utf-8\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Qwen3-VL-MOE model.\"\"\"\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import logging\n+from ..qwen3_moe.modeling_qwen3_moe import (\n+    Qwen3MoeDecoderLayer,\n+    Qwen3MoePreTrainedModel,\n+    Qwen3MoeRMSNorm,\n+)\n+from ..qwen3_vl.configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLVisionConfig\n+from ..qwen3_vl.modeling_qwen3_vl import (\n+    Qwen3VLForConditionalGeneration,\n+    Qwen3VLModel,\n+    Qwen3VLTextAttention,\n+    Qwen3VLTextModel,\n+    Qwen3VLVisionModel,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Qwen3VLMoeTextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3VLMoeTextModel`]. It is used to instantiate a\n+    Qwen3-VL-MOE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen3-VL-30B-A3B-Instruct [Qwen/Qwen3-VL-30B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 151936):\n+            Vocabulary size of the Qwen2MoE model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Qwen2MoeModel`]\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 5632):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 16):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 128000):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_theta (`float`, *optional*, defaults to 5000000.0):\n+            The base period of the RoPE embeddings.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        decoder_sparse_step (`int`, *optional*, defaults to 1):\n+            The frequency of the MoE layer.\n+        moe_intermediate_size (`int`, *optional*, defaults to 1408):\n+            Intermediate size of the routed expert.\n+        num_experts_per_tok (`int`, *optional*, defaults to 4):\n+            Number of selected experts.\n+        num_experts (`int`, *optional*, defaults to 60):\n+            Number of routed experts.\n+        norm_topk_prob (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the topk probabilities.\n+        mlp_only_layers (`List[int]`, *optional*, defaults to `[]`):\n+            Indicate which layers use Qwen3VLMoeMLP rather than Qwen3VLMoeSparseMoeBlock\n+            The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n+            If `mlp_only_layers` is empty, `decoder_sparse_step` is used to determine the sparsity.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        head_dim (`int`, *optional*):\n+            The dimension of the head. If not specified, will default to `hidden_size // num_attention_heads`.\n+\n+    ```python\n+    >>> from transformers import Qwen3VLMoeForConditionalGeneration, Qwen3VLMoeConfig\n+\n+    >>> # Initializing a Qwen3VLMoe style configuration\n+    >>> configuration = Qwen3VLMoeConfig()\n+\n+    >>> # Initializing a model from the Qwen3-VL-30B-A3B style configuration\n+    >>> model = Qwen3VLMoeForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen3_vl_moe_text\"\n+    base_config_key = \"text_config\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `Qwen3VLMoe`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=151936,\n+        hidden_size=2048,\n+        intermediate_size=5632,\n+        num_hidden_layers=24,\n+        num_attention_heads=16,\n+        num_key_value_heads=16,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=128000,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n+        rope_theta=5000000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        decoder_sparse_step=1,\n+        moe_intermediate_size=1408,\n+        num_experts_per_tok=4,\n+        num_experts=60,\n+        norm_topk_prob=True,\n+        mlp_only_layers=None,\n+        rope_scaling=None,\n+        head_dim=None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.rope_scaling = rope_scaling\n+        self.head_dim = head_dim or hidden_size // num_attention_heads\n+\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"mrope_interleaved\"})\n+\n+        # MoE arguments\n+        self.decoder_sparse_step = decoder_sparse_step\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.norm_topk_prob = norm_topk_prob\n+        self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n+\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+\n+\n+class Qwen3VLMoeVisionConfig(Qwen3VLVisionConfig):\n+    pass\n+\n+\n+class Qwen3VLMoeConfig(Qwen3VLConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3VLMoeModel`]. It is used to instantiate a\n+    Qwen3-VL-MOE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    Qwen3-VL-30B-A3B-Instruct [Qwen/Qwen3-VL-30B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen3VLMoeTextConfig`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen3VLMoeVisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 151655):\n+            The image token index to encode the image prompt.\n+        video_token_id (`int`, *optional*, defaults to 151656):\n+            The video token index to encode the image prompt.\n+        vision_start_token_id (`int`, *optional*, defaults to 151652):\n+            The start token index to encode the image prompt.\n+        vision_end_token_id (`int`, *optional*, defaults to 151653):\n+            The end token index to encode the image prompt.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie the word embeddings.\n+\n+    ```python\n+    >>> from transformers import Qwen3VLMoeForConditionalGeneration, Qwen3VLMoeConfig\n+\n+    >>> # Initializing a Qwen3-VL-MOE style configuration\n+    >>> configuration = Qwen3VLMoeConfig()\n+\n+    >>> # Initializing a model from the Qwen3-VL-30B-A3B style configuration\n+    >>> model = Qwen3VLMoeForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"qwen3_vl_moe\"\n+    sub_configs = {\"vision_config\": Qwen3VLMoeVisionConfig, \"text_config\": Qwen3VLMoeTextConfig}\n+\n+\n+class Qwen3VLMoeTextRMSNorm(Qwen3MoeRMSNorm):\n+    pass\n+\n+\n+class Qwen3VLMoeTextRouter(nn.Linear):\n+    def __init__(self, config):\n+        super().__init__(config.hidden_size, config.num_experts, bias=False)\n+        self.hidden_size = config.hidden_size\n+        self.top_k = config.num_experts_per_tok\n+        # since all the models use norm_topk_prob, we don't need to have a extra check for it\n+        # self.norm_topk_prob = config.norm_topk_prob\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.reshape(-1, self.hidden_size)\n+        router_logits = super().forward(hidden_states)\n+        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n+        routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n+        return router_weights, router_logits, router_indices\n+\n+\n+class Qwen3VLMoeTextExperts(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        self.intermediate_size = config.moe_intermediate_size\n+        self.hidden_size = config.hidden_size\n+        self.expert_dim = self.intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n+        self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, routing_weights: torch.Tensor, router_indices: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        When training it is more efficient to just loop over the experts and compute the output for each expert\n+        as otherwise the memory would explode.\n+\n+        For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs.\n+\n+        Args:\n+            hidden_states (torch.Tensor): (batch_size * token_num, hidden_size)\n+            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n+            router_indices (torch.Tensor): (batch_size * token_num, top_k)\n+        Returns:\n+            torch.Tensor\n+        \"\"\"\n+        batch_size = hidden_states.shape[0]\n+        hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n+        if self.training:\n+            next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n+            with torch.no_grad():\n+                expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=self.num_experts)\n+                expert_mask = expert_mask.permute(2, 1, 0)\n+                # we sum on the top_k and on the sequence length to get which experts\n+                # are hit this time around\n+                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+            for expert_idx in expert_hit[:]:\n+                with torch.no_grad():\n+                    _, token_idx = torch.where(expert_mask[expert_idx[0]])\n+                current_state = hidden_states[token_idx]\n+                gate_up = current_state @ self.gate_up_proj[expert_idx]\n+                gate, up = gate_up.chunk(2, dim=-1)\n+                gated_output = up * self.act_fn(gate)\n+                out = gated_output @ self.down_proj[expert_idx]\n+                weighted_output = out[0] * routing_weights[token_idx, expert_idx, None]\n+                next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n+            next_states = next_states.view(batch_size, -1, self.hidden_size)\n+        else:\n+            hidden_states = hidden_states.repeat(self.num_experts, 1)\n+            hidden_states = hidden_states.view(self.num_experts, -1, self.hidden_size)\n+            gate_up = torch.bmm(hidden_states, self.gate_up_proj)\n+            gate, up = gate_up.chunk(2, dim=-1)  # not supported for DTensors\n+            next_states = torch.bmm((up * self.act_fn(gate)), self.down_proj)\n+            next_states = next_states.reshape(self.num_experts, batch_size, -1, self.hidden_size)\n+            next_states = (\n+                next_states * routing_weights.transpose(0, 1).view(self.num_experts, batch_size, -1)[..., None]\n+            )\n+            next_states = next_states.sum(dim=0)\n+        return next_states\n+\n+\n+class Qwen3VLMoeTextSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.num_experts = config.num_experts\n+        self.gate = Qwen3VLMoeTextRouter(config)\n+        self.experts = Qwen3VLMoeTextExperts(config)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        router_weights, router_logits, router_indices = self.gate(hidden_states)\n+        routed_out = self.experts(hidden_states, router_weights, router_indices)\n+        return routed_out, router_logits\n+\n+\n+class Qwen3VLMoeTextAttention(Qwen3VLTextAttention):\n+    pass\n+\n+\n+class Qwen3VLMoeTextDecoderLayer(Qwen3MoeDecoderLayer):\n+    pass\n+\n+\n+class Qwen3VLMoePreTrainedModel(Qwen3MoePreTrainedModel):\n+    config: Qwen3VLMoeConfig\n+    _no_split_modules = [\"Qwen3VLMoeTextDecoderLayer\", \"Qwen3VLMoeVisionBlock\"]\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights.\"\"\"\n+        PreTrainedModel._init_weights(self, module)\n+        if hasattr(self.config, \"initializer_range\"):\n+            std = self.config.initializer_range\n+        else:\n+            std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n+        if isinstance(module, Qwen3VLMoeTextExperts):\n+            module.gate_up_proj.data.normal_(mean=0.0, std=std)\n+            module.down_proj.data.normal_(mean=0.0, std=std)\n+\n+\n+class Qwen3VLMoeVisionModel(Qwen3VLVisionModel):\n+    pass\n+\n+\n+class Qwen3VLMoeTextModel(Qwen3VLTextModel):\n+    pass\n+\n+\n+class Qwen3VLMoeModel(Qwen3VLModel):\n+    pass\n+\n+\n+class Qwen3VLMoeForConditionalGeneration(Qwen3VLForConditionalGeneration):\n+    pass\n+\n+\n+__all__ = [\n+    \"Qwen3VLMoeConfig\",\n+    \"Qwen3VLMoeTextConfig\",\n+    \"Qwen3VLMoeVisionModel\",\n+    \"Qwen3VLMoeForConditionalGeneration\",\n+    \"Qwen3VLMoeModel\",\n+    \"Qwen3VLMoePreTrainedModel\",\n+    \"Qwen3VLMoeTextModel\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/qwen3_vl/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2F__init__.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2"
        },
        {
            "sha": "35031bf542aac470606a02ed1336ef1f55904dbd",
            "filename": "tests/models/qwen3_vl/test_modeling_qwen3_vl.py",
            "status": "added",
            "additions": 299,
            "deletions": 0,
            "changes": 299,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl%2Ftest_modeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl%2Ftest_modeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2Ftest_modeling_qwen3_vl.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,299 @@\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Qwen3-VL model.\"\"\"\n+\n+import copy\n+import unittest\n+\n+from transformers import (\n+    Qwen3VLConfig,\n+    Qwen3VLForConditionalGeneration,\n+    Qwen3VLModel,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    require_torch,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    ModelTesterMixin,\n+    floats_tensor,\n+    ids_tensor,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class Qwen3VLVisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        seq_length=7,\n+        num_channels=3,\n+        ignore_index=-100,\n+        image_size=16,\n+        text_config={\n+            \"bos_token_id\": 0,\n+            \"eos_token_id\": 1,\n+            \"pad_token_id\": 2,\n+            \"hidden_act\": \"silu\",\n+            \"head_dim\": 8,\n+            \"hidden_size\": 32,\n+            \"vocab_size\": 99,\n+            \"intermediate_size\": 37,\n+            \"max_position_embeddings\": 512,\n+            \"model_type\": \"qwen3_vl\",\n+            \"num_attention_heads\": 4,\n+            \"num_hidden_layers\": 4,\n+            \"num_key_value_heads\": 2,\n+            \"rope_theta\": 10000,\n+            \"tie_word_embeddings\": True,\n+            \"rope_scaling\": {\"rope_type\": \"default\", \"mrope_section\": [16, 8, 8], \"mrope_interleaved\": True},\n+        },\n+        vision_config={\n+            \"depth\": 2,\n+            \"in_chans\": 3,\n+            \"hidden_act\": \"gelu_pytorch_tanh\",\n+            \"intermediate_size\": 32,\n+            \"out_hidden_size\": 32,\n+            \"hidden_size\": 32,\n+            \"num_heads\": 4,\n+            \"patch_size\": 16,\n+            \"spatial_merge_size\": 1,\n+            \"temporal_patch_size\": 2,\n+            \"num_position_embeddings\": 16,\n+            \"deepstack_visual_indexes\": [0, 1],\n+        },\n+        image_token_id=3,\n+        video_token_id=4,\n+        vision_start_token_id=5,\n+        vision_end_token_id=6,\n+        tie_word_embeddings=True,\n+        is_training=True,\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.is_training = is_training\n+\n+        self.vision_config = vision_config\n+        self.text_config = text_config\n+\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.bos_token_id = text_config[\"bos_token_id\"]\n+        self.eos_token_id = text_config[\"eos_token_id\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+        self.head_dim = text_config[\"head_dim\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.intermediate_size = text_config[\"intermediate_size\"]\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.num_key_value_heads = text_config[\"num_key_value_heads\"]\n+        self.rope_theta = text_config[\"rope_theta\"]\n+        self.rope_scaling = text_config[\"rope_scaling\"]\n+        self.hidden_act = text_config[\"hidden_act\"]\n+        self.max_position_embeddings = text_config[\"max_position_embeddings\"]\n+        self.model_type = text_config[\"model_type\"]\n+\n+        self.vision_start_token_id = vision_start_token_id\n+        self.vision_end_token_id = vision_end_token_id\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+        self.tie_word_embeddings = tie_word_embeddings\n+\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.num_image_tokens = 32\n+        self.seq_length = seq_length + self.num_image_tokens\n+\n+    def get_config(self):\n+        return Qwen3VLConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            image_token_id=self.image_token_id,\n+            video_token_id=self.video_token_id,\n+            vision_start_token_id=self.vision_start_token_id,\n+            vision_end_token_id=self.vision_end_token_id,\n+            tie_word_embeddings=self.tie_word_embeddings,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        patch_size = config.vision_config.patch_size\n+        temporal_patch_size = config.vision_config.temporal_patch_size\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size * (self.image_size**2) // (patch_size**2),\n+                self.num_channels * (patch_size**2) * temporal_patch_size,\n+            ]\n+        )\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+\n+        input_ids[:, -1] = self.pad_token_id\n+        input_ids[input_ids == self.video_token_id] = self.pad_token_id\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[input_ids == self.vision_start_token_id] = self.pad_token_id\n+        input_ids[:, self.num_image_tokens] = self.image_token_id\n+        input_ids[:, self.num_image_tokens - 1] = self.vision_start_token_id\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size, device=torch_device),\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Qwen3VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `Qwen3VLForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (\n+        (\n+            Qwen3VLModel,\n+            Qwen3VLForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    test_pruning = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = Qwen3VLVisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Qwen3VLConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)\n+\n+            # remove one image but leave the image token in text\n+            patch_size = config.vision_config.patch_size\n+            one_img_length = (self.model_tester.image_size**2) // (patch_size**2)\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-one_img_length:, ...]\n+            curr_input_dict[\"image_grid_thw\"] = curr_input_dict[\"image_grid_thw\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**curr_input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:one_img_length]\n+            image_grid_thw = curr_input_dict[\"image_grid_thw\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(\n+                    input_ids=input_ids,\n+                    pixel_values=pixel_values,\n+                    image_grid_thw=image_grid_thw,\n+                )\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            image_grid_thw = torch.cat([image_grid_thw, image_grid_thw], dim=0)\n+            _ = model(\n+                input_ids=input_ids,\n+                pixel_values=pixel_values,\n+                image_grid_thw=image_grid_thw,\n+            )\n+\n+    def test_video_forward(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        B = self.model_tester.batch_size\n+        C = config.vision_config.in_chans\n+        T = config.vision_config.temporal_patch_size\n+        P = config.vision_config.patch_size\n+\n+        input_ids = ids_tensor([B, self.model_tester.seq_length], self.model_tester.vocab_size)\n+\n+        F = 4\n+        patch_H = self.model_tester.image_size // P\n+        patch_W = self.model_tester.image_size // P\n+        patch_T = F // T\n+        patches_per_video = patch_T * patch_H * patch_W\n+        pathed_per_frame = patch_H * patch_W\n+        pixel_values_videos = floats_tensor(\n+            [\n+                # first dim: batch_size * num_patches\n+                B * patches_per_video,\n+                # second dim: in_channels * temporal_patch_size * patch_size^2\n+                C * T * (P**2),\n+            ]\n+        )\n+\n+        # qwen3vl use timestamps for video, so split it into patch_T sub-videos\n+        video_grid_thw = torch.tensor([[1, patch_H, patch_W] for _ in range(patch_T)] * B)\n+\n+        # sanity check\n+        self.assertEqual(pixel_values_videos.shape[0], video_grid_thw.prod(dim=1).sum().item())\n+\n+        # Insert video token sequence\n+        input_ids[:, -1] = self.model_tester.pad_token_id\n+        input_ids[input_ids == self.model_tester.video_token_id] = self.model_tester.pad_token_id\n+        input_ids[input_ids == self.model_tester.image_token_id] = self.model_tester.pad_token_id\n+        input_ids[input_ids == self.model_tester.vision_start_token_id] = self.model_tester.pad_token_id\n+        input_ids[:, self.model_tester.num_image_tokens] = self.model_tester.video_token_id\n+\n+        insertion_point = self.model_tester.num_image_tokens\n+\n+        self.assertLessEqual((B * patches_per_video) + insertion_point, self.model_tester.seq_length)\n+        for b in range(B):\n+            # each frame is separated by a vision_start_token_id\n+            for frame_idx in range(patch_T):\n+                input_ids[b, insertion_point + frame_idx * (pathed_per_frame + 1)] = (\n+                    self.model_tester.vision_start_token_id\n+                )\n+                input_ids[\n+                    b,\n+                    insertion_point + frame_idx * (pathed_per_frame + 1) + 1 : insertion_point\n+                    + (frame_idx + 1) * (pathed_per_frame + 1),\n+                ] = self.model_tester.video_token_id\n+\n+        for model_class in self.all_model_classes:\n+            # TODO:we should remove this because we use timestamps for video\n+            model = model_class(config).to(torch_device)\n+            outputs = model(\n+                input_ids=input_ids,\n+                pixel_values_videos=pixel_values_videos,\n+                video_grid_thw=video_grid_thw,\n+            )\n+            self.assertIsNotNone(outputs)"
        },
        {
            "sha": "87636dcf607d98a4a81ce16ef05834329ecccfe1",
            "filename": "tests/models/qwen3_vl/test_processing_qwen3_vl.py",
            "status": "added",
            "additions": 379,
            "deletions": 0,
            "changes": 379,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,379 @@\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import pytest\n+\n+from transformers import AutoProcessor, Qwen2TokenizerFast\n+from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import Qwen2VLImageProcessorFast, Qwen3VLProcessor\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@require_vision\n+@require_torch\n+@require_torchvision\n+@unittest.skip(\"The checkpoint is not yet released\")\n+class Qwen3VLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Qwen3VLProcessor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        processor = Qwen3VLProcessor.from_pretrained(\n+            \"Qwen/Qwen3-VL-4B-Instruct\", patch_size=4, max_pixels=56 * 56, min_pixels=28 * 28\n+        )\n+        processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_video_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n+\n+    def get_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n+    def test_save_load_pretrained_default(self):\n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        video_processor = self.get_video_processor()\n+\n+        processor = Qwen3VLProcessor(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+        processor = Qwen3VLProcessor.from_pretrained(self.tmpdirname, use_fast=True)\n+\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n+        self.assertEqual(processor.image_processor.to_json_string(), image_processor.to_json_string())\n+        self.assertIsInstance(processor.tokenizer, Qwen2TokenizerFast)\n+        self.assertIsInstance(processor.image_processor, Qwen2VLImageProcessorFast)\n+\n+    def test_image_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        video_processor = self.get_video_processor()\n+\n+        processor = Qwen3VLProcessor(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        input_image_proc = image_processor(image_input, return_tensors=\"pt\")\n+        input_processor = processor(images=image_input, text=\"dummy\", return_tensors=\"pt\")\n+\n+        for key in input_image_proc:\n+            self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)\n+\n+    def test_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        video_processor = self.get_video_processor()\n+\n+        processor = Qwen3VLProcessor(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=input_str, images=image_input)\n+\n+        self.assertListEqual(\n+            list(inputs.keys()),\n+            [\"input_ids\", \"attention_mask\", \"pixel_values\", \"image_grid_thw\"],\n+        )\n+\n+        # test if it raises when no input is passed\n+        with pytest.raises(ValueError):\n+            processor()\n+\n+        # test if it raises when no text is passed\n+        with pytest.raises(TypeError):\n+            processor(images=image_input)\n+\n+    def test_model_input_names(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        video_processor = self.get_video_processor()\n+\n+        processor = Qwen3VLProcessor(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        video_inputs = self.prepare_video_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, videos=video_inputs, do_sample_frames=False)\n+\n+        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n+\n+    @require_torch\n+    @require_av\n+    def _test_apply_chat_template(\n+        self,\n+        modality: str,\n+        batch_size: int,\n+        return_tensors: str,\n+        input_name: str,\n+        processor_name: str,\n+        input_data: list[str],\n+    ):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        if processor_name not in self.processor_class.attributes:\n+            self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n+\n+        batch_messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [{\"type\": \"text\", \"text\": \"Describe this.\"}],\n+                },\n+            ]\n+        ] * batch_size\n+\n+        # Test that jinja can be applied\n+        formatted_prompt = processor.apply_chat_template(batch_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), batch_size)\n+\n+        # Test that tokenizing with template and directly with `self.tokenizer` gives same output\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            batch_messages, add_generation_prompt=True, tokenize=True, return_tensors=return_tensors\n+        )\n+        add_special_tokens = True\n+        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n+            add_special_tokens = False\n+        tok_output = processor.tokenizer(\n+            formatted_prompt, return_tensors=return_tensors, add_special_tokens=add_special_tokens\n+        )\n+        expected_output = tok_output.input_ids\n+        self.assertListEqual(expected_output.tolist(), formatted_prompt_tokenized.tolist())\n+\n+        # Test that kwargs passed to processor's `__call__` are actually used\n+        tokenized_prompt_100 = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            truncation=True,\n+            return_tensors=return_tensors,\n+            max_length=100,\n+        )\n+        self.assertEqual(len(tokenized_prompt_100[0]), 100)\n+\n+        # Test that `return_dict=True` returns text related inputs in the dict\n+        out_dict_text = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+        )\n+        self.assertTrue(all(key in out_dict_text for key in [\"input_ids\", \"attention_mask\"]))\n+        self.assertEqual(len(out_dict_text[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict_text[\"attention_mask\"]), batch_size)\n+\n+        # Test that with modality URLs and `return_dict=True`, we get modality inputs in the dict\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": modality, \"url\": url}]\n+\n+        out_dict = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+            max_frames=2,  # by default no more than 2 frames, otherwise too slow\n+        )\n+        input_name = getattr(self, input_name)\n+        self.assertTrue(input_name in out_dict)\n+        self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n+\n+        if modality == \"video\":\n+            # qwen pixels don't scale with bs same way as other models, calculate expected video token count based on video_grid_thw\n+            expected_video_token_count = 0\n+            for thw in out_dict[\"video_grid_thw\"]:\n+                expected_video_token_count += thw[0] * thw[1] * thw[2]\n+            mm_len = expected_video_token_count\n+        else:\n+            mm_len = batch_size * 192\n+        self.assertEqual(len(out_dict[input_name]), mm_len)\n+\n+        return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}\n+        for k in out_dict:\n+            self.assertIsInstance(out_dict[k], return_tensor_to_type[return_tensors])\n+\n+    @require_av\n+    @unittest.skip(\"qwen3_vl can't sample frames from image frames directly, user can use `qwen-vl-utils`\")\n+    def test_apply_chat_template_video_1(self):\n+        pass\n+\n+    @require_av\n+    @unittest.skip(\"qwen3_vl can't sample frames from image frames directly, user can use `qwen-vl-utils`\")\n+    def test_apply_chat_template_video_2(self):\n+        pass\n+\n+    @require_av\n+    def test_apply_chat_template_video_frame_sampling(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\"},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Add video URL for return dict and load with `num_frames` arg\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+        }\n+        num_frames = 3\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            num_frames=num_frames,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 360)\n+\n+        # Load with `fps` arg\n+        fps = 1\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            fps=fps,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 900)\n+\n+        # Load with `fps` and `num_frames` args, should raise an error\n+        with self.assertRaises(ValueError):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                fps=fps,\n+                num_frames=num_frames,\n+            )\n+\n+        # Load without any arg should load the whole video\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 27000)\n+\n+        # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n+        # because we assume they come from one video\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": [\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+            ],\n+        }\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            do_sample_frames=False,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 160)\n+\n+    def test_kwargs_overrides_custom_image_processor_kwargs(self):\n+        processor = self.get_processor()\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=input_str, images=image_input, max_pixels=56 * 56 * 4, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 612)\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 100)"
        },
        {
            "sha": "9230f0f9502ef2ad75fd86b1f57fb4dba3b2dd68",
            "filename": "tests/models/qwen3_vl/test_video_processing_qwen3_vl.py",
            "status": "added",
            "additions": 330,
            "deletions": 0,
            "changes": 330,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl%2Ftest_video_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl%2Ftest_video_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2Ftest_video_processing_qwen3_vl.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,330 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n+\n+\n+if is_torch_available():\n+    from PIL import Image\n+\n+if is_vision_available() and is_torchvision_available():\n+    from transformers import Qwen3VLVideoProcessor\n+    from transformers.models.qwen3_vl.video_processing_qwen3_vl import smart_resize\n+\n+\n+class Qwen3VLVideoProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=5,\n+        num_frames=8,\n+        num_channels=3,\n+        min_resolution=32,\n+        max_resolution=80,\n+        temporal_patch_size=2,\n+        patch_size=16,\n+        merge_size=2,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        do_convert_rgb=True,\n+    ):\n+        size = size if size is not None else {\"longest_edge\": 20, \"shortest_edge\": 10}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_frames = num_frames\n+        self.num_channels = num_channels\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+        self.temporal_patch_size = temporal_patch_size\n+        self.patch_size = patch_size\n+        self.merge_size = merge_size\n+\n+    def prepare_video_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+            \"do_sample_frames\": True,\n+        }\n+\n+    def prepare_video_metadata(self, videos):\n+        video_metadata = []\n+        for video in videos:\n+            if isinstance(video, list):\n+                num_frames = len(video)\n+            elif hasattr(video, \"shape\"):\n+                if len(video.shape) == 4:  # (T, H, W, C)\n+                    num_frames = video.shape[0]\n+                else:\n+                    num_frames = 1\n+            else:\n+                num_frames = self.num_frames\n+\n+            metadata = {\n+                \"fps\": 2,\n+                \"duration\": num_frames / 2,\n+                \"total_num_frames\": num_frames,\n+            }\n+            video_metadata.append(metadata)\n+        return video_metadata\n+\n+    def expected_output_video_shape(self, videos):\n+        grid_t = self.num_frames // self.temporal_patch_size\n+        hidden_dim = self.num_channels * self.temporal_patch_size * self.patch_size * self.patch_size\n+        seq_len = 0\n+        for video in videos:\n+            if isinstance(video, list) and isinstance(video[0], Image.Image):\n+                video = np.stack([np.array(frame) for frame in video])\n+            elif hasattr(video, \"shape\"):\n+                pass\n+            else:\n+                video = np.array(video)\n+\n+            if hasattr(video, \"shape\") and len(video.shape) >= 3:\n+                if len(video.shape) == 4:\n+                    t, height, width = video.shape[:3]\n+                elif len(video.shape) == 3:\n+                    height, width = video.shape[:2]\n+                    t = 1\n+                else:\n+                    t, height, width = self.num_frames, self.min_resolution, self.min_resolution\n+            else:\n+                t, height, width = self.num_frames, self.min_resolution, self.min_resolution\n+\n+            resized_height, resized_width = smart_resize(\n+                t,\n+                height,\n+                width,\n+                factor=self.patch_size * self.merge_size,\n+                min_pixels=self.size[\"shortest_edge\"],\n+                max_pixels=self.size[\"longest_edge\"],\n+            )\n+            grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n+            seq_len += grid_t * grid_h * grid_w\n+        return [seq_len, hidden_dim]\n+\n+    def prepare_video_inputs(self, equal_resolution=False, return_tensors=\"pil\"):\n+        videos = prepare_video_inputs(\n+            batch_size=self.batch_size,\n+            num_frames=self.num_frames,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            return_tensors=return_tensors,\n+        )\n+        return videos\n+\n+\n+@require_torch\n+@require_vision\n+class Qwen3VLVideoProcessingTest(VideoProcessingTestMixin, unittest.TestCase):\n+    fast_video_processing_class = Qwen3VLVideoProcessor if is_torchvision_available() else None\n+    input_name = \"pixel_values_videos\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.video_processor_tester = Qwen3VLVideoProcessingTester(self)\n+\n+    @property\n+    def video_processor_dict(self):\n+        return self.video_processor_tester.prepare_video_processor_dict()\n+\n+    def test_video_processor_from_dict_with_kwargs(self):\n+        video_processor = self.fast_video_processing_class.from_dict(self.video_processor_dict)\n+        self.assertEqual(video_processor.size, {\"longest_edge\": 20, \"shortest_edge\": 10})\n+\n+        video_processor = self.fast_video_processing_class.from_dict(\n+            self.video_processor_dict, size={\"longest_edge\": 42, \"shortest_edge\": 42}\n+        )\n+        self.assertEqual(video_processor.size, {\"longest_edge\": 42, \"shortest_edge\": 42})\n+\n+    def test_call_pil(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"pil\"\n+            )\n+\n+            for video in video_inputs:\n+                self.assertIsInstance(video[0], Image.Image)\n+\n+            video_metadata = self.video_processor_tester.prepare_video_metadata(video_inputs)\n+            encoded_videos = video_processing(\n+                video_inputs[0], video_metadata=[video_metadata[0]], return_tensors=\"pt\"\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+            encoded_videos = video_processing(video_inputs, video_metadata=video_metadata, return_tensors=\"pt\")[\n+                self.input_name\n+            ]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_call_numpy(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"np\"\n+            )\n+\n+            video_metadata = self.video_processor_tester.prepare_video_metadata(video_inputs)\n+            encoded_videos = video_processing(\n+                video_inputs[0], video_metadata=[video_metadata[0]], return_tensors=\"pt\"\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+            encoded_videos = video_processing(video_inputs, video_metadata=video_metadata, return_tensors=\"pt\")[\n+                self.input_name\n+            ]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_call_pytorch(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"pt\"\n+            )\n+            video_metadata = self.video_processor_tester.prepare_video_metadata(video_inputs)\n+            encoded_videos = video_processing(\n+                video_inputs[0], video_metadata=[video_metadata[0]], return_tensors=\"pt\"\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+            encoded_videos = video_processing(video_inputs, video_metadata=video_metadata, return_tensors=\"pt\")[\n+                self.input_name\n+            ]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    @unittest.skip(\"Skip for now, the test needs adjustment for Qwen3VL\")\n+    def test_call_numpy_4_channels(self):\n+        for video_processing_class in self.video_processor_list:\n+            # Test that can process videos which have an arbitrary number of channels\n+            # Initialize video_processing\n+            video_processor = video_processing_class(**self.video_processor_dict)\n+\n+            # create random numpy tensors\n+            self.video_processor_tester.num_channels = 4\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"np\"\n+            )\n+\n+            # Test not batched input\n+            encoded_videos = video_processor(\n+                video_inputs[0],\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=0,\n+                image_std=1,\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+            # Test batched\n+            encoded_videos = video_processor(\n+                video_inputs,\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=0,\n+                image_std=1,\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_nested_input(self):\n+        \"\"\"Tests that the processor can work with nested list where each video is a list of arrays\"\"\"\n+        for video_processing_class in self.video_processor_list:\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"np\"\n+            )\n+\n+            video_inputs_nested = [list(video) for video in video_inputs]\n+            video_metadata = self.video_processor_tester.prepare_video_metadata(video_inputs)\n+\n+            # Test not batched input\n+            encoded_videos = video_processing(\n+                video_inputs_nested[0], video_metadata=[video_metadata[0]], return_tensors=\"pt\"\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+            # Test batched\n+            encoded_videos = video_processing(video_inputs_nested, video_metadata=video_metadata, return_tensors=\"pt\")[\n+                self.input_name\n+            ]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_call_sample_frames(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processor_dict = self.video_processor_dict.copy()\n+            video_processing = video_processing_class(**video_processor_dict)\n+\n+            prev_num_frames = self.video_processor_tester.num_frames\n+            self.video_processor_tester.num_frames = 8\n+            prev_min_resolution = getattr(self.video_processor_tester, \"min_resolution\", None)\n+            prev_max_resolution = getattr(self.video_processor_tester, \"max_resolution\", None)\n+            self.video_processor_tester.min_resolution = 56\n+            self.video_processor_tester.max_resolution = 112\n+\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False,\n+                return_tensors=\"torch\",\n+            )\n+\n+            metadata = [[{\"total_num_frames\": 8, \"fps\": 4}]]\n+            batched_metadata = metadata * len(video_inputs)\n+\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", video_metadata=metadata)[\n+                self.input_name\n+            ]\n+            encoded_videos_batched = video_processing(\n+                video_inputs, return_tensors=\"pt\", video_metadata=batched_metadata\n+            )[self.input_name]\n+\n+            self.assertIsNotNone(encoded_videos)\n+            self.assertIsNotNone(encoded_videos_batched)\n+            self.assertEqual(len(encoded_videos.shape), 2)\n+            self.assertEqual(len(encoded_videos_batched.shape), 2)\n+\n+            self.video_processor_tester.num_frames = prev_num_frames\n+            if prev_min_resolution is not None:\n+                self.video_processor_tester.min_resolution = prev_min_resolution\n+            if prev_max_resolution is not None:\n+                self.video_processor_tester.max_resolution = prev_max_resolution"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/qwen3_vl_moe/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl_moe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl_moe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl_moe%2F__init__.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2"
        },
        {
            "sha": "adae69a81fa832d1e805e8b94511d3d567bf450c",
            "filename": "tests/models/qwen3_vl_moe/test_modeling_qwen3_vl_moe.py",
            "status": "added",
            "additions": 298,
            "deletions": 0,
            "changes": 298,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -0,0 +1,298 @@\n+# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Qwen3VLMoe model.\"\"\"\n+\n+import copy\n+import unittest\n+\n+from transformers import (\n+    Qwen3VLMoeConfig,\n+    Qwen3VLMoeForConditionalGeneration,\n+    Qwen3VLMoeModel,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    require_torch,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    ModelTesterMixin,\n+    floats_tensor,\n+    ids_tensor,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class Qwen3VLMoeVisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        seq_length=7,\n+        num_channels=3,\n+        ignore_index=-100,\n+        image_size=16,\n+        text_config={\n+            \"bos_token_id\": 0,\n+            \"eos_token_id\": 1,\n+            \"pad_token_id\": 2,\n+            \"hidden_act\": \"silu\",\n+            \"hidden_size\": 32,\n+            \"vocab_size\": 99,\n+            \"intermediate_size\": 37,\n+            \"max_position_embeddings\": 512,\n+            \"model_type\": \"qwen3_vl_moe\",\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 2,\n+            \"num_hidden_layers\": 4,\n+            \"moe_intermediate_size\": 16,\n+            \"num_experts_per_tok\": 4,\n+            \"num_experts\": 8,\n+            \"rope_theta\": 10000,\n+            \"tie_word_embeddings\": True,\n+            \"rope_scaling\": {\"rope_type\": \"default\", \"mrope_section\": [16, 8, 8], \"mrope_interleaved\": True},\n+        },\n+        vision_config={\n+            \"depth\": 2,\n+            \"in_chans\": 3,\n+            \"hidden_act\": \"gelu_pytorch_tanh\",\n+            \"intermediate_size\": 32,\n+            \"out_hidden_size\": 32,\n+            \"hidden_size\": 32,\n+            \"num_heads\": 4,\n+            \"patch_size\": 16,\n+            \"spatial_merge_size\": 1,\n+            \"temporal_patch_size\": 2,\n+            \"num_position_embeddings\": 16,\n+            \"deepstack_visual_indexes\": [0, 1],\n+        },\n+        image_token_id=3,\n+        video_token_id=4,\n+        vision_start_token_id=5,\n+        vision_end_token_id=6,\n+        tie_word_embeddings=True,\n+        is_training=True,\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.is_training = is_training\n+\n+        self.vision_config = vision_config\n+        self.text_config = text_config\n+\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.bos_token_id = text_config[\"bos_token_id\"]\n+        self.eos_token_id = text_config[\"eos_token_id\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.intermediate_size = text_config[\"intermediate_size\"]\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.num_key_value_heads = text_config[\"num_key_value_heads\"]\n+        self.rope_theta = text_config[\"rope_theta\"]\n+        self.rope_scaling = text_config[\"rope_scaling\"]\n+        self.hidden_act = text_config[\"hidden_act\"]\n+        self.max_position_embeddings = text_config[\"max_position_embeddings\"]\n+        self.model_type = text_config[\"model_type\"]\n+\n+        self.vision_start_token_id = vision_start_token_id\n+        self.vision_end_token_id = vision_end_token_id\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+        self.tie_word_embeddings = tie_word_embeddings\n+\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.num_image_tokens = 32\n+        self.seq_length = seq_length + self.num_image_tokens\n+\n+    def get_config(self):\n+        return Qwen3VLMoeConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            image_token_id=self.image_token_id,\n+            video_token_id=self.video_token_id,\n+            vision_start_token_id=self.vision_start_token_id,\n+            vision_end_token_id=self.vision_end_token_id,\n+            tie_word_embeddings=self.tie_word_embeddings,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        patch_size = config.vision_config.patch_size\n+        temporal_patch_size = config.vision_config.temporal_patch_size\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size * (self.image_size**2) // (patch_size**2),\n+                self.num_channels * (patch_size**2) * temporal_patch_size,\n+            ]\n+        )\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+\n+        input_ids[:, -1] = self.pad_token_id\n+        input_ids[input_ids == self.video_token_id] = self.pad_token_id\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[input_ids == self.vision_start_token_id] = self.pad_token_id\n+        input_ids[:, self.num_image_tokens] = self.image_token_id\n+        input_ids[:, self.num_image_tokens - 1] = self.vision_start_token_id\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size, device=torch_device),\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Qwen3VLMoeModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `Qwen3VLMoeForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (\n+        (\n+            Qwen3VLMoeModel,\n+            Qwen3VLMoeForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    test_pruning = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = Qwen3VLMoeVisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Qwen3VLMoeConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)\n+\n+            # remove one image but leave the image token in text\n+            patch_size = config.vision_config.patch_size\n+            one_img_length = (self.model_tester.image_size**2) // (patch_size**2)\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-one_img_length:, ...]\n+            curr_input_dict[\"image_grid_thw\"] = curr_input_dict[\"image_grid_thw\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**curr_input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:one_img_length]\n+            image_grid_thw = curr_input_dict[\"image_grid_thw\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(\n+                    input_ids=input_ids,\n+                    pixel_values=pixel_values,\n+                    image_grid_thw=image_grid_thw,\n+                )\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            image_grid_thw = torch.cat([image_grid_thw, image_grid_thw], dim=0)\n+            _ = model(\n+                input_ids=input_ids,\n+                pixel_values=pixel_values,\n+                image_grid_thw=image_grid_thw,\n+            )\n+\n+    def test_video_forward(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        B = self.model_tester.batch_size\n+        C = config.vision_config.in_chans\n+        T = config.vision_config.temporal_patch_size\n+        P = config.vision_config.patch_size\n+\n+        input_ids = ids_tensor([B, self.model_tester.seq_length], self.model_tester.vocab_size)\n+\n+        F = 4\n+        patch_H = self.model_tester.image_size // P\n+        patch_W = self.model_tester.image_size // P\n+        patch_T = F // T\n+        patches_per_video = patch_T * patch_H * patch_W\n+        pathed_per_frame = patch_H * patch_W\n+        pixel_values_videos = floats_tensor(\n+            [\n+                # first dim: batch_size * num_patches\n+                B * patches_per_video,\n+                # second dim: in_channels * temporal_patch_size * patch_size^2\n+                C * T * (P**2),\n+            ]\n+        )\n+        video_grid_thw = torch.tensor([[1, patch_H, patch_W] for _ in range(patch_T)] * B)\n+\n+        # sanity check\n+        self.assertEqual(pixel_values_videos.shape[0], video_grid_thw.prod(dim=1).sum().item())\n+\n+        # Insert video token sequence\n+        input_ids[:, -1] = self.model_tester.pad_token_id\n+        input_ids[input_ids == self.model_tester.video_token_id] = self.model_tester.pad_token_id\n+        input_ids[input_ids == self.model_tester.image_token_id] = self.model_tester.pad_token_id\n+        input_ids[input_ids == self.model_tester.vision_start_token_id] = self.model_tester.pad_token_id\n+        input_ids[:, self.model_tester.num_image_tokens] = self.model_tester.video_token_id\n+\n+        insertion_point = self.model_tester.num_image_tokens\n+\n+        self.assertLessEqual((B * patches_per_video) + insertion_point, self.model_tester.seq_length)\n+        for b in range(B):\n+            # each frame is separated by a vision_start_token_id\n+            for frame_idx in range(patch_T):\n+                input_ids[b, insertion_point + frame_idx * (pathed_per_frame + 1)] = (\n+                    self.model_tester.vision_start_token_id\n+                )\n+                input_ids[\n+                    b,\n+                    insertion_point + frame_idx * (pathed_per_frame + 1) + 1 : insertion_point\n+                    + (frame_idx + 1) * (pathed_per_frame + 1),\n+                ] = self.model_tester.video_token_id\n+\n+        for model_class in self.all_model_classes:\n+            # TODO:we should remove this because we use timestamps for video\n+            model = model_class(config).to(torch_device)\n+            outputs = model(\n+                input_ids=input_ids,\n+                pixel_values_videos=pixel_values_videos,\n+                video_grid_thw=video_grid_thw,\n+            )\n+            self.assertIsNotNone(outputs)"
        },
        {
            "sha": "e932e5bfc24cebf0d3895e67d63f22eb2ca4df40",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 13,
            "deletions": 7,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0dbe095b0fdceff933dbca7978ed76716d97fb2/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0dbe095b0fdceff933dbca7978ed76716d97fb2/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=c0dbe095b0fdceff933dbca7978ed76716d97fb2",
            "patch": "@@ -71,6 +71,8 @@\n     \"Qwen2AudioEncoder\",\n     \"Qwen2VisionTransformerPretrainedModel\",\n     \"Qwen2_5_VisionTransformerPretrainedModel\",\n+    \"Qwen3VLVisionModel\",\n+    \"Qwen3VLMoeVisionModel\",\n     \"SwitchTransformersStack\",\n     \"TFDPRSpanPredictor\",\n     \"MaskFormerSwinModel\",\n@@ -151,13 +153,17 @@\n         \"ChameleonVQVAE\",  # VQVAE here is used only for encoding (discretizing) and is tested as part of bigger model\n         \"Qwen2VLModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2VLForConditionalGeneration.\n         \"Qwen2_5_VLModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5_VLForConditionalGeneration.\n-        \"Qwen2_5OmniForConditionalGeneration\",  # Not a regular model. Testted in Qwen2_5OmniModelIntegrationTest\n-        \"Qwen2_5OmniTalkerForConditionalGeneration\",  #  Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n-        \"Qwen2_5OmniTalkerModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n-        \"Qwen2_5OmniThinkerTextModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n-        \"Qwen2_5OmniToken2WavModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n-        \"Qwen2_5OmniToken2WavDiTModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n-        \"Qwen2_5OmniToken2WavBigVGANModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntegrationTest.\n+        \"Qwen3VLModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen3VLForConditionalGeneration.\n+        \"Qwen3VLMoeModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen3VLMoeForConditionalGeneration.\n+        \"Qwen3VLTextModel\",  # Building part of bigger (tested) model.\n+        \"Qwen3VLMoeTextModel\",  # Building part of bigger (tested) model.\n+        \"Qwen2_5OmniForConditionalGeneration\",  # Not a regular model. Testted in Qwen2_5OmniModelIntergrationTest\n+        \"Qwen2_5OmniTalkerForConditionalGeneration\",  #  Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n+        \"Qwen2_5OmniTalkerModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n+        \"Qwen2_5OmniThinkerTextModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n+        \"Qwen2_5OmniToken2WavModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n+        \"Qwen2_5OmniToken2WavDiTModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n+        \"Qwen2_5OmniToken2WavBigVGANModel\",  # Building part of bigger (tested) model. Tested implicitly through Qwen2_5OmniModelIntergrationTest.\n         \"MllamaTextModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"MllamaVisionModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"Llama4TextModel\",  # Building part of bigger (tested) model. # TODO: add tests"
        }
    ],
    "stats": {
        "total": 8046,
        "additions": 8039,
        "deletions": 7
    }
}