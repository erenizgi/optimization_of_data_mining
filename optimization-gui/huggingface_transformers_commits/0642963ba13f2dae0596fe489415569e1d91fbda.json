{
    "author": "IlyasMoutawwakil",
    "message": "batched and grouped experts implementations (#42697)\n\n* meo implementation\n\n* support more MoEs\n\n* tests\n\n* add comments\n\n* add grouped_mm support\n\n* typing act_fn and adding stride 16 note\n\n* style\n\n* fix dbrx config\n\n* fix config test\n\n* add licence and better stride conditions\n\n* comment\n\n* no need to pad tesnors to 16 byte strides if we made sure our tiny testing models have 16 byte aligned weights\n\n* use a class decorator with a registration interface\n\n* remove line\n\n* remove unnecessary\n\n* register config with the decorator\n\n* fix redundant\n\n* reduce changes some more\n\n* fix\n\n* fix\n\n* import from integrations\n\n* remove empty lines\n\n* use histc instead of bincount\n\n* fix cpu histc not supporting long\n\n* docs\n\n* added benchmark to docs\n\n* add to from_pretrained's docstring\n\n* make grouped_mm the deafault when possible\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/experts_interface.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestion from @stevhliu\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestion from @stevhliu\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* make qwen3 vl moe inherit its experts and sparse moe blocks from qwen3 moe, making it use experts implementation\n\n* create _supports_grouped_mm flag and use it for testing\n\n* fix copies\n\n* better grouped mm checks\n\n* fix model size failure\n\n* better docs\n\n* get rid of class property _supports_grouped_mm\n\n* add method calling checks and fix models that didn't have experts\n\n* fix copies\n\n* fix\n\n* fix\n\n* more cleanup\n\n* clean\n\n* document compilation behaviour\n\n* docs\n\n* fix new moe after merge\n\n* fix the new ernie 4.5 vl moe testing\n\n* support fullgraph automatic compilation for MoEs\n\n* fix lazy initialization\n\n* disable fullgraph for granitemoe and jetmoe because of topk gating\n\n* avoid implicit fallback in experts implementation and only do it when auto-compiling\n\n* style\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "0642963ba13f2dae0596fe489415569e1d91fbda",
    "files": [
        {
            "sha": "231eb15262ca1a792fa10bb881872d2a1d1c4ad4",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -74,6 +74,8 @@\n       title: Overview\n     - local: attention_interface\n       title: Attention backends\n+    - local: experts_interface\n+      title: Experts backends\n     - local: continuous_batching\n       title: Continuous batching\n     - local: kernel_doc/overview"
        },
        {
            "sha": "18947e030f01453114eb57e8f2b787063493df41",
            "filename": "docs/source/en/experts_interface.md",
            "status": "added",
            "additions": 166,
            "deletions": 0,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/docs%2Fsource%2Fen%2Fexperts_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/docs%2Fsource%2Fen%2Fexperts_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fexperts_interface.md?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -0,0 +1,166 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Experts backends\n+\n+All Mixture-of-Experts (MoE) implementations perform the same high-level computation. For each token, a router selects *k* experts. The token hidden state is then projected through the selected experts' parameters and aggregated with routing weights. The difference between experts backends is *how* those expert matrix multiplications execute.\n+\n+The [`ExpertsInterface`] provides optimized experts backends. It decouples the experts implementation from the model code to simplify experimentation with different functions. Add new backends through the same interface.\n+\n+| experts backend | description                                                                                                                                  |\n+| --------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n+| `\"eager\"`       | Reference implementation that loops over active experts and applies projections per-expert.                                                  |\n+| `\"batched_mm\"`  | Uses [torch.bmm](https://docs.pytorch.org/docs/stable/generated/torch.bmm.html) to compute per-(token, expert) projections in a batched way. |\n+| `\"grouped_mm\"`  | Uses `torch._grouped_mm` to group tokens by expert and run grouped GEMMs (requires PyTorch 2.9+).                                            |\n+\n+`batched_mm` is fastest for very small inputs and compilation speeds it up further. `grouped_mm` performs best for larger inputs.\n+\n+## Set an experts backend\n+\n+Use the `experts_implementation` argument in [`~PreTrainedModel.from_pretrained`] to instantiate a model with a specific experts backend.\n+\n+```py\n+from transformers import AutoModelForCausalLM\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"Qwen/Qwen1.5-MoE-A2.7B\",\n+    dtype=\"bfloat16\",\n+    experts_implementation=\"batched_mm\",\n+)\n+```\n+\n+Switch between experts backends at runtime without reloading the model using [`~PreTrainedModel.set_experts_implementation`].\n+\n+```py\n+model.set_experts_implementation(\"eager\")\n+```\n+\n+## Backbone-specific experts backend\n+\n+Multimodal models can have multiple sub-configs (for example, different backbones). You can set a different experts backend per sub-config by passing a `dict` to `experts_implementation` at load time.\n+\n+Keys in the mapping must match sub-config names.\n+\n+```py\n+from transformers import AutoModelForImageTextToText\n+\n+experts_implementation_per_backbone = {\n+    \"text_config\": \"grouped_mm\",\n+    \"vision_config\": \"eager\",\n+}\n+\n+model = AutoModelForImageTextToText.from_pretrained(\n+    \"Qwen/Qwen3-VL-Moe\",\n+    experts_implementation=experts_implementation_per_backbone,\n+)\n+```\n+\n+Set the experts backend globally with an empty key.\n+\n+```py\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"Qwen/Qwen1.5-MoE-A2.7B\",\n+    experts_implementation={\"\": \"batched_mm\"},\n+)\n+```\n+\n+## torch.compile\n+\n+All three backends (`\"eager\"`, `\"batched_mm\"`, `\"grouped_mm\"`) are compatible with `torch.compile` to certain extents. The following table summarizes compatibility:\n+\n+| Implementation | compilation modes                    | dtypes                           | `fullgraph=True` |\n+| -------------- | ------------------------------------ | -------------------------------- | ---------------- |\n+| `grouped_mm`   | `None`, `max-autotune-no-cudagraphs` | `bfloat16`                       | Yes              |\n+| `batched_mm`   | all                                  | `bfloat16`, `float16`, `float32` | Yes              |\n+| `eager`        | all                                  | `bfloat16`, `float16`, `float32` | No               |\n+\n+Notes:\n+\n+- The `grouped_mm` experts backend currently only supports `bfloat16` when compiled with `torch.compile`. Additionally, it is not compatible with CUDA graphs, so you must use `mode=None` or `mode=\"max-autotune-no-cudagraphs\"` when compiling.\n+- The `eager` experts backend uses a data-dependent operation to find which experts are used in a forward pass. This operation is not compatible with full graph compilation (`fullgraph=True`).\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"Qwen/Qwen1.5-MoE-A2.7B\",\n+    dtype=\"bfloat16\",\n+    experts_implementation=\"grouped_mm\",\n+).eval().cuda()\n+\n+# Works for grouped_mm (no CUDA graphs)\n+model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n+```\n+\n+## Benchmarks\n+\n+This [benchmark](https://github.com/user-attachments/files/24125816/bench.py) compares different input sizes and experts implementations with and without `torch.compile`.\n+\n+### Batch Size 1, Sequence Length 16\n+\n+| Torch Compile              | Implementation | Mean Latency (ms)                            | Median Latency (ms)                          | P90 Latency (ms)                             | Peak Mem (MB) |\n+| -------------------------- | -------------- | -------------------------------------------- | -------------------------------------------- | -------------------------------------------- | ------------- |\n+| False                      | eager          | 271.80                                       | 272.94                                       | 295.34                                       | 27324.65      |\n+| True                       | eager          | 351.86                                       | 351.64                                       | 384.64                                       | 27329.29      |\n+| max-autotune-no-cudagraphs | eager          | 352.52                                       | 352.15                                       | 382.79                                       | 27329.29      |\n+| False                      | batched_mm     | 52.03                                        | 52.07                                        | 52.67                                        | 28382.50      |\n+| True                       | batched_mm     | 53.04                                        | 53.04                                        | 53.11                                        | 28029.63      |\n+| max-autotune-no-cudagraphs | batched_mm     | **<span style=\"color: green;\">23.87</span>** | **<span style=\"color: green;\">23.86</span>** | **<span style=\"color: green;\">24.02</span>** | **27329.29**  |\n+| False                      | grouped_mm     | 64.27                                        | 64.09                                        | 65.49                                        | 27329.29      |\n+| True                       | grouped_mm     | 59.45                                        | 59.52                                        | 60.99                                        | 27329.29      |\n+| max-autotune-no-cudagraphs | grouped_mm     | 59.61                                        | 59.55                                        | 60.89                                        | 27329.29      |\n+\n+### Batch Size 1, Sequence Length 128\n+\n+| Torch Compile              | Implementation | Mean Latency (ms)                            | Median Latency (ms)                          | P90 Latency (ms)                             | Peak Mem (MB) |\n+| -------------------------- | -------------- | -------------------------------------------- | -------------------------------------------- | -------------------------------------------- | ------------- |\n+| False                      | eager          | 471.73                                       | 472.65                                       | 487.97                                       | 27396.46      |\n+| True                       | eager          | <span style=\"color: red;\">637.32</span>      | 613.70                                       | <span style=\"color: red;\">845.01</span>      | 27429.82      |\n+| max-autotune-no-cudagraphs | eager          | 620.21                                       | 619.35                                       | 657.74                                       | 27429.82      |\n+| False                      | batched_mm     | 316.67                                       | 316.94                                       | 317.92                                       | 35854.56      |\n+| True                       | batched_mm     | 370.29                                       | 370.29                                       | 370.57                                       | 33031.64      |\n+| max-autotune-no-cudagraphs | batched_mm     | 151.87                                       | 150.38                                       | 158.01                                       | 27429.82      |\n+| False                      | grouped_mm     | 78.50                                        | 78.53                                        | 80.00                                        | **27429.82**  |\n+| True                       | grouped_mm     | 72.95                                        | 72.99                                        | 74.60                                        | **27429.82**  |\n+| max-autotune-no-cudagraphs | grouped_mm     | **<span style=\"color: green;\">72.71</span>** | **<span style=\"color: green;\">72.89</span>** | **<span style=\"color: green;\">73.55</span>** | **27429.82**  |\n+\n+### Batch Size 4, Sequence Length 16\n+\n+| Torch Compile              | Implementation | Mean Latency (ms)                            | Median Latency (ms)                          | P90 Latency (ms)                             | Peak Mem (MB) |\n+| -------------------------- | -------------- | -------------------------------------------- | -------------------------------------------- | -------------------------------------------- | ------------- |\n+| False                      | eager          | 431.87                                       | 433.38                                       | 448.01                                       | 27391.57      |\n+| True                       | eager          | <span style=\"color: red;\">566.63</span>      | <span style=\"color: red;\">569.74</span>      | <span style=\"color: red;\">598.98</span>      | 27372.12      |\n+| max-autotune-no-cudagraphs | eager          | 563.13                                       | 567.79                                       | 588.25                                       | 27372.12      |\n+| False                      | batched_mm     | 163.41                                       | 163.38                                       | 164.84                                       | 31585.54      |\n+| True                       | batched_mm     | 189.18                                       | 189.08                                       | 189.79                                       | 30173.45      |\n+| max-autotune-no-cudagraphs | batched_mm     | 79.15                                        | 79.10                                        | 79.74                                        | 27372.11      |\n+| False                      | grouped_mm     | 75.23                                        | 75.18                                        | 76.74                                        | 27372.11      |\n+| True                       | grouped_mm     | 70.35                                        | 70.40                                        | 71.71                                        | **27372.12**  |\n+| max-autotune-no-cudagraphs | grouped_mm     | **<span style=\"color: green;\">70.26</span>** | **<span style=\"color: green;\">70.43</span>** | **<span style=\"color: green;\">71.32</span>** | **27372.12**  |\n+\n+### Batch Size 4, Sequence Length 128\n+\n+| Torch Compile              | Implementation | Mean Latency (ms)                            | Median Latency (ms)                          | P90 Latency (ms)                             | Peak Mem (MB)                             |\n+| -------------------------- | -------------- | -------------------------------------------- | -------------------------------------------- | -------------------------------------------- | ----------------------------------------- |\n+| False                      | eager          | 526.88                                       | 522.75                                       | 570.01                                       | 27632.62                                  |\n+| True                       | eager          | 678.18                                       | 677.54                                       | 690.97                                       | 27762.46                                  |\n+| max-autotune-no-cudagraphs | eager          | 676.22                                       | 677.07                                       | 681.91                                       | 27762.45                                  |\n+| False                      | batched_mm     | 1235.25                                      | 1235.33                                      | 1237.90                                      | <span style=\"color: red;\">61465.85</span> |\n+| True                       | batched_mm     | <span style=\"color: red;\">1505.00</span>     | <span style=\"color: red;\">1503.31</span>     | <span style=\"color: red;\">1536.10</span>     | 50174.26                                  |\n+| max-autotune-no-cudagraphs | batched_mm     | 572.37                                       | 570.81                                       | 589.74                                       | **27762.45**                              |\n+| False                      | grouped_mm     | 80.95                                        | 81.06                                        | 81.70                                        | **27762.45**                              |\n+| True                       | grouped_mm     | **<span style=\"color: green;\">79.67</span>** | **<span style=\"color: green;\">79.69</span>** | **<span style=\"color: green;\">80.54</span>** | **27762.45**                              |\n+| max-autotune-no-cudagraphs | grouped_mm     | 83.29                                        | 79.83                                        | 111.83                                       | **27762.46**                              |"
        },
        {
            "sha": "9f12b27d7cc19be93fec3f9bc636098278ff64a4",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -37,7 +37,7 @@ def __repr__(self):\n         return f\"{self.__class__.__name__}\"\n \n     @abstractmethod\n-    def lazy_initialization(self, key_states: torch.Tensor): ...\n+    def lazy_initialization(self, key_states: torch.Tensor, value_states: torch.Tensor) -> None: ...\n \n     @abstractmethod\n     def update(\n@@ -89,7 +89,7 @@ class DynamicLayer(CacheLayerMixin):\n \n     is_sliding = False\n \n-    def lazy_initialization(self, key_states: torch.Tensor):\n+    def lazy_initialization(self, key_states: torch.Tensor, value_states: torch.Tensor) -> None:\n         self.dtype, self.device = key_states.dtype, key_states.device\n         self.keys = torch.tensor([], dtype=self.dtype, device=self.device)\n         self.values = torch.tensor([], dtype=self.dtype, device=self.device)\n@@ -114,7 +114,7 @@ def update(\n         \"\"\"\n         # Lazy initialization\n         if not self.is_initialized:\n-            self.lazy_initialization(key_states)\n+            self.lazy_initialization(key_states, value_states)\n \n         self.keys = torch.cat([self.keys, key_states], dim=-2)\n         self.values = torch.cat([self.values, value_states], dim=-2)\n@@ -178,8 +178,8 @@ def __init__(self, sliding_window: int):\n         self.cumulative_length = 0\n         self._sliding_window_tensor = torch.tensor(self.sliding_window, dtype=torch.long)\n \n-    def lazy_initialization(self, key_states: torch.Tensor) -> None:\n-        super().lazy_initialization(key_states)\n+    def lazy_initialization(self, key_states: torch.Tensor, value_states: torch.Tensor) -> None:\n+        super().lazy_initialization(key_states, value_states)\n         self._sliding_window_tensor = self._sliding_window_tensor.to(self.device)\n \n     def update(\n@@ -201,7 +201,7 @@ def update(\n         \"\"\"\n         # Lazy initialization\n         if not self.is_initialized:\n-            self.lazy_initialization(key_states)\n+            self.lazy_initialization(key_states, value_states)\n \n         self.cumulative_length += key_states.shape[-2]\n \n@@ -267,7 +267,7 @@ def __init__(self, max_cache_len: int):\n         super().__init__()\n         self.max_cache_len = max_cache_len\n \n-    def lazy_initialization(self, key_states: torch.Tensor):\n+    def lazy_initialization(self, key_states: torch.Tensor, value_states: torch.Tensor) -> None:\n         \"\"\"\n         Lazy initialization of the keys and values tensors. This allows to get all properties (dtype, device,\n         num_heads in case of TP etc...) at runtime directly, which is extremely practical as it avoids moving\n@@ -281,16 +281,18 @@ def lazy_initialization(self, key_states: torch.Tensor):\n         i.e. `mode=\"reduce-overhead\"` is known to fail). But it will in general work correctly, and prefill should\n         not be compiled anyway for performances!\n         \"\"\"\n-        self.max_batch_size, self.num_heads, _, self.head_dim = key_states.shape\n         self.dtype, self.device = key_states.dtype, key_states.device\n+        self.max_batch_size, self.num_heads = key_states.shape[:2]\n+        self.v_head_dim = value_states.shape[-1]\n+        self.k_head_dim = key_states.shape[-1]\n \n         self.keys = torch.zeros(\n-            (self.max_batch_size, self.num_heads, self.max_cache_len, self.head_dim),\n+            (self.max_batch_size, self.num_heads, self.max_cache_len, self.k_head_dim),\n             dtype=self.dtype,\n             device=self.device,\n         )\n         self.values = torch.zeros(\n-            (self.max_batch_size, self.num_heads, self.max_cache_len, self.head_dim),\n+            (self.max_batch_size, self.num_heads, self.max_cache_len, self.v_head_dim),\n             dtype=self.dtype,\n             device=self.device,\n         )\n@@ -323,7 +325,7 @@ def update(\n         \"\"\"\n         # Lazy initialization\n         if not self.is_initialized:\n-            self.lazy_initialization(key_states)\n+            self.lazy_initialization(key_states, value_states)\n \n         # Some old models give None for `cache_position` or even omit passing `cache_kwargs` when used as cross-attention,\n         # in which case we should copy the whole Layer (key_states.shape[-2] == self.max_cache_len)\n@@ -398,7 +400,7 @@ def update(\n         \"\"\"\n         # Lazy initialization\n         if not self.is_initialized:\n-            self.lazy_initialization(key_states)\n+            self.lazy_initialization(key_states, value_states)\n \n         # Some old models give None for `cache_position` or even omit passing `cache_kwargs` when used as cross-attention,\n         # in which case we should copy the whole Layer (key_states.shape[-2] == self.max_cache_len)\n@@ -533,7 +535,7 @@ def update(\n \n         # Lazy initialization\n         if not self.is_initialized:\n-            self.lazy_initialization(key_states)\n+            self.lazy_initialization(key_states, value_states)\n             self._quantized_keys = self._quantize(key_states.contiguous(), axis=self.axis_key)\n             self._quantized_values = self._quantize(value_states.contiguous(), axis=self.axis_value)\n             return key_states, value_states\n@@ -795,10 +797,10 @@ def early_initialization(\n         # Note that the initialization needs all dimensions (except -2), as well as device and dtype, so we use\n         # this fake tensor approach. It has size 0 on the -2 dimension, so it does not allocate any data (it only\n         # creates an empty tensor with correct shape, dtype and device), which is very efficient and practical\n-        fake_keys_tensor = torch.zeros((batch_size, num_heads, 0, head_dim), dtype=dtype, device=device)\n+        fake_kv_tensor = torch.zeros((batch_size, num_heads, 0, head_dim), dtype=dtype, device=device)\n         # Init all layers\n         for layer in self.layers:\n-            layer.lazy_initialization(fake_keys_tensor)\n+            layer.lazy_initialization(fake_kv_tensor, fake_kv_tensor)\n \n     def get_seq_length(self, layer_idx: int = 0) -> int:\n         \"\"\"Returns the sequence length of the cache for the given layer.\"\"\""
        },
        {
            "sha": "db917900cb3efcdfc238714bd75248510ef719f1",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 29,
            "deletions": 1,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -324,6 +324,9 @@ def __init__(\n         # Attention implementation to use, if relevant (it sets it recursively on sub-configs)\n         self._attn_implementation = kwargs.pop(\"attn_implementation\", None)\n \n+        # Experts implementation to use, if relevant (it sets it recursively on sub-configs)\n+        self._experts_implementation = kwargs.pop(\"experts_implementation\", None)\n+\n         # Drop the transformers version info\n         self.transformers_version = kwargs.pop(\"transformers_version\", None)\n \n@@ -417,6 +420,28 @@ def _attn_implementation(self, value: str | dict | None):\n                 )\n                 subconfig._attn_implementation = sub_implementation\n \n+    @property\n+    def _experts_implementation(self):\n+        return self._experts_implementation_internal\n+\n+    @_experts_implementation.setter\n+    def _experts_implementation(self, value: str | dict | None):\n+        \"\"\"We set it recursively on the sub-configs as well\"\"\"\n+        # Set if for current config\n+        current_moe = getattr(self, \"_experts_implementation\", None)\n+        experts_implementation = value if not isinstance(value, dict) else value.get(\"\", current_moe)\n+        self._experts_implementation_internal = experts_implementation\n+\n+        # Set it recursively on the subconfigs\n+        for subconfig_key in self.sub_configs:\n+            subconfig = getattr(self, subconfig_key, None)\n+            if subconfig is not None:\n+                current_subconfig_moe = getattr(subconfig, \"_experts_implementation\", None)\n+                sub_implementation = (\n+                    value if not isinstance(value, dict) else value.get(subconfig_key, current_subconfig_moe)\n+                )\n+                subconfig._experts_implementation = sub_implementation\n+\n     @property\n     def torch_dtype(self):\n         logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n@@ -756,8 +781,9 @@ def from_dict(\n             # If both are present, use `dtype`\n             kwargs[\"dtype\"] = kwargs.get(\"dtype\", torch_dtype)\n \n-        # We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\n+        # We remove them from kwargs so that they do not appear in `return_unused_kwargs`.\n         config_dict[\"attn_implementation\"] = kwargs.pop(\"attn_implementation\", None)\n+        config_dict[\"experts_implementation\"] = kwargs.pop(\"experts_implementation\", None)\n \n         config = cls(**config_dict)\n \n@@ -1082,6 +1108,8 @@ def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n             del d[\"_commit_hash\"]\n         if \"_attn_implementation_internal\" in d:\n             del d[\"_attn_implementation_internal\"]\n+        if \"_experts_implementation_internal\" in d:\n+            del d[\"_experts_implementation_internal\"]\n         # Do not serialize `base_model_tp_plan` for now\n         if \"base_model_tp_plan\" in d:\n             del d[\"base_model_tp_plan\"]"
        },
        {
            "sha": "ea0590c108b5f5d787c8c3fb577616e225b26d53",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -2178,8 +2178,10 @@ def _valid_auto_compile_criteria(self, model_kwargs: dict[str, Any], generation_\n                 \"will be skipped.\"\n             )\n \n-            # Finally: if we can compile, disable tokenizers parallelism and check for FA2 + static cache\n+        if can_compile:\n+            # Finally: if we can compile, disable tokenizers parallelism\n             os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n+\n             # If we use FA2 and a static cache, we cannot compile with fullgraph\n             if self.config._attn_implementation == \"flash_attention_2\":\n                 # only raise warning if the user passed an explicit compile-config\n@@ -2190,6 +2192,15 @@ def _valid_auto_compile_criteria(self, model_kwargs: dict[str, Any], generation_\n                     )\n                     generation_config.compile_config.fullgraph = False\n \n+            # If we use grouped_mm and dtype different than bfloat16, we fallback to batched_mm\n+            if self.config._experts_implementation == \"grouped_mm\":\n+                if self.dtype != torch.bfloat16:\n+                    logger.warning_once(\n+                        \"torch._grouped_mm currently only supports bfloat16 when being compiled with torch.compile. \"\n+                        \"Falling back to batched_mm implementation for compilation.\"\n+                    )\n+                    self.set_experts_implementation(\"batched_mm\")\n+\n         return can_compile\n \n     def _get_deprecated_gen_repo("
        },
        {
            "sha": "7cb6c644f460f61b6a6449dca520aa8f1c867c89",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -69,6 +69,7 @@\n     \"hqq\": [\"prepare_for_hqq_linear\"],\n     \"hub_kernels\": [\n         \"LayerRepository\",\n+        \"lazy_load_kernel\",\n         \"register_kernel_mapping\",\n         \"replace_kernel_forward_from_hub\",\n         \"use_kernel_forward_from_hub\",\n@@ -116,6 +117,11 @@\n         \"run_hp_search_ray\",\n         \"run_hp_search_wandb\",\n     ],\n+    \"moe\": [\n+        \"batched_mm_experts_forward\",\n+        \"grouped_mm_experts_forward\",\n+        \"use_experts_implementation\",\n+    ],\n     \"mxfp4\": [\n         \"Mxfp4GptOssExperts\",\n         \"convert_moe_packed_tensors\",\n@@ -211,6 +217,7 @@\n     from .hqq import prepare_for_hqq_linear\n     from .hub_kernels import (\n         LayerRepository,\n+        lazy_load_kernel,\n         register_kernel_mapping,\n         replace_kernel_forward_from_hub,\n         use_kernel_forward_from_hub,\n@@ -258,6 +265,11 @@\n         run_hp_search_ray,\n         run_hp_search_wandb,\n     )\n+    from .moe import (\n+        batched_mm_experts_forward,\n+        grouped_mm_experts_forward,\n+        use_experts_implementation,\n+    )\n     from .mxfp4 import (\n         Mxfp4GptOssExperts,\n         dequantize,"
        },
        {
            "sha": "87302c656be028160d97e9df5111459df24d6187",
            "filename": "src/transformers/integrations/moe.py",
            "status": "added",
            "additions": 240,
            "deletions": 0,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fintegrations%2Fmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fintegrations%2Fmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -0,0 +1,240 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from functools import wraps\n+\n+from ..utils.generic import GeneralInterface\n+from ..utils.import_utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+# Examples of experts class with its eager mm implementation\n+# class Experts(nn.Module):\n+#     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n+\n+#     def __init__(self, config):\n+#         super().__init__()\n+#         self.num_experts = config.n_routed_experts\n+#         self.hidden_dim = config.hidden_size\n+#         self.intermediate_dim = config.moe_intermediate_size\n+#         self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+#         self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+#         self.act_fn = ACT2FN[config.hidden_act]\n+\n+#     def forward(\n+#         self,\n+#         hidden_states: torch.Tensor,\n+#         top_k_index: torch.Tensor,\n+#         top_k_weights: torch.Tensor,\n+#     ) -> torch.Tensor:\n+#         final_hidden_states = torch.zeros_like(hidden_states)\n+#         with torch.no_grad():\n+#             expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n+#             expert_mask = expert_mask.permute(2, 1, 0)\n+#             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+\n+#         for expert_idx in expert_hit:\n+#             expert_idx = expert_idx[0]\n+#             if expert_idx == self.num_experts:\n+#                 continue\n+#             top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n+#             current_state = hidden_states[token_idx]\n+#             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+#             current_hidden_states = self.act_fn(gate) * up\n+#             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+#             current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n+#             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n+#         return final_hidden_states\n+\n+\n+def batched_mm_experts_forward(\n+    self: torch.nn.Module,\n+    hidden_states: torch.Tensor,\n+    top_k_index: torch.Tensor,\n+    top_k_weights: torch.Tensor,\n+) -> torch.Tensor:\n+    device = hidden_states.device\n+    num_top_k = top_k_index.size(-1)\n+    num_tokens = hidden_states.size(0)\n+    num_experts = self.gate_up_proj.size(0)\n+    final_hidden_states = torch.zeros_like(hidden_states)\n+\n+    # Flatten top_k_index to get expert_ids per selected sample\n+    expert_ids = top_k_index.reshape(-1)\n+    token_idx = torch.arange(num_tokens, device=device).unsqueeze(1).expand(-1, num_top_k).reshape(-1)\n+\n+    # Resolve routing weights per selected sample, allowing top_k_weights to be either:\n+    # - (num_tokens, num_top_k) Qwen2MoE style\n+    # - (num_tokens, num_experts) DeepseekV2 style\n+    if top_k_weights.shape == (num_tokens, num_top_k):\n+        sample_weights = top_k_weights.reshape(-1)  # (S,)\n+    elif top_k_weights.shape == (num_tokens, num_experts):\n+        sample_weights = top_k_weights[token_idx, expert_ids]  # (S,)\n+    else:\n+        raise ValueError(\n+            f\"top_k_weights has an invalid/unsupported shape. It should be either (num_tokens, num_top_k)({num_tokens}, {num_top_k}) \"\n+            f\"or (num_tokens, num_experts)({num_tokens}, {num_experts}), but got {top_k_weights.shape}.\"\n+        )\n+\n+    # Get current hidden states for selected samples\n+    current_hidden_states = hidden_states[token_idx]  # (S, hidden_dim)\n+\n+    # Select projection matrices for selected experts\n+    selected_gate_up = self.gate_up_proj[expert_ids]  # (S, hidden_dim, 2 * intermediate_dim)\n+    selected_down = self.down_proj[expert_ids]  # (S, hidden_dim, intermediate_dim)\n+\n+    # --- Up projection per expert (batched) ---\n+    gate_up_out = torch.bmm(selected_gate_up, current_hidden_states.unsqueeze(-1)).squeeze(-1)\n+    if hasattr(self, \"gate_up_proj_bias\") and self.gate_up_proj_bias is not None:\n+        gate_up_out = gate_up_out + self.gate_up_proj_bias[expert_ids]\n+\n+    # Split into gate and up components\n+    gate, up = gate_up_out.chunk(2, dim=-1)  # both have shape (S, intermediate_dim)\n+\n+    # Apply activation\n+    hidden_after_activation = self.act_fn(gate) * up  # (S, intermediate_dim)\n+\n+    # --- Down projection per expert (batched) ---\n+    out_per_sample = torch.bmm(selected_down, hidden_after_activation.unsqueeze(-1)).squeeze(-1)\n+    if hasattr(self, \"down_proj_bias\") and self.down_proj_bias is not None:\n+        out_per_sample = out_per_sample + self.down_proj_bias[expert_ids]\n+\n+    # Apply routing weights\n+    out_per_sample = out_per_sample * sample_weights.unsqueeze(-1)  # (S, hidden_dim)\n+\n+    # Accumulate results back to the final_hidden_states using original token indices\n+    final_hidden_states.index_add_(0, token_idx, out_per_sample.to(final_hidden_states.dtype))\n+\n+    return final_hidden_states\n+\n+\n+def grouped_mm_experts_forward(\n+    self: torch.nn.Module,\n+    hidden_states: torch.Tensor,\n+    top_k_index: torch.Tensor,\n+    top_k_weights: torch.Tensor,\n+) -> torch.Tensor:\n+    if not hasattr(torch, \"_grouped_mm\"):\n+        raise ImportError(\n+            \"torch._grouped_mm is not available. Please make sure you are using a PyTorch version that includes it (2.9+).\"\n+        )\n+\n+    device = hidden_states.device\n+    num_top_k = top_k_index.size(-1)\n+    num_tokens = hidden_states.size(0)\n+    num_experts = self.gate_up_proj.size(0)\n+    final_hidden_states = torch.zeros_like(hidden_states)\n+\n+    # Flatten top_k_index to get expert_ids per selected sample\n+    expert_ids = top_k_index.reshape(-1)\n+    token_idx = torch.arange(num_tokens, device=device).unsqueeze(1).expand(-1, num_top_k).reshape(-1)\n+\n+    # Get permutation to group by expert\n+    perm = torch.argsort(expert_ids, stable=True)\n+    inv_perm = torch.argsort(perm, stable=True)\n+\n+    # Resolve routing weights per selected sample, allowing top_k_weights to be either:\n+    # - (num_tokens, num_top_k) Qwen2MoE style\n+    # - (num_tokens, num_experts) DeepseekV2 style\n+    if top_k_weights.shape == (num_tokens, num_top_k):\n+        sample_weights = top_k_weights.reshape(-1)  # (S,)\n+    elif top_k_weights.shape == (num_tokens, num_experts):\n+        sample_weights = top_k_weights[token_idx, expert_ids]  # (S,)\n+    else:\n+        raise ValueError(\n+            f\"top_k_weights has an invalid/unsupported shape. It should be either (num_tokens, num_top_k)({num_tokens}, {num_top_k}) \"\n+            f\"or (num_tokens, num_experts)({num_tokens}, {num_experts}), but got {top_k_weights.shape}.\"\n+        )\n+\n+    # Get current hidden states for selected samples\n+    current_hidden_states = hidden_states[token_idx]  # (S, hidden_dim)\n+\n+    # Group by expert for grouped_mm\n+    expert_ids_g = expert_ids[perm]\n+    sample_weights_g = sample_weights[perm]\n+    current_states_g = current_hidden_states[perm]\n+\n+    # Compute offsets for grouped_mm\n+    # using histc instead of bincount to avoid cuda graph issues\n+    # (grouped_mm_experts_forward still fails with cuda graphs but because of _grouped_mm internals)\n+    num_tokens_per_expert = torch.histc(expert_ids_g.float(), bins=num_experts, min=0, max=num_experts - 1)\n+    offsets = torch.cumsum(num_tokens_per_expert, dim=0, dtype=torch.int32)\n+\n+    # --- Up projection per expert (grouped_mm) ---\n+    gate_up_out = torch._grouped_mm(current_states_g, self.gate_up_proj.transpose(-2, -1), offs=offsets)\n+    if hasattr(self, \"gate_up_proj_bias\") and self.gate_up_proj_bias is not None:\n+        # we should be able to pass bias to the grouped_mm call, but it's still not fully supported\n+        gate_up_out = gate_up_out + self.gate_up_proj_bias[expert_ids_g]\n+\n+    # Split into gate and up components\n+    gate, up = gate_up_out.chunk(2, dim=-1)  # both have shape (S, intermediate_dim)\n+\n+    # Apply activation\n+    hidden_after_activation = self.act_fn(gate) * up  # (S, intermediate_dim)\n+\n+    # --- Down projection per expert (grouped_mm) ---\n+    out_per_sample_g = torch._grouped_mm(hidden_after_activation, self.down_proj.transpose(-2, -1), offs=offsets)\n+    if hasattr(self, \"down_proj_bias\") and self.down_proj_bias is not None:\n+        # we should be able to pass bias to the grouped_mm call, but it's still not fully supported\n+        out_per_sample_g = out_per_sample_g + self.down_proj_bias[expert_ids_g]\n+\n+    # Apply routing weights\n+    out_per_sample_g = out_per_sample_g * sample_weights_g.unsqueeze(-1)\n+\n+    # Restore original order\n+    out_per_sample = out_per_sample_g[inv_perm]\n+\n+    # Accumulate results back to the final_hidden_states using original token indices\n+    final_hidden_states.index_add_(0, token_idx, out_per_sample.to(final_hidden_states.dtype))\n+\n+    return final_hidden_states\n+\n+\n+class ExpertsInterface(GeneralInterface):\n+    \"\"\"Interface for registering custom experts implementations.\"\"\"\n+\n+    _global_mapping = {\n+        \"batched_mm\": batched_mm_experts_forward,\n+        \"grouped_mm\": grouped_mm_experts_forward,\n+    }\n+\n+\n+ALL_EXPERTS_FUNCTIONS = ExpertsInterface()\n+\n+\n+def use_experts_implementation(experts_class: type[torch.nn.Module]) -> type[torch.nn.Module]:\n+    original_init = experts_class.__init__\n+    original_forward = experts_class.forward\n+\n+    @wraps(original_init)\n+    def __init__(self, config, *args, **kwargs):\n+        original_init(self, config, *args, **kwargs)\n+        self.config = config\n+\n+    @wraps(original_forward)\n+    def forward(self, *args, **kwargs):\n+        experts_forward = original_forward\n+\n+        if self.config._experts_implementation != \"eager\":\n+            experts_forward = ALL_EXPERTS_FUNCTIONS[self.config._experts_implementation]\n+\n+        return experts_forward(self, *args, **kwargs)\n+\n+    experts_class.__init__ = __init__\n+    experts_class.forward = forward\n+    return experts_class"
        },
        {
            "sha": "7710d587a7481a70c12da27a1517f33925011e29",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 126,
            "deletions": 0,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -109,6 +109,7 @@\n     is_accelerate_available,\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n+    is_grouped_mm_available,\n     is_kernels_available,\n     is_torch_flex_attn_available,\n     is_torch_greater_or_equal,\n@@ -1278,6 +1279,11 @@ def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n         self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(\n             self.config._attn_implementation, is_init_check=True\n         )\n+        # Check the experts implementation is supported, or set it if not yet set (on the internal attr, to avoid\n+        # setting it recursively)\n+        self.config._experts_implementation_internal = self._check_and_adjust_experts_implementation(\n+            self.config._experts_implementation\n+        )\n         if self.can_generate():\n             self.generation_config = GenerationConfig.from_model_config(config)\n \n@@ -1465,9 +1471,14 @@ def _from_config(cls, config, **kwargs):\n         if \"attn_implementation\" in kwargs:\n             config._attn_implementation = kwargs.pop(\"attn_implementation\")\n \n+        # If passing `experts_implementation` as kwargs, respect it (it will be applied recursively on subconfigs)\n+        if \"experts_implementation\" in kwargs:\n+            config._experts_implementation = kwargs.pop(\"experts_implementation\")\n+\n         init_contexts = []\n         if dtype is not None:\n             init_contexts.append(local_torch_dtype(dtype, cls.__name__))\n+\n         if is_deepspeed_zero3_enabled() and not _is_quantized and not _is_ds_init_called:\n             logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n             # this immediately partitions the model across all gpus, to avoid the overhead in time\n@@ -1729,6 +1740,22 @@ def _sdpa_can_dispatch(self, is_init_check: bool = False) -> bool:\n \n         return True\n \n+    def _grouped_mm_can_dispatch(self) -> bool:\n+        \"\"\"\n+        Check the availability of Grouped MM for a given model.\n+        \"\"\"\n+\n+        if not self._can_set_experts_implementation():\n+            raise ValueError(f\"{self.__class__.__name__} does not support setting experts implementation.\")\n+\n+        if not is_grouped_mm_available():\n+            raise ImportError(\n+                \"PyTorch Grouped MM requirements in Transformers are not met. Please install torch>=2.9.0.\"\n+            )\n+\n+        # If no error raised by this point, we can return `True`\n+        return True\n+\n     def _flex_attn_can_dispatch(self, is_init_check: bool = False) -> bool:\n         \"\"\"\n         Check the availability of Flex Attention for a given model.\n@@ -1837,6 +1864,19 @@ def _check_and_adjust_attn_implementation(\n \n         return applicable_attn_implementation\n \n+    def _check_and_adjust_experts_implementation(self, experts_implementation: Optional[str]) -> str:\n+        \"\"\"\n+        Check that the `experts_implementation` exists and is supported by the models.\n+\n+        Args:\n+            experts_implementation (`str` or `None`):\n+                The experts implementation to check for existence/validity.\n+        Returns:\n+            `str`: The final experts implementation to use.\n+        \"\"\"\n+        applicable_experts_implementation = self.get_correct_experts_implementation(experts_implementation)\n+        return applicable_experts_implementation\n+\n     def get_correct_attn_implementation(self, requested_attention: Optional[str], is_init_check: bool = False) -> str:\n         applicable_attention = \"sdpa\" if requested_attention is None else requested_attention\n         if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n@@ -1871,6 +1911,26 @@ def get_correct_attn_implementation(self, requested_attention: Optional[str], is\n \n         return applicable_attention\n \n+    def get_correct_experts_implementation(self, requested_experts: Optional[str]) -> str:\n+        applicable_experts = \"grouped_mm\" if requested_experts is None else requested_experts\n+        if applicable_experts not in [\"eager\", \"grouped_mm\", \"batched_mm\"]:\n+            message = (\n+                f'Specified `experts_implementation=\"{applicable_experts}\"` is not supported. The only possible arguments are '\n+                '`experts_implementation=\"eager\"`, `\"experts_implementation=grouped_mm\"` and `\"experts_implementation=batched_mm\"`.'\n+            )\n+            raise ValueError(message)\n+\n+        # Perform relevant checks\n+        if applicable_experts == \"grouped_mm\":\n+            try:\n+                self._grouped_mm_can_dispatch()\n+            except (ValueError, ImportError) as e:\n+                if requested_experts == \"grouped_mm\":\n+                    raise e\n+                applicable_experts = \"eager\"\n+\n+        return applicable_experts\n+\n     @classmethod\n     def _can_set_attn_implementation(cls) -> bool:\n         \"\"\"Detect whether the class supports setting its attention implementation dynamically. It is an ugly check based on\n@@ -1889,6 +1949,17 @@ def _can_set_attn_implementation(cls) -> bool:\n             # If no attention layer, assume `True`. Most probably a multimodal model or inherits from existing models\n             return True\n \n+    @classmethod\n+    def _can_set_experts_implementation(cls) -> bool:\n+        \"\"\"Detect whether the class supports setting its experts implementation dynamically. It is an ugly check based on\n+        opening the file, but avoids maintaining yet another property flag.\n+        \"\"\"\n+        class_file = sys.modules[cls.__module__].__file__\n+        with open(class_file, \"r\") as f:\n+            code = f.read()\n+        # heuristic -> if we the use_experts_implementation decorator is used, then we can set it\n+        return \"@use_experts_implementation\" in code\n+\n     def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n         \"\"\"\n         Set the requested `attn_implementation` for this model.\n@@ -1988,6 +2059,50 @@ def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n                     if hasattr(subconfig, \"_attn_was_changed\"):\n                         del subconfig._attn_was_changed\n \n+    def set_experts_implementation(self, experts_implementation: Union[str, dict]):\n+        \"\"\"\n+        Set the requested `experts_implementation` for this model.\n+\n+        Args:\n+            experts_implementation (`str` or `dict`):\n+                The experts implementation to set for this model. It can be either a `str`, in which case it will be\n+                dispatched to all submodels if relevant, or a `dict` where keys are the sub_configs name, in which case each\n+                submodel will dispatch the corresponding value.\n+        \"\"\"\n+        requested_implementation = (\n+            experts_implementation\n+            if not isinstance(experts_implementation, dict)\n+            else experts_implementation.get(\"\", self.config._experts_implementation)\n+        )\n+\n+        if requested_implementation != self.config._experts_implementation:\n+            requested_implementation = self._check_and_adjust_experts_implementation(requested_implementation)\n+            # Apply the change (on the internal attr, to avoid setting it recursively)\n+            self.config._experts_implementation_internal = requested_implementation\n+\n+        # Apply it to all submodels as well\n+        for submodule in self.modules():\n+            # We found a submodel (which is not self) with a different config (otherwise, it may be the same \"actual model\",\n+            # e.g. ForCausalLM has a Model inside, but no need to check it again)\n+            if (\n+                submodule is not self\n+                and isinstance(submodule, PreTrainedModel)\n+                and submodule.config.__class__ != self.config.__class__\n+            ):\n+                # Set the experts on the submodule\n+                sub_implementation = requested_implementation\n+                if isinstance(experts_implementation, dict):\n+                    for subconfig_key in self.config.sub_configs:\n+                        # We need to check for exact object match here, with `is`\n+                        if getattr(self.config, subconfig_key) is submodule.config:\n+                            sub_implementation = experts_implementation.get(\n+                                subconfig_key, submodule.config._experts_implementation\n+                            )\n+                            break\n+                # Check the module can use correctly, otherwise we raise an error if requested experts can't be set for submodule\n+                sub_implementation = submodule.get_correct_experts_implementation(sub_implementation)\n+                submodule.config._experts_implementation_internal = sub_implementation\n+\n     def enable_input_require_grads(self):\n         \"\"\"\n         Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping\n@@ -3646,6 +3761,14 @@ def from_pretrained(\n                   \"org/model@main\"\n                   \"org/model:custom_kernel\"\n                   \"org/model@v1.2.3:custom_kernel\"\n+            experts_implementation (`str`, *optional*):\n+                The experts implementation to use in the model (if relevant). Can be any of:\n+\n+                - `\"eager\"` (sequential implementation of the experts matrix multiplications).\n+                - `\"batched_mm\"` (using [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html)).\n+                - `\"grouped_mm\"` (using [`torch._grouped_mm`](https://docs.pytorch.org/docs/main/generated/torch.nn.functional.grouped_mm.html)).\n+\n+                By default, if available, `grouped_mm` will be used for torch>=2.9.0. The default is otherwise the sequential `\"eager\"` implementation.\n \n             > Parameters for big model inference\n \n@@ -3869,6 +3992,9 @@ def from_pretrained(\n         if \"attn_implementation\" in kwargs:\n             config._attn_implementation = kwargs.pop(\"attn_implementation\")\n \n+        if \"experts_implementation\" in kwargs:\n+            config._experts_implementation = kwargs.pop(\"experts_implementation\")\n+\n         hf_quantizer, config, device_map = get_hf_quantizer(\n             config, quantization_config, device_map, weights_only, user_agent\n         )"
        },
        {
            "sha": "d15efa17526afab83fb537ba9d70c46ac2efc7fe",
            "filename": "src/transformers/models/afmoe/modeling_afmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -29,8 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n-from ...integrations.hub_kernels import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, MoeModelOutputWithPast"
        },
        {
            "sha": "3734494876d2e8ae0570e439f746a11199466831",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -35,8 +35,7 @@\n from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n-from ...integrations.hub_kernels import lazy_load_kernel\n+from ...integrations import lazy_load_kernel, use_kernel_forward_from_hub, use_kernelized_func\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast"
        },
        {
            "sha": "8f3283793bf936f404f6523a0e6eae631bf2e375",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -43,7 +43,7 @@\n )\n \n from ... import initialization as init\n-from ...integrations.hub_kernels import lazy_load_kernel\n+from ...integrations import lazy_load_kernel\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel"
        },
        {
            "sha": "00a39a0774f197682e28a6c2a50532ccc441a287",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -104,7 +104,15 @@ def __init__(\n         self.moe_loss_weight = moe_loss_weight\n         self.moe_normalize_expert_weights = moe_normalize_expert_weights\n \n-        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\", \"torch_dtype\", \"dtype\"]:\n+        for k in [\n+            \"model_type\",\n+            \"attn_implementation\",\n+            \"experts_implementation\",\n+            \"transformers_version\",\n+            \"_commit_hash\",\n+            \"torch_dtype\",\n+            \"dtype\",\n+        ]:\n             if k in kwargs:\n                 kwargs.pop(k)\n         if len(kwargs) != 0:"
        },
        {
            "sha": "7e7f5414b6010047f56ba740664af3efe08e2482",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -30,18 +30,19 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub\n+from ...integrations import use_experts_implementation, use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_deepseek_v2 import DeepseekV2Config\n \n \n+@use_experts_implementation\n class DeepseekV2Experts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -453,7 +454,9 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": DeepseekV2DecoderLayer,"
        },
        {
            "sha": "9ffec2a419593adb6e8d942c8016324886af8182",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -24,7 +24,7 @@\n from ...cache_utils import Cache\n from ...modeling_rope_utils import RopeParameters, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import logging\n+from ...utils import is_grouped_mm_available, logging\n from ...utils.generic import maybe_autocast\n from ..llama.configuration_llama import LlamaConfig\n from ..llama.modeling_llama import (\n@@ -437,7 +437,9 @@ def __init__(self, config: DeepseekV2Config, layer_idx: int):\n \n \n class DeepseekV2PreTrainedModel(LlamaPreTrainedModel):\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "70f480ac906cbb797597a46bedc5e405e8f0379f",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -16,7 +16,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub\n+from ...integrations import use_experts_implementation, use_kernel_forward_from_hub, use_kernel_func_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -28,7 +28,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_deepseek_v3 import DeepseekV3Config\n \n@@ -150,6 +150,7 @@ def forward(self, hidden_states):\n         return router_logits\n \n \n+@use_experts_implementation\n class DeepseekV3NaiveMoe(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -542,7 +543,9 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": DeepseekV3DecoderLayer,"
        },
        {
            "sha": "b092350f81c55b3f5e1dba8e0669d51aa2241865",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -12,7 +12,7 @@\n from ...modeling_layers import GenericForSequenceClassification, GenericForTokenClassification\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import logging\n+from ...utils import is_grouped_mm_available, logging\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n     LlamaForCausalLM,\n@@ -304,7 +304,9 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n \n \n class DeepseekV3PreTrainedModel(LlamaPreTrainedModel):\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _keep_in_fp32_modules_strict = [\"e_score_correction_bias\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "160f0f4a1e95145334088c1dae00786a157dad31",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -29,15 +29,20 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_dots1 import Dots1Config\n \n@@ -308,6 +313,7 @@ def forward(self, hidden_states):\n         return router_logits\n \n \n+@use_experts_implementation\n class Dots1NaiveMoe(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -463,7 +469,9 @@ class Dots1PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Dots1DecoderLayer,"
        },
        {
            "sha": "80efdf9219fc2d1428f1bb6e4a298dc5bd65c699",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -29,14 +29,14 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n+from ...integrations import use_experts_implementation, use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_ernie4_5_moe import Ernie4_5_MoeConfig\n \n@@ -317,6 +317,7 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n+@use_experts_implementation\n class Ernie4_5_MoeExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -372,7 +373,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         )\n \n         with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n-            router_logits = F.linear(hidden_states.float(), self.weight)\n+            router_logits = F.linear(hidden_states.float(), self.weight.float())\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n             _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n             routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n@@ -476,7 +477,9 @@ class Ernie4_5_MoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(Ernie4_5_MoeTopKRouter, index=0),"
        },
        {
            "sha": "ff4616098f071eac73e2a348212078ad741aed09",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 27,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -104,32 +104,6 @@ def __init__(self, config):\n         self.num_experts = config.moe_num_experts\n         self.intermediate_dim = config.moe_intermediate_size\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        top_k_index: torch.Tensor,\n-        top_k_weights: torch.Tensor,\n-    ) -> torch.Tensor:\n-        final_hidden_states = torch.zeros_like(hidden_states)\n-        with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n-            expert_mask = expert_mask.permute(2, 1, 0)\n-            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-\n-        for expert_idx in expert_hit:\n-            expert_idx = expert_idx[0]\n-            if expert_idx == self.num_experts:\n-                continue\n-            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n-            current_state = hidden_states[token_idx]\n-            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n-            current_hidden_states = self.act_fn(gate) * up\n-            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n-            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n-\n-        return final_hidden_states\n-\n \n class Ernie4_5_MoeTopKRouter(nn.Module):\n     def __init__(self, config):\n@@ -147,7 +121,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         )\n \n         with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n-            router_logits = F.linear(hidden_states.float(), self.weight)\n+            router_logits = F.linear(hidden_states.float(), self.weight.float())\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n             _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n             routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)"
        },
        {
            "sha": "041331a6d7c78c84c6430429c5d464cb247824b4",
            "filename": "src/transformers/models/ernie4_5_vl_moe/modeling_ernie4_5_vl_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fmodeling_ernie4_5_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fmodeling_ernie4_5_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_vl_moe%2Fmodeling_ernie4_5_vl_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -31,7 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n+from ...integrations import use_experts_implementation, use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -357,7 +357,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         )\n \n         with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n-            router_logits = F.linear(hidden_states.float(), self.weight)\n+            router_logits = F.linear(hidden_states.float(), self.weight.float())\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n             _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n             routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n@@ -368,6 +368,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         return router_logits, selected_experts, routing_weights\n \n \n+@use_experts_implementation\n class Ernie4_5_VL_MoeMoeExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n "
        },
        {
            "sha": "1e79b18b63dd688469a568a694186ab89aef1dff",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -31,7 +31,7 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n-from ...integrations.hub_kernels import lazy_load_kernel\n+from ...integrations import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging"
        },
        {
            "sha": "5a88e5e5b3253840b754f4f4b9362debd6a32ba5",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -30,14 +30,14 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n+from ...integrations import use_experts_implementation, use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_flex_olmo import FlexOlmoConfig\n \n@@ -293,6 +293,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_experts_implementation\n class FlexOlmoExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -421,7 +422,9 @@ class FlexOlmoPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),"
        },
        {
            "sha": "08064796496e035fefd74cbd6d921cc8e9977b38",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -30,15 +30,15 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n+from ...integrations import use_experts_implementation, use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_glm4_moe import Glm4MoeConfig\n \n@@ -332,6 +332,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+@use_experts_implementation\n class Glm4MoeNaiveMoe(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -486,7 +487,9 @@ class Glm4MoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Glm4MoeDecoderLayer,"
        },
        {
            "sha": "770702a0e9ab0ba93bffb0d7aff6daa3b1768997",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -273,7 +273,7 @@ class Glm4MoeDecoderLayer(DeepseekV3DecoderLayer):\n \n \n class Glm4MoePreTrainedModel(DeepseekV3PreTrainedModel):\n-    _can_compile_fullgraph = False\n+    pass\n \n \n class Glm4MoeModel(DeepseekV3Model):"
        },
        {
            "sha": "4ee99dbd5eff79897e780822dfbf55953318b883",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -32,15 +32,21 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n+from ...integrations import use_experts_implementation, use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import ModelOutput, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    is_grouped_mm_available,\n+    is_torchdynamo_compiling,\n+)\n from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_glm4v_moe import Glm4vMoeConfig, Glm4vMoeTextConfig, Glm4vMoeVisionConfig\n \n@@ -395,6 +401,7 @@ def forward(self, hidden_states):\n         return router_logits\n \n \n+@use_experts_implementation\n class Glm4vMoeTextNaiveMoe(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -586,7 +593,9 @@ class Glm4vMoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n \n     _can_record_outputs = {"
        },
        {
            "sha": "ddd845ac69398ad5118271b34cfb55f7d450f9b5",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -28,8 +28,7 @@\n from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernelized_func\n-from ...integrations.hub_kernels import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub, use_kernelized_func\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import (\n     GenericForSequenceClassification,"
        },
        {
            "sha": "06fb428df416370bb06620e7ef9d7da5840e45e4",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -21,7 +21,7 @@\n \n from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n-from ...integrations.hub_kernels import use_kernel_forward_from_hub\n+from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_outputs import (\n     MoeModelOutputWithPast,"
        },
        {
            "sha": "d2b46f4813f4d26d4b5604bc6101bd64c5369e40",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -456,8 +456,7 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # TopK gating fails fullgraph compilation at \"expert_size = expert_size.tolist()\"\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": GraniteMoeDecoderLayer,"
        },
        {
            "sha": "58e38321fdecc3d946637a030bfb40a39a1b231d",
            "filename": "src/transformers/models/granitemoe/modular_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -146,8 +146,7 @@ class GraniteMoePreTrainedModel(LlamaPreTrainedModel, PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # TopK gating fails fullgraph compilation at \"expert_size = expert_size.tolist()\"\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "ac2770b44ff83559eb9382064ec66e22d48c42c9",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -31,8 +31,12 @@\n from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n-from ...integrations.hub_kernels import lazy_load_kernel\n+from ...integrations import (\n+    lazy_load_kernel,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -1235,8 +1239,7 @@ class GraniteMoeHybridPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # TopK gating fails fullgraph compilation at \"expert_size = expert_size.tolist()\"\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": GraniteMoeHybridDecoderLayer,"
        },
        {
            "sha": "bbfff3eab95a8a4aeac3c56963699c1fd932c42c",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -462,8 +462,7 @@ class GraniteMoeSharedPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # TopK gating fails fullgraph compilation at \"expert_size = expert_size.tolist()\"\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": GraniteMoeSharedDecoderLayer,"
        },
        {
            "sha": "a9b7126e6264cf4cd59e74b55c5b1d74fe49983f",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -30,14 +30,19 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import check_model_inputs, maybe_autocast\n from .configuration_hunyuan_v1_moe import HunYuanMoEV1Config\n \n@@ -244,6 +249,7 @@ def forward(self, hidden_states):\n         return logits\n \n \n+@use_experts_implementation\n class HunYuanMoEV1Experts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -371,7 +377,9 @@ class HunYuanMoEV1PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": HunYuanMoEV1DecoderLayer,"
        },
        {
            "sha": "632ecedac152130842d95c0b801d5c1516d6eed1",
            "filename": "src/transformers/models/hunyuan_v1_moe/modular_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -25,7 +25,7 @@\n from ...cache_utils import Cache\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, logging\n+from ...utils import TransformersKwargs, is_grouped_mm_available, logging\n from ..hunyuan_v1_dense.modeling_hunyuan_v1_dense import HunYuanDenseV1RotaryEmbedding\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -177,7 +177,9 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n \n \n class HunYuanMoEV1PreTrainedModel(LlamaPreTrainedModel):\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "b9ed4a5fa5bfd9507db1a9efadc00eba8b03cd7d",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -32,8 +32,13 @@\n from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n-from ...integrations.hub_kernels import lazy_load_kernel\n+from ...integrations import (\n+    lazy_load_kernel,\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -593,6 +598,7 @@ def forward(self, x):\n         return down_proj\n \n \n+@use_experts_implementation\n class JambaExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n "
        },
        {
            "sha": "cf5fac0eed864dac341d4d7f635b6bdd53a161d2",
            "filename": "src/transformers/models/jamba/modular_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -25,7 +25,7 @@\n \n from ... import initialization as init\n from ...activations import ACT2FN\n-from ...integrations.hub_kernels import lazy_load_kernel\n+from ...integrations import lazy_load_kernel\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast"
        },
        {
            "sha": "197ec47e638b09da285c246c05965dd69b413a32",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -576,7 +576,7 @@ class JetMoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = False  # TopK gating fails fullgraph compilation at \"expert_size = expert_size.tolist()\"\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"gate\", index=1),"
        },
        {
            "sha": "5ace41d875c05d1a891872eb3cc940381d5c99d5",
            "filename": "src/transformers/models/jetmoe/modular_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -435,6 +435,7 @@ class JetMoePreTrainedModel(MixtralPreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _can_compile_fullgraph = False  # TopK gating fails fullgraph compilation at \"expert_size = expert_size.tolist()\"\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "3017bd5ecd4675f21adef79c5c4b861b8f0daaa9",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -27,7 +27,12 @@\n from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -145,6 +150,7 @@ def forward(self, x):\n         return self.w2(F.silu(self.w1(x)) * self.w3(x))\n \n \n+@use_experts_implementation\n class Lfm2MoeExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -155,6 +161,7 @@ def __init__(self, config):\n         self.intermediate_dim = config.moe_intermediate_size\n         self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n         self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n+        self.act_fn = F.silu\n \n     def forward(\n         self,\n@@ -175,7 +182,7 @@ def forward(\n             top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n-            current_hidden_states = F.silu(gate) * up\n+            current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n             current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n@@ -671,7 +678,7 @@ class Lfm2MoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = False  # uses a non-compilable custom cache class Lfm2MoeHybridConvCache\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Lfm2MoeDecoderLayer,"
        },
        {
            "sha": "1eabe7b5857ea51d147edd01361b20d6f953a0f3",
            "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 28,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -72,33 +72,7 @@ def __init__(self, config: Lfm2MoeConfig, intermediate_size: Optional[int] = Non\n class Lfm2MoeExperts(Qwen2MoeExperts):\n     def __init__(self, config):\n         super().__init__(config)\n-        del self.act_fn\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        top_k_index: torch.Tensor,\n-        top_k_weights: torch.Tensor,\n-    ) -> torch.Tensor:\n-        final_hidden_states = torch.zeros_like(hidden_states)\n-        with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n-            expert_mask = expert_mask.permute(2, 1, 0)\n-            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-\n-        for expert_idx in expert_hit:\n-            expert_idx = expert_idx[0]\n-            if expert_idx == self.num_experts:\n-                continue\n-            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n-            current_state = hidden_states[token_idx]\n-            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n-            current_hidden_states = F.silu(gate) * up\n-            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n-            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n-\n-        return final_hidden_states\n+        self.act_fn = F.silu\n \n \n class Lfm2MoeSparseMoeBlock(nn.Module):\n@@ -160,7 +134,7 @@ def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n \n \n class Lfm2MoePreTrainedModel(LlamaPreTrainedModel):\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = False  # uses a non-compilable custom cache class Lfm2MoeHybridConvCache\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "a9437766dd9af061623e90a4b44f9f81d2fee1b0",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -26,7 +26,7 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n-from ...integrations.hub_kernels import lazy_load_kernel\n+from ...integrations import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ("
        },
        {
            "sha": "cccd8b4a748de6c2e7e39f29b1803c7fe4b24aeb",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -24,7 +24,7 @@\n from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...integrations.hub_kernels import lazy_load_kernel\n+from ...integrations import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ("
        },
        {
            "sha": "e30c4f718cb2001a79131aa431d09a9ca62e510f",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -31,7 +31,12 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -473,6 +478,7 @@ def forward(self, hidden_states):\n         return router_logits, router_scores, router_indices\n \n \n+@use_experts_implementation\n class MiniMaxExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -596,7 +602,7 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = False  # uses a non-compilable custom cache class MiniMaxCache\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(MiniMaxTopKRouter, layer_name=\"mlp.gate\", index=0),"
        },
        {
            "sha": "e1e0a459ee807f07dec22d4df251a25be097df6b",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -521,7 +521,7 @@ def forward(\n \n \n class MiniMaxPreTrainedModel(MixtralPreTrainedModel):\n-    _can_compile_fullgraph = False\n+    _can_compile_fullgraph = False  # uses a non-compilable custom cache class MiniMaxCache\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(MiniMaxTopKRouter, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": MiniMaxDecoderLayer,"
        },
        {
            "sha": "6b1f30abccec8d1178aa72185d03c5bc01a0c445",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -37,7 +37,12 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -50,11 +55,12 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import OutputRecorder, maybe_autocast\n from .configuration_mixtral import MixtralConfig\n \n \n+@use_experts_implementation\n class MixtralExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -403,7 +409,9 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(MixtralTopKRouter, index=0),"
        },
        {
            "sha": "fe6caa4de1569da3af530b749bdb1cb0fb6c51c1",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -28,12 +28,13 @@\n from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n+from ...integrations import use_experts_implementation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, logging\n+from ...utils import TransformersKwargs, is_grouped_mm_available, logging\n from ...utils.generic import OutputRecorder\n from ..mistral.modeling_mistral import (\n     MistralAttention,\n@@ -134,6 +135,7 @@ def load_balancing_loss_func(\n     return overall_loss * num_experts\n \n \n+@use_experts_implementation\n class MixtralExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -263,7 +265,9 @@ def forward(\n \n \n class MixtralPreTrainedModel(MistralPreTrainedModel):\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(MixtralTopKRouter, index=0),\n         \"hidden_states\": MixtralDecoderLayer,"
        },
        {
            "sha": "ac655d83eea14160b527ec85b3523452ceff8563",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -27,14 +27,19 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_olmoe import OlmoeConfig\n \n@@ -298,6 +303,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_experts_implementation\n class OlmoeExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -431,7 +437,9 @@ class OlmoePreTrainedModel(PreTrainedModel):\n         \"hidden_states\": OlmoeDecoderLayer,\n         \"attentions\": OlmoeAttention,\n     }\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "2be222d7a671f9c02bca6d57b64b3b578e70e5d5",
            "filename": "src/transformers/models/olmoe/modular_olmoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -24,7 +24,7 @@\n from ...modeling_outputs import MoeModelOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_grouped_mm_available, logging\n from ...utils.generic import OutputRecorder\n from ..gemma.modeling_gemma import GemmaMLP\n from ..llama.modeling_llama import (\n@@ -165,7 +165,9 @@ class OlmoePreTrainedModel(PreTrainedModel):\n         \"hidden_states\": OlmoeDecoderLayer,\n         \"attentions\": OlmoeAttention,\n     }\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "770170f4f748a4a97a1a584800ca3517da93d8c4",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -30,14 +30,19 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_phimoe import PhimoeConfig\n \n@@ -327,6 +332,7 @@ def backward(\n         )\n \n \n+@use_experts_implementation\n class PhimoeExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -617,7 +623,9 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(PhimoeTopKRouter, layer_name=\"mlp.router\", index=0),"
        },
        {
            "sha": "6c5f1f4844c6923287c675cb6b58d5dd2564886a",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -35,7 +35,12 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n@@ -47,7 +52,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_qwen2_moe import Qwen2MoeConfig\n \n@@ -292,6 +297,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_experts_implementation\n class Qwen2MoeExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -432,7 +438,9 @@ class Qwen2MoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(Qwen2MoeTopKRouter, index=0),"
        },
        {
            "sha": "2aa07f7252a5a7f6d6ba8b082062459be3019c7b",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -30,7 +30,12 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -43,7 +48,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_qwen3_moe import Qwen3MoeConfig\n \n@@ -212,6 +217,7 @@ def forward(self, x):\n         return down_proj\n \n \n+@use_experts_implementation\n class Qwen3MoeExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n \n@@ -365,7 +371,9 @@ class Qwen3MoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(Qwen3MoeTopKRouter, layer_name=\"mlp.gate\", index=0),"
        },
        {
            "sha": "16598eb01e864547637bf436f62aa8373a1abddd",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernelized_func\n+from ...integrations import use_experts_implementation, use_kernelized_func\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import (\n@@ -819,6 +819,7 @@ def forward(self, x):\n         return down_proj\n \n \n+@use_experts_implementation\n class Qwen3NextExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n "
        },
        {
            "sha": "abda2b845de40836f8869bba7f72fefcba26264a",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -35,7 +35,12 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -49,7 +54,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple\n+from ...utils import auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs, maybe_autocast\n from .configuration_qwen3_omni_moe import (\n     Qwen3OmniMoeAudioEncoderConfig,\n@@ -1338,6 +1343,7 @@ def apply_interleaved_mrope(self, freqs, mrope_section):\n         return freqs_t\n \n \n+@use_experts_implementation\n class Qwen3OmniMoeThinkerTextExperts(nn.Module):\n     \"\"\"\n     ModuleList of experts.\n@@ -1616,7 +1622,9 @@ class Qwen3OmniMoeThinkerTextPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(Qwen3OmniMoeThinkerTextTopKRouter, layer_name=\"mlp.gate\", index=0),\n@@ -2767,6 +2775,7 @@ def forward(self, x):\n         return down_proj\n \n \n+@use_experts_implementation\n class Qwen3OmniMoeTalkerTextExperts(nn.Module):\n     \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n "
        },
        {
            "sha": "e25f4aebdbdb772218efa97fa1232132ab51f70c",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 67,
            "deletions": 94,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -31,15 +31,20 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...integrations import use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func\n+from ...integrations import (\n+    use_experts_implementation,\n+    use_kernel_forward_from_hub,\n+    use_kernel_func_from_hub,\n+    use_kernelized_func,\n+)\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available\n from ...utils.generic import OutputRecorder, check_model_inputs, maybe_autocast\n from .configuration_qwen3_vl_moe import Qwen3VLMoeConfig, Qwen3VLMoeTextConfig, Qwen3VLMoeVisionConfig\n \n@@ -65,92 +70,77 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+@use_experts_implementation\n class Qwen3VLMoeTextExperts(nn.Module):\n+    \"\"\"Collection of expert weights stored as 3D tensors.\"\"\"\n+\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_experts\n-        self.intermediate_size = config.moe_intermediate_size\n-        self.hidden_size = config.hidden_size\n-        self.expert_dim = self.intermediate_size\n-        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n-        self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n+        self.hidden_dim = config.hidden_size\n+        self.intermediate_dim = config.moe_intermediate_size\n+        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n+        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n         self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(\n-        self, hidden_states: torch.Tensor, routing_weights: torch.Tensor, router_indices: torch.Tensor\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n-        \"\"\"\n-        When training it is more efficient to just loop over the experts and compute the output for each expert\n-        as otherwise the memory would explode.\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+\n+        for expert_idx in expert_hit:\n+            expert_idx = expert_idx[0]\n+            if expert_idx == self.num_experts:\n+                continue\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n+            current_state = hidden_states[token_idx]\n+            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n+            current_hidden_states = self.act_fn(gate) * up\n+            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n+            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n+\n+        return final_hidden_states\n \n-        For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs.\n \n-        Args:\n-            hidden_states (torch.Tensor): (batch_size * token_num, hidden_size)\n-            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n-            router_indices (torch.Tensor): (batch_size * token_num, top_k)\n-        Returns:\n-            torch.Tensor\n-        \"\"\"\n-        batch_size = hidden_states.shape[0]\n-        hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n-        if self.training:\n-            next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n-            with torch.no_grad():\n-                expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=self.num_experts)\n-                expert_mask = expert_mask.permute(2, 1, 0)\n-                # we sum on the top_k and on the sequence length to get which experts\n-                # are hit this time around\n-                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-            for expert_idx in expert_hit[:]:\n-                with torch.no_grad():\n-                    _, token_idx = torch.where(expert_mask[expert_idx[0]])\n-                current_state = hidden_states[token_idx]\n-                gate_up = current_state @ self.gate_up_proj[expert_idx]\n-                gate, up = gate_up.chunk(2, dim=-1)\n-                gated_output = up * self.act_fn(gate)\n-                out = gated_output @ self.down_proj[expert_idx]\n-                weighted_output = out[0] * routing_weights[token_idx, expert_idx, None]\n-                next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n-            next_states = next_states.view(batch_size, -1, self.hidden_size)\n-        else:\n-            hidden_states = hidden_states.repeat(self.num_experts, 1)\n-            hidden_states = hidden_states.view(self.num_experts, -1, self.hidden_size)\n-            gate_up = torch.bmm(hidden_states, self.gate_up_proj)\n-            gate, up = gate_up.chunk(2, dim=-1)  # not supported for DTensors\n-            next_states = torch.bmm((up * self.act_fn(gate)), self.down_proj)\n-            next_states = next_states.reshape(self.num_experts, batch_size, -1, self.hidden_size)\n-            next_states = (\n-                next_states * routing_weights.transpose(0, 1).view(self.num_experts, batch_size, -1)[..., None]\n-            )\n-            next_states = next_states.sum(dim=0)\n-        return next_states\n+class Qwen3VLMoeTextTopKRouter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_experts\n+        self.hidden_dim = config.hidden_size\n+        self.weight = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n+        router_logits = F.linear(hidden_states, self.weight)  # (seq_len, num_experts)\n+        router_logits = torch.nn.functional.softmax(router_logits, dtype=torch.float, dim=-1)\n+        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n+        router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n+        router_top_value = router_top_value.to(router_logits.dtype)\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class Qwen3VLMoeTextSparseMoeBlock(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config: Qwen3VLMoeTextConfig):\n         super().__init__()\n-        self.hidden_size = config.hidden_size\n-        self.num_experts = config.num_experts\n-        self.top_k = config.num_experts_per_tok\n-        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n         self.experts = Qwen3VLMoeTextExperts(config)\n+        self.gate = Qwen3VLMoeTextTopKRouter(config)\n \n-        # since all the models use norm_topk_prob, we don't need to have a extra check for it\n-        # self.norm_topk_prob = config.norm_topk_prob\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        batch_size = hidden_states.shape[0]\n-        hidden_states = hidden_states.reshape(-1, self.hidden_size)\n-        router_logits = self.gate(hidden_states)\n-        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n-        routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(router_logits.dtype)\n-        router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n-        hidden_states = hidden_states.reshape(batch_size, -1, self.hidden_size)\n-        routed_out = self.experts(hidden_states, router_weights, router_indices)\n-        return routed_out\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n+        _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n+        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n \n \n def rotate_half(x):\n@@ -368,27 +358,6 @@ def forward(\n         return hidden_states\n \n \n-class Qwen3VLMoeTextTopKRouter(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.top_k = config.num_experts_per_tok\n-        self.num_experts = config.num_experts\n-        self.norm_topk_prob = config.norm_topk_prob\n-        self.hidden_dim = config.hidden_size\n-        self.weight = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n-\n-    def forward(self, hidden_states):\n-        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n-        router_logits = F.linear(hidden_states, self.weight)  # (seq_len, num_experts)\n-        router_logits = torch.nn.functional.softmax(router_logits, dtype=torch.float, dim=-1)\n-        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n-        if self.norm_topk_prob:\n-            router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n-        router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = router_top_value\n-        return router_logits, router_scores, router_indices\n-\n-\n @auto_docstring\n class Qwen3VLMoePreTrainedModel(PreTrainedModel):\n     config: Qwen3VLMoeConfig\n@@ -399,7 +368,9 @@ class Qwen3VLMoePreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_compile_fullgraph = (\n+        is_grouped_mm_available()\n+    )  # https://huggingface.co/docs/transformers/experts_interface#torchcompile\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(Qwen3VLMoeTextTopKRouter, layer_name=\"mlp.gate\", index=0),\n@@ -418,6 +389,8 @@ def _init_weights(self, module):\n         if isinstance(module, Qwen3VLMoeTextExperts):\n             init.normal_(module.gate_up_proj, mean=0.0, std=std)\n             init.normal_(module.down_proj, mean=0.0, std=std)\n+        elif isinstance(module, Qwen3VLMoeTextTopKRouter):\n+            init.normal_(module.weight, mean=0.0, std=std)\n         elif isinstance(module, Qwen3VLMoeVisionRotaryEmbedding):\n             inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n             init.copy_(module.inv_freq, inv_freq)"
        },
        {
            "sha": "e8b032eaaf76f827faa354a26e73820e2c58a2a6",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 24,
            "deletions": 81,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -18,9 +18,9 @@\n \n import torch\n import torch.nn as nn\n+import torch.nn.functional as F\n \n from ... import initialization as init\n-from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n@@ -29,8 +29,10 @@\n from ...utils import TransformersKwargs, can_return_tuple, logging\n from ..qwen3_moe.modeling_qwen3_moe import (\n     Qwen3MoeDecoderLayer,\n+    Qwen3MoeExperts,\n     Qwen3MoePreTrainedModel,\n     Qwen3MoeRMSNorm,\n+    Qwen3MoeSparseMoeBlock,\n     load_balancing_loss_func,\n )\n from ..qwen3_vl.configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLVisionConfig\n@@ -258,92 +260,31 @@ class Qwen3VLMoeTextRMSNorm(Qwen3MoeRMSNorm):\n     pass\n \n \n-class Qwen3VLMoeTextExperts(nn.Module):\n+class Qwen3VLMoeTextExperts(Qwen3MoeExperts):\n+    pass\n+\n+\n+class Qwen3VLMoeTextTopKRouter(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n+        self.top_k = config.num_experts_per_tok\n         self.num_experts = config.num_experts\n-        self.intermediate_size = config.moe_intermediate_size\n-        self.hidden_size = config.hidden_size\n-        self.expert_dim = self.intermediate_size\n-        self.gate_up_proj = nn.Parameter(torch.zeros(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n-        self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n-        self.act_fn = ACT2FN[config.hidden_act]\n+        self.hidden_dim = config.hidden_size\n+        self.weight = nn.Parameter(torch.zeros(self.num_experts, self.hidden_dim))\n \n-    def forward(\n-        self, hidden_states: torch.Tensor, routing_weights: torch.Tensor, router_indices: torch.Tensor\n-    ) -> torch.Tensor:\n-        \"\"\"\n-        When training it is more efficient to just loop over the experts and compute the output for each expert\n-        as otherwise the memory would explode.\n-\n-        For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs.\n-\n-        Args:\n-            hidden_states (torch.Tensor): (batch_size * token_num, hidden_size)\n-            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n-            router_indices (torch.Tensor): (batch_size * token_num, top_k)\n-        Returns:\n-            torch.Tensor\n-        \"\"\"\n-        batch_size = hidden_states.shape[0]\n-        hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n-        if self.training:\n-            next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n-            with torch.no_grad():\n-                expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=self.num_experts)\n-                expert_mask = expert_mask.permute(2, 1, 0)\n-                # we sum on the top_k and on the sequence length to get which experts\n-                # are hit this time around\n-                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-            for expert_idx in expert_hit[:]:\n-                with torch.no_grad():\n-                    _, token_idx = torch.where(expert_mask[expert_idx[0]])\n-                current_state = hidden_states[token_idx]\n-                gate_up = current_state @ self.gate_up_proj[expert_idx]\n-                gate, up = gate_up.chunk(2, dim=-1)\n-                gated_output = up * self.act_fn(gate)\n-                out = gated_output @ self.down_proj[expert_idx]\n-                weighted_output = out[0] * routing_weights[token_idx, expert_idx, None]\n-                next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n-            next_states = next_states.view(batch_size, -1, self.hidden_size)\n-        else:\n-            hidden_states = hidden_states.repeat(self.num_experts, 1)\n-            hidden_states = hidden_states.view(self.num_experts, -1, self.hidden_size)\n-            gate_up = torch.bmm(hidden_states, self.gate_up_proj)\n-            gate, up = gate_up.chunk(2, dim=-1)  # not supported for DTensors\n-            next_states = torch.bmm((up * self.act_fn(gate)), self.down_proj)\n-            next_states = next_states.reshape(self.num_experts, batch_size, -1, self.hidden_size)\n-            next_states = (\n-                next_states * routing_weights.transpose(0, 1).view(self.num_experts, batch_size, -1)[..., None]\n-            )\n-            next_states = next_states.sum(dim=0)\n-        return next_states\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n+        router_logits = F.linear(hidden_states, self.weight)  # (seq_len, num_experts)\n+        router_logits = torch.nn.functional.softmax(router_logits, dtype=torch.float, dim=-1)\n+        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n+        router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n+        router_top_value = router_top_value.to(router_logits.dtype)\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n-class Qwen3VLMoeTextSparseMoeBlock(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.hidden_size = config.hidden_size\n-        self.num_experts = config.num_experts\n-        self.top_k = config.num_experts_per_tok\n-        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n-        self.experts = Qwen3VLMoeTextExperts(config)\n-\n-        # since all the models use norm_topk_prob, we don't need to have a extra check for it\n-        # self.norm_topk_prob = config.norm_topk_prob\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        batch_size = hidden_states.shape[0]\n-        hidden_states = hidden_states.reshape(-1, self.hidden_size)\n-        router_logits = self.gate(hidden_states)\n-        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n-        routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(router_logits.dtype)\n-        router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n-        hidden_states = hidden_states.reshape(batch_size, -1, self.hidden_size)\n-        routed_out = self.experts(hidden_states, router_weights, router_indices)\n-        return routed_out\n+class Qwen3VLMoeTextSparseMoeBlock(Qwen3MoeSparseMoeBlock):\n+    pass\n \n \n class Qwen3VLMoeTextAttention(Qwen3VLTextAttention):\n@@ -369,6 +310,8 @@ def _init_weights(self, module):\n         if isinstance(module, Qwen3VLMoeTextExperts):\n             init.normal_(module.gate_up_proj, mean=0.0, std=std)\n             init.normal_(module.down_proj, mean=0.0, std=std)\n+        elif isinstance(module, Qwen3VLMoeTextTopKRouter):\n+            init.normal_(module.weight, mean=0.0, std=std)\n         elif isinstance(module, Qwen3VLMoeVisionRotaryEmbedding):\n             inv_freq = 1.0 / (module.theta ** (torch.arange(0, module.dim, 2, dtype=torch.float) / module.dim))\n             init.copy_(module.inv_freq, inv_freq)"
        },
        {
            "sha": "659f3ec96c545aad84dc5f00bc384127e0402a15",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -145,6 +145,7 @@\n     is_gguf_available,\n     is_gptqmodel_available,\n     is_grokadamw_available,\n+    is_grouped_mm_available,\n     is_habana_gaudi1,\n     is_hadamard_available,\n     is_hqq_available,"
        },
        {
            "sha": "529e2f2b84200c3acb13bfbd345323dd22ccd30e",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -552,6 +552,11 @@ def is_torch_flex_attn_available() -> bool:\n     return is_torch_available() and version.parse(get_torch_version()) >= version.parse(\"2.5.0\")\n \n \n+@lru_cache\n+def is_grouped_mm_available() -> bool:\n+    return is_torch_available() and version.parse(get_torch_version()) >= version.parse(\"2.9.0\")\n+\n+\n @lru_cache\n def is_kenlm_available() -> bool:\n     return _is_package_available(\"kenlm\")"
        },
        {
            "sha": "6a9dcdf010b71ef1d1dd26b7170d248f33b295f6",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -176,7 +176,7 @@ def __init__(\n         num_hidden_layers=2,\n         num_attention_heads=2,\n         num_key_value_heads=2,\n-        intermediate_size=37,\n+        intermediate_size=32,\n         hidden_act=\"gelu\",\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,\n@@ -193,7 +193,7 @@ def __init__(\n         scope=None,\n         expert_interval=1,\n         moe_layer_start_index=0,\n-        moe_intermediate_size=12,\n+        moe_intermediate_size=16,\n         shared_expert_intermediate_size=36,\n         shared_expert_gate=True,\n         moe_num_shared_experts=2,"
        },
        {
            "sha": "2f78c9a9e407b9164fc5ea9c5c4cee88870b1960",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 28,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -1220,27 +1220,15 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n                 \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n             }\n \n-            text_config = model.config.get_text_config()\n-            head_dim = (\n-                getattr(text_config, \"head_dim\", None) or text_config.hidden_size // text_config.num_attention_heads\n-            )\n-            num_key_value_heads = (\n-                text_config.num_attention_heads\n-                if getattr(text_config, \"num_key_value_heads\", None) is None\n-                else text_config.num_key_value_heads\n-            )\n-            num_hidden_layers = text_config.num_hidden_layers\n-\n             inputs_embeds = model.get_input_embeddings()(input_ids)\n             outputs = model.generate(inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict)\n \n             # we should get `max_length - 1` in shape, not `max_length - embeds_length`.\n             # -1 because the last generated token isn't yet in the cache.\n+            text_config = model.config.get_text_config()\n             max_length = max_new_tokens + inputs_embeds.shape[1] - 1\n-            cache_shape = [batch_size, num_key_value_heads, max_length, head_dim]\n             self.assertIsInstance(outputs.past_key_values, StaticCache)\n-            self.assertEqual(len(outputs.past_key_values), num_hidden_layers)\n-            self.assertListEqual(list(outputs.past_key_values.layers[0].keys.shape), cache_shape)\n+            self._check_past_key_values_for_generate(batch_size, outputs.past_key_values, max_length, text_config)\n \n     @pytest.mark.generate\n     def test_generate_continue_from_past_key_values(self):\n@@ -1445,22 +1433,12 @@ def test_generate_with_static_cache(self):\n                 )\n \n                 # Check 1: The cache shapes must match the expected shapes\n+                text_config = model.config.get_text_config()\n                 max_cache_len = seq_length + max_new_tokens - 1  # cache len = gen len - 1, the last token has no cache\n-                text_config = config.text_config if hasattr(config, \"text_config\") else config\n-                head_dim = (\n-                    getattr(text_config, \"head_dim\", None)\n-                    or text_config.hidden_size // text_config.num_attention_heads\n-                )\n-                num_key_value_heads = (\n-                    text_config.num_attention_heads\n-                    if getattr(text_config, \"num_key_value_heads\", None) is None\n-                    else text_config.num_key_value_heads\n-                )\n-                num_hidden_layers = text_config.num_hidden_layers\n-                cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n                 self.assertTrue(isinstance(static_cache_generation.past_key_values, StaticCache))\n-                self.assertTrue(len(static_cache_generation.past_key_values) == num_hidden_layers)\n-                self.assertTrue(static_cache_generation.past_key_values.layers[0].keys.shape == cache_shape)\n+                self._check_past_key_values_for_generate(\n+                    batch_size, static_cache_generation.past_key_values, max_cache_len, text_config\n+                )\n \n                 # Check 2: The outputs must be similar to the case with dynamic cache\n                 dynamic_cache_generation = model.generate(**generation_kwargs, **inputs_dict)"
        },
        {
            "sha": "9353642edbaa7c59fcfa5e6885321cbd7c2d002d",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -60,8 +60,8 @@ def __init__(\n         use_labels=True,\n         vocab_size=99,\n         hidden_size=32,\n-        intermediate_size=37,\n-        moe_intermediate_size=12,\n+        intermediate_size=32,\n+        moe_intermediate_size=16,\n         num_hidden_layers=2,\n         num_attention_heads=4,\n         num_key_value_heads=4,\n@@ -76,7 +76,7 @@ def __init__(\n         n_group=2,\n         topk_group=1,\n         num_experts_per_tok=8,\n-        first_k_dense_replace=2,\n+        first_k_dense_replace=1,\n         norm_topk_prob=True,\n         aux_loss_alpha=0.001,\n         hidden_act=\"silu\","
        },
        {
            "sha": "4a79938c17454b1e94cfe7e2ae29463d656288ec",
            "filename": "tests/models/ernie4_5_vl_moe/test_modeling_ernie4_5_vl_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Fernie4_5_vl_moe%2Ftest_modeling_ernie4_5_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Fernie4_5_vl_moe%2Ftest_modeling_ernie4_5_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_vl_moe%2Ftest_modeling_ernie4_5_vl_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -93,7 +93,7 @@ def __init__(\n             self.text_config = {\n                 \"vocab_size\": 99,\n                 \"hidden_size\": 16,\n-                \"intermediate_size\": 22,\n+                \"intermediate_size\": 32,\n                 \"num_hidden_layers\": 2,\n                 \"num_attention_heads\": 2,\n                 \"num_key_value_heads\": 1,\n@@ -102,7 +102,7 @@ def __init__(\n                 \"tie_word_embeddings\": True,\n                 \"rope_parameters\": {\"type\": \"default\", \"rope_theta\": 500_000.0, \"mrope_section\": [1, 1, 2]},\n                 \"mlp_layer_types\": [\"dense\", \"sparse\"],\n-                \"moe_intermediate_size\": [22, 22],\n+                \"moe_intermediate_size\": [32, 32],\n                 \"moe_k\": 2,\n                 \"moe_num_experts\": 8,\n                 \"moe_num_shared_experts\": 2,\n@@ -115,7 +115,7 @@ def __init__(\n                 \"depth\": 2,\n                 \"hidden_size\": 32,\n                 \"hidden_act\": \"silu\",\n-                \"intermediate_size\": 22,\n+                \"intermediate_size\": 32,\n                 \"num_heads\": 2,\n                 \"spatial_merge_size\": 1,\n             }"
        },
        {
            "sha": "8626f70a314e2bb9191d249c452921bd5021b561",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -110,7 +110,7 @@ def __init__(\n         attn_layer_period=8,\n         num_attention_heads=2,\n         num_key_value_heads=2,\n-        intermediate_size=37,\n+        intermediate_size=40,\n         hidden_act=\"gelu\",\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,"
        },
        {
            "sha": "5edd638566fb3aa2bef278285df9000634bca392",
            "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -47,10 +47,14 @@ class Lfm2MoeModelTester(CausalLMModelTester):\n     def __init__(\n         self,\n         parent,\n+        num_dense_layers=1,\n+        num_hidden_layers=2,\n         layer_types=[\"full_attention\", \"conv\"],\n     ):\n         super().__init__(parent)\n         self.layer_types = layer_types\n+        self.num_dense_layers = num_dense_layers\n+        self.num_hidden_layers = num_hidden_layers\n \n \n @require_torch"
        },
        {
            "sha": "8da7e9b6d23ee201d74e697155f225f84b2630ff",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -70,7 +70,7 @@ def __init__(\n         norm_topk_prob=False,\n         output_router_logits=False,\n         router_aux_loss_coef=0.001,\n-        intermediate_size=12,\n+        intermediate_size=16,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "f6306237906239ca972546aed436230ff2d6144c",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -25,6 +25,7 @@\n from collections import defaultdict\n from contextlib import contextmanager\n from copy import deepcopy\n+from unittest.mock import Mock, patch\n \n import numpy as np\n import pytest\n@@ -51,6 +52,7 @@\n     is_deepspeed_zero3_enabled,\n     unset_hf_deepspeed_config,\n )\n+from transformers.integrations.moe import batched_mm_experts_forward, grouped_mm_experts_forward\n from transformers.modeling_layers import GradientCheckpointingLayer\n from transformers.modeling_utils import FLASH_ATTN_KERNEL_FALLBACK, _get_tied_weight_keys\n from transformers.models.auto import get_values\n@@ -509,6 +511,118 @@ def _can_output_attn(model):\n                 )\n \n \n+TEST_EAGER_MATCHES_BATCHED_AND_GROUPED_INFERENCE_PARAMETERIZATION = [\n+    (\n+        # test name for the test runner\n+        f\"{dtype}\",\n+        # parameterization\n+        *(dtype,),\n+    )\n+    for dtype in (\"fp16\", \"fp32\", \"bf16\")\n+]\n+\n+\n+def _get_output_tensors(outputs):\n+    output_tensors = []\n+\n+    if hasattr(outputs, \"logits\"):\n+        output_tensors.append(outputs.logits)\n+    if hasattr(outputs, \"last_hidden_state\"):\n+        output_tensors.append(outputs.last_hidden_state)\n+    if hasattr(outputs, \"start_logits\"):\n+        output_tensors.append(outputs.start_logits)\n+    if hasattr(outputs, \"end_logits\"):\n+        output_tensors.append(outputs.end_logits)\n+\n+    return output_tensors\n+\n+\n+def _test_eager_matches_batched_and_grouped_inference(self, name, dtype):\n+    if not self.all_model_classes[0]._can_set_experts_implementation():\n+        self.skipTest(f\"{self.all_model_classes[0].__name__} does not support grouped_mm\")\n+\n+    # convert shorthand name to torch.dtype\n+    if dtype == \"fp16\":\n+        dtype = torch.float16\n+    elif dtype == \"bf16\":\n+        dtype = torch.bfloat16\n+    elif dtype == \"fp32\":\n+        dtype = torch.float32\n+\n+    if not is_torch_fp16_available_on_device(torch_device) and dtype == torch.float16:\n+        self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n+\n+    if not is_torch_bf16_available_on_device(torch_device) and dtype == torch.bfloat16:\n+        self.skipTest(\n+            f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n+        )\n+\n+    for model_class in self.all_model_classes:\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        set_config_for_less_flaky_test(config)\n+        model = model_class(config)\n+        set_model_for_less_flaky_test(model)\n+\n+        # Load with dtype\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+            model = model_class.from_pretrained(tmpdirname, dtype=dtype).eval().to(torch_device)\n+\n+        with torch.no_grad():\n+            inputs_dict = {k: v.to(dtype) if torch.is_floating_point(v) else v for k, v in inputs_dict.items()}\n+            prepared_inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            mock_batched_mm_forward = Mock(wraps=batched_mm_experts_forward)\n+            mock_grouped_mm_forward = Mock(wraps=grouped_mm_experts_forward)\n+            with (\n+                # This is needed because we call the functions through the interface's global mapping\n+                patch.dict(\n+                    \"transformers.integrations.moe.ALL_EXPERTS_FUNCTIONS._global_mapping\",\n+                    {\"batched_mm\": mock_batched_mm_forward, \"grouped_mm\": mock_grouped_mm_forward},\n+                ),\n+            ):\n+                model.set_experts_implementation(\"eager\")\n+                self.assertEqual(model.config._experts_implementation, \"eager\")\n+                outputs_eager = model(**prepared_inputs)\n+                mock_batched_mm_forward.assert_not_called()\n+                mock_grouped_mm_forward.assert_not_called()\n+\n+                mock_batched_mm_forward.reset_mock()\n+                mock_grouped_mm_forward.reset_mock()\n+\n+                model.set_experts_implementation(\"batched_mm\")\n+                self.assertEqual(model.config._experts_implementation, \"batched_mm\")\n+                outputs_batched_mm = model(**prepared_inputs)\n+                mock_grouped_mm_forward.assert_not_called()\n+                mock_batched_mm_forward.assert_called()\n+\n+                mock_batched_mm_forward.reset_mock()\n+                mock_grouped_mm_forward.reset_mock()\n+\n+                model.set_experts_implementation(\"grouped_mm\")\n+                self.assertEqual(model.config._experts_implementation, \"grouped_mm\")\n+                outputs_grouped_mm = model(**prepared_inputs)\n+                mock_batched_mm_forward.assert_not_called()\n+                mock_grouped_mm_forward.assert_called()\n+\n+                mock_batched_mm_forward.reset_mock()\n+                mock_grouped_mm_forward.reset_mock()\n+\n+        # extract output tensors for comparison\n+        outputs_eager = _get_output_tensors(outputs_eager)\n+        outputs_batched_mm = _get_output_tensors(outputs_batched_mm)\n+        outputs_grouped_mm = _get_output_tensors(outputs_grouped_mm)\n+\n+        # make sure we have collected some tensors from the outputs\n+        self.assertTrue(outputs_eager, \"No outputs from eager implementation\")\n+        self.assertTrue(outputs_batched_mm, \"No outputs from batched_mm implementation\")\n+        self.assertTrue(outputs_grouped_mm, \"No outputs from grouped_mm implementation\")\n+\n+        # make sure all implementations give numerically close outputs\n+        torch.testing.assert_close(outputs_eager, outputs_batched_mm, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs_eager, outputs_grouped_mm, rtol=1e-4, atol=1e-4)\n+\n+\n def _config_zero_init(config):\n     configs_no_init = copy.deepcopy(config)\n     for key in configs_no_init.__dict__:\n@@ -3344,6 +3458,10 @@ def test_eager_matches_sdpa_inference(\n             self, name, dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n         )\n \n+    @parameterized.expand(TEST_EAGER_MATCHES_BATCHED_AND_GROUPED_INFERENCE_PARAMETERIZATION)\n+    def test_eager_matches_batched_and_grouped_inference(self, name, dtype):\n+        _test_eager_matches_batched_and_grouped_inference(self, name, dtype)\n+\n     @require_torch_accelerator\n     @slow\n     def test_sdpa_can_dispatch_on_flash(self):"
        },
        {
            "sha": "d018f95893b4f92f9386df1b5219268aea1be231",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0642963ba13f2dae0596fe489415569e1d91fbda/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=0642963ba13f2dae0596fe489415569e1d91fbda",
            "patch": "@@ -163,6 +163,7 @@ def test_config_common_kwargs_is_complete(self):\n                 \"_name_or_path\",\n                 \"_commit_hash\",\n                 \"_attn_implementation_internal\",\n+                \"_experts_implementation_internal\",\n                 \"transformers_version\",\n             ],\n         )"
        }
    ],
    "stats": {
        "total": 1434,
        "additions": 1060,
        "deletions": 374
    }
}