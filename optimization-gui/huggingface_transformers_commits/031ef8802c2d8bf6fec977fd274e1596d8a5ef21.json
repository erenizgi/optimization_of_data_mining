{
    "author": "Joaquinecc",
    "message": "fix FSDP + torch.compile bug when saving pretrained model  (#37725)\n\n* args keep_torch_compile=False in _save and _wwrap_method\n\n* Fix FSDP execution on evaluation  for torch_compile mode\n\n* add test trainer FSDP + Torch Compile\n\n* fix quality code\n\n* make style\n\n* Revert \" make style\"\n\nThis reverts commit 77e797f8829c50992cc21496be3d9a3e480e1c97.\n\n* make style",
    "sha": "031ef8802c2d8bf6fec977fd274e1596d8a5ef21",
    "files": [
        {
            "sha": "743cc353abe71d8871a1aafd2176b42afab05f88",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/031ef8802c2d8bf6fec977fd274e1596d8a5ef21/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/031ef8802c2d8bf6fec977fd274e1596d8a5ef21/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=031ef8802c2d8bf6fec977fd274e1596d8a5ef21",
            "patch": "@@ -1986,7 +1986,7 @@ def _wrap_model(self, model, training=True, dataloader=None):\n             return smp.DistributedModel(model, backward_passes_per_step=self.args.gradient_accumulation_steps)\n \n         # train/eval could be run multiple-times - if already wrapped, don't re-wrap it again\n-        if self.accelerator.unwrap_model(model) is not model:\n+        if self.accelerator.unwrap_model(model, keep_torch_compile=False) is not model:\n             return model\n \n         # Mixed precision training with apex\n@@ -3998,8 +3998,8 @@ def _save(self, output_dir: Optional[str] = None, state_dict=None):\n             if state_dict is None:\n                 state_dict = self.model.state_dict()\n \n-            if isinstance(self.accelerator.unwrap_model(self.model), supported_classes):\n-                self.accelerator.unwrap_model(self.model).save_pretrained(\n+            if isinstance(self.accelerator.unwrap_model(self.model, keep_torch_compile=False), supported_classes):\n+                self.accelerator.unwrap_model(self.model, keep_torch_compile=False).save_pretrained(\n                     output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors\n                 )\n             else:\n@@ -4296,7 +4296,8 @@ def evaluation_loop(\n             start_time = time.time()\n             model = (\n                 self.accelerator.prepare(model)\n-                if self.is_deepspeed_enabled or (self.is_fsdp_enabled and self.accelerator.mixed_precision != \"fp8\")\n+                if self.is_deepspeed_enabled\n+                or (self.is_fsdp_enabled and self.accelerator.mixed_precision != \"fp8\" and not self.args.torch_compile)\n                 else self.accelerator.prepare_model(model, evaluation_mode=True)\n             )\n             self.model_preparation_time = round(time.time() - start_time, 4)"
        },
        {
            "sha": "1e30f9f4549d684cecd84b55eca8bea82a16534b",
            "filename": "tests/trainer/test_trainer_fsdp.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/031ef8802c2d8bf6fec977fd274e1596d8a5ef21/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/031ef8802c2d8bf6fec977fd274e1596d8a5ef21/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_fsdp.py?ref=031ef8802c2d8bf6fec977fd274e1596d8a5ef21",
            "patch": "@@ -147,6 +147,34 @@ def test_trainer(self):\n         # successful return here == success - any errors would have caused an error in the sub-call\n \n \n+class TestFSDPTrainerTorchCompile(TestCasePlus):\n+    @require_torch_multi_accelerator\n+    @require_accelerate\n+    @run_first\n+    def test_trainer(self):\n+        output_dir = self.get_auto_remove_tmp_dir()\n+        cmd = [\n+            \"accelerate\",\n+            \"launch\",\n+            \"--use_fsdp\",\n+            \"--main_process_port\",\n+            f\"{get_torch_dist_unique_port()}\",\n+            \"--num_processes\",\n+            f\"{backend_device_count(torch_device)}\",\n+            \"--fsdp_transformer_layer_cls_to_wrap\",\n+            \"GPT2Block\",\n+            f\"{self.test_file_dir}/test_trainer_fsdp.py\",\n+            \"--torch_compile_mode\",\n+            \"default\",\n+            \"--output_dir\",\n+            f\"{output_dir}\",\n+            \"--report_to\",\n+            \"none\",\n+        ]\n+        execute_subprocess_async(cmd, env=self.get_env())\n+        # successful return here == success - any errors would have caused an error in the sub-call\n+\n+\n if __name__ == \"__main__\":\n     parser = HfArgumentParser((Seq2SeqTrainingArguments,))\n     training_args = parser.parse_args_into_dataclasses()[0]"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 33,
        "deletions": 4
    }
}