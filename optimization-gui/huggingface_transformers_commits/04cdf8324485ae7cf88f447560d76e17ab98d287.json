{
    "author": "ydshieh",
    "message": "Update some tests for torch 2.7.1 (#38701)\n\n* fix 1\n\n* fix 2\n\n* fix 3\n\n* fix 4\n\n* fp16\n\n* break\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "04cdf8324485ae7cf88f447560d76e17ab98d287",
    "files": [
        {
            "sha": "842c3ecddb6ec1ca4c1502022c41fdeb670db638",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=04cdf8324485ae7cf88f447560d76e17ab98d287",
            "patch": "@@ -463,11 +463,11 @@ def test_model_7b_batched(self):\n                     'What constellation is this image showing?The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.',\n                 ],\n                 (\"cuda\", 7): [\n-                    'Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue line extending across the center of the image. The line is labeled \"390 light years\" and is accompanied by a small black and',\n+                    'Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue dot representing the position of the star Alpha Centauri. Alpha Centauri is the brightest star in the constellation Centaurus and is located',\n                     'What constellation is this image showing?The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.',\n                 ],\n                 (\"cuda\", 8): [\n-                    'Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue dot in the center representing the star Alpha Centauri. The star map is a representation of the night sky, showing the positions of stars in',\n+                    'Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue dot representing the position of the star Alpha Centauri. Alpha Centauri is the brightest star in the constellation Centaurus and is located',\n                     'What constellation is this image showing?The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.',\n                 ],\n             }"
        },
        {
            "sha": "53a37108da8352afbe5d7163f89f5c6112017552",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=04cdf8324485ae7cf88f447560d76e17ab98d287",
            "patch": "@@ -299,7 +299,7 @@ def test_model_integration_test(self):\n         \"\"\"\n         model = ColQwen2ForRetrieval.from_pretrained(\n             self.model_name,\n-            torch_dtype=torch.bfloat16,\n+            torch_dtype=torch.float16,\n             load_in_8bit=True,\n         ).eval()\n \n@@ -331,14 +331,14 @@ def test_model_integration_test(self):\n         expectations = Expectations(\n             {\n                 (\"cuda\", 7): [\n-                    [15.5000, 8.1250, 14.9375],\n-                    [9.0625, 17.1250, 10.6875],\n-                    [15.9375, 12.1875, 20.2500],\n+                    [15.0938, 8.3203, 15.0391],\n+                    [9.6328, 16.9062, 10.5312],\n+                    [15.6562, 12.2656, 20.2969],\n                 ],\n                 (\"cuda\", 8): [\n-                    [15.1250, 8.6875, 15.0625],\n-                    [9.2500, 17.2500, 10.3750],\n-                    [15.9375, 12.3750, 20.2500],\n+                    [15.0703, 8.7422, 15.0312],\n+                    [9.5078, 16.8906, 10.6250],\n+                    [15.6484, 12.3984, 20.4688],\n                 ],\n             }\n         )"
        },
        {
            "sha": "cb9b8c689272d728761b36a2d428466017771290",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 71,
            "deletions": 55,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=04cdf8324485ae7cf88f447560d76e17ab98d287",
            "patch": "@@ -292,35 +292,36 @@ def tearDown(self):\n     def test_qwen2_small_model_integration_generate(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n         prompt = (\n             \"<|im_start|>user\\n<IMG_CONTEXT>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n         )\n-        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n         with torch.no_grad():\n             generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n-        expected_output = \"The image shows two cats lying on a pink blanket. The cat on the left is a tabby\"\n+        expected_output = \"The image shows two cats lying on a pink surface, which appears to be a bed or couch.\"\n+\n         self.assertEqual(decoded_output, expected_output)\n \n     def test_qwen2_small_model_integration_forward(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n         prompt = (\n             \"<|im_start|>user\\n<IMG_CONTEXT>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n         )\n-        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n \n         # Forward\n         with torch.inference_mode():\n@@ -329,9 +330,9 @@ def test_qwen2_small_model_integration_forward(self):\n         actual_logits = output.logits[0, -1, :5].cpu()\n         expected_logits_all = Expectations(\n             {\n-                (\"xpu\", 3): torch.tensor([11.7500, 14.7500, 14.1250, 10.5625, 6.7812], dtype=torch.bfloat16),\n-                (\"cuda\", 7): torch.tensor([11.9375, 14.7500, 14.4375, 10.8125,  7.0938], dtype=torch.bfloat16),\n-                (\"cuda\", 8): torch.tensor([11.8750, 14.8125, 14.3125, 10.8125,  6.9375], dtype=torch.bfloat16),\n+                (\"xpu\", 3): torch.tensor([11.7500, 14.7500, 14.1250, 10.5625, 6.7812], dtype=torch.float16),\n+                (\"cuda\", 7): torch.tensor([11.9531, 14.7031, 14.2734, 10.6562,  6.9219], dtype=torch.float16),\n+                (\"cuda\", 8): torch.tensor([11.9609, 14.7188, 14.2734, 10.6484,  6.9141], dtype=torch.float16),\n             }\n         )  # fmt: skip\n         expected_logits = expected_logits_all.get_expectation()\n@@ -347,10 +348,10 @@ def test_qwen2_small_model_integration_forward(self):\n     def test_qwen2_small_model_integration_generate_text_only(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         prompt = \"<|im_start|>user\\nWrite a haiku<|im_end|>\\n<|im_start|>assistant\\n\"\n-        inputs = processor(text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        inputs = processor(text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n         with torch.no_grad():\n             generate_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n             decoded_output = processor.decode(\n@@ -360,8 +361,8 @@ def test_qwen2_small_model_integration_generate_text_only(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light.\",\n-                (\"cuda\", 7): \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light.\",\n-                (\"cuda\", 8): \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light begins.\",\n+                (\"cuda\", 7): 'Whispers of dawn,\\nSilent whispers of night,\\nPeace in the stillness.',\n+                (\"cuda\", 8): 'Whispers of dawn,\\nSilent whispers of night,\\nPeace in the stillness.',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -371,7 +372,7 @@ def test_qwen2_small_model_integration_generate_text_only(self):\n     def test_qwen2_small_model_integration_generate_chat_template(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         messages = [\n             {\n@@ -385,20 +386,21 @@ def test_qwen2_small_model_integration_generate_chat_template(self):\n \n         inputs = processor.apply_chat_template(\n             messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n-        ).to(torch_device, dtype=torch.bfloat16)\n+        ).to(torch_device, dtype=torch.float16)\n         with torch.no_grad():\n             generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n-        expected_output = \"The image shows two cats lying on a pink blanket. The cat on the left is a tabby\"\n+        expected_output = \"The image shows two cats lying on a pink surface, which appears to be a bed or couch.\"\n+\n         self.assertEqual(decoded_output, expected_output)\n \n     @require_deterministic_for_xpu\n     def test_qwen2_small_model_integration_batched_generate(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         # Prepare inputs\n         prompt = [\n@@ -409,14 +411,15 @@ def test_qwen2_small_model_integration_batched_generate(self):\n         image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n \n         inputs = processor(text=prompt, images=[[image1], [image2]], padding=True, return_tensors=\"pt\").to(\n-            torch_device, dtype=torch.bfloat16\n+            torch_device, dtype=torch.float16\n         )\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n \n         # Check first output\n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n         expected_output = \"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\"  # fmt: skip\n+\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -428,7 +431,7 @@ def test_qwen2_small_model_integration_batched_generate(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate\"',\n-                (\"cuda\", 7): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Arch,\"',\n+                (\"cuda\", 7): 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate of',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -442,7 +445,7 @@ def test_qwen2_small_model_integration_batched_generate(self):\n     def test_qwen2_small_model_integration_batched_generate_multi_image(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         # Prepare inputs\n         prompt = [\n@@ -466,7 +469,7 @@ def test_qwen2_small_model_integration_batched_generate_multi_image(self):\n         )\n \n         inputs = processor(text=prompt, images=[[image1], [image2, image3]], padding=True, return_tensors=\"pt\").to(\n-            torch_device, dtype=torch.bfloat16\n+            torch_device, dtype=torch.float16\n         )\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n@@ -548,7 +551,7 @@ def test_qwen2_medium_model_integration_video(self):\n     def test_qwen2_small_model_integration_interleaved_images_videos(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, torch_dtype=torch.bfloat16, device_map=torch_device\n+            self.small_model_checkpoint, torch_dtype=torch.float16, device_map=torch_device\n         )\n         messages = [\n             [\n@@ -600,7 +603,7 @@ def test_qwen2_small_model_integration_interleaved_images_videos(self):\n             return_tensors=\"pt\",\n             padding=True,\n             num_frames=8,\n-        ).to(torch_device, dtype=torch.bfloat16)\n+        ).to(torch_device, dtype=torch.float16)\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n \n@@ -609,10 +612,11 @@ def test_qwen2_small_model_integration_interleaved_images_videos(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image:**\\n   - The Statue of Liberty is prominently featured on an\",\n-                (\"cuda\", 7): \"user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image:**\\n   - The Statue of Liberty is prominently featured on an\",\n+                (\"cuda\", 7): 'user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image:**\\n   - The Statue of Liberty is prominently featured on an',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -623,7 +627,7 @@ def test_qwen2_small_model_integration_interleaved_images_videos(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot.\",\n-                (\"cuda\", 7): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot.\",\n+                (\"cuda\", 7): 'user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nA forehand shot',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -635,7 +639,9 @@ def test_qwen2_small_model_integration_interleaved_images_videos(self):\n \n         # Check third output\n         decoded_output = processor.decode(output[2], skip_special_tokens=True)\n-        expected_output = \"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\"  # fmt: skip\n+        expected_output = (\n+            \"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\"\n+        )\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -657,15 +663,15 @@ def tearDown(self):\n     def test_llama_small_model_integration_generate(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n         prompt = (\n             \"<|im_start|>user\\n<IMG_CONTEXT>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n         )\n-        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n         with torch.no_grad():\n             generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n             decoded_output = processor.decode(\n@@ -677,15 +683,15 @@ def test_llama_small_model_integration_generate(self):\n     def test_llama_small_model_integration_forward(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n         prompt = (\n             \"<|im_start|>user\\n<IMG_CONTEXT>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n         )\n-        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n \n         # Forward\n         with torch.inference_mode():\n@@ -695,12 +701,12 @@ def test_llama_small_model_integration_forward(self):\n \n         expected_logits_all = Expectations(\n             {\n-                (\"xpu\", 3): torch.tensor([-9.8750, -0.5703, 1.4297, -10.3125, -10.3125], dtype=torch.bfloat16),\n-                (\"cuda\", 7): torch.tensor([-9.8750, -0.5703, 1.4297, -10.3125, -10.3125], dtype=torch.bfloat16),\n-                (\"cuda\", 8): torch.tensor([-9.8750,  -0.5117,   1.4297, -10.3750, -10.3750], dtype=torch.bfloat16),\n+                (\"xpu\", 3): torch.tensor([-9.8750, -0.5703, 1.4297, -10.3125, -10.3125], dtype=torch.float16),\n+                (\"cuda\", 7): torch.tensor([-9.8750,  -0.4861,   1.4648, -10.3359, -10.3359], dtype=torch.float16),\n+                (\"cuda\", 8): torch.tensor([-9.8906,  -0.4995,   1.4473, -10.3359, -10.3438], dtype=torch.float16),\n             }\n         )  # fmt: skip\n-        expected_logits = torch.tensor(expected_logits_all.get_expectation(), dtype=torch.bfloat16)\n+        expected_logits = torch.tensor(expected_logits_all.get_expectation(), dtype=torch.float16)\n \n         # The original implementation and the transformers implementation do not match exactly, hence the higher tolerance.\n         # The difference is likely due to the different implementations of the attention mechanism (different order of operations)\n@@ -716,22 +722,30 @@ def test_llama_small_model_integration_forward(self):\n     def test_llama_small_model_integration_generate_text_only(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         prompt = \"<|im_start|>user\\nWrite a haiku<|im_end|>\\n<|im_start|>assistant\\n\"\n-        inputs = processor(text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        inputs = processor(text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.float16)\n         with torch.no_grad():\n             generate_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n-        expected_output = \"Autumn leaves fall,\\nNature's breath, a season's sigh,\\nSilent woods awake.\"\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"cuda\", 7): \"Autumn leaves fall,\\nNature's breath, a gentle sigh,\\nSilent whispers.\",\n+                (\"cuda\", 8): \"Autumn leaves fall,\\nNature's breath, a silent sigh,\\nWinter's chill approaches.\",\n+            }\n+        )\n+        expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(decoded_output, expected_output)\n \n     def test_llama_small_model_integration_generate_chat_template(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         messages = [\n             {\n@@ -745,7 +759,7 @@ def test_llama_small_model_integration_generate_chat_template(self):\n \n         inputs = processor.apply_chat_template(\n             messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n-        ).to(torch_device, dtype=torch.bfloat16)\n+        ).to(torch_device, dtype=torch.float16)\n         with torch.no_grad():\n             generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n             decoded_output = processor.decode(\n@@ -757,7 +771,7 @@ def test_llama_small_model_integration_generate_chat_template(self):\n     def test_llama_small_model_integration_batched_generate(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         # Prepare inputs\n         prompt = [\n@@ -768,7 +782,7 @@ def test_llama_small_model_integration_batched_generate(self):\n         image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n \n         inputs = processor(text=prompt, images=[[image1], [image2]], padding=True, return_tensors=\"pt\").to(\n-            torch_device, dtype=torch.bfloat16\n+            torch_device, dtype=torch.float16\n         )\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n@@ -778,11 +792,12 @@ def test_llama_small_model_integration_batched_generate(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden path leads to calm lake,\\nNature's peaceful grace.\",\n-                (\"cuda\", 7): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden path leads to calm lake,\\nNature's peaceful grace.\",\n-                (\"cuda\", 8): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nA wooden path leads to the sea,\\nPeaceful, still waters.\",\n+                (\"cuda\", 7): 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.',\n+                (\"cuda\", 8): 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -791,7 +806,7 @@ def test_llama_small_model_integration_batched_generate(self):\n \n         # Check second output\n         decoded_output = processor.decode(output[1], skip_special_tokens=True)\n-        expected_output = 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese gate in the background, adorned with red and gold colors and Chinese characters'  # fmt: skip\n+        expected_output = \"user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese gate in the background, adorned with red and gold colors and Chinese characters\"\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -801,7 +816,7 @@ def test_llama_small_model_integration_batched_generate(self):\n     def test_llama_small_model_integration_batched_generate_multi_image(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.float16\n         )\n         # Prepare inputs\n         prompt = [\n@@ -825,15 +840,16 @@ def test_llama_small_model_integration_batched_generate_multi_image(self):\n         )\n \n         inputs = processor(text=prompt, images=[[image1], [image2, image3]], padding=True, return_tensors=\"pt\").to(\n-            torch_device, dtype=torch.bfloat16\n+            torch_device, dtype=torch.float16\n         )\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n \n         # Check first output\n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n         # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n-        expected_output = \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden path leads to calm lake,\\nNature's peaceful grace.\"  # fmt: skip\n+        expected_output = \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\"\n+\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -842,7 +858,7 @@ def test_llama_small_model_integration_batched_generate_multi_image(self):\n \n         # Check second output\n         decoded_output = processor.decode(output[1], skip_special_tokens=True)\n-        expected_output = 'user\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After closely examining the images again, I can see that there are several differences'  # fmt: skip\n+        expected_output = \"user\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After closely examining the images again, I can see that there are several differences\"\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -893,7 +909,7 @@ def test_llama_medium_model_integration_video(self):\n     def test_llama_small_model_integration_interleaved_images_videos(self):\n         processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n         model = InternVLForConditionalGeneration.from_pretrained(\n-            self.small_model_checkpoint, torch_dtype=torch.bfloat16, device_map=torch_device\n+            self.small_model_checkpoint, torch_dtype=torch.float16, device_map=torch_device\n         )\n         messages = [\n             [\n@@ -945,7 +961,7 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n             return_tensors=\"pt\",\n             padding=True,\n             num_frames=8,\n-        ).to(torch_device, dtype=torch.bfloat16)\n+        ).to(torch_device, dtype=torch.float16)\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n \n@@ -954,8 +970,8 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After re-examining the images, I can see that they are actually\",\n-                (\"cuda\", 7): \"user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After re-examining the images, I can see that they are actually\",\n-                (\"cuda\", 8): \"user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After closely examining the images again, I can see that there are several differences\",\n+                (\"cuda\", 7): 'user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. Upon closer inspection, the differences between the two images are:\\n\\n1. **',\n+                (\"cuda\", 8): 'user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After re-examining the images, I can see that there are no',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -970,8 +986,8 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their\",\n-                (\"cuda\", 7): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their\",\n-                (\"cuda\", 8): \"user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their\",\n+                (\"cuda\", 7): 'user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common stroke in tennis where the player swings the racket across their',\n+                (\"cuda\", 8): 'user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common stroke in tennis where the player swings the racket across their',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -986,8 +1002,8 @@ def test_llama_small_model_integration_interleaved_images_videos(self):\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n-                (\"cuda\", 7): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n-                (\"cuda\", 8): \"user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.\",\n+                (\"cuda\", 7): 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.',\n+                (\"cuda\", 8): 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()"
        },
        {
            "sha": "7f4d147cb26b0e4b5b9388a388122761921e2322",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=04cdf8324485ae7cf88f447560d76e17ab98d287",
            "patch": "@@ -248,7 +248,7 @@ def test_export_static_cache(self):\n         tokenizer = AutoTokenizer.from_pretrained(qwen_model, pad_token=\"</s>\", padding_side=\"right\")\n         if is_torch_greater_or_equal(\"2.7.0\"):\n             strict = False  # Due to https://github.com/pytorch/pytorch/issues/150994\n-            EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% plain, unsalted, unsweetened, and unflavored.\"]\n+            EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% plain, unflavoured, and unadulterated.\"]\n         else:\n             strict = True\n             EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% plain, unflavoured, and unadulterated. It is\"]"
        },
        {
            "sha": "e98566db1c69b75d41a7954c873f4d06c8bec198",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04cdf8324485ae7cf88f447560d76e17ab98d287/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=04cdf8324485ae7cf88f447560d76e17ab98d287",
            "patch": "@@ -422,11 +422,13 @@ def test_xglm_sample(self):\n         output_ids = model.generate(input_ids, do_sample=True, num_beams=1)\n         output_str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n \n-        EXPECTED_OUTPUT_STR = (\n-            \"Today is a nice day and the water is still cold. We just stopped off for some fresh coffee. This place\"\n-            \" looks like a\"\n-        )\n-        self.assertEqual(output_str, EXPECTED_OUTPUT_STR)\n+        EXPECTED_OUTPUT_STRS = [\n+            # torch 2.6\n+            \"Today is a nice day and the water is still cold. We just stopped off for some fresh coffee. This place looks like a\",\n+            # torch 2.7\n+            \"Today is a nice day and the sun is shining. A nice day with warm rainy and windy weather today.\",\n+        ]\n+        self.assertIn(output_str, EXPECTED_OUTPUT_STRS)\n \n     @require_torch_accelerator\n     @require_torch_fp16"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 88,
        "deletions": 70
    }
}