{
    "author": "faaany",
    "message": "make `test_speculative_decoding_non_distil` device-agnostic (#38010)\n\n* make device-agnostic\n\n* use condition\n\n---------\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "c7c2f08994b1cacd77ead61e1627b7017df13bf9",
    "files": [
        {
            "sha": "519aed351138d4206ff1affb0c2123d786923f69",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7c2f08994b1cacd77ead61e1627b7017df13bf9/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7c2f08994b1cacd77ead61e1627b7017df13bf9/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=c7c2f08994b1cacd77ead61e1627b7017df13bf9",
            "patch": "@@ -2495,9 +2495,9 @@ def test_speculative_decoding_distil(self):\n         self.assertTrue(total_time_non_assist > total_time_assist, \"Make sure that assistant decoding is faster\")\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_speculative_decoding_non_distil(self):\n-        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n+        torch_dtype = torch.float16 if torch_device in [\"cuda\", \"xpu\"] else torch.float32\n         model_id = \"openai/whisper-large-v2\"\n         model = WhisperForConditionalGeneration.from_pretrained(\n             model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}