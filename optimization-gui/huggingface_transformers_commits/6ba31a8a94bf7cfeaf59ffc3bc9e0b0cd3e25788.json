{
    "author": "muellerzr",
    "message": "Enable users to use their own loss functions + deal with prefetching for grad accum (#34198)\n\n* bookmark\r\n\r\n* Bookmark\r\n\r\n* Bookmark\r\n\r\n* Actually implement\r\n\r\n* Pass in kwarg explicitly\r\n\r\n* Adjust for if we do or don't have labels\r\n\r\n* Bookmark fix for od\r\n\r\n* bookmark\r\n\r\n* Fin\r\n\r\n* closer\r\n\r\n* Negate accelerate grad accum div\r\n\r\n* Fixup not training long enough\r\n\r\n* Add in compute_loss to take full model output\r\n\r\n* Document\r\n\r\n* compute_loss -> compute_loss_fn\r\n\r\n* Add a test\r\n\r\n* Refactor\r\n\r\n* Refactor\r\n\r\n* Uncomment tests\r\n\r\n* Update tests/trainer/test_trainer.py\r\n\r\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>",
    "sha": "6ba31a8a94bf7cfeaf59ffc3bc9e0b0cd3e25788",
    "files": [
        {
            "sha": "58a20f66f4e81bb91c119af7a571588eea2207a3",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 167,
            "deletions": 123,
            "changes": 290,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ba31a8a94bf7cfeaf59ffc3bc9e0b0cd3e25788/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ba31a8a94bf7cfeaf59ffc3bc9e0b0cd3e25788/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=6ba31a8a94bf7cfeaf59ffc3bc9e0b0cd3e25788",
            "patch": "@@ -340,12 +340,16 @@ class Trainer:\n             The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n             be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n             inner layers, dropout probabilities etc).\n+        compute_loss_func (`Callable`, *optional*):\n+            A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated\n+            batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, here is one using\n+            the loss function from `transformers`\n         compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n             The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n             a dictionary string to metric values. *Note* When passing TrainingArgs with `batch_eval_metrics` set to\n             `True`, your compute_metrics function must take a boolean `compute_result` argument. This will be triggered\n             after the last eval batch to signal that the function needs to calculate and return the global summary\n-            statistics rather than accumulating the batch-level statistics.\n+            statistics rather than accumulating the batch-level statistics\n         callbacks (List of [`TrainerCallback`], *optional*):\n             A list of callbacks to customize the training loop. Will add those to the list of default callbacks\n             detailed in [here](callback).\n@@ -394,6 +398,7 @@ def __init__(\n             Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin]\n         ] = None,\n         model_init: Optional[Callable[[], PreTrainedModel]] = None,\n+        compute_loss_func: Optional[Callable] = None,\n         compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n         callbacks: Optional[List[TrainerCallback]] = None,\n         optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n@@ -415,6 +420,7 @@ def __init__(\n                 f\"You have set `args.eval_strategy` to {args.eval_strategy} but you didn't pass an `eval_dataset` to `Trainer`. Either set `args.eval_strategy` to `no` or pass an `eval_dataset`. \"\n             )\n         self.args = args\n+        self.compute_loss_func = compute_loss_func\n         # Seed must be set before instantiating the model when using model\n         enable_full_determinism(self.args.seed) if self.args.full_determinism else set_seed(self.args.seed)\n \n@@ -2369,16 +2375,16 @@ def _inner_training_loop(\n \n         total_batched_samples = 0\n         for epoch in range(epochs_trained, num_train_epochs):\n-            epoch_iterator = train_dataloader\n-            if hasattr(epoch_iterator, \"set_epoch\"):\n-                epoch_iterator.set_epoch(epoch)\n+            epoch_dataloader = train_dataloader\n+            if hasattr(epoch_dataloader, \"set_epoch\"):\n+                epoch_dataloader.set_epoch(epoch)\n \n             # Reset the past mems state at the beginning of each epoch if necessary.\n             if args.past_index >= 0:\n                 self._past = None\n \n             steps_in_epoch = (\n-                len(epoch_iterator)\n+                len(epoch_dataloader)\n                 if len_dataloader is not None\n                 else args.max_steps * args.gradient_accumulation_steps\n             )\n@@ -2390,142 +2396,154 @@ def _inner_training_loop(\n             rng_to_sync = False\n             steps_skipped = 0\n             if steps_trained_in_current_epoch > 0:\n-                epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)\n+                epoch_dataloader = skip_first_batches(epoch_dataloader, steps_trained_in_current_epoch)\n                 steps_skipped = steps_trained_in_current_epoch\n                 steps_trained_in_current_epoch = 0\n                 rng_to_sync = True\n \n             step = -1\n-            for step, inputs in enumerate(epoch_iterator):\n-                total_batched_samples += 1\n-\n-                if self.args.include_num_input_tokens_seen:\n-                    main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n-                    if main_input_name not in inputs:\n-                        logger.warning(\n-                            \"Tried to track the number of tokens seen, however the current model is \"\n-                            \"not configured properly to know what item is the input. To fix this, add \"\n-                            \"a `main_input_name` attribute to the model class you are using.\"\n-                        )\n+            epoch_iterator = iter(epoch_dataloader)\n+            # We chunkify the epoch iterator into gradient accumulation steps `n` batches\n+            remainder = num_examples % args.gradient_accumulation_steps\n+            num_items_in_batch = None\n+            if remainder == 0:\n+                remainder = args.gradient_accumulation_steps\n+            update_step = -1\n+            total_updates = steps_in_epoch // args.gradient_accumulation_steps + 1\n+            for _ in range(total_updates):\n+                update_step += 1\n+                num_batches = args.gradient_accumulation_steps if update_step != (total_updates - 1) else remainder\n+                batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)\n+                for inputs in batch_samples:\n+                    step += 1\n+                    total_batched_samples += 1\n+                    # Since we perform prefetching, we need to manually set sync_gradients\n+                    if total_batched_samples % args.gradient_accumulation_steps != 0:\n+                        self.accelerator.gradient_state._set_sync_gradients(False)\n                     else:\n-                        self.state.num_input_tokens_seen += (\n-                            torch.sum(\n-                                self.accelerator.gather(\n-                                    torch.tensor(\n-                                        inputs[main_input_name].numel(), device=self.args.device, dtype=torch.int64\n-                                    )\n-                                )\n+                        self.accelerator.gradient_state._set_sync_gradients(True)\n+\n+                    if self.args.include_num_input_tokens_seen:\n+                        main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n+                        if main_input_name not in inputs:\n+                            logger.warning(\n+                                \"Tried to track the number of tokens seen, however the current model is \"\n+                                \"not configured properly to know what item is the input. To fix this, add \"\n+                                \"a `main_input_name` attribute to the model class you are using.\"\n                             )\n-                            .cpu()\n-                            .item()\n-                        )\n-                if rng_to_sync:\n-                    self._load_rng_state(resume_from_checkpoint)\n-                    rng_to_sync = False\n-\n-                # Skip past any already trained steps if resuming training\n-                if steps_trained_in_current_epoch > 0:\n-                    steps_trained_in_current_epoch -= 1\n-                    if steps_trained_progress_bar is not None:\n-                        steps_trained_progress_bar.update(1)\n-                    if steps_trained_in_current_epoch == 0:\n+                        else:\n+                            input_tokens = inputs[main_input_name].numel()\n+                            input_tokens = torch.tensor(input_tokens, device=self.args.device, dtype=torch.int64)\n+                            self.state.num_input_tokens_seen += self.accelerator.gather(input_tokens).cpu().item()\n+                    if rng_to_sync:\n                         self._load_rng_state(resume_from_checkpoint)\n-                    continue\n-                elif steps_trained_progress_bar is not None:\n-                    steps_trained_progress_bar.close()\n-                    steps_trained_progress_bar = None\n-\n-                if step % args.gradient_accumulation_steps == 0:\n-                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n-\n-                with self.accelerator.accumulate(model):\n-                    tr_loss_step = self.training_step(model, inputs)\n-\n-                if (\n-                    args.logging_nan_inf_filter\n-                    and not is_torch_xla_available()\n-                    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n-                ):\n-                    # if loss is nan or inf simply add the average of previous logged losses\n-                    tr_loss = tr_loss + tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n-                else:\n-                    if tr_loss.device != tr_loss_step.device:\n-                        raise ValueError(\n-                            f\"Calculated loss must be on the original device: {tr_loss.device} but device in use is {tr_loss_step.device}\"\n-                        )\n-                    tr_loss = tr_loss + tr_loss_step\n+                        rng_to_sync = False\n+\n+                    # Skip past any already trained steps if resuming training\n+                    if steps_trained_in_current_epoch > 0:\n+                        steps_trained_in_current_epoch -= 1\n+                        if steps_trained_progress_bar is not None:\n+                            steps_trained_progress_bar.update(1)\n+                        if steps_trained_in_current_epoch == 0:\n+                            self._load_rng_state(resume_from_checkpoint)\n+                        continue\n+                    elif steps_trained_progress_bar is not None:\n+                        steps_trained_progress_bar.close()\n+                        steps_trained_progress_bar = None\n+\n+                    if step % args.gradient_accumulation_steps == 0:\n+                        self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n+\n+                    with self.accelerator.accumulate(model):\n+                        tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n+\n+                    if (\n+                        args.logging_nan_inf_filter\n+                        and not is_torch_xla_available()\n+                        and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n+                    ):\n+                        # if loss is nan or inf simply add the average of previous logged losses\n+                        tr_loss = tr_loss + tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n+                    else:\n+                        if tr_loss.device != tr_loss_step.device:\n+                            raise ValueError(\n+                                f\"Calculated loss must be on the original device: {tr_loss.device} but device in use is {tr_loss_step.device}\"\n+                            )\n+                        tr_loss = tr_loss + tr_loss_step\n \n-                self.current_flos += float(self.floating_point_ops(inputs))\n+                    self.current_flos += float(self.floating_point_ops(inputs))\n \n-                is_last_step_and_steps_less_than_grad_acc = (\n-                    steps_in_epoch <= args.gradient_accumulation_steps and (step + 1) == steps_in_epoch\n-                )\n+                    is_last_step_and_steps_less_than_grad_acc = (\n+                        steps_in_epoch <= args.gradient_accumulation_steps and (step + 1) == steps_in_epoch\n+                    )\n \n-                if (\n-                    total_batched_samples % args.gradient_accumulation_steps == 0\n-                    or\n-                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n-                    is_last_step_and_steps_less_than_grad_acc\n-                ):\n-                    # the `or` condition of `is_last_step_and_steps_less_than_grad_acc` is not covered\n-                    # in accelerate. So, explicitly enable sync gradients to True in that case.\n-                    if is_last_step_and_steps_less_than_grad_acc:\n+                    if (\n+                        (total_batched_samples) % args.gradient_accumulation_steps == 0\n+                        or\n+                        # last step in epoch but step is always smaller than gradient_accumulation_steps\n+                        is_last_step_and_steps_less_than_grad_acc\n+                    ):\n+                        # Since we perform prefetching, we need to manually set sync_gradients to True\n                         self.accelerator.gradient_state._set_sync_gradients(True)\n \n-                    # Gradient clipping\n-                    if args.max_grad_norm is not None and args.max_grad_norm > 0:\n-                        # deepspeed does its own clipping\n-\n-                        if is_sagemaker_mp_enabled() and args.fp16:\n-                            _grad_norm = self.optimizer.clip_master_grads(args.max_grad_norm)\n-                        elif self.use_apex:\n-                            # Revert to normal clipping otherwise, handling Apex or full precision\n-                            _grad_norm = nn.utils.clip_grad_norm_(\n-                                amp.master_params(self.optimizer),\n-                                args.max_grad_norm,\n-                            )\n-                        else:\n-                            _grad_norm = self.accelerator.clip_grad_norm_(\n-                                model.parameters(),\n-                                args.max_grad_norm,\n-                            )\n-\n-                        if (\n-                            is_accelerate_available()\n-                            and self.accelerator.distributed_type == DistributedType.DEEPSPEED\n-                        ):\n-                            grad_norm = model.get_global_grad_norm()\n-                            # In some cases the grad norm may not return a float\n-                            if hasattr(grad_norm, \"item\"):\n-                                grad_norm = grad_norm.item()\n-                        else:\n-                            grad_norm = _grad_norm\n+                        # Gradient clipping\n+                        if args.max_grad_norm is not None and args.max_grad_norm > 0:\n+                            # deepspeed does its own clipping\n+\n+                            if is_sagemaker_mp_enabled() and args.fp16:\n+                                _grad_norm = self.optimizer.clip_master_grads(args.max_grad_norm)\n+                            elif self.use_apex:\n+                                # Revert to normal clipping otherwise, handling Apex or full precision\n+                                _grad_norm = nn.utils.clip_grad_norm_(\n+                                    amp.master_params(self.optimizer),\n+                                    args.max_grad_norm,\n+                                )\n+                            else:\n+                                _grad_norm = self.accelerator.clip_grad_norm_(\n+                                    model.parameters(),\n+                                    args.max_grad_norm,\n+                                )\n \n-                    self.control = self.callback_handler.on_pre_optimizer_step(args, self.state, self.control)\n+                            if (\n+                                is_accelerate_available()\n+                                and self.accelerator.distributed_type == DistributedType.DEEPSPEED\n+                            ):\n+                                grad_norm = model.get_global_grad_norm()\n+                                # In some cases the grad norm may not return a float\n+                                if hasattr(grad_norm, \"item\"):\n+                                    grad_norm = grad_norm.item()\n+                            else:\n+                                grad_norm = _grad_norm\n \n-                    self.optimizer.step()\n+                        self.control = self.callback_handler.on_pre_optimizer_step(args, self.state, self.control)\n \n-                    self.control = self.callback_handler.on_optimizer_step(args, self.state, self.control)\n+                        self.optimizer.step()\n \n-                    optimizer_was_run = not self.accelerator.optimizer_step_was_skipped\n-                    if optimizer_was_run:\n-                        # Delay optimizer scheduling until metrics are generated\n-                        if not isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n-                            self.lr_scheduler.step()\n+                        self.control = self.callback_handler.on_optimizer_step(args, self.state, self.control)\n \n-                    model.zero_grad()\n-                    self.state.global_step += 1\n-                    self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch\n-                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n+                        optimizer_was_run = not self.accelerator.optimizer_step_was_skipped\n+                        if optimizer_was_run:\n+                            # Delay optimizer scheduling until metrics are generated\n+                            if not isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n+                                self.lr_scheduler.step()\n \n-                    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n-                else:\n-                    self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n+                        model.zero_grad()\n+                        self.state.global_step += 1\n+                        self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch\n+                        self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n+                        self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n+                    else:\n+                        self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n \n-                if self.control.should_epoch_stop or self.control.should_training_stop:\n                     # PyTorch/XLA relies on the data loader to insert the mark_step for\n                     # each step. Since we are breaking the loop early, we need to manually\n                     # insert the mark_step here.\n+                    if self.control.should_epoch_stop or self.control.should_training_stop:\n+                        if is_torch_xla_available():\n+                            xm.mark_step()\n+                        break\n+                # We also need to break out of the nested loop\n+                if self.control.should_epoch_stop or self.control.should_training_stop:\n                     if is_torch_xla_available():\n                         xm.mark_step()\n                     break\n@@ -3514,7 +3532,9 @@ def autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True):\n \n         return ctx_manager\n \n-    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n+    def training_step(\n+        self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], num_items_in_batch=None\n+    ) -> torch.Tensor:\n         \"\"\"\n         Perform a training step on a batch of inputs.\n \n@@ -3542,7 +3562,7 @@ def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor,\n             return loss_mb.reduce_mean().detach().to(self.args.device)\n \n         with self.compute_loss_context_manager():\n-            loss = self.compute_loss(model, inputs)\n+            loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n \n         del inputs\n         if (\n@@ -3575,20 +3595,23 @@ def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor,\n             with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                 scaled_loss.backward()\n         else:\n+            loss *= self.args.gradient_accumulation_steps\n             self.accelerator.backward(loss, **kwargs)\n \n         return loss.detach() / self.args.gradient_accumulation_steps\n \n-    def compute_loss(self, model, inputs, return_outputs=False):\n+    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n         \"\"\"\n         How the loss is computed by Trainer. By default, all models return the loss in the first element.\n \n         Subclass and override for custom behavior.\n         \"\"\"\n-        if self.label_smoother is not None and \"labels\" in inputs:\n+        if (self.label_smoother is not None or self.compute_loss_func is not None) and \"labels\" in inputs:\n             labels = inputs.pop(\"labels\")\n         else:\n             labels = None\n+        # if num_items_in_batch is not None:\n+        #     inputs[\"num_items_in_batch\"] = num_items_in_batch\n         outputs = model(**inputs)\n         # Save past state if it exists\n         # TODO: this needs to be fixed and made cleaner later.\n@@ -3601,7 +3624,10 @@ def compute_loss(self, model, inputs, return_outputs=False):\n                 model_name = unwrapped_model.base_model.model._get_name()\n             else:\n                 model_name = unwrapped_model._get_name()\n-            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n+            # User-defined compute_loss function\n+            if self.compute_loss_func is not None:\n+                loss = self.compute_loss_func(outputs, labels, num_items_in_batch=num_items_in_batch)\n+            elif model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n                 loss = self.label_smoother(outputs, labels, shift_labels=True)\n             else:\n                 loss = self.label_smoother(outputs, labels)\n@@ -4993,3 +5019,21 @@ def _fsdp_qlora_plugin_updates(self):\n                 fsdp_plugin.set_mixed_precision(\n                     self.model.hf_quantizer.quantization_config.bnb_4bit_quant_storage, override=True\n                 )\n+\n+    def get_batch_samples(self, epoch_iterator, num_batches):\n+        batch_samples = []\n+        num_items_in_batch = None\n+        for _ in range(num_batches):\n+            try:\n+                batch_samples += [next(epoch_iterator)]\n+            except StopIteration:\n+                break\n+        if len(batch_samples) > 0 and \"labels\" in batch_samples[0]:\n+            # For now we don't support object detection\n+            try:\n+                num_items_in_batch = sum(\n+                    [data_batch[\"labels\"][..., 1:].ne(-100).sum().item() for data_batch in batch_samples]\n+                )\n+            except TypeError:\n+                pass\n+        return batch_samples, num_items_in_batch"
        },
        {
            "sha": "5c03355785d2b5818055ded0a331b25a615fdf63",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 158,
            "deletions": 1,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ba31a8a94bf7cfeaf59ffc3bc9e0b0cd3e25788/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ba31a8a94bf7cfeaf59ffc3bc9e0b0cd3e25788/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=6ba31a8a94bf7cfeaf59ffc3bc9e0b0cd3e25788",
            "patch": "@@ -42,13 +42,15 @@\n     AutoImageProcessor,\n     AutoProcessor,\n     AutoTokenizer,\n+    DataCollatorForLanguageModeling,\n     IntervalStrategy,\n     PretrainedConfig,\n     TrainerCallback,\n     TrainingArguments,\n     get_polynomial_decay_schedule_with_warmup,\n     is_torch_available,\n     logging,\n+    set_seed,\n )\n from transformers.hyperparameter_search import ALL_HYPERPARAMETER_SEARCH_BACKENDS\n from transformers.testing_utils import (\n@@ -153,6 +155,19 @@\n PATH_SAMPLE_TEXT = f\"{get_tests_dir()}/fixtures/sample_text.txt\"\n \n \n+class StoreLossCallback(TrainerCallback):\n+    \"\"\"\n+    Simple callback to store the loss.\n+    \"\"\"\n+\n+    def __init__(self):\n+        self.losses = []\n+\n+    def on_log(self, args, state, control, logs=None, **kwargs):\n+        if \"loss\" in logs:\n+            self.losses.append(logs[\"loss\"])\n+\n+\n class MockCudaOOMCallback(TrainerCallback):\n     \"\"\"\n     Simple callback to simulate CUDA OOM error if\n@@ -168,6 +183,26 @@ def on_step_end(self, args, state, control, **kwargs):\n             raise RuntimeError(\"CUDA out of memory.\")\n \n \n+def ForCausalLMLoss(logits, labels, vocab_size, num_items_in_batch, disable_num_items_in_batch=False):\n+    # Upcast to float if we need to compute the loss to avoid potential precision issues\n+    logits = logits.float()\n+    # Shift so that tokens < n predict n\n+    shift_logits = logits[..., :-1, :].contiguous()\n+    shift_labels = labels[..., 1:].contiguous()\n+\n+    # Flatten the tokens\n+    shift_logits = shift_logits.view(-1, vocab_size)\n+    shift_labels = shift_labels.view(-1)\n+    # Enable model parallelism\n+    shift_labels = shift_labels.to(shift_logits.device)\n+    if num_items_in_batch is None or disable_num_items_in_batch:\n+        loss = nn.functional.cross_entropy(shift_logits, shift_labels, ignore_index=-100, reduction=\"mean\")\n+    else:\n+        loss = nn.functional.cross_entropy(shift_logits, shift_labels, ignore_index=-100, reduction=\"sum\")\n+        loss = loss / num_items_in_batch\n+    return loss\n+\n+\n class RegressionDataset:\n     def __init__(self, a=2, b=3, length=64, seed=42, label_names=None):\n         np.random.seed(seed)\n@@ -438,6 +473,31 @@ def forward(self, input_x, labels=None, **kwargs):\n             loss = nn.functional.mse_loss(y, labels)\n             return (loss, y)\n \n+    class BasicTextGenerationModel(nn.Module):\n+        def __init__(self, vocab_size, hidden_size):\n+            super().__init__()\n+            self.embedding = nn.Embedding(vocab_size, hidden_size)\n+            self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n+            self.fc = nn.Linear(hidden_size, vocab_size)\n+\n+        def forward(self, input_ids, **kwargs):\n+            embedded = self.embedding(input_ids)\n+            lstm_out, _ = self.lstm(embedded)\n+            logits = self.fc(lstm_out)\n+            return logits\n+\n+    def create_dummy_dataset_for_text_generation(vocab_size, seq_length, num_samples):\n+        import datasets\n+        import numpy as np\n+\n+        # Create random input sequences\n+        input_ids = np.random.randint(0, vocab_size, (num_samples, seq_length))\n+\n+        # Create a datasets.Dataset\n+        dataset = datasets.Dataset.from_dict({\"input_ids\": input_ids, \"labels\": input_ids})\n+\n+        return dataset\n+\n     class TstLayer(nn.Module):\n         def __init__(self, hidden_size):\n             super().__init__()\n@@ -676,8 +736,105 @@ def test_model_init(self):\n         trainer.train()\n         self.check_trained_model(trainer.model, alternate_seed=True)\n \n+    @slow\n+    def test_gradient_accumulation_loss_alignment(self):\n+        set_seed(42)\n+        import datasets\n+\n+        model_name = \"distilgpt2\"\n+        dataset_name = \"wikitext\"\n+        dataset_config = \"wikitext-2-raw-v1\"\n+        dataset = datasets.load_dataset(dataset_name, dataset_config, split=\"train[:500]\")\n+        dataset = dataset.train_test_split(test_size=0.2)\n+        tokenizer = AutoTokenizer.from_pretrained(model_name)\n+\n+        def tokenize_function(examples):\n+            return tokenizer(examples[\"text\"])\n+\n+        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n+\n+        tokenizer.pad_token = tokenizer.eos_token\n+        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n+\n+        model = AutoModelForCausalLM.from_pretrained(model_name)\n+\n+        def compute_loss(logits, labels, vocab_size, num_items_in_batch, disable_num_items_in_batch=False):\n+            return ForCausalLMLoss(\n+                logits[\"logits\"], labels, vocab_size, num_items_in_batch, disable_num_items_in_batch\n+            )\n+\n+        loss_fn = partial(compute_loss, vocab_size=model.config.vocab_size, disable_num_items_in_batch=False)\n+\n+        base_loss_callback = StoreLossCallback()\n+\n+        args_kwargs = {\n+            \"report_to\": \"none\",\n+            \"logging_steps\": 1,\n+            \"max_steps\": 20,\n+            \"learning_rate\": 3e-4,\n+            \"disable_tqdm\": True,\n+        }\n+\n+        args = TrainingArguments(\n+            \"./generation\",\n+            **args_kwargs,\n+        )\n+        trainer = Trainer(\n+            model,\n+            args,\n+            train_dataset=tokenized_dataset[\"train\"],\n+            callbacks=[base_loss_callback],\n+            compute_loss_func=loss_fn,\n+            data_collator=data_collator,\n+        )\n+        trainer.train()\n+\n+        grad_accum_loss_callback = StoreLossCallback()\n+        args = TrainingArguments(\n+            \"./generation\",\n+            **args_kwargs,\n+            gradient_accumulation_steps=2,\n+            per_device_train_batch_size=4,\n+        )\n+        set_seed(42)\n+        model = AutoModelForCausalLM.from_pretrained(model_name)\n+        trainer = Trainer(\n+            model,\n+            args,\n+            train_dataset=tokenized_dataset[\"train\"],\n+            callbacks=[grad_accum_loss_callback],\n+            compute_loss_func=loss_fn,\n+            data_collator=data_collator,\n+        )\n+        trainer.train()\n+\n+        set_seed(42)\n+        model = AutoModelForCausalLM.from_pretrained(model_name)\n+        broken_loss_callback = StoreLossCallback()\n+        loss_fn = partial(compute_loss, vocab_size=model.config.vocab_size, disable_num_items_in_batch=True)\n+        trainer = Trainer(\n+            model,\n+            args,\n+            train_dataset=tokenized_dataset[\"train\"],\n+            callbacks=[broken_loss_callback],\n+            compute_loss_func=loss_fn,\n+            data_collator=data_collator,\n+        )\n+        trainer.train()\n+\n+        # Calculate the difference between the base loss and the grad_accum loss\n+        diff_truth = [base - grad for base, grad in zip(base_loss_callback.losses, grad_accum_loss_callback.losses)]\n+        diff_broken = [base - grad for base, grad in zip(base_loss_callback.losses, broken_loss_callback.losses)]\n+        # These should be quite close\n+        for diff in diff_truth:\n+            self.assertLess(abs(diff), 0.1, f\"Difference {diff} is not within 0.1\")\n+\n+        # These should be very off\n+        for diff in diff_broken:\n+            self.assertGreater(abs(diff), 0.1, f\"Difference {diff} is not greater than 0.1\")\n+\n     def test_gradient_accumulation(self):\n-        # Training with half the batch size but accumulation steps as 2 should give the same results.\n+        # Training with half the batch size but accumulation steps as 2 should give the same training losses.\n         trainer = get_regression_trainer(\n             gradient_accumulation_steps=2, per_device_train_batch_size=4, learning_rate=0.1\n         )"
        }
    ],
    "stats": {
        "total": 449,
        "additions": 325,
        "deletions": 124
    }
}