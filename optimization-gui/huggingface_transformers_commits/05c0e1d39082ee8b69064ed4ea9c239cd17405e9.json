{
    "author": "itazap",
    "message": "rm slow tokenizers (#40936)\n\n* fixes missed\n\n* gemma test fix\n\n* refactor\n\n* rm legacy from llama\n\n* added renaming\n\n* add _model\n\n* update legacy\n\n* update legacy\n\n* fix docstring\n\n* always load blank, then set _tokenizer if we have it\n\n* new toks\n\n* update all berttokenizer based models\n\n* apply feedback - delete bert duplicates\n\n* more models --> fast only\n\n* more convert_slow models\n\n* fix common test refs\n\n* updating fast only tokenizers\n\n* openai and pegasus\n\n* enable sentencepiecebackend\n\n* more models\n\n* code gen\n\n* t5\n\n* code gen tests\n\n* speecht5\n\n* mbart\n\n* mbart50\n\n* more models\n\n* more models\n\n* layouglmv2\n\n* update tests\n\n* update tests\n\n* update tests\n\n* pretrainedtokenizer\n\n* whisper\n\n* whisper\n\n* layoutxlm and storing backends\n\n* refactor sentencepiecebackend and additional_special_tokens\n\n* renaming tokenization_utils --> tokenization_python\n\n* udpate tests\n\n* bert test\n\n* blenderbot\n\n* clip\n\n* codegen\n\n* code_llama\n\n* cohere\n\n* deberata, deberat v2, funnel\n\n* gpt2\n\n* batch update tests\n\n* pegasus qwen2 roberta\n\n* more models\n\n* layout tests\n\n* some renaming\n\n* fix references to utils_fast\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix some tests\n\n* regression\n\n* fix refs\n\n* fix refs\n\n* missed the most crucial file in my last commit\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* batch encode fix\n\n* fix some tests\n\n* BC for batch_decode bc too many refs\n\n* more tests\n\n* fix more tests\n\n* fix for processors\n\n* fixing more models\n\n* deleted mbart50 by accident\n\n* seamless m4t\n\n* albert fix\n\n* whisper\n\n* layout3\n\n* attempt to fix cached tokenizers on CI\n\n* trying another fix on CI\n\n* again try to work around CI\n\n* bertweet\n\n* tapas\n\n* mbart50\n\n* luke\n\n* mluke\n\n* markuplm\n\n* markuplm\n\n* fix some more auto tests\n\n* some random model failures\n\n* mistralcommontestser\n\n* more fixes\n\n* ref fix\n\n* siglip\n\n* marian\n\n* plbart\n\n* update utils toks\n\n* seamless m4t\n\n* roc bert\n\n* udpate byt5 test\n\n* xlm\n\n* esm\n\n* roformer\n\n* code llama\n\n* biogpt\n\n* m2m100\n\n* dpr and flaubert\n\n* xlm and speech to text\n\n* tok backend pass object\n\n* tokenizer object pass\n\n* wav2vec2\n\n* wav2vec2\n\n* cpmant\n\n* update utils tokenizers\n\n* cpmant\n\n* bartpho\n\n* test apply chat template assistant mask\n\n* apply chat template video\n\n* apply chat template assistant mask\n\n* test torch\n\n* update from slow in base and fix donut processor errors\n\n* auto to point to tokenizers backend, fix kosmos2\n\n* some non model fixes for old slow models that no longer have their own tokenizer file as they are the same as bert\n\n* missed file from last commit\n\n* idefics2\n\n* fixup\n\n* fixup\n\n* pretrained tokenizer fast test update\n\n* stash\n\n* bad merged\n\n* cherry pick more stuff that did not merge well\n\n* fix gptsw3\n\n* nit warn for now\n\n* update error raising\n\n* just ran fixup\n\n* bring back bert legacy\n\n* fix\n\n* nit\n\n* fix 56 errors on blenderbotsmall?\n\n* 18 for blenderbotsmall\n\n* tok auto\n\n* missed clip\n\n* fix tests\n\n* something missed\n\n* token healing\n\n* tok common tests update - nonmodel\n\n* try to fix non-model test in test_tokenization_utils\n\n* fix hub tests\n\n* try to fix hub tests\n\n* custom vocab related fixed\n\n* bert jap\n\n* BERT JAP\n\n* rename bert legacy to bert legacy\n\n* Wav2vec2\n\n* fix in tok python to update total vocab size - fixes speech t5\n\n* blender bot small\n\n* forgot test file\n\n* test failures\n\n* marian\n\n* gpt2 tiktoken\n\n* big bird / marian\n\n* udop\n\n* forgot couple changes\n\n* test_serve fix\n\n* missing import\n\n* a couple processors fixes\n\n* style partly\n\n* fix to fetch tests ci\n\n* Revert branch back to commit f5bc69ef state\n\n* revert branch to styling\n\n* update mistral after merge\n\n* fixes for non model tests\n\n* some processor test fixes\n\n* more processor test fixes\n\n* more processor fixes\n\n* hub tests\n\n* python tok utils\n\n* fix hub test\n\n* make style for now\n\n* remove problemattic fic copies\n\n* python utils/check_copies.py --fix_and_overwrite\n\n* more styling\n\n* fixup\n\n* silence docstirng\n\n* fix import?\n\n* fix imports\n\n* add the local test as well\n\n* throw spm error\n\n* llamas\n\n* fix a couple tests\n\n* broke ci\n\n* broke ci\n\n* broke ci\n\n* broke ci\n\n* add logs to debug gemma on ci\n\n* gemma and llama\n\n* gemma\n\n* revert las commit\n\n* gemma debug\n\n* gemma debug\n\n* gemma\n\n* safely import spiece backend\n\n* tok tests\n\n* check none\n\n* setup and qual\n\n* ruff\n\n* del dev files\n\n* tok auto\n\n* fill docstrings\n\n* update auto\n\n* blenderbot small nit\n\n* add migration guide\n\n* move mixtral patch to `TokenizersBackend`, move `TokenizerExtractor`\n\n* rename MistralCommonTokenizer to MistralCommonB ackend\n\n* nit\n\n* fix failures\n\n* fixup\n\n* remoove one old test\n\n* mark the slow one as slow\n\n* very small fixes\n\n* update auto mapping for missing ones\n\n* fixup lorsd\n\n* fixup doc and stuff\n\n* should be the final fixe\n\n* processing update\n\n* update\n\n* FIX or brute AI fix the llava test\n\n* style\n\n* slow?\n\n* fix is offline mode?\n\n* fix mt5\n\n* One tok utils (#42462)\n\n* consolidate python and utils tokenization files, they are copies\n\n* ruff and ref\n\n* Format\n\n* fix cohere\n\n* ?\n\n* up\n\n* am I dumbb?\n\n* grumble\n\n---------\n\nCo-authored-by: Arthur <arthur.zucker@gmail.com>",
    "sha": "05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
    "files": [
        {
            "sha": "474aa478e2f3be7ab4dfa11912d044c2f730f5fd",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 201,
            "deletions": 0,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -74,6 +74,207 @@ While this is being implemented, expect varying levels of support across differe\n \n Linked PR: https://github.com/huggingface/transformers/pull/41580\n \n+\n+\n+\n+## Tokenization\n+\n+Just as we moved towards a single backend library for model definition, we want `Tokenizer` to be a lot more intuitive.\n+With v5, you can now initialize an empty `LlamaTokenizer` and train it directly on your new task! \n+\n+Defining a new tokenizer object should be as simple as this:\n+```python\n+from transformers import TokenizersBackend, generate_merges\n+from tokenizers import pre_tokenizers, Tokenizer\n+from tokenizers.model import BPE\n+\n+class Llama5Tokenizer(TokenizersBackend):\n+    def __init__(self,        unk_token=\"<unk>\",bos_token=\"<s>\", eos_token=\"</s>\", vocab=None, merges=None ):\n+        if vocab is None:\n+            self._vocab = {\n+                str(unk_token): 0,\n+                str(bos_token): 1,\n+                str(eos_token): 2,\n+            }\n+\n+        else:\n+            self._vocab = vocab\n+\n+        if merges is not None:\n+            self._merges = merges\n+        else:\n+            self._merges = generate_merges(filtered_vocab)\n+\n+        self._tokenizer = Tokenizer(\n+            BPE(vocab=self._vocab, merges=self._merges, fuse_unk=True)\n+        )\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(\n+            replacement=\"‚ñÅ\", prepend_scheme=_get_prepend_scheme(self.add_prefix_space, self), split=False\n+        )\n+        super().__init__(\n+            tokenizer_object=self._tokenizer,\n+            unk_token=unk_token,\n+            bos_token=bos_token,\n+            eos_token=eos_token,\n+        )\n+```\n+\n+And now if you call `Llama5Tokenizer()` you just get an empty, trainable tokenizer that follows the definition of the authors of `Llama5` (it does not exist yet :wink:).\n+\n+The above is the main motivation towards refactoring tokenization: we want people to just instantiate a tokenizer like they would a model, empty or not and with exactly what they defined.\n+\n+### Non-tokenizers\n+If you tokenizers is not common, or you just don't want to rely on `sentencepiece` nor `tokenizers` you can just import the `PythonBackend` (previousl `PreTrainedTokenzier`) which has all the API and logic for added tokens, encoding and decoding wieht them etc. \n+\n+If you want to have en less features, you can use the common `PreTrainedTokenizerBase` mixin, which mostly defines `transformers` tokenizer API: `encode`, `decode`, `vocab_size`, `get_vocab`, `convert_tokens_to_ids`, `convert_ids_to_tokens`, `from_pretrained`, `save_pretrained`, etc.\n+\n+### Backend Architecture Changes\n+\n+**Moving away from \"slow\" vs \"fast\" tokenizers:**\n+\n+Previously, transformers maintained two parallel implementations for many tokenizers:\n+- \"Slow\" tokenizers (`tokenization_<model>.py`) - Python-based implementations, often using [SentencePiece](https://github.com/google/sentencepiece) as the backend.\n+- \"Fast\" tokenizers (`tokenization_<model>_fast.py`) - Rust-based implementations using the ü§ó [tokenizers](https://github.com/huggingface/tokenizers) library.\n+\n+In v5, we consolidate to a single tokenizer file per model: `tokenization_<model>.py`. This file will use the most appropriate backend available:\n+\n+1. **TokenizersBackend** (preferred): Rust-based tokenizers from the ü§ó [tokenizers](https://github.com/huggingface/tokenizers) library. In general its performances are better, but it also offers a lot more features that are comonly adopted across the ecosystem, like handling additional tokens, easily update the state of the tokenizer, automatic parallelisation etc. \n+2. **SentencePieceBackend**: For models requiring SentencePiece\n+3. **PythonBackend**: Pure Python implementations\n+4. **MistralCommonBackend**: Relies on `MistralCommon`'s toknenization library. (Previously `MistralCommonTokenizer`)\n+\n+The `AutoTokenizer` automatically selects the appropriate backend based on available files and dependencies. This is transparent, you continue to use `AutoTokenizer.from_pretrained()` as before. This allows transformers to be future-proof and modular to easily support future backends.\n+\n+\n+### API Changes\n+\n+**1. Direct tokenizer initialization with vocab and merges:**\n+\n+In v5, you can now initialize tokenizers directly with vocabulary and merges, enabling training custom tokenizers from scratch:\n+\n+```python\n+# v5: Initialize a blank tokenizer for training\n+from transformers import LlamaTokenizer\n+\n+# Create a tokenizer with custom vocabulary and merges\n+vocab = {\"<unk>\": 0, \"<s>\": 1, \"</s>\": 2, \"hello\": 3, \"world\": 4}\n+merges = [(\"h\", \"e\"), (\"l\", \"l\"), (\"o\", \" \")]\n+\n+tokenizer = LlamaTokenizer(vocab=vocab, merges=merges)\n+\n+# Or initialize a blank tokenizer to train on your own dataset\n+tokenizer = LlamaTokenizer()  # Creates a blank Llama-like tokenizer\n+```\n+But you can no longer pass a vocab file. As this accounts for `from_pretrained` use-case.\n+\n+**2. Simplified decoding API:**\n+\n+The `batch_decode` method has been unified with `decode`. Both single and batch decoding now use the same method:\n+```python\n+from transformers import AutoTokenizer\n+tokenizer = AutoTokenizer.from_pretrained(\"t5-small\") \n+inputs = [\"hey how are you?\", \"fine\"]\n+tokenizer.decode(tokenizer.encode(inputs))\n+```\n+Gives:\n+```diff\n+- 'hey how are you?</s> fine</s>'\n++ ['hey how are you?</s>', 'fine</s>']\n+```\n+\n+This is mostly because people get `list[list[int]]` out of `generate`, but then they would use `decode` because they use `encode` and would get:\n+```python\n+   ...: tokenizer.decode([[1,2], [1,4]])\n+---------------------------------------------------------------------------\n+TypeError                                 Traceback (most recent call last)\n+Cell In[2], line 4\n+      2 tokenizer = AutoTokenizer.from_pretrained(\"t5-small\") \n+      3 inputs = [\"hey how are you?\", \"fine\"]\n+----> 4 tokenizer.decode([[1,2], [1,4]])\n+\n+File /raid/arthur/transformers/src/transformers/tokenization_utils_base.py:3948, in PreTrainedTokenizerBase.decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\n+   3945 # Convert inputs to python lists\n+   3946 token_ids = to_py_obj(token_ids)\n+-> 3948 return self._decode(\n+   3949     token_ids=token_ids,\n+   3950     skip_special_tokens=skip_special_tokens,\n+   3951     clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+   3952     **kwargs,\n+   3953 )\n+\n+File /raid/arthur/transformers/src/transformers/tokenization_utils_fast.py:682, in PreTrainedTokenizerFast._decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\n+    680 if isinstance(token_ids, int):\n+    681     token_ids = [token_ids]\n+--> 682 text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n+    684 clean_up_tokenization_spaces = (\n+    685     clean_up_tokenization_spaces\n+    686     if clean_up_tokenization_spaces is not None\n+    687     else self.clean_up_tokenization_spaces\n+    688 )\n+    689 if clean_up_tokenization_spaces:\n+\n+TypeError: argument 'ids': 'list' object cannot be interpreted as an integer\n+```\n+\n+**3. Unified encoding API:**\n+\n+The `encode_plus` is deprecated ‚Üí call directly with `__call__`\n+\n+**3. `apply_chat_template` returns `BatchEncoding`:**\n+\n+Previously, `apply_chat_template` returned `input_ids` for backward compatibility. In v5, it now consistently returns a `BatchEncoding` dict like other tokenizer methods:\n+\n+```python\n+# v5\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Hello!\"},\n+    {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n+]\n+\n+# Now returns BatchEncoding with input_ids, attention_mask, etc.\n+outputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n+print(outputs.keys())  # dict_keys(['input_ids', 'attention_mask'])\n+```\n+\n+#### Removed legacy configuration file saving:\n+\n+- `special_tokens_map.json` - special tokens are now stored in `tokenizer_config.json`.\n+- `added_tokens.json` - added tokens are now stored in `tokenizer.json`.\n+- `added_tokens_decoder` is only stored when there is no `tokenizer.json`.\n+\n+When loading older tokenizers, these files are still read for backward compatibility, but new saves use the consolidated format.\n+\n+### Model-Specific Changes\n+\n+Several models that had identical tokenizers now import from their base implementation:\n+\n+- **LayoutLM** ‚Üí uses BertTokenizer\n+- **LED** ‚Üí uses BartTokenizer  \n+- **Longformer** ‚Üí uses RobertaTokenizer\n+- **LXMert** ‚Üí uses BertTokenizer\n+- **MT5** ‚Üí uses T5Tokenizer\n+- **MVP** ‚Üí uses BartTokenizer\n+\n+We're just gonna remove these files at term.\n+\n+**Removed T5-specific workarounds:**\n+\n+The internal `_eventually_correct_t5_max_length` method has been removed. T5 tokenizers now handle max length consistently with other models.\n+\n+### Testing Changes\n+\n+Model-specific tokenization test files now focus on integration tests.\n+Common tokenization API tests (e.g., `add_tokens`, `encode`, `decode`) are now centralized and automatically applied across all tokenizers. This reduces test duplication and ensures consistent behavior\n+\n+\n+For legacy implementations, the original BERT Python tokenizer code (including `WhitespaceTokenizer`, `BasicTokenizer`, etc.) is preserved in `bert_legacy.py` for reference purposes.\n+\n+**Linked PRs:**\n+- https://github.com/huggingface/transformers/issues/40938\n+- https://github.com/huggingface/transformers/pull/40936\n+- https://github.com/huggingface/transformers/pull/41626\n+\n+\n ## Library-wide changes with lesser impact\n \n ### `use_auth_token`"
        },
        {
            "sha": "ba2a69552a225a20ca8a864c05e9c2a42987ad7d",
            "filename": "docs/source/en/internal/tokenization_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Finternal%2Ftokenization_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Finternal%2Ftokenization_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Ftokenization_utils.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,8 +18,7 @@ rendered properly in your Markdown viewer.\n \n This page lists all the utility functions used by the tokenizers, mainly the class\n [`~tokenization_utils_base.PreTrainedTokenizerBase`] that implements the common methods between\n-[`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`] and the mixin\n-[`~tokenization_utils_base.SpecialTokensMixin`].\n+[`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`].\n \n Most of those are only useful if you are studying the code of the tokenizers in the library.\n \n@@ -29,9 +28,6 @@ Most of those are only useful if you are studying the code of the tokenizers in\n     - __call__\n     - all\n \n-## SpecialTokensMixin\n-\n-[[autodoc]] tokenization_utils_base.SpecialTokensMixin\n \n ## Enums and namedtuples\n "
        },
        {
            "sha": "76492abe464bac4b533f3ad2c12ea8b42dff8db3",
            "filename": "docs/source/en/main_classes/tokenizer.md",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -28,8 +28,7 @@ The base classes [`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`]\n implement the common methods for encoding string inputs in model inputs (see below) and instantiating/saving python and\n \"Fast\" tokenizers either from a local file or directory or from a pretrained tokenizer provided by the library\n (downloaded from HuggingFace's AWS S3 repository). They both rely on\n-[`~tokenization_utils_base.PreTrainedTokenizerBase`] that contains the common methods, and\n-[`~tokenization_utils_base.SpecialTokensMixin`].\n+[`~tokenization_utils_base.PreTrainedTokenizerBase`] that contains the common methods.\n \n [`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`] thus implement the main\n methods for using all the tokenizers:\n@@ -98,6 +97,18 @@ loaded very simply into ü§ó transformers. Take a look at the [Using tokenizers\n     - push_to_hub\n     - all\n \n+## PythonBackend\n+\n+[[autodoc]] PythonBackend\n+\n+## TokenizersBackend\n+\n+[[autodoc]] TokenizersBackend\n+\n+## SentencePieceBackend\n+\n+[[autodoc]] SentencePieceBackend\n+\n ## BatchEncoding\n \n [[autodoc]] BatchEncoding"
        },
        {
            "sha": "fdda4ef243a43cd6155080c8096910d1add45e4b",
            "filename": "docs/source/en/model_doc/bert.md",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -100,11 +100,13 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n ## BertTokenizer\n \n [[autodoc]] BertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n+## BertTokenizerLegacy\n+\n+[[autodoc]] BertTokenizerLegacy\n+\n ## BertTokenizerFast\n \n [[autodoc]] BertTokenizerFast"
        },
        {
            "sha": "1c308e8887bec92d1939f10c66417229d7f7a7e9",
            "filename": "docs/source/en/model_doc/big_bird.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -104,9 +104,7 @@ print(f\"The predicted token is: {predicted_token}\")\n ## BigBirdTokenizer\n \n [[autodoc]] BigBirdTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## BigBirdTokenizerFast"
        },
        {
            "sha": "fd44fd2a176c559de6e3d602d13b8aac79b893a2",
            "filename": "docs/source/en/model_doc/blenderbot-small.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -68,9 +68,7 @@ the left.\n ## BlenderbotSmallTokenizer\n \n [[autodoc]] BlenderbotSmallTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## BlenderbotSmallTokenizerFast"
        },
        {
            "sha": "c7ed007978037d0fe9b31c5e7183db4112098146",
            "filename": "docs/source/en/model_doc/blenderbot.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -84,12 +84,10 @@ An example:\n ## BlenderbotTokenizer\n \n [[autodoc]] BlenderbotTokenizer\n-    - build_inputs_with_special_tokens\n \n ## BlenderbotTokenizerFast\n \n [[autodoc]] BlenderbotTokenizerFast\n-    - build_inputs_with_special_tokens\n \n ## BlenderbotModel\n "
        },
        {
            "sha": "a234211d095deebdcf95c52d9c7ec0d7f3e4e871",
            "filename": "docs/source/en/model_doc/bloom.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -63,10 +63,6 @@ See also:\n [[autodoc]] BloomConfig\n     - all\n \n-## BloomTokenizerFast\n-\n-[[autodoc]] BloomTokenizerFast\n-    - all\n \n ## BloomModel\n "
        },
        {
            "sha": "832ed26e66d12b55e2d1254ad91387985cb8d3f2",
            "filename": "docs/source/en/model_doc/camembert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -122,9 +122,7 @@ print(f\"The predicted token is: {predicted_token}\")\n ## CamembertTokenizer\n \n [[autodoc]] CamembertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CamembertTokenizerFast"
        },
        {
            "sha": "b58502a2b4535eb820a8fcd3b8fbe38140e6c4cf",
            "filename": "docs/source/en/model_doc/clip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -99,9 +99,7 @@ print(f\"Most likely label: {most_likely_label} with probability: {probs[0][most_\n ## CLIPTokenizer\n \n [[autodoc]] CLIPTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CLIPTokenizerFast"
        },
        {
            "sha": "21773c59ae4778df0bb614f103d5c2fdc3bcbafc",
            "filename": "docs/source/en/model_doc/code_llama.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcode_llama.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -167,16 +167,12 @@ visualizer(\"\"\"def func(a, b):\n ## CodeLlamaTokenizer\n \n [[autodoc]] CodeLlamaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CodeLlamaTokenizerFast\n \n [[autodoc]] CodeLlamaTokenizerFast\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - update_post_processor\n     - save_vocabulary"
        },
        {
            "sha": "0f09933f6351e65e6e3eff0b9c887ee7feeaa502",
            "filename": "docs/source/en/model_doc/codegen.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -77,7 +77,6 @@ hello_world()\n ## CodeGenTokenizer\n \n [[autodoc]] CodeGenTokenizer\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CodeGenTokenizerFast"
        },
        {
            "sha": "05285413cc87c9c8a23a6f04db256aaf8b815dd9",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -129,14 +129,10 @@ visualizer(\"Plants create energy through a process known as\")\n \n [[autodoc]] CohereConfig\n \n-## CohereTokenizerFast\n-\n-[[autodoc]] CohereTokenizerFast\n-    - build_inputs_with_special_tokens\n-    - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n-    - update_post_processor\n-    - save_vocabulary\n+## CohereTokenizer\n+\n+[[autodoc]] CohereTokenizer\n+\n \n ## CohereModel\n "
        },
        {
            "sha": "9efe1b9af958386244ebe405d47108d7003015a8",
            "filename": "docs/source/en/model_doc/convbert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -62,9 +62,7 @@ ConvBERT training tips are similar to those of BERT. For usage tips refer to [BE\n ## ConvBertTokenizer\n \n [[autodoc]] ConvBertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## ConvBertTokenizerFast"
        },
        {
            "sha": "d7a6fb45c6e23bf4df7bf2df30693cf4d8be3e68",
            "filename": "docs/source/en/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -125,16 +125,12 @@ print(f\"Predicted label: {predicted_label}\")\n ## DebertaV2Tokenizer\n \n [[autodoc]] DebertaV2Tokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## DebertaV2TokenizerFast\n \n [[autodoc]] DebertaV2TokenizerFast\n-    - build_inputs_with_special_tokens\n-    - create_token_type_ids_from_sequences\n \n ## DebertaV2Model\n "
        },
        {
            "sha": "d9432fae366b6bacc761ba69e177bba5a2864796",
            "filename": "docs/source/en/model_doc/deberta.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -104,16 +104,12 @@ echo -e '{\"text\": \"A soccer game with multiple people playing.\", \"text_pair\": \"S\n ## DebertaTokenizer\n \n [[autodoc]] DebertaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## DebertaTokenizerFast\n \n [[autodoc]] DebertaTokenizerFast\n-    - build_inputs_with_special_tokens\n-    - create_token_type_ids_from_sequences\n \n ## DebertaModel\n "
        },
        {
            "sha": "973a4e8488aec26602b48ab75c1c79fbde9bcb14",
            "filename": "docs/source/en/model_doc/fnet.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -65,9 +65,7 @@ sequence length for fine-tuning and inference.\n ## FNetTokenizer\n \n [[autodoc]] FNetTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## FNetTokenizerFast"
        },
        {
            "sha": "ff731b7589ba8eedb5d3c585c0f70241ea2f99e4",
            "filename": "docs/source/en/model_doc/funnel.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -74,9 +74,7 @@ This model was contributed by [sgugger](https://huggingface.co/sgugger). The ori\n ## FunnelTokenizer\n \n [[autodoc]] FunnelTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## FunnelTokenizerFast"
        },
        {
            "sha": "53934a045fad9f8993ec9099ef2deff8908ff326",
            "filename": "docs/source/en/model_doc/gpt_neox.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -171,6 +171,10 @@ following speedups during training and inference.\n \n [[autodoc]] GPTNeoXConfig\n \n+## GPTNeoXTokenizer\n+\n+[[autodoc]] GPTNeoXTokenizer\n+\n ## GPTNeoXTokenizerFast\n \n [[autodoc]] GPTNeoXTokenizerFast"
        },
        {
            "sha": "9090b1f9dfefabee3a90b7329577f439e2e980a1",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -152,9 +152,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ## LEDTokenizer\n \n [[autodoc]] LEDTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## LEDTokenizerFast"
        },
        {
            "sha": "f753445572efeb5cd38f43a6847dc6cbd77872d4",
            "filename": "docs/source/en/model_doc/llama.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -131,17 +131,13 @@ visualizer(\"Plants create energy through a process known as\")\n ## LlamaTokenizer\n \n [[autodoc]] LlamaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## LlamaTokenizerFast\n \n [[autodoc]] LlamaTokenizerFast\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - update_post_processor\n     - save_vocabulary\n "
        },
        {
            "sha": "db0349885848371f162fdd4c4fda9a08565a43f2",
            "filename": "docs/source/en/model_doc/llama2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -147,17 +147,13 @@ visualizer(\"Plants create energy through a process known as\")\n ## LlamaTokenizer\n \n [[autodoc]] LlamaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## LlamaTokenizerFast\n \n [[autodoc]] LlamaTokenizerFast\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - update_post_processor\n     - save_vocabulary\n "
        },
        {
            "sha": "6a0b72e5de53c4c600c061bb2bb59d2fa982b093",
            "filename": "docs/source/en/model_doc/mbart.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmbart.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -120,7 +120,6 @@ print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n ## MBartTokenizer\n \n [[autodoc]] MBartTokenizer\n-    - build_inputs_with_special_tokens\n \n ## MBartTokenizerFast\n "
        },
        {
            "sha": "3534ff26c42765be06993cc692391e3a3f6cce20",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -136,9 +136,9 @@ Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/bl\n \n [[autodoc]] MistralConfig\n \n-## MistralCommonTokenizer\n+## MistralCommonBackend\n \n-[[autodoc]] MistralCommonTokenizer\n+[[autodoc]] MistralCommonBackend\n \n ## MistralModel\n "
        },
        {
            "sha": "7911200ad7446813a726d91a733d986dbd3cd72d",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -242,9 +242,9 @@ messages = [\n \n [[autodoc]] Mistral3Config\n \n-## MistralCommonTokenizer\n+## MistralCommonBackend\n \n-[[autodoc]] MistralCommonTokenizer\n+[[autodoc]] MistralCommonBackend\n \n ## Mistral3Model\n "
        },
        {
            "sha": "4167a68063f7b1a4645794899b6b345151ae49ae",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -198,9 +198,9 @@ A list of official Hugging Face and community (indicated by üåé) resources to h\n \n [[autodoc]] MixtralConfig\n \n-## MistralCommonTokenizer\n+## MistralCommonBackend\n \n-[[autodoc]] MistralCommonTokenizer\n+[[autodoc]] MistralCommonBackend\n \n ## MixtralModel\n "
        },
        {
            "sha": "938ce6219af0d93b1498f6c25d27c209ec9b70c9",
            "filename": "docs/source/en/model_doc/mpnet.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -64,9 +64,7 @@ separate your segments with the separation token `tokenizer.sep_token` (or `[sep\n ## MPNetTokenizer\n \n [[autodoc]] MPNetTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## MPNetTokenizerFast"
        },
        {
            "sha": "eda5fd867dd6b74589d0a3afca64f8041f1cfb00",
            "filename": "docs/source/en/model_doc/mt5.md",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -127,18 +127,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n [[autodoc]] MT5Config\n \n-## MT5Tokenizer\n-\n-[[autodoc]] MT5Tokenizer\n-\n-See [`T5Tokenizer`] for all details.\n-\n-## MT5TokenizerFast\n-\n-[[autodoc]] MT5TokenizerFast\n-\n-See [`T5TokenizerFast`] for all details.\n-\n ## MT5Model\n \n [[autodoc]] MT5Model"
        },
        {
            "sha": "dcb6e5d5703183f6dd81c321bbe30cddf1adb605",
            "filename": "docs/source/en/model_doc/nllb.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -151,7 +151,6 @@ visualizer(\"UN Chief says there is no military solution in Syria\")\n ## NllbTokenizer\n \n [[autodoc]] NllbTokenizer\n-    - build_inputs_with_special_tokens\n \n ## NllbTokenizerFast\n "
        },
        {
            "sha": "41ee4757b34a3950959b0cdf311de558c3e8bd04",
            "filename": "docs/source/en/model_doc/nougat.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -111,6 +111,10 @@ The model is identical to [Donut](donut) in terms of architecture.\n [[autodoc]] NougatImageProcessorFast\n     - preprocess\n \n+## NougatTokenizer\n+\n+[[autodoc]] NougatTokenizer\n+\n ## NougatTokenizerFast\n \n [[autodoc]] NougatTokenizerFast"
        },
        {
            "sha": "548058c3ec18ecd682ca819efcd0f69ed68e87c2",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -138,9 +138,9 @@ print(output)\n \n [[autodoc]] PixtralVisionConfig\n \n-## MistralCommonTokenizer\n+## MistralCommonBackend\n \n-[[autodoc]] MistralCommonTokenizer\n+[[autodoc]] MistralCommonBackend\n \n ## PixtralVisionModel\n "
        },
        {
            "sha": "237316dc4a31ce2210e2dd79867b354b0885da33",
            "filename": "docs/source/en/model_doc/rembert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -62,17 +62,13 @@ also similar to the Albert one rather than the BERT one.\n ## RemBertTokenizer\n \n [[autodoc]] RemBertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## RemBertTokenizerFast\n \n [[autodoc]] RemBertTokenizerFast\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## RemBertModel"
        },
        {
            "sha": "0a285eba9abb52be294ce6ed443986f94ca65c40",
            "filename": "docs/source/en/model_doc/roberta.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -99,15 +99,12 @@ echo -e \"Plants create <mask> through a process known as photosynthesis.\" | tran\n ## RobertaTokenizer\n \n [[autodoc]] RobertaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## RobertaTokenizerFast\n \n [[autodoc]] RobertaTokenizerFast\n-    - build_inputs_with_special_tokens\n \n ## RobertaModel\n "
        },
        {
            "sha": "86b0293c4b9e3058d58679d76738f038d00880fe",
            "filename": "docs/source/en/model_doc/seamless_m4t.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -176,9 +176,7 @@ This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The o\n \n [[autodoc]] SeamlessM4TTokenizer\n     - __call__\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## SeamlessM4TTokenizerFast"
        },
        {
            "sha": "83e500cd79e2cb7799fa9a75bd0604d356367eb5",
            "filename": "docs/source/en/model_doc/splinter.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fsplinter.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fsplinter.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsplinter.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -67,9 +67,7 @@ This model was contributed by [yuvalkirstain](https://huggingface.co/yuvalkirsta\n ## SplinterTokenizer\n \n [[autodoc]] SplinterTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## SplinterTokenizerFast"
        },
        {
            "sha": "251874ae9a128a47bc46fa859babb232b7e67d10",
            "filename": "docs/source/en/model_doc/squeezebert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fsqueezebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fsqueezebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsqueezebert.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -70,9 +70,7 @@ This model was contributed by [forresti](https://huggingface.co/forresti).\n ## SqueezeBertTokenizer\n \n [[autodoc]] SqueezeBertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## SqueezeBertTokenizerFast"
        },
        {
            "sha": "ef62fff9d21b775118e31d322bf63a070b44ad5a",
            "filename": "docs/source/en/model_doc/t5.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -118,9 +118,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ## T5Tokenizer\n \n [[autodoc]] T5Tokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## T5TokenizerFast"
        },
        {
            "sha": "088c5e1b24622c483e6fed6855f6531f7833f2aa",
            "filename": "docs/source/en/model_doc/whisper.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -100,9 +100,7 @@ transcription[0]\n \n [[autodoc]] WhisperTokenizer\n     - set_prefix_tokens\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n     - batch_decode\n     - decode\n@@ -113,9 +111,7 @@ transcription[0]\n \n [[autodoc]] WhisperTokenizerFast\n     - set_prefix_tokens\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n     - batch_decode\n     - decode"
        },
        {
            "sha": "6e5c1648cd465de37092c0a7b9c1940f318b927d",
            "filename": "docs/source/en/model_doc/xglm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fxglm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fxglm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxglm.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -57,9 +57,7 @@ This model was contributed by [Suraj](https://huggingface.co/valhalla). The orig\n ## XGLMTokenizer\n \n [[autodoc]] XGLMTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## XGLMTokenizerFast"
        },
        {
            "sha": "a57210cd8351e31e0c94f3d9be94e89061ef73ce",
            "filename": "docs/source/en/model_doc/xlm-roberta.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-roberta.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -179,9 +179,7 @@ This implementation is the same as RoBERTa. Refer to the [documentation of RoBER\n ## XLMRobertaTokenizer\n \n [[autodoc]] XLMRobertaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## XLMRobertaTokenizerFast"
        },
        {
            "sha": "c9a06977b985dbdbf30c5b0fab3e58cd77018488",
            "filename": "docs/source/en/model_doc/xlnet.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlnet.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -69,9 +69,7 @@ This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The o\n ## XLNetTokenizer\n \n [[autodoc]] XLNetTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## XLNetTokenizerFast"
        },
        {
            "sha": "e8f44090074b2085f05a1a5c502a8579e4b66489",
            "filename": "docs/source/ja/internal/tokenization_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fja%2Finternal%2Ftokenization_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fja%2Finternal%2Ftokenization_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Finternal%2Ftokenization_utils.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,8 +18,7 @@ rendered properly in your Markdown viewer.\n \n „Åì„ÅÆ„Éö„Éº„Ç∏„Å´„ÅØ„ÄÅ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Å´„Çà„Å£„Å¶‰ΩøÁî®„Åï„Çå„Çã„Åô„Åπ„Å¶„ÅÆ„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞ (‰∏ª„Å´„ÇØ„É©„Çπ) „Åå„É™„Çπ„Éà„Åï„Çå„Åæ„Åô„ÄÇ\n [`~tokenization_utils_base.PreTrainedTokenizerBase`] Èñì„ÅÆÂÖ±ÈÄö„É°„ÇΩ„ÉÉ„Éâ„ÇíÂÆüË£Ö„Åó„Åæ„Åô„ÄÇ\n-[`PreTrainedTokenizer`] „Å® [`PreTrainedTokenizerFast`] „Åä„Çà„Å≥„Éü„ÉÉ„ÇØ„Çπ„Ç§„É≥\n-[`~tokenization_utils_base.SpecialTokensMixin`]„ÄÇ\n+[`PreTrainedTokenizer`] „Å® [`PreTrainedTokenizerFast`] „Åä„Çà„Å≥„Éü„ÉÉ„ÇØ„Çπ„Ç§„É≥„ÄÇ\n \n „Åì„Çå„Çâ„ÅÆ„Åª„Å®„Çì„Å©„ÅØ„ÄÅ„É©„Ç§„Éñ„É©„É™ÂÜÖ„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅÆ„Ç≥„Éº„Éâ„ÇíÂ≠¶Áøí„Åô„ÇãÂ†¥Âêà„Å´„ÅÆ„ÅøÂΩπ„Å´Á´ã„Å°„Åæ„Åô„ÄÇ\n \n@@ -29,10 +28,6 @@ rendered properly in your Markdown viewer.\n     - __call__\n     - all\n \n-## SpecialTokensMixin\n-\n-[[autodoc]] tokenization_utils_base.SpecialTokensMixin\n-\n ## Enums and namedtuples\n \n [[autodoc]] tokenization_utils_base.TruncationStrategy"
        },
        {
            "sha": "99eebc4a96e6c0f4e36e5ebbf9eaebc3b5cea7c4",
            "filename": "docs/source/ja/main_classes/tokenizer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fja%2Fmain_classes%2Ftokenizer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fja%2Fmain_classes%2Ftokenizer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Ftokenizer.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -28,8 +28,7 @@ Rust „É©„Ç§„Éñ„É©„É™ [ü§ó Tokenizers](https://github.com/huggingface/tokenizers\n „É¢„Éá„É´ÂÖ•Âäõ„ÅÆÊñáÂ≠óÂàóÂÖ•Âäõ„Çí„Ç®„É≥„Ç≥„Éº„Éâ„Åó (‰ª•‰∏ã„ÇíÂèÇÁÖß)„ÄÅPython „Çí„Ç§„É≥„Çπ„Çø„É≥„ÇπÂåñ/‰øùÂ≠ò„Åô„Çã„Åü„ÇÅ„ÅÆ‰∏ÄËà¨ÁöÑ„Å™„É°„ÇΩ„ÉÉ„Éâ„ÇíÂÆüË£Ö„Åó„Åæ„Åô„ÄÇ\n „É≠„Éº„Ç´„É´ „Éï„Ç°„Ç§„É´„Åæ„Åü„ÅØ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÄÅ„Åæ„Åü„ÅØ„É©„Ç§„Éñ„É©„É™„Å´„Çà„Å£„Å¶Êèê‰æõ„Åï„Çå„Çã‰∫ãÂâç„Éà„É¨„Éº„Éã„É≥„Ç∞Ê∏à„Åø„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Åã„Çâ„ÅÆ„ÄåÈ´òÈÄü„Äç„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº\n (HuggingFace „ÅÆ AWS S3 „É™„Éù„Ç∏„Éà„É™„Åã„Çâ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ)„ÄÇ‰∫å‰∫∫„Å®„ÇÇÈ†º„Çä„Å´„Åó„Å¶„ÅÑ„Çã„ÅÆ„ÅØ„ÄÅ\n-ÂÖ±ÈÄö„É°„ÇΩ„ÉÉ„Éâ„ÇíÂê´„ÇÄ [`~tokenization_utils_base.PreTrainedTokenizerBase`]\n-[`~tokenization_utils_base.SpecialTokensMixin`]„ÄÇ\n+ÂÖ±ÈÄö„É°„ÇΩ„ÉÉ„Éâ„ÇíÂê´„ÇÄ [`~tokenization_utils_base.PreTrainedTokenizerBase`]„ÄÇ\n \n „Åó„Åü„Åå„Å£„Å¶„ÄÅ[`PreTrainedTokenizer`] „Å® [`PreTrainedTokenizerFast`] „ÅØ„É°„Ç§„É≥„ÇíÂÆüË£Ö„Åó„Åæ„Åô„ÄÇ\n „Åô„Åπ„Å¶„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Çí‰ΩøÁî®„Åô„Çã„Åü„ÇÅ„ÅÆ„É°„ÇΩ„ÉÉ„Éâ:"
        },
        {
            "sha": "d9f0e28eaf4e7015d4a11e58716f9f4b92200e72",
            "filename": "docs/source/ja/model_doc/bloom.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fja%2Fmodel_doc%2Fbloom.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fja%2Fmodel_doc%2Fbloom.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbloom.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -56,12 +56,6 @@ BLOOM „Çí‰Ωø„ÅÑÂßã„ÇÅ„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å§ÂÖ¨Âºè Hugging Face „Åä„Çà„Å≥„Ç≥„Éü„É•„Éã\n [[autodoc]] BloomConfig\n     - all\n \n-## BloomTokenizerFast\n-\n-[[autodoc]] BloomTokenizerFast\n-    - all\n-\n-\n \n ## BloomModel\n "
        },
        {
            "sha": "561048127d1a27b6edfb2583bafd2909f844e7d8",
            "filename": "docs/source/ko/internal/tokenization_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fko%2Finternal%2Ftokenization_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fko%2Finternal%2Ftokenization_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Ftokenization_utils.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º ÏúÑÌïú Ïú†Ìã∏Î¶¨Ìã∞ [[utilities-for-tokenizers]]\n \n-Ïù¥ ÌéòÏù¥ÏßÄÎäî ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Î™®Îì† Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§ÏùÑ ÎÇòÏó¥ÌïòÎ©∞, Ï£ºÎ°ú [`PreTrainedTokenizer`]ÏôÄ [`PreTrainedTokenizerFast`] ÏÇ¨Ïù¥Ïùò Í≥µÌÜµ Î©îÏÜåÎìúÎ•º Íµ¨ÌòÑÌïòÎäî [`~tokenization_utils_base.PreTrainedTokenizerBase`] ÌÅ¥ÎûòÏä§ÏôÄ [`~tokenization_utils_base.SpecialTokensMixin`]ÏùÑ Îã§Î£πÎãàÎã§.\n+Ïù¥ ÌéòÏù¥ÏßÄÎäî ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Î™®Îì† Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§ÏùÑ ÎÇòÏó¥ÌïòÎ©∞, Ï£ºÎ°ú [`PreTrainedTokenizer`]ÏôÄ [`PreTrainedTokenizerFast`] ÏÇ¨Ïù¥Ïùò Í≥µÌÜµ Î©îÏÜåÎìúÎ•º Íµ¨ÌòÑÌïòÎäî [`~tokenization_utils_base.PreTrainedTokenizerBase`] ÌÅ¥ÎûòÏä§ ÏùÑ Îã§Î£πÎãàÎã§.\n \n Ïù¥ Ìï®ÏàòÎì§ ÎåÄÎ∂ÄÎ∂ÑÏùÄ ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏΩîÎìúÎ•º Ïó∞Íµ¨Ìï† ÎïåÎßå Ïú†Ïö©Ìï©ÎãàÎã§.\n \n@@ -26,9 +26,6 @@ rendered properly in your Markdown viewer.\n    - __call__\n    - all\n \n-## SpecialTokensMixin [[transformers.SpecialTokensMixin]]\n-\n-[[autodoc]] tokenization_utils_base.SpecialTokensMixin\n \n ## Enums Î∞è namedtuples [[transformers.tokenization_utils_base.TruncationStrategy]]\n "
        },
        {
            "sha": "307e34c83a111ba2fd734ec49060c6e99970d48e",
            "filename": "docs/source/ko/main_classes/tokenizer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fko%2Fmain_classes%2Ftokenizer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fko%2Fmain_classes%2Ftokenizer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Ftokenizer.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,7 @@ rendered properly in your Markdown viewer.\n 1. ÌäπÌûà Î∞∞Ïπò ÌÜ†ÌÅ∞ÌôîÎ•º ÏàòÌñâÌï† Îïå ÏÜçÎèÑÍ∞Ä ÌÅ¨Í≤å Ìñ•ÏÉÅÎê©ÎãàÎã§.\n 2. ÏõêÎ≥∏ Î¨∏ÏûêÏó¥(Î¨∏Ïûê Î∞è Îã®Ïñ¥)Í≥º ÌÜ†ÌÅ∞ Í≥µÍ∞Ñ ÏÇ¨Ïù¥Î•º Îß§ÌïëÌïòÎäî Ï∂îÍ∞ÄÏ†ÅÏù∏ Î©îÏÜåÎìúÎ•º Ï†úÍ≥µÌï©ÎãàÎã§. (Ïòà: ÌäπÏ†ï Î¨∏ÏûêÎ•º Ìè¨Ìï®ÌïòÎäî ÌÜ†ÌÅ∞Ïùò Ïù∏Îç±Ïä§Î•º ÏñªÍ±∞ÎÇò, ÌäπÏ†ï ÌÜ†ÌÅ∞Ïóê Ìï¥ÎãπÌïòÎäî Î¨∏Ïûê Î≤îÏúÑÎ•º Í∞ÄÏ†∏Ïò§Îäî Îì±).\n \n-Í∏∞Î≥∏ ÌÅ¥ÎûòÏä§Ïù∏ [`PreTrainedTokenizer`]ÏôÄ [`PreTrainedTokenizerFast`]Îäî Î¨∏ÏûêÏó¥ ÏûÖÎ†•ÏùÑ Ïù∏ÏΩîÎî©ÌïòÎäî Î©îÏÜåÎìúÎ•º Íµ¨ÌòÑÌïòÎ©∞(ÏïÑÎûò Ï∞∏Ï°∞), Î°úÏª¨ ÌååÏùºÏù¥ÎÇò ÎîîÎ†âÌÜ†Î¶¨, ÎòêÎäî ÎùºÏù¥Î∏åÎü¨Î¶¨ÏóêÏÑú Ï†úÍ≥µÌïòÎäî ÏÇ¨Ï†Ñ ÌõàÎ†®Îêú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä(HuggingFaceÏùò AWS S3 Ï†ÄÏû•ÏÜåÏóêÏÑú Îã§Ïö¥Î°úÎìúÎêú)Î°úÎ∂ÄÌÑ∞ ÌååÏù¥Ïç¨ Î∞è \"Fast\" ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º Ïù∏Ïä§ÌÑ¥Ïä§ÌôîÌïòÍ±∞ÎÇò Ï†ÄÏû•ÌïòÎäî Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. Ïù¥ Îëê ÌÅ¥ÎûòÏä§Îäî Í≥µÌÜµ Î©îÏÜåÎìúÎ•º Ìè¨Ìï®ÌïòÎäî [`~tokenization_utils_base.PreTrainedTokenizerBase`]ÏôÄ [`~tokenization_utils_base.SpecialTokensMixin`]Ïóê ÏùòÏ°¥Ìï©ÎãàÎã§.\n+Í∏∞Î≥∏ ÌÅ¥ÎûòÏä§Ïù∏ [`PreTrainedTokenizer`]ÏôÄ [`PreTrainedTokenizerFast`]Îäî Î¨∏ÏûêÏó¥ ÏûÖÎ†•ÏùÑ Ïù∏ÏΩîÎî©ÌïòÎäî Î©îÏÜåÎìúÎ•º Íµ¨ÌòÑÌïòÎ©∞(ÏïÑÎûò Ï∞∏Ï°∞), Î°úÏª¨ ÌååÏùºÏù¥ÎÇò ÎîîÎ†âÌÜ†Î¶¨, ÎòêÎäî ÎùºÏù¥Î∏åÎü¨Î¶¨ÏóêÏÑú Ï†úÍ≥µÌïòÎäî ÏÇ¨Ï†Ñ ÌõàÎ†®Îêú ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä(HuggingFaceÏùò AWS S3 Ï†ÄÏû•ÏÜåÏóêÏÑú Îã§Ïö¥Î°úÎìúÎêú)Î°úÎ∂ÄÌÑ∞ ÌååÏù¥Ïç¨ Î∞è \"Fast\" ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º Ïù∏Ïä§ÌÑ¥Ïä§ÌôîÌïòÍ±∞ÎÇò Ï†ÄÏû•ÌïòÎäî Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. Ïù¥ Îëê ÌÅ¥ÎûòÏä§Îäî Í≥µÌÜµ Î©îÏÜåÎìúÎ•º Ìè¨Ìï®ÌïòÎäî [`~tokenization_utils_base.PreTrainedTokenizerBase`]Ïóê ÏùòÏ°¥Ìï©ÎãàÎã§.\n \n [`PreTrainedTokenizer`]ÏôÄ [`PreTrainedTokenizerFast`]Îäî Î™®Îì† ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÏóêÏÑú ÏÇ¨Ïö©ÎêòÎäî Ï£ºÏöî Î©îÏÜåÎìúÎì§ÏùÑ Íµ¨ÌòÑÌï©ÎãàÎã§:\n "
        },
        {
            "sha": "3c98a028064b8f36d10bbdfa6fc8f2ccaa101cca",
            "filename": "docs/source/ko/model_doc/cohere.md",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fko%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fko%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fcohere.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -115,15 +115,7 @@ print(gen_text)\n ## CohereConfig[[transformers.CohereConfig]]\n \n [[autodoc]] CohereConfig\n-\n-## CohereTokenizerFast[[transformers.CohereTokenizerFast]]\n-\n-[[autodoc]] CohereTokenizerFast\n-    - build_inputs_with_special_tokens\n-    - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n-    - update_post_processor\n-    - save_vocabulary\n+ave_vocabulary\n \n ## CohereModel[[transformers.CohereModel]]\n "
        },
        {
            "sha": "9bfb3ee5a98a918fe272df8185383396adfd9b53",
            "filename": "docs/source/zh/internal/tokenization_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fzh%2Finternal%2Ftokenization_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fzh%2Finternal%2Ftokenization_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Finternal%2Ftokenization_utils.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -30,9 +30,6 @@ rendered properly in your Markdown viewer.\n     - __call__\n     - all\n \n-## SpecialTokensMixin\n-\n-[[autodoc]] tokenization_utils_base.SpecialTokensMixin\n \n ## EnumsÂíånamedtuples(ÂëΩÂêçÂÖÉÁªÑ)\n "
        },
        {
            "sha": "6b016fbece726109f3efd32ff45006d068ed3441",
            "filename": "docs/source/zh/main_classes/tokenizer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fzh%2Fmain_classes%2Ftokenizer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/docs%2Fsource%2Fzh%2Fmain_classes%2Ftokenizer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Ftokenizer.md?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,7 @@ tokenizerË¥üË¥£ÂáÜÂ§áËæìÂÖ•‰ª•‰æõÊ®°Âûã‰ΩøÁî®„ÄÇËØ•Â∫ìÂåÖÂê´ÊâÄÊúâÊ®°ÂûãÁöÑtoken\n 1. Âú®ÊâπÈáèÂàÜËØçÊó∂ÊòæËëóÊèêÈÄü\n 2. Âú®ÂéüÂßãÂ≠óÁ¨¶‰∏≤ÔºàÂ≠óÁ¨¶ÂíåÂçïËØçÔºâÂíåtokenÁ©∫Èó¥‰πãÈó¥ËøõË°åÊò†Â∞ÑÁöÑÂÖ∂‰ªñÊñπÊ≥ïÔºà‰æãÂ¶ÇÔºåËé∑ÂèñÂåÖÂê´ÁªôÂÆöÂ≠óÁ¨¶ÁöÑtokenÁöÑÁ¥¢ÂºïÊàñ‰∏éÁªôÂÆötokenÂØπÂ∫îÁöÑÂ≠óÁ¨¶ËåÉÂõ¥Ôºâ„ÄÇ\n \n-Âü∫Á±ª [PreTrainedTokenizer] Âíå [PreTrained TokenizerFast] ÂÆûÁé∞‰∫ÜÂú®Ê®°ÂûãËæìÂÖ•‰∏≠ÁºñÁ†ÅÂ≠óÁ¨¶‰∏≤ËæìÂÖ•ÁöÑÂ∏∏Áî®ÊñπÊ≥ïÔºàËßÅ‰∏ãÊñáÔºâÔºåÂπ∂‰ªéÊú¨Âú∞Êñá‰ª∂ÊàñÁõÆÂΩïÊàñ‰ªéÂ∫ìÊèê‰æõÁöÑÈ¢ÑËÆ≠ÁªÉÁöÑ tokenizerÔºà‰ªé HuggingFace ÁöÑ AWS S3 Â≠òÂÇ®Â∫ì‰∏ãËΩΩÔºâÂÆû‰æãÂåñ/‰øùÂ≠ò python Âíå‚ÄúFast‚Äù tokenizer„ÄÇÂÆÉ‰ª¨ÈÉΩ‰æùËµñ‰∫éÂåÖÂê´Â∏∏Áî®ÊñπÊ≥ïÁöÑ [`~tokenization_utils_base.PreTrainedTokenizerBase`]Âíå[`~tokenization_utils_base.SpecialTokensMixin`]„ÄÇ\n+Âü∫Á±ª [PreTrainedTokenizer] Âíå [PreTrained TokenizerFast] ÂÆûÁé∞‰∫ÜÂú®Ê®°ÂûãËæìÂÖ•‰∏≠ÁºñÁ†ÅÂ≠óÁ¨¶‰∏≤ËæìÂÖ•ÁöÑÂ∏∏Áî®ÊñπÊ≥ïÔºàËßÅ‰∏ãÊñáÔºâÔºåÂπ∂‰ªéÊú¨Âú∞Êñá‰ª∂ÊàñÁõÆÂΩïÊàñ‰ªéÂ∫ìÊèê‰æõÁöÑÈ¢ÑËÆ≠ÁªÉÁöÑ tokenizerÔºà‰ªé HuggingFace ÁöÑ AWS S3 Â≠òÂÇ®Â∫ì‰∏ãËΩΩÔºâÂÆû‰æãÂåñ/‰øùÂ≠ò python Âíå‚ÄúFast‚Äù tokenizer„ÄÇÂÆÉ‰ª¨ÈÉΩ‰æùËµñ‰∫éÂåÖÂê´Â∏∏Áî®ÊñπÊ≥ïÁöÑ [`~tokenization_utils_base.PreTrainedTokenizerBase`]„ÄÇ\n \n Âõ†Ê≠§Ôºå[`PreTrainedTokenizer`] Âíå [`PreTrainedTokenizerFast`] ÂÆûÁé∞‰∫Ü‰ΩøÁî®ÊâÄÊúâtokenizersÁöÑ‰∏ªË¶ÅÊñπÊ≥ïÔºö\n "
        },
        {
            "sha": "ac6136fe4c1213ab8ea8a0c022ad13ff57d9c413",
            "filename": "examples/pytorch/question-answering/run_qa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -38,7 +38,6 @@\n     DataCollatorWithPadding,\n     EvalPrediction,\n     HfArgumentParser,\n-    PreTrainedTokenizerFast,\n     TrainingArguments,\n     default_data_collator,\n     set_seed,\n@@ -335,7 +334,8 @@ def main():\n     )\n \n     # Tokenizer check: this script requires a fast tokenizer.\n-    if not isinstance(tokenizer, PreTrainedTokenizerFast):\n+    # Check if tokenizer has _tokenizer attribute (from tokenizers library) or is_fast property\n+    if not (hasattr(tokenizer, \"_tokenizer\") or getattr(tokenizer, \"is_fast\", False)):\n         raise TypeError(\n             \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\""
        },
        {
            "sha": "9d9477f8f09990bedacef4e2911681b503aec2cd",
            "filename": "examples/pytorch/text-generation/run_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -37,7 +37,6 @@\n from transformers import (\n     AutoTokenizer,\n     BloomForCausalLM,\n-    BloomTokenizerFast,\n     CTRLLMHeadModel,\n     CTRLTokenizer,\n     GenerationMixin,\n@@ -72,7 +71,7 @@\n     \"xlnet\": (XLNetLMHeadModel, XLNetTokenizer),\n     \"xlm\": (XLMWithLMHeadModel, XLMTokenizer),\n     \"gptj\": (GPTJForCausalLM, AutoTokenizer),\n-    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n+    \"bloom\": (BloomForCausalLM, AutoTokenizer),\n     \"llama\": (LlamaForCausalLM, AutoTokenizer),\n     \"opt\": (OPTForCausalLM, GPT2Tokenizer),\n }"
        },
        {
            "sha": "2f1ab1a1a0901c74b7c145ee44c62b3e60fd2faf",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -49,7 +49,6 @@\n     DataCollatorForTokenClassification,\n     HfArgumentParser,\n     PreTrainedConfig,\n-    PreTrainedTokenizerFast,\n     Trainer,\n     TrainingArguments,\n     set_seed,\n@@ -389,7 +388,8 @@ def get_label_list(labels):\n     )\n \n     # Tokenizer check: this script requires a fast tokenizer.\n-    if not isinstance(tokenizer, PreTrainedTokenizerFast):\n+    # Check if tokenizer has _tokenizer attribute (from tokenizers library) or is_fast property\n+    if not (hasattr(tokenizer, \"_tokenizer\") or getattr(tokenizer, \"is_fast\", False)):\n         raise TypeError(\n             \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n             \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\""
        },
        {
            "sha": "c69b0c03847dcc7679b784e085845841d13b1408",
            "filename": "examples/pytorch/translation/run_translation.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2Frun_translation.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -60,6 +60,7 @@\n     default_data_collator,\n     set_seed,\n )\n+from transformers.tokenization_utils_sentencepiece import SentencePieceBackend\n from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n@@ -403,7 +404,9 @@ def main():\n         model.resize_token_embeddings(len(tokenizer))\n \n     # Set decoder_start_token_id\n-    if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n+    if model.config.decoder_start_token_id is None and isinstance(\n+        tokenizer, (MBartTokenizer, MBartTokenizerFast, SentencePieceBackend)\n+    ):\n         if isinstance(tokenizer, MBartTokenizer):\n             model.config.decoder_start_token_id = tokenizer.lang_code_to_id[data_args.target_lang]\n         else:"
        },
        {
            "sha": "6dad9d7c05cbaa1b2b552c994d6da6475813e59b",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 17,
            "deletions": 7,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -58,6 +58,7 @@\n # Base objects, independent of any specific backend\n _import_structure = {\n     \"audio_utils\": [],\n+    \"cli\": [],\n     \"configuration_utils\": [\"PreTrainedConfig\", \"PretrainedConfig\"],\n     \"convert_slow_tokenizers_checkpoints_to_fast\": [],\n     \"data\": [\n@@ -172,13 +173,13 @@\n     \"processing_utils\": [\"ProcessorMixin\"],\n     \"quantizers\": [],\n     \"testing_utils\": [],\n-    \"tokenization_utils\": [\"PreTrainedTokenizer\"],\n+    \"tokenization_python\": [\"PreTrainedTokenizer\", \"PythonBackend\"],\n+    \"tokenization_utils_sentencepiece\": [\"SentencePieceBackend\"],\n     \"tokenization_utils_base\": [\n         \"AddedToken\",\n         \"BatchEncoding\",\n         \"CharSpan\",\n         \"PreTrainedTokenizerBase\",\n-        \"SpecialTokensMixin\",\n         \"TokenSpan\",\n     ],\n     \"trainer_callback\": [\n@@ -274,7 +275,10 @@\n     ]\n else:\n     # Fast tokenizers structure\n-    _import_structure[\"tokenization_utils_fast\"] = [\"PreTrainedTokenizerFast\"]\n+    _import_structure[\"tokenization_utils_tokenizers\"] = [\n+        \"TokenizersBackend\",\n+        \"PreTrainedTokenizerFast\",\n+    ]\n \n \n try:\n@@ -302,7 +306,7 @@\n         name for name in dir(dummy_mistral_common_objects) if not name.startswith(\"_\")\n     ]\n else:\n-    _import_structure[\"tokenization_mistral_common\"] = [\"MistralCommonTokenizer\"]\n+    _import_structure[\"tokenization_mistral_common\"] = [\"MistralCommonBackend\"]\n \n # Vision-specific objects\n try:\n@@ -677,14 +681,20 @@\n     from .pytorch_utils import apply_chunking_to_forward as apply_chunking_to_forward\n \n     # Tokenization\n-    from .tokenization_utils import PreTrainedTokenizer as PreTrainedTokenizer\n+    from .tokenization_python import PreTrainedTokenizer as PreTrainedTokenizer\n+    from .tokenization_python import PythonBackend as PythonBackend\n     from .tokenization_utils_base import AddedToken as AddedToken\n     from .tokenization_utils_base import BatchEncoding as BatchEncoding\n     from .tokenization_utils_base import CharSpan as CharSpan\n     from .tokenization_utils_base import PreTrainedTokenizerBase as PreTrainedTokenizerBase\n-    from .tokenization_utils_base import SpecialTokensMixin as SpecialTokensMixin\n     from .tokenization_utils_base import TokenSpan as TokenSpan\n-    from .tokenization_utils_fast import PreTrainedTokenizerFast as PreTrainedTokenizerFast\n+\n+    # Tokenization\n+    from .tokenization_utils_sentencepiece import SentencePieceBackend as SentencePieceBackend\n+    from .tokenization_utils_tokenizers import PreTrainedTokenizerFast as PreTrainedTokenizerFast\n+    from .tokenization_utils_tokenizers import (\n+        TokenizersBackend as TokenizersBackend,\n+    )\n \n     # Trainer\n     from .trainer import Trainer as Trainer"
        },
        {
            "sha": "f6a7944ca38a3f2e813819b51446f7118527fd46",
            "filename": "src/transformers/cli/add_new_model_like.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -142,7 +142,7 @@ def __init__(self, lowercase_name: str):\n \n         # Get tokenizer class\n         if self.lowercase_name in TOKENIZER_MAPPING_NAMES:\n-            self.tokenizer_class, self.fast_tokenizer_class = TOKENIZER_MAPPING_NAMES[self.lowercase_name]\n+            self.fast_tokenizer_class = TOKENIZER_MAPPING_NAMES[self.lowercase_name]\n             self.fast_tokenizer_class = (\n                 None if self.fast_tokenizer_class == \"PreTrainedTokenizerFast\" else self.fast_tokenizer_class\n             )"
        },
        {
            "sha": "26bc4eb1f285b8f07b4c1e330d888ba1d4aae172",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1590,7 +1590,6 @@ def tokenizer(self, proto):\n         return tokenizer\n \n \n-# Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n@@ -1625,16 +1624,14 @@ def __init__(\n         vocab_file=None,\n         pattern=r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n         add_prefix_space=False,\n-        additional_special_tokens=None,\n+        extra_special_tokens=None,\n         **kwargs,\n     ):\n         self.vocab_file = vocab_file\n         self.pattern = pattern\n         self.add_prefix_space = add_prefix_space\n-        self.additional_special_tokens = (\n-            additional_special_tokens.keys()\n-            if isinstance(additional_special_tokens, dict)\n-            else additional_special_tokens\n+        self.extra_special_tokens = (\n+            extra_special_tokens.keys() if isinstance(extra_special_tokens, dict) else extra_special_tokens\n         )\n \n     def extract_vocab_merges_from_model(self, tiktoken_url: str):\n@@ -1686,7 +1683,7 @@ def converted(self) -> Tokenizer:\n         tokenizer.decoder = decoders.ByteLevel()\n \n         tokenizer.add_special_tokens(\n-            [AddedToken(token, normalized=False, special=True) for token in self.additional_special_tokens]\n+            [AddedToken(token, normalized=False, special=True) for token in self.extra_special_tokens]\n         )\n \n         tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n@@ -1861,7 +1858,7 @@ def convert_slow_tokenizer(transformer_tokenizer, from_tiktoken=False) -> Tokeni\n             logger.info(\"Converting from Tiktoken\")\n             return TikTokenConverter(\n                 vocab_file=transformer_tokenizer.vocab_file,\n-                additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n+                extra_special_tokens=transformer_tokenizer.extra_special_tokens,\n             ).converted()\n         except Exception:\n             raise ValueError("
        },
        {
            "sha": "46886e6fc0396cff06719e2613358df984a9f565",
            "filename": "src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fconvert_slow_tokenizers_checkpoints_to_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fconvert_slow_tokenizers_checkpoints_to_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizers_checkpoints_to_fast.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -28,11 +28,21 @@\n logger = logging.get_logger(__name__)\n \n \n-TOKENIZER_CLASSES = {\n-    # Phi3 uses Llama tokenizer\n-    name: getattr(transformers, \"LlamaTokenizerFast\" if name == \"Phi3Tokenizer\" else name + \"Fast\")\n-    for name in SLOW_TO_FAST_CONVERTERS\n-}\n+TOKENIZER_CLASSES = {}\n+for name in SLOW_TO_FAST_CONVERTERS:\n+    # Special cases for tokenizers that don't have their own Fast tokenizer\n+    if name == \"Phi3Tokenizer\":\n+        tokenizer_class_name = \"LlamaTokenizerFast\"\n+    elif name == \"ElectraTokenizer\":\n+        tokenizer_class_name = \"BertTokenizerFast\"\n+    else:\n+        tokenizer_class_name = name + \"Fast\"\n+\n+    try:\n+        TOKENIZER_CLASSES[name] = getattr(transformers, tokenizer_class_name)\n+    except AttributeError:\n+        # Skip tokenizers that don't have a Fast version\n+        pass\n \n \n def convert_slow_checkpoint_to_fast(tokenizer_name, checkpoint_name, dump_path, force_download):"
        },
        {
            "sha": "644c582b5ed1eeeff8fd6ac77881e00e21abd114",
            "filename": "src/transformers/data/datasets/glue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -102,7 +102,6 @@ def __init__(\n         label_list = self.processor.get_labels()\n         if args.task_name in [\"mnli\", \"mnli-mm\"] and tokenizer.__class__.__name__ in (\n             \"RobertaTokenizer\",\n-            \"RobertaTokenizerFast\",\n             \"XLMRobertaTokenizer\",\n             \"BartTokenizer\",\n             \"BartTokenizerFast\","
        },
        {
            "sha": "ba8936a5d64223156ea9635048f969df72d87bae",
            "filename": "src/transformers/data/datasets/squad.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -22,7 +22,7 @@\n from torch.utils.data import Dataset\n \n from ...models.auto.modeling_auto import MODEL_FOR_QUESTION_ANSWERING_MAPPING\n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import check_torch_load_is_safe, logging\n from ..processors.squad import SquadFeatures, SquadV1Processor, SquadV2Processor, squad_convert_examples_to_features\n "
        },
        {
            "sha": "c6865f2d99a5310ce2c3b51365e9dd579ce7788f",
            "filename": "src/transformers/data/processors/glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -19,7 +19,7 @@\n import warnings\n from enum import Enum\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n from .utils import DataProcessor, InputExample, InputFeatures\n "
        },
        {
            "sha": "bc40f6b3ad391364ae60ee513cefa68f21428f81",
            "filename": "src/transformers/data/processors/squad.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,7 @@\n import numpy as np\n from tqdm import tqdm\n \n-from ...models.bert.tokenization_bert import whitespace_tokenize\n+from ...models.bert.tokenization_bert_legacy import whitespace_tokenize\n from ...tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase, TruncationStrategy\n from ...utils import is_torch_available, is_torch_hpu_available, logging\n from .utils import DataProcessor\n@@ -125,7 +125,6 @@ def squad_convert_example_to_features(\n             \"RobertaTokenizer\",\n             \"LongformerTokenizer\",\n             \"BartTokenizer\",\n-            \"RobertaTokenizerFast\",\n             \"LongformerTokenizerFast\",\n             \"BartTokenizerFast\",\n         ]:\n@@ -161,7 +160,8 @@ def squad_convert_example_to_features(\n         if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET\n         else tokenizer.model_max_length - tokenizer.max_len_single_sentence\n     )\n-    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair\n+    max_len_sentences_pair = tokenizer.model_max_length - tokenizer.num_special_tokens_to_add(pair=True)\n+    sequence_pair_added_tokens = tokenizer.model_max_length - max_len_sentences_pair\n \n     span_doc_tokens = all_doc_tokens\n     while len(spans) * doc_stride < len(all_doc_tokens):\n@@ -175,7 +175,7 @@ def squad_convert_example_to_features(\n             pairs = truncated_query\n             truncation = TruncationStrategy.ONLY_FIRST.value\n \n-        encoded_dict = tokenizer.encode_plus(  # TODO(thom) update this logic\n+        encoded_dict = tokenizer(  # TODO(thom) update this logic\n             texts,\n             pairs,\n             truncation=truncation,"
        },
        {
            "sha": "c695858169e95f26dbbb814d885df0196d36ce08",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -490,7 +490,7 @@ def convert_source_tokens_to_target_tokens(\n         Returns:\n             The converted token IDs.\n         \"\"\"\n-        text = source_tokenizer.batch_decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+        text = source_tokenizer.decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n         dest_ids = destination_tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n         return dest_ids.to(input_ids.device)\n \n@@ -978,7 +978,7 @@ def _prepare_assistant_input_ids(self, target_input_ids: torch.LongTensor) -> to\n             # we have only one new token and we can directly convert it\n             assistant_new_ids = self._atm_translator.target_to_assistant_input_ids.get(target_new_ids[0].item())\n         if assistant_new_ids is None:\n-            target_new_text = self.target_tokenizer.batch_decode(\n+            target_new_text = self.target_tokenizer.decode(\n                 target_new_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n             )\n             assistant_new_ids = self.assistant_tokenizer("
        },
        {
            "sha": "f97828a0862b73a8aabc5dc93e45558019b33d83",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -44,7 +44,7 @@\n from ..integrations.fsdp import is_fsdp_managed_module\n from ..masking_utils import create_masks_for_generate\n from ..pytorch_utils import isin_mps_friendly\n-from ..tokenization_utils import ExtensionsTrie\n+from ..tokenization_python import ExtensionsTrie\n from ..utils import (\n     ModelOutput,\n     TransformersKwargs,\n@@ -2734,7 +2734,7 @@ def heal_tokens(\n \n         # assumption: leading/trailing whitespace is not meaningful, so the prompts are\n         # stripped before re-tokenizing to desensitize generation to whitespace artefacts\n-        prompts = [p.strip() for p in tokenizer.batch_decode(input_ids, skip_special_tokens=True)]\n+        prompts = [p.strip() for p in tokenizer.decode(input_ids, skip_special_tokens=True)]\n         input_ids = tokenizer(\n             prompts,\n             return_tensors=\"pt\","
        },
        {
            "sha": "68807f20425be14074f2849e52254a15fcca4e5a",
            "filename": "src/transformers/integrations/mistral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmistral.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1,8 +1,8 @@\n from tokenizers import Regex, Tokenizer, decoders, pre_tokenizers, processors\n from tokenizers.models import BPE\n \n-from transformers import LlamaTokenizerFast\n from transformers.convert_slow_tokenizer import bytes_to_unicode\n+from transformers.tokenization_utils_tokenizers import PreTrainedTokenizerFast\n \n \n class MistralConverter:\n@@ -85,16 +85,18 @@ def convert_tekken_tokenizer(tokenizer_file: str):\n     # Extract vocab and special tokens\n     vocab = mistral_tokenizer.instruct_tokenizer.tokenizer._tekken_token2id_nospecial\n     all_special = [\n-        token.value if hasattr(token, \"value\") else token\n+        token.get(\"token_str\", str(token))\n+        if isinstance(token, dict)\n+        else (token.value if hasattr(token, \"value\") else str(token))\n         for token in mistral_tokenizer.instruct_tokenizer.tokenizer._all_special_tokens\n     ]\n     specials_tokens = {token: all_special.index(token) for token in all_special}\n     specials_tokens.update(vocab)\n     vocab = specials_tokens\n \n     # Convert\n-    tokenizer = LlamaTokenizerFast(\n-        tokenizer_object=MistralConverter(vocab=vocab, additional_special_tokens=all_special).converted(),\n+    tokenizer = PreTrainedTokenizerFast(\n+        tokenizer_object=MistralConverter(vocab=vocab, additional_special_tokens=all_special).converted()\n     )\n \n     # Post-process"
        },
        {
            "sha": "9dd19052d29a009281417a9fef479af56b1d3c9f",
            "filename": "src/transformers/integrations/tiktoken.py",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fintegrations%2Ftiktoken.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fintegrations%2Ftiktoken.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftiktoken.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -2,7 +2,7 @@\n from typing import Any\n \n from transformers.convert_slow_tokenizer import TikTokenConverter\n-from transformers.tokenization_utils_fast import TIKTOKEN_VOCAB_FILE, TOKENIZER_FILE\n+from transformers.tokenization_utils_tokenizers import TIKTOKEN_VOCAB_FILE, TOKENIZER_FILE\n \n \n def convert_tiktoken_to_fast(encoding: Any, output_dir: str):\n@@ -23,6 +23,9 @@ def convert_tiktoken_to_fast(encoding: Any, output_dir: str):\n     save_file = output_dir / \"tiktoken\" / TIKTOKEN_VOCAB_FILE\n     tokenizer_file = output_dir / TOKENIZER_FILE\n \n+    # Create parent directory for save_file\n+    save_file.parent.mkdir(parents=True, exist_ok=True)\n+\n     save_file_absolute = str(save_file.absolute())\n     output_file_absolute = str(tokenizer_file.absolute())\n \n@@ -34,10 +37,17 @@ def convert_tiktoken_to_fast(encoding: Any, output_dir: str):\n             encoding = get_encoding(encoding)\n \n         dump_tiktoken_bpe(encoding._mergeable_ranks, save_file_absolute)\n-    except ImportError:\n-        raise ValueError(\"`tiktoken` is required to save a `tiktoken` file. Install it with `pip install tiktoken`.\")\n+    except ImportError as e:\n+        error_msg = str(e)\n+        if \"blobfile\" in error_msg.lower():\n+            raise ValueError(\n+                \"`blobfile` is required to save a `tiktoken` file. Install it with `pip install blobfile`.\"\n+            ) from e\n+        raise ValueError(\n+            \"`tiktoken` is required to save a `tiktoken` file. Install it with `pip install tiktoken`.\"\n+        ) from e\n \n     tokenizer = TikTokenConverter(\n-        vocab_file=save_file_absolute, pattern=encoding._pat_str, additional_special_tokens=encoding._special_tokens\n+        vocab_file=save_file_absolute, pattern=encoding._pat_str, extra_special_tokens=encoding._special_tokens\n     ).converted()\n     tokenizer.save(output_file_absolute)"
        },
        {
            "sha": "606fc7dcc0f83b43b4a611c854afae43d76db1f9",
            "filename": "src/transformers/models/albert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Falbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Falbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,6 @@\n     from .configuration_albert import *\n     from .modeling_albert import *\n     from .tokenization_albert import *\n-    from .tokenization_albert_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "479efa963548a8cb813d95fb4a8958a741bb627b",
            "filename": "src/transformers/models/albert/tokenization_albert.py",
            "status": "modified",
            "additions": 102,
            "deletions": 237,
            "changes": 339,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,41 +14,30 @@\n # limitations under the License.\n \"\"\"Tokenization classes for ALBERT model.\"\"\"\n \n-import os\n-import unicodedata\n-from shutil import copyfile\n-from typing import Any, Optional\n+from typing import Optional\n \n-import sentencepiece as spm\n+from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from tokenizers.models import Unigram\n \n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n-from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n \n+VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\", \"tokenizer_file\": \"tokenizer.json\"}\n \n-SPIECE_UNDERLINE = \"‚ñÅ\"\n \n-\n-@requires(backends=(\"sentencepiece\",))\n-class AlbertTokenizer(PreTrainedTokenizer):\n+class AlbertTokenizer(TokenizersBackend):\n     \"\"\"\n-    Construct an ALBERT tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).\n-\n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n+    Construct a \"fast\" ALBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on\n+    [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models). This\n+    tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should refer to\n+    this superclass for more information regarding those methods\n \n     Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n         do_lower_case (`bool`, *optional*, defaults to `True`):\n             Whether or not to lowercase the input when tokenizing.\n-        remove_space (`bool`, *optional*, defaults to `True`):\n-            Whether or not to strip the text when tokenizing (removing excess spaces before and after the string).\n         keep_accents (`bool`, *optional*, defaults to `False`):\n             Whether or not to keep accents when tokenizing.\n         bos_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n@@ -62,15 +51,8 @@ class AlbertTokenizer(PreTrainedTokenizer):\n             </Tip>\n \n         eos_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The end of sequence token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n-            The token used is the `sep_token`.\n-\n-            </Tip>\n-\n+            The end of sequence token. .. note:: When building a sequence using special tokens, this is not the token\n+            that is used for the end of sequence. The token used is the `sep_token`.\n         unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n@@ -86,235 +68,118 @@ class AlbertTokenizer(PreTrainedTokenizer):\n         mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        sp_model_kwargs (`dict`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n-\n-    Attributes:\n-        sp_model (`SentencePieceProcessor`):\n-            The *SentencePiece* processor that is used for every conversion (string, tokens and IDs).\n+        add_prefix_space (`bool`, *optional*, defaults to `True`):\n+            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n+            other word.\n+        trim_offsets (`bool`, *optional*, defaults to `True`):\n+            Whether the post processing step should trim offsets to avoid including whitespaces.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        vocab_file (`str`, *optional*):\n+            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a .model extension) that\n+            contains the vocabulary necessary to instantiate a tokenizer.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n+    model_input_names = [\"input_ids\", \"attention_mask\"]\n+    slow_tokenizer_class = None\n \n     def __init__(\n         self,\n-        vocab_file,\n-        do_lower_case=True,\n-        remove_space=True,\n-        keep_accents=False,\n-        bos_token=\"[CLS]\",\n-        eos_token=\"[SEP]\",\n-        unk_token=\"<unk>\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"<pad>\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n+        do_lower_case: bool = True,\n+        keep_accents: bool = False,\n+        bos_token: str = \"[CLS]\",\n+        eos_token: str = \"[SEP]\",\n+        unk_token: str = \"<unk>\",\n+        sep_token: str = \"[SEP]\",\n+        pad_token: str = \"<pad>\",\n+        cls_token: str = \"[CLS]\",\n+        mask_token: str = \"[MASK]\",\n+        add_prefix_space: bool = True,\n+        trim_offsets: bool = True,\n+        vocab: Optional[dict] = None,\n+        vocab_file: Optional[str] = None,\n         **kwargs,\n-    ) -> None:\n-        # Mask token behave like a normal word, i.e. include the space before it and\n-        # is included in the raw text, there should be a match in a non-normalized sentence.\n-        mask_token = (\n-            AddedToken(mask_token, lstrip=True, rstrip=False, normalized=False)\n-            if isinstance(mask_token, str)\n-            else mask_token\n-        )\n-\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n+    ):\n+        self.vocab_file = vocab_file\n+        self.add_prefix_space = add_prefix_space\n+        self.trim_offsets = trim_offsets\n \n         self.do_lower_case = do_lower_case\n-        self.remove_space = remove_space\n         self.keep_accents = keep_accents\n-        self.vocab_file = vocab_file\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(vocab_file)\n-\n-        super().__init__(\n-            do_lower_case=do_lower_case,\n-            remove_space=remove_space,\n-            keep_accents=keep_accents,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            sp_model_kwargs=self.sp_model_kwargs,\n-            **kwargs,\n-        )\n-\n-    @property\n-    def vocab_size(self) -> int:\n-        return len(self.sp_model)\n \n-    def get_vocab(self) -> dict[str, int]:\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(self.vocab_file)\n-\n-    def preprocess_text(self, inputs):\n-        if self.remove_space:\n-            outputs = \" \".join(inputs.strip().split())\n+        if vocab is not None:\n+            self._vocab_scores = [(token, 0.0) for token in vocab.keys()] if isinstance(vocab, dict) else list(vocab)\n         else:\n-            outputs = inputs\n-        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n+            self._vocab_scores = [\n+                (str(pad_token), 0.0),\n+                (str(unk_token), 0.0),\n+                (str(cls_token), 0.0),\n+                (str(sep_token), 0.0),\n+                (str(mask_token), 0.0),\n+            ]\n+\n+        self._tokenizer = Tokenizer(\n+            Unigram(\n+                self._vocab_scores,\n+                unk_id=1,\n+                byte_fallback=False,\n+            )\n+        )\n \n+        list_normalizers = [\n+            normalizers.Replace(\"``\", '\"'),\n+            normalizers.Replace(\"''\", '\"'),\n+            normalizers.NFKD(),\n+            normalizers.StripAccents(),\n+            normalizers.Lowercase(),\n+            normalizers.Replace(Regex(\" {2,}\"), \" \"),\n+        ]\n         if not self.keep_accents:\n-            outputs = unicodedata.normalize(\"NFKD\", outputs)\n-            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n+            list_normalizers.append(normalizers.NFKD())\n+            list_normalizers.append(normalizers.StripAccents())\n         if self.do_lower_case:\n-            outputs = outputs.lower()\n-\n-        return outputs\n-\n-    def _tokenize(self, text: str) -> list[str]:\n-        \"\"\"Tokenize a string.\"\"\"\n-        text = self.preprocess_text(text)\n-        pieces = self.sp_model.encode(text, out_type=str)\n-        new_pieces = []\n-        for piece in pieces:\n-            if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n-                # Logic to handle special cases see https://github.com/google-research/bert/blob/master/README.md#tokenization\n-                # `9,9` -> ['‚ñÅ9', ',', '9'] instead of [`_9,`, '9']\n-                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n-                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n-                    if len(cur_pieces[0]) == 1:\n-                        cur_pieces = cur_pieces[1:]\n-                    else:\n-                        cur_pieces[0] = cur_pieces[0][1:]\n-                cur_pieces.append(piece[-1])\n-                new_pieces.extend(cur_pieces)\n-            else:\n-                new_pieces.append(piece)\n-\n-        return new_pieces\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.sp_model.PieceToId(token)\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.sp_model.IdToPiece(index)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        current_sub_tokens = []\n-        out_string = \"\"\n-        prev_is_special = False\n-        for token in tokens:\n-            # make sure that special tokens are not decoded using sentencepiece model\n-            if token in self.all_special_tokens:\n-                if not prev_is_special:\n-                    out_string += \" \"\n-                out_string += self.sp_model.decode(current_sub_tokens) + token\n-                prev_is_special = True\n-                current_sub_tokens = []\n-            else:\n-                current_sub_tokens.append(token)\n-                prev_is_special = False\n-        out_string += self.sp_model.decode(current_sub_tokens)\n-        return out_string.strip()\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An ALBERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n+            list_normalizers.append(normalizers.Lowercase())\n \n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return cls + token_ids_0 + sep\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n+        list_normalizers.append(normalizers.Replace(Regex(\" {2,}\"), \" \"))\n+        self._tokenizer.normalizer = normalizers.Sequence(list_normalizers)\n \n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n+        prepend_scheme = \"always\" if add_prefix_space else \"never\"\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+            [\n+                pre_tokenizers.WhitespaceSplit(),\n+                pre_tokenizers.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme),\n+            ]\n+        )\n \n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n+        self._tokenizer.decoder = decoders.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme)\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=\"[CLS]:0 $A:0 [SEP]:0\",\n+            pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n+            special_tokens=[\n+                (\"[CLS]\", self._tokenizer.token_to_id(str(cls_token))),\n+                (\"[SEP]\", self._tokenizer.token_to_id(str(sep_token))),\n+            ],\n         )\n \n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n+        tokenizer_object = self._tokenizer\n \n-        return (out_vocab_file,)\n+        super().__init__(\n+            tokenizer_object=tokenizer_object,\n+            do_lower_case=self.do_lower_case,\n+            keep_accents=self.keep_accents,\n+            bos_token=bos_token,\n+            eos_token=eos_token,\n+            sep_token=sep_token,\n+            cls_token=cls_token,\n+            unk_token=unk_token,\n+            pad_token=pad_token,\n+            mask_token=mask_token,\n+            add_prefix_space=add_prefix_space,\n+            trim_offsets=trim_offsets,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"AlbertTokenizer\"]"
        },
        {
            "sha": "ed9add51d20743948dc1fe51ad6f5fe0c1ed1543",
            "filename": "src/transformers/models/albert/tokenization_albert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 178,
            "changes": 178,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,178 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for ALBERT model.\"\"\"\n-\n-import os\n-from shutil import copyfile\n-from typing import Optional\n-\n-from ...tokenization_utils import AddedToken\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import is_sentencepiece_available, logging\n-\n-\n-if is_sentencepiece_available():\n-    from .tokenization_albert import AlbertTokenizer\n-else:\n-    AlbertTokenizer = None\n-\n-logger = logging.get_logger(__name__)\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-SPIECE_UNDERLINE = \"‚ñÅ\"\n-\n-\n-class AlbertTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" ALBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on\n-    [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models). This\n-    tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods\n-\n-    Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        remove_space (`bool`, *optional*, defaults to `True`):\n-            Whether or not to strip the text when tokenizing (removing excess spaces before and after the string).\n-        keep_accents (`bool`, *optional*, defaults to `False`):\n-            Whether or not to keep accents when tokenizing.\n-        bos_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the beginning of\n-            sequence. The token used is the `cls_token`.\n-\n-            </Tip>\n-\n-        eos_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The end of sequence token. .. note:: When building a sequence using special tokens, this is not the token\n-            that is used for the end of sequence. The token used is the `sep_token`.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = AlbertTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=True,\n-        remove_space=True,\n-        keep_accents=False,\n-        bos_token=\"[CLS]\",\n-        eos_token=\"[SEP]\",\n-        unk_token=\"<unk>\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"<pad>\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        **kwargs,\n-    ):\n-        # Mask token behave like a normal word, i.e. include the space before it and\n-        # is included in the raw text, there should be a match in a non-normalized sentence.\n-        mask_token = (\n-            AddedToken(mask_token, lstrip=True, rstrip=False, normalized=False)\n-            if isinstance(mask_token, str)\n-            else mask_token\n-        )\n-\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            remove_space=remove_space,\n-            keep_accents=keep_accents,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            **kwargs,\n-        )\n-\n-        self.do_lower_case = do_lower_case\n-        self.remove_space = remove_space\n-        self.keep_accents = keep_accents\n-        self.vocab_file = vocab_file\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An ALBERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return cls + token_ids_0 + sep\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not self.can_save_slow_tokenizer:\n-            raise ValueError(\n-                \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \"\n-                \"tokenizer.\"\n-            )\n-\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-\n-        return (out_vocab_file,)\n-\n-\n-__all__ = [\"AlbertTokenizerFast\"]"
        },
        {
            "sha": "1001e647803b851e0844d3d3f661f5b111f6db9b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils import PreTokenizedInput, TextInput\n+from ...tokenization_python import PreTokenizedInput, TextInput\n from ...utils import TensorType, TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoTokenizer\n from ..llama.configuration_llama import LlamaConfig"
        },
        {
            "sha": "23f13f3129c28c9cf9c88a62a21181023136dbaa",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -25,7 +25,7 @@\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils import PreTokenizedInput, TextInput\n+from ...tokenization_python import PreTokenizedInput, TextInput\n from ...utils import TensorType\n from ..auto import AutoTokenizer\n "
        },
        {
            "sha": "93d8dfabdd7e53dc2260b6fb0d98608aeede7179",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -25,7 +25,7 @@\n from ...feature_extraction_utils import FeatureExtractionMixin\n from ...image_processing_utils import ImageProcessingMixin\n from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils import TOKENIZER_CONFIG_FILE\n+from ...tokenization_python import TOKENIZER_CONFIG_FILE\n from ...utils import FEATURE_EXTRACTOR_NAME, PROCESSOR_NAME, VIDEO_PROCESSOR_NAME, cached_file, logging\n from ...video_processing_utils import BaseVideoProcessor\n from .auto_factory import _LazyAutoMapping"
        },
        {
            "sha": "deac13558652647d3b47e77f3b5f09c2495bb2cf",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 788,
            "deletions": 841,
            "changes": 1629,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"Auto Tokenizer class.\"\"\"\n \n import importlib\n+import inspect\n import json\n import os\n from collections import OrderedDict\n@@ -25,16 +26,16 @@\n from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...modeling_gguf_pytorch_utils import load_gguf_checkpoint\n-from ...tokenization_utils import PreTrainedTokenizer\n-from ...tokenization_utils_base import TOKENIZER_CONFIG_FILE\n+from ...tokenization_python import PreTrainedTokenizer, PythonBackend\n+from ...tokenization_utils_base import TOKENIZER_CONFIG_FILE, find_sentencepiece_model_file, load_vocab_and_merges\n from ...utils import (\n-    cached_file,\n     extract_commit_hash,\n     is_g2p_en_available,\n     is_sentencepiece_available,\n     is_tokenizers_available,\n     logging,\n )\n+from ...utils.hub import cached_file, has_file\n from ..encoder_decoder import EncoderDecoderConfig\n from .auto_factory import _LazyAutoMapping\n from .configuration_auto import (\n@@ -47,777 +48,303 @@\n \n \n if is_tokenizers_available():\n-    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n+    from ...tokenization_utils_tokenizers import TokenizersBackend\n else:\n-    PreTrainedTokenizerFast = None\n+    TokenizersBackend = None\n \n+if is_sentencepiece_available():\n+    from ...tokenization_utils_sentencepiece import SentencePieceBackend\n+else:\n+    SentencePieceBackend = None\n \n logger = logging.get_logger(__name__)\n \n-# Explicit rather than inferred generics to significantly improves completion suggestion performance for language servers.\n-TOKENIZER_MAPPING_NAMES = OrderedDict[str, tuple[Optional[str], Optional[str]]](\n+# V5: Simplified mapping - single tokenizer class per model type (always prefer tokenizers-based)\n+REGISTERED_TOKENIZER_CLASSES: dict[str, type[Any]] = {}\n+REGISTERED_FAST_ALIASES: dict[str, type[Any]] = {}\n+\n+TOKENIZER_MAPPING_NAMES = OrderedDict[str, Optional[str]](\n     [\n-        (\n-            \"aimv2\",\n-            (\n-                \"CLIPTokenizer\",\n-                \"CLIPTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"albert\",\n-            (\n-                \"AlbertTokenizer\" if is_sentencepiece_available() else None,\n-                \"AlbertTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"align\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"altclip\", (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"arcee\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"aria\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"aya_vision\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"bark\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"bart\", (\"BartTokenizer\", \"BartTokenizerFast\")),\n-        (\n-            \"barthez\",\n-            (\n-                \"BarthezTokenizer\" if is_sentencepiece_available() else None,\n-                \"BarthezTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"bartpho\", (\"BartphoTokenizer\", None)),\n-        (\"bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"bert-generation\", (\"BertGenerationTokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\"bert-japanese\", (\"BertJapaneseTokenizer\", None)),\n-        (\"bertweet\", (\"BertweetTokenizer\", None)),\n-        (\n-            \"big_bird\",\n-            (\n-                \"BigBirdTokenizer\" if is_sentencepiece_available() else None,\n-                \"BigBirdTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"bigbird_pegasus\", (\"PegasusTokenizer\", \"PegasusTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"biogpt\", (\"BioGptTokenizer\", None)),\n-        (\"bitnet\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"blenderbot\", (\"BlenderbotTokenizer\", \"BlenderbotTokenizerFast\")),\n-        (\"blenderbot-small\", (\"BlenderbotSmallTokenizer\", None)),\n-        (\"blip\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"blip-2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"bloom\", (None, \"BloomTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"blt\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"bridgetower\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"bros\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"byt5\", (\"ByT5Tokenizer\", None)),\n-        (\n-            \"camembert\",\n-            (\n-                \"CamembertTokenizer\" if is_sentencepiece_available() else None,\n-                \"CamembertTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"canine\", (\"CanineTokenizer\", None)),\n-        (\n-            \"chameleon\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"chinese_clip\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"clap\",\n-            (\n-                \"RobertaTokenizer\",\n-                \"RobertaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"clip\",\n-            (\n-                \"CLIPTokenizer\",\n-                \"CLIPTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"clipseg\",\n-            (\n-                \"CLIPTokenizer\",\n-                \"CLIPTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"clvp\", (\"ClvpTokenizer\", None)),\n-        (\n-            \"code_llama\",\n-            (\n-                \"CodeLlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"CodeLlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"codegen\", (\"CodeGenTokenizer\", \"CodeGenTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"cohere\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"cohere2\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"cohere2_vision\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"colpali\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"colqwen2\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"convbert\", (\"ConvBertTokenizer\", \"ConvBertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"cpm\",\n-            (\n-                \"CpmTokenizer\" if is_sentencepiece_available() else None,\n-                \"CpmTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"cpmant\", (\"CpmAntTokenizer\", None)),\n-        (\"csm\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"ctrl\", (\"CTRLTokenizer\", None)),\n-        (\n-            \"cwm\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"data2vec-audio\", (\"Wav2Vec2CTCTokenizer\", None)),\n-        (\"data2vec-text\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"dbrx\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"deberta\", (\"DebertaTokenizer\", \"DebertaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"deberta-v2\",\n-            (\n-                \"DebertaV2Tokenizer\" if is_sentencepiece_available() else None,\n-                \"DebertaV2TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"deepseek_v2\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"deepseek_v3\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"deepseek_vl\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"deepseek_vl_hybrid\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"dia\", (\"DiaTokenizer\", None)),\n-        (\n-            \"diffllama\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"distilbert\", (\"DistilBertTokenizer\", \"DistilBertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"donut\", (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"dpr\",\n-            (\n-                \"DPRQuestionEncoderTokenizer\",\n-                \"DPRQuestionEncoderTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"electra\", (\"ElectraTokenizer\", \"ElectraTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"emu3\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"ernie\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"ernie4_5\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"ernie4_5_moe\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"esm\", (\"EsmTokenizer\", None)),\n-        (\"evolla\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"exaone4\",\n-            (\n-                \"GPT2Tokenizer\" if is_tokenizers_available() else None,\n-                \"GPT2TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"falcon\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"falcon_mamba\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"fastspeech2_conformer\",\n-            (\"FastSpeech2ConformerTokenizer\" if is_g2p_en_available() else None, None),\n-        ),\n-        (\"flaubert\", (\"FlaubertTokenizer\", None)),\n-        (\"flava\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"flex_olmo\", (None, \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"florence2\", (\"BartTokenizer\", \"BartTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"fnet\", (\"FNetTokenizer\", \"FNetTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"fsmt\", (\"FSMTTokenizer\", None)),\n-        (\"funnel\", (\"FunnelTokenizer\", \"FunnelTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"fuyu\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"gemma\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"gemma2\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"gemma3\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"gemma3_text\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"gemma3n\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"gemma3n_text\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"git\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"glm\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"glm4\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"glm46v\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"glm4_moe\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"glm4v\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"glm4v_moe\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"got_ocr2\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"gpt-sw3\", (\"GPTSw3Tokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\"gpt2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"gpt_bigcode\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"gpt_neo\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"gpt_neox\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"gpt_neox_japanese\", (\"GPTNeoXJapaneseTokenizer\", None)),\n-        (\"gpt_oss\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"gptj\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"granite\", (\"GPT2Tokenizer\", None)),\n-        (\"granite_speech\", (\"GPT2Tokenizer\", None)),\n-        (\"granitemoe\", (\"GPT2Tokenizer\", None)),\n-        (\"granitemoehybrid\", (\"GPT2Tokenizer\", None)),\n-        (\"granitemoeshared\", (\"GPT2Tokenizer\", None)),\n-        (\"grounding-dino\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"groupvit\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"helium\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"herbert\", (\"HerbertTokenizer\", \"HerbertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"hubert\", (\"Wav2Vec2CTCTokenizer\", None)),\n-        (\"ibert\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"idefics\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"idefics2\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"idefics3\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"instructblip\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"instructblipvideo\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"internvl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"jamba\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"janus\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"jetmoe\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"kosmos-2\",\n-            (\n-                \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n-                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"kosmos-2.5\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"kyutai_speech_to_text\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"layoutlm\", (\"LayoutLMTokenizer\", \"LayoutLMTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"layoutlmv2\", (\"LayoutLMv2Tokenizer\", \"LayoutLMv2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"layoutlmv3\", (\"LayoutLMv3Tokenizer\", \"LayoutLMv3TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"layoutxlm\", (\"LayoutXLMTokenizer\", \"LayoutXLMTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"led\", (\"LEDTokenizer\", \"LEDTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"lfm2\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"lfm2_vl\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"lilt\", (\"LayoutLMv3Tokenizer\", \"LayoutLMv3TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"llama\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"llama4\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"llama4_text\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"llava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"llava_next\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"llava_next_video\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"llava_onevision\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"longformer\", (\"LongformerTokenizer\", \"LongformerTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"longt5\",\n-            (\n-                \"T5Tokenizer\" if is_sentencepiece_available() else None,\n-                \"T5TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"luke\", (\"LukeTokenizer\", None)),\n-        (\"lxmert\", (\"LxmertTokenizer\", \"LxmertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"m2m_100\", (\"M2M100Tokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\"mamba\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"mamba2\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"marian\", (\"MarianTokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\"markuplm\", (\"MarkupLMTokenizer\", \"MarkupLMTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"mbart\",\n-            (\n-                \"MBartTokenizer\" if is_sentencepiece_available() else None,\n-                \"MBartTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"mbart50\",\n-            (\n-                \"MBart50Tokenizer\" if is_sentencepiece_available() else None,\n-                \"MBart50TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"megatron-bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"metaclip_2\",\n-            (\n-                \"XLMRobertaTokenizer\",\n-                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"mgp-str\", (\"MgpstrTokenizer\", None)),\n-        (\n-            \"minimax\",\n-            (\n-                \"GPT2Tokenizer\" if is_sentencepiece_available() else None,\n-                \"GPT2TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"ministral\",\n-            (\n-                \"MistralCommonTokenizer\"\n-                if is_mistral_common_available()\n-                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n-            ),\n-        ),\n+        (\"aimv2\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"albert\", \"AlbertTokenizer\" if is_tokenizers_available() else None),\n+        (\"align\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"arcee\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"aria\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"aya_vision\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n+        (\"bark\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"bart\", \"RobertaTokenizer\" if is_tokenizers_available() else None),\n+        (\"barthez\", \"BarthezTokenizer\" if is_tokenizers_available() else None),\n+        (\"bartpho\", \"BartphoTokenizer\"),\n+        (\"bert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"bert-generation\", \"BertGenerationTokenizer\" if is_sentencepiece_available() else None),\n+        (\"bert-japanese\", \"BertJapaneseTokenizer\"),\n+        (\"bertweet\", \"BertweetTokenizer\"),\n+        (\"big_bird\", \"BigBirdTokenizer\" if is_tokenizers_available() else None),\n+        (\"bigbird_pegasus\", \"PegasusTokenizer\" if is_tokenizers_available() else None),\n+        (\"biogpt\", \"BioGptTokenizer\"),\n+        (\"bitnet\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"blenderbot\", \"BlenderbotTokenizer\" if is_tokenizers_available() else None),\n+        (\"blenderbot-small\", \"BlenderbotSmallTokenizer\"),\n+        (\"blip\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"blip-2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"bloom\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"blt\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"bridgetower\", \"RobertaTokenizer\"),\n+        (\"bros\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"byt5\", \"ByT5Tokenizer\"),\n+        (\"camembert\", \"CamembertTokenizer\" if is_tokenizers_available() else None),\n+        (\"canine\", \"CanineTokenizer\"),\n+        (\"chameleon\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"chinese_clip\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"clap\", \"RobertaTokenizer\"),\n+        (\"clip\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n+        (\"clipseg\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n+        (\"clvp\", \"ClvpTokenizer\"),\n+        (\"code_llama\", \"CodeLlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"codegen\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"cohere\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n+        (\"cohere2\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n+        (\"colpali\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"colqwen2\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"convbert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"cpm\", \"CpmTokenizer\" if is_tokenizers_available() else None),\n+        (\"cpmant\", \"CpmAntTokenizer\"),\n+        (\"csm\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"ctrl\", \"CTRLTokenizer\"),\n+        (\"data2vec-audio\", \"Wav2Vec2CTCTokenizer\"),\n+        (\"data2vec-text\", \"RobertaTokenizer\"),\n+        (\"dbrx\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"deberta\", \"DebertaTokenizer\" if is_tokenizers_available() else None),\n+        (\"deberta-v2\", \"DebertaV2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"deepseek_v2\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"deepseek_v3\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"deepseek_vl\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"deepseek_vl_hybrid\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"dia\", \"DiaTokenizer\"),\n+        (\"diffllama\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"distilbert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"dpr\", \"DPRQuestionEncoderTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"electra\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"emu3\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"ernie\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"ernie4_5\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"ernie4_5_moe\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"esm\", \"EsmTokenizer\"),\n+        (\"exaone4\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"falcon\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"falcon_mamba\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"fastspeech2_conformer\", \"FastSpeech2ConformerTokenizer\" if is_g2p_en_available() else None),\n+        (\"flaubert\", \"FlaubertTokenizer\"),\n+        (\"flava\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"flex_olmo\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"florence2\", \"BartTokenizer\" if is_tokenizers_available() else None),\n+        (\"fnet\", \"FNetTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"fsmt\", \"FSMTTokenizer\"),\n+        (\"funnel\", \"FunnelTokenizer\" if is_tokenizers_available() else None),\n+        (\"gemma\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"gemma2\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"gemma3\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"gemma3_text\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"gemma3n\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"gemma3n_text\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"git\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"glm\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"glm4\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"glm4_moe\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"glm4v\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"glm4v_moe\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"got_ocr2\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"gpt-sw3\", \"GPTSw3Tokenizer\" if is_sentencepiece_available() else None),\n+        (\"gpt2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"gpt_bigcode\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"gpt_neo\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"gpt_neox\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n+        (\"gpt_neox_japanese\", \"GPTNeoXJapaneseTokenizer\"),\n+        (\"gpt_oss\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"gptj\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"granite\", \"GPT2Tokenizer\"),\n+        (\"granitemoe\", \"GPT2Tokenizer\"),\n+        (\"granitemoehybrid\", \"GPT2Tokenizer\"),\n+        (\"granitemoeshared\", \"GPT2Tokenizer\"),\n+        (\"grounding-dino\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"groupvit\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"helium\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"herbert\", \"HerbertTokenizer\" if is_tokenizers_available() else None),\n+        (\"hubert\", \"Wav2Vec2CTCTokenizer\"),\n+        (\"ibert\", \"RobertaTokenizer\"),\n+        (\"idefics\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"idefics2\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"idefics3\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"instructblip\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"instructblipvideo\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"internvl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"jamba\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"janus\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"jetmoe\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"kosmos-2\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n+        (\"kosmos-2.5\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"layoutlm\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"layoutlmv2\", \"LayoutLMv2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"layoutlmv3\", \"LayoutLMv3Tokenizer\" if is_tokenizers_available() else None),\n+        (\"layoutxlm\", \"LayoutXLMTokenizer\" if is_tokenizers_available() else None),\n+        (\"led\", \"LEDTokenizer\" if is_tokenizers_available() else None),\n+        (\"lfm2_vl\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"lilt\", \"RobertaTokenizer\" if is_tokenizers_available() else None),\n+        (\"llama\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"llama4\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"llama4_text\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"llava\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"llava_next\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"llava_next_video\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"llava_onevision\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"longformer\", \"RobertaTokenizer\" if is_tokenizers_available() else None),\n+        (\"longt5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n+        (\"luke\", \"LukeTokenizer\"),\n+        (\"lxmert\", \"LxmertTokenizer\" if is_tokenizers_available() else None),\n+        (\"m2m_100\", \"M2M100Tokenizer\" if is_sentencepiece_available() else None),\n+        (\"mamba\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"mamba2\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"marian\", \"MarianTokenizer\" if is_sentencepiece_available() else None),\n+        (\"mbart\", \"MBartTokenizer\" if is_tokenizers_available() else None),\n+        (\"mbart50\", \"MBart50Tokenizer\" if is_tokenizers_available() else None),\n+        (\"mega\", \"RobertaTokenizer\"),\n+        (\"megatron-bert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"metaclip_2\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"mgp-str\", \"MgpstrTokenizer\"),\n+        (\"minimax\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\n             \"mistral\",\n-            (\n-                \"MistralCommonTokenizer\"\n-                if is_mistral_common_available()\n-                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n-            ),\n-        ),\n-        (\n-            \"mistral3\",\n-            (\n-                \"MistralCommonTokenizer\"\n-                if is_mistral_common_available()\n-                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n-            ),\n+            \"MistralCommonBackend\"\n+            if is_mistral_common_available()\n+            else (\"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n         ),\n         (\n             \"mixtral\",\n-            (\n-                \"MistralCommonTokenizer\"\n-                if is_mistral_common_available()\n-                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n-            ),\n-        ),\n-        (\"mllama\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"mluke\", (\"MLukeTokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\"mm-grounding-dino\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"mobilebert\", (\"MobileBertTokenizer\", \"MobileBertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"modernbert\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"moonshine\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"moshi\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"mpnet\", (\"MPNetTokenizer\", \"MPNetTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"mpt\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"mra\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"mt5\",\n-            (\n-                \"MT5Tokenizer\" if is_sentencepiece_available() else None,\n-                \"MT5TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"musicgen\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"musicgen_melody\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"mvp\", (\"MvpTokenizer\", \"MvpTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"myt5\", (\"MyT5Tokenizer\", None)),\n-        (\"nanochat\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"nemotron\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"nllb\",\n-            (\n-                \"NllbTokenizer\" if is_sentencepiece_available() else None,\n-                \"NllbTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"nllb-moe\",\n-            (\n-                \"NllbTokenizer\" if is_sentencepiece_available() else None,\n-                \"NllbTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"nougat\", (None, \"NougatTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"nystromformer\",\n-            (\n-                \"AlbertTokenizer\" if is_sentencepiece_available() else None,\n-                \"AlbertTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"olmo\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"olmo2\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"olmo3\", (None, \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"olmoe\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"omdet-turbo\",\n-            (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n-        ),\n-        (\"oneformer\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"openai-gpt\",\n-            (\"OpenAIGPTTokenizer\", \"OpenAIGPTTokenizerFast\" if is_tokenizers_available() else None),\n-        ),\n-        (\"opt\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"ovis2\", (None, \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"owlv2\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"owlvit\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"paligemma\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"parakeet\", (None, \"ParakeetTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"pegasus\",\n-            (\n-                \"PegasusTokenizer\" if is_sentencepiece_available() else None,\n-                \"PegasusTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"pegasus_x\",\n-            (\n-                \"PegasusTokenizer\" if is_sentencepiece_available() else None,\n-                \"PegasusTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"perceiver\",\n-            (\n-                \"PerceiverTokenizer\",\n-                None,\n-            ),\n-        ),\n-        (\"perception_lm\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"persimmon\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"phi\", (\"CodeGenTokenizer\", \"CodeGenTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"phi3\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"phi4_multimodal\", (None, \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"phimoe\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"phobert\", (\"PhobertTokenizer\", None)),\n-        (\"pix2struct\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n+            \"MistralCommonBackend\"\n+            if is_mistral_common_available()\n+            else (\"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        ),\n+        (\"mllama\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"mluke\", \"MLukeTokenizer\" if is_sentencepiece_available() else None),\n+        (\"mm-grounding-dino\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"mobilebert\", \"MobileBertTokenizer\" if is_tokenizers_available() else None),\n+        (\"modernbert\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"moonshine\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"moshi\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"mpnet\", \"MPNetTokenizer\" if is_tokenizers_available() else None),\n+        (\"mpt\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"mra\", \"RobertaTokenizer\"),\n+        (\"mt5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n+        (\"musicgen\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n+        (\"musicgen_melody\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n+        (\"mvp\", \"MvpTokenizer\" if is_tokenizers_available() else None),\n+        (\"myt5\", \"MyT5Tokenizer\"),\n+        (\"nemotron\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"nezha\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"nllb\", \"NllbTokenizer\" if is_tokenizers_available() else None),\n+        (\"nllb-moe\", \"NllbTokenizer\" if is_tokenizers_available() else None),\n+        (\"nougat\", \"NougatTokenizer\" if is_tokenizers_available() else None),\n+        (\"nystromformer\", \"AlbertTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"olmo\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"olmo2\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"olmo3\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"olmoe\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"omdet-turbo\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"oneformer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"openai-gpt\", \"OpenAIGPTTokenizer\" if is_tokenizers_available() else None),\n+        (\"opt\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"ovis2\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"owlv2\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"owlvit\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"paligemma\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"pegasus\", \"PegasusTokenizer\" if is_tokenizers_available() else None),\n+        (\"pegasus_x\", \"PegasusTokenizer\" if is_tokenizers_available() else None),\n+        (\"perceiver\", \"PerceiverTokenizer\"),\n+        (\"persimmon\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"phi\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"phi3\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"phimoe\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"phobert\", \"PhobertTokenizer\"),\n+        (\"pix2struct\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n         (\n             \"pixtral\",\n-            (\n-                None,\n-                \"MistralCommonTokenizer\"\n-                if is_mistral_common_available()\n-                else (\"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n-            ),\n-        ),\n-        (\"plbart\", (\"PLBartTokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\"pop2piano\", (\"Pop2PianoTokenizer\", None)),\n-        (\"prophetnet\", (\"ProphetNetTokenizer\", None)),\n-        (\n-            \"qwen2\",\n-            (\n-                \"Qwen2Tokenizer\",\n-                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"qwen2_5_omni\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"qwen2_5_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"qwen2_audio\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"qwen2_moe\",\n-            (\n-                \"Qwen2Tokenizer\",\n-                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"qwen2_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"qwen3\",\n-            (\n-                \"Qwen2Tokenizer\",\n-                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"qwen3_moe\",\n-            (\n-                \"Qwen2Tokenizer\",\n-                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"qwen3_next\",\n-            (\n-                \"Qwen2Tokenizer\",\n-                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"qwen3_omni_moe\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"qwen3_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"qwen3_vl_moe\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"rag\", (\"RagTokenizer\", None)),\n-        (\n-            \"recurrent_gemma\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"reformer\",\n-            (\n-                \"ReformerTokenizer\" if is_sentencepiece_available() else None,\n-                \"ReformerTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"rembert\",\n-            (\n-                \"RemBertTokenizer\" if is_sentencepiece_available() else None,\n-                \"RemBertTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"roberta\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"roberta-prelayernorm\",\n-            (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None),\n-        ),\n-        (\"roc_bert\", (\"RoCBertTokenizer\", None)),\n-        (\"roformer\", (\"RoFormerTokenizer\", \"RoFormerTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"rwkv\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"seamless_m4t\",\n-            (\n-                \"SeamlessM4TTokenizer\" if is_sentencepiece_available() else None,\n-                \"SeamlessM4TTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"seamless_m4t_v2\",\n-            (\n-                \"SeamlessM4TTokenizer\" if is_sentencepiece_available() else None,\n-                \"SeamlessM4TTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"shieldgemma2\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"siglip\", (\"SiglipTokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\n-            \"siglip2\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"smollm3\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"smolvlm\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"speech_to_text\", (\"Speech2TextTokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\"speecht5\", (\"SpeechT5Tokenizer\" if is_sentencepiece_available() else None, None)),\n-        (\"splinter\", (\"SplinterTokenizer\", \"SplinterTokenizerFast\")),\n-        (\n-            \"squeezebert\",\n-            (\"SqueezeBertTokenizer\", \"SqueezeBertTokenizerFast\" if is_tokenizers_available() else None),\n-        ),\n-        (\"stablelm\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"starcoder2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"switch_transformers\",\n-            (\n-                \"T5Tokenizer\" if is_sentencepiece_available() else None,\n-                \"T5TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"t5\",\n-            (\n-                \"T5Tokenizer\" if is_sentencepiece_available() else None,\n-                \"T5TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"t5gemma\",\n-            (\n-                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"tapas\", (\"TapasTokenizer\", None)),\n-        (\"trocr\", (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"tvp\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"udop\",\n-            (\n-                \"UdopTokenizer\" if is_sentencepiece_available() else None,\n-                \"UdopTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"umt5\",\n-            (\n-                \"T5Tokenizer\" if is_sentencepiece_available() else None,\n-                \"T5TokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"video_llama_3\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"video_llava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"vilt\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"vipllava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"vision_text_dual_encoder\",\n-            (\"PreTrainedTokenizer\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n-        ),\n-        (\"visual_bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"vits\", (\"VitsTokenizer\", None)),\n+            \"MistralCommonBackend\"\n+            if is_mistral_common_available()\n+            else (\"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        ),\n+        (\"plbart\", \"PLBartTokenizer\" if is_tokenizers_available() else None),\n+        (\"prophetnet\", \"ProphetNetTokenizer\"),\n+        (\"qdqbert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"qwen2\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen2_5_omni\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen2_5_vl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen2_audio\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen2_moe\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen2_vl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen3\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen3_moe\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen3_next\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen3_omni_moe\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen3_vl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"qwen3_vl_moe\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"rag\", \"RagTokenizer\"),\n+        (\"realm\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"recurrent_gemma\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"reformer\", \"ReformerTokenizer\" if is_tokenizers_available() else None),\n+        (\"rembert\", \"RemBertTokenizer\" if is_tokenizers_available() else None),\n+        (\"retribert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"roberta\", \"RobertaTokenizer\"),\n+        (\"roberta-prelayernorm\", \"RobertaTokenizer\"),\n+        (\"roc_bert\", \"RoCBertTokenizer\"),\n+        (\"roformer\", \"RoFormerTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"rwkv\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"seamless_m4t\", \"SeamlessM4TTokenizer\" if is_tokenizers_available() else None),\n+        (\"seamless_m4t_v2\", \"SeamlessM4TTokenizer\" if is_tokenizers_available() else None),\n+        (\"shieldgemma2\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"siglip\", \"SiglipTokenizer\" if is_sentencepiece_available() else None),\n+        (\"siglip2\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"smollm3\", \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"speech_to_text\", \"Speech2TextTokenizer\" if is_sentencepiece_available() else None),\n+        (\"speecht5\", \"SpeechT5Tokenizer\" if is_sentencepiece_available() else None),\n+        (\"splinter\", \"SplinterTokenizer\"),\n+        (\"squeezebert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"stablelm\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"starcoder2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n+        (\"switch_transformers\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n+        (\"t5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n+        (\"t5gemma\", \"GemmaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"tapas\", \"TapasTokenizer\"),\n+        (\"trocr\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n+        (\"tvp\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"udop\", \"UdopTokenizer\" if is_tokenizers_available() else None),\n+        (\"umt5\", \"T5Tokenizer\" if is_tokenizers_available() else None),\n+        (\"video_llava\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"vilt\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"vipllava\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"visual_bert\", \"BertTokenizer\" if is_tokenizers_available() else None),\n+        (\"vits\", \"VitsTokenizer\"),\n         (\n             \"voxtral\",\n-            (\n-                \"MistralCommonTokenizer\" if is_mistral_common_available() else None,\n-                \"PreTrainedTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n-            ),\n-        ),\n-        (\"wav2vec2\", (\"Wav2Vec2CTCTokenizer\", None)),\n-        (\"wav2vec2-bert\", (\"Wav2Vec2CTCTokenizer\", None)),\n-        (\"wav2vec2-conformer\", (\"Wav2Vec2CTCTokenizer\", None)),\n-        (\"wav2vec2_phoneme\", (\"Wav2Vec2PhonemeCTCTokenizer\", None)),\n-        (\"wav2vec2_with_lm\", (\"Wav2Vec2CTCTokenizer\", None)),\n-        (\"whisper\", (\"WhisperTokenizer\", \"WhisperTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"xclip\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"xglm\",\n-            (\n-                \"XGLMTokenizer\" if is_sentencepiece_available() else None,\n-                \"XGLMTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"xlm\", (\"XLMTokenizer\", None)),\n-        (\n-            \"xlm-roberta\",\n-            (\n-                \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n-                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"xlm-roberta-xl\",\n-            (\n-                \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n-                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"xlnet\",\n-            (\n-                \"XLNetTokenizer\" if is_sentencepiece_available() else None,\n-                \"XLNetTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\"xlstm\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-        (\n-            \"xmod\",\n-            (\n-                \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n-                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"yoso\",\n-            (\n-                \"AlbertTokenizer\" if is_sentencepiece_available() else None,\n-                \"AlbertTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"zamba\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n-        (\n-            \"zamba2\",\n-            (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-            ),\n-        ),\n+            \"MistralCommonBackend\"\n+            if is_mistral_common_available()\n+            else (\"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+        ),\n+        (\"wav2vec2\", \"Wav2Vec2CTCTokenizer\"),\n+        (\"wav2vec2-bert\", \"Wav2Vec2CTCTokenizer\"),\n+        (\"wav2vec2-conformer\", \"Wav2Vec2CTCTokenizer\"),\n+        (\"wav2vec2_phoneme\", \"Wav2Vec2PhonemeCTCTokenizer\"),\n+        (\"whisper\", \"WhisperTokenizer\" if is_tokenizers_available() else None),\n+        (\"xclip\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"xglm\", \"XGLMTokenizer\" if is_tokenizers_available() else None),\n+        (\"xlm\", \"XLMTokenizer\"),\n+        (\"xlm-roberta\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n+        (\"xlm-roberta-xl\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n+        (\"xlnet\", \"XLNetTokenizer\" if is_tokenizers_available() else None),\n+        (\"xlstm\", \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"xmod\", \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"yoso\", \"AlbertTokenizer\" if is_tokenizers_available() else None),\n+        (\"zamba\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"zamba2\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None),\n     ]\n )\n \n@@ -826,14 +353,38 @@\n CONFIG_TO_TYPE = {v: k for k, v in CONFIG_MAPPING_NAMES.items()}\n \n \n+def load_vocab(vocab_file):\n+    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n+    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n+        return json.load(reader)\n+\n+\n+def load_merges(merges_file):\n+    \"\"\"Loads a merges file into a list.\"\"\"\n+    merges = []\n+    with open(merges_file, \"r\", encoding=\"utf-8\") as reader:\n+        for line in reader:\n+            line = line.strip()\n+            if line and not line.startswith(\"#\"):\n+                merges.append(tuple(line.split()))\n+    return merges\n+\n+\n def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n+    if class_name in REGISTERED_FAST_ALIASES:\n+        return REGISTERED_FAST_ALIASES[class_name]\n+\n+    if class_name in REGISTERED_TOKENIZER_CLASSES:\n+        return REGISTERED_TOKENIZER_CLASSES[class_name]\n+\n     if class_name == \"PreTrainedTokenizerFast\":\n-        return PreTrainedTokenizerFast\n+        return TokenizersBackend\n \n-    for module_name, tokenizers in TOKENIZER_MAPPING_NAMES.items():\n-        if class_name in tokenizers:\n+    # V5: TOKENIZER_MAPPING_NAMES now maps to single strings, not tuples\n+    for module_name, tokenizer_class in TOKENIZER_MAPPING_NAMES.items():\n+        if tokenizer_class == class_name:\n             module_name = model_type_to_module_name(module_name)\n-            if module_name in [\"mistral\", \"mixtral\", \"ministral\"] and class_name == \"MistralCommonTokenizer\":\n+            if module_name in [\"mistral\", \"mixtral\", \"ministral\"] and class_name == \"MistralCommonBackend\":\n                 module = importlib.import_module(\".tokenization_mistral_common\", \"transformers\")\n             else:\n                 module = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n@@ -842,11 +393,11 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n             except AttributeError:\n                 continue\n \n-    for tokenizers in TOKENIZER_MAPPING._extra_content.values():\n-        for tokenizer in tokenizers:\n-            if getattr(tokenizer, \"__name__\", None) == class_name:\n-                return tokenizer\n+    for tokenizer in TOKENIZER_MAPPING._extra_content.values():\n+        if getattr(tokenizer, \"__name__\", None) == class_name:\n+            return tokenizer\n \n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n@@ -856,6 +407,402 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n     return None\n \n \n+def _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs):\n+    # Delegate to shared helper to avoid duplication\n+    return find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n+\n+\n+def _load_tokenizers_backend(tokenizer_class, pretrained_model_name_or_path, inputs, kwargs):\n+    \"\"\"\n+    Load a tokenizer using only the tokenizers backend (no SentencePiece fallback).\n+\n+    This function attempts to load with the following priority:\n+    1. If tokenizer.json exists, load directly\n+    2. If any .model file (SPM) exists, try extracting vocab and merges\n+    3. If vocab.json and merges.txt exist, load with those\n+    4. If vocab.txt exists (WordPiece models), load with that\n+\n+    Args:\n+        tokenizer_class: The tokenizer class to instantiate\n+        pretrained_model_name_or_path: Path or model id\n+        inputs: Additional positional arguments for tokenizer init\n+        kwargs: Additional keyword arguments\n+\n+    Returns:\n+        An instantiated tokenizer object\n+\n+    Raises:\n+        ValueError: If tokenizer could not be loaded with tokenizers backend\n+    \"\"\"\n+    files_loaded = []\n+\n+    # Try tokenizer.json first\n+    try:\n+        tokenizer_json_exists = has_file(\n+            pretrained_model_name_or_path,\n+            \"tokenizer.json\",\n+            revision=kwargs.get(\"revision\"),\n+            token=kwargs.get(\"token\"),\n+            cache_dir=kwargs.get(\"cache_dir\"),\n+            local_files_only=kwargs.get(\"local_files_only\", False),\n+        )\n+    except Exception:\n+        tokenizer_json_exists = False\n+\n+    if tokenizer_json_exists:\n+        files_loaded.append(\"tokenizer.json\")\n+        kwargs[\"backend\"] = \"tokenizers\"\n+        kwargs[\"files_loaded\"] = files_loaded\n+        # Some old models have uploaded a tokenizer.json but haven't updated tokenizer_config.json to point to the correct tokenizer class\n+        tokenizer_class = (\n+            TokenizersBackend\n+            if tokenizer_class.__name__ in (\"PythonBackend\", \"PreTrainedTokenizer\")\n+            else tokenizer_class\n+        )\n+        return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+\n+    # Try tekken.json (Mistral format)\n+    try:\n+        if has_file(\n+            pretrained_model_name_or_path,\n+            \"tekken.json\",\n+            revision=kwargs.get(\"revision\"),\n+            token=kwargs.get(\"token\"),\n+            cache_dir=kwargs.get(\"cache_dir\"),\n+            local_files_only=kwargs.get(\"local_files_only\", False),\n+        ):\n+            from ...integrations.mistral import convert_tekken_tokenizer\n+\n+            tekken_file = cached_file(\n+                pretrained_model_name_or_path,\n+                \"tekken.json\",\n+                **{\n+                    k: v\n+                    for k, v in kwargs.items()\n+                    if k\n+                    in [\"cache_dir\", \"force_download\", \"proxies\", \"token\", \"revision\", \"local_files_only\", \"subfolder\"]\n+                },\n+            )\n+            if tekken_file is not None:\n+                files_loaded.append(\"tekken.json\")\n+                kwargs[\"backend\"] = \"tokenizers\"\n+                kwargs[\"files_loaded\"] = files_loaded\n+                return convert_tekken_tokenizer(tekken_file)\n+    except (ImportError, Exception):\n+        pass\n+\n+    # Try extracting from SentencePiece model\n+    spm_file = _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n+    if spm_file is not None:\n+        try:\n+            resolved_spm = cached_file(\n+                pretrained_model_name_or_path,\n+                spm_file,\n+                cache_dir=kwargs.get(\"cache_dir\"),\n+                force_download=kwargs.get(\"force_download\", False),\n+                proxies=kwargs.get(\"proxies\"),\n+                token=kwargs.get(\"token\"),\n+                revision=kwargs.get(\"revision\"),\n+                local_files_only=kwargs.get(\"local_files_only\", False),\n+                subfolder=kwargs.get(\"subfolder\", \"\"),\n+            )\n+        except Exception:\n+            resolved_spm = None\n+\n+        if resolved_spm is not None:\n+            try:\n+                from ...tokenization_utils_sentencepiece import SentencePieceExtractor\n+\n+                fast_sig = inspect.signature(getattr(tokenizer_class, \"__init__\", tokenizer_class))\n+                if \"vocab\" in fast_sig.parameters:\n+                    try:\n+                        vocab_ids, vocab_scores, merges = SentencePieceExtractor(resolved_spm).extract()\n+                        files_loaded.append(spm_file)\n+                        kwargs[\"backend\"] = \"tokenizers\"\n+                        kwargs[\"files_loaded\"] = files_loaded\n+                        # If tokenizer needs both vocab and merges (BPE models)\n+                        if \"merges\" in fast_sig.parameters:\n+                            return tokenizer_class.from_pretrained(\n+                                pretrained_model_name_or_path, *inputs, vocab=vocab_scores, merges=merges, **kwargs\n+                            )\n+                        # If tokenizer only needs vocab (Unigram models like NLLB, SeamlessM4T)\n+                        else:\n+                            return tokenizer_class.from_pretrained(\n+                                pretrained_model_name_or_path, *inputs, vocab=vocab_scores, **kwargs\n+                            )\n+                    except Exception:\n+                        pass\n+            except ImportError as e:\n+                if \"sentencepiece\" in str(e).lower() or \"SentencePiece\" in str(e):\n+                    raise ImportError(\n+                        f\"This checkpoint only contains a SentencePiece model file ({spm_file}), but the `sentencepiece` library is not installed. \"\n+                        f\"Please install sentencepiece to load this tokenizer: `pip install sentencepiece`\"\n+                    ) from e\n+                raise\n+            except Exception:\n+                pass\n+\n+    vocab, merges, loaded = load_vocab_and_merges(pretrained_model_name_or_path, **kwargs)\n+    if vocab is not None:\n+        files_loaded.extend(loaded)\n+        if issubclass(tokenizer_class, PreTrainedTokenizer):\n+            kwargs[\"backend\"] = \"python\"\n+        else:\n+            kwargs[\"backend\"] = \"tokenizers\"\n+        kwargs[\"files_loaded\"] = files_loaded\n+        if merges is not None:\n+            return tokenizer_class.from_pretrained(\n+                pretrained_model_name_or_path, *inputs, vocab=vocab, merges=merges, **kwargs\n+            )\n+        else:\n+            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, vocab=vocab, **kwargs)\n+\n+    # Try vocab.txt (WordPiece models like SplinterTokenizer)\n+    try:\n+        resolved_vocab_txt = cached_file(\n+            pretrained_model_name_or_path,\n+            \"vocab.txt\",\n+            cache_dir=kwargs.get(\"cache_dir\"),\n+            force_download=kwargs.get(\"force_download\", False),\n+            proxies=kwargs.get(\"proxies\"),\n+            token=kwargs.get(\"token\"),\n+            revision=kwargs.get(\"revision\"),\n+            local_files_only=kwargs.get(\"local_files_only\", False),\n+            subfolder=kwargs.get(\"subfolder\", \"\"),\n+        )\n+    except Exception:\n+        resolved_vocab_txt = None\n+\n+    if resolved_vocab_txt is not None:\n+        try:\n+            fast_sig = inspect.signature(getattr(tokenizer_class, \"__init__\", tokenizer_class))\n+            if \"vocab\" in fast_sig.parameters:\n+                # Load vocab.txt: each line is a token, line number is the ID\n+                vocab = OrderedDict()\n+                with open(resolved_vocab_txt, \"r\", encoding=\"utf-8\") as reader:\n+                    tokens = reader.readlines()\n+                for index, token in enumerate(tokens):\n+                    token = token.rstrip(\"\\n\")\n+                    vocab[token] = index\n+                files_loaded.append(\"vocab.txt\")\n+                kwargs[\"backend\"] = \"tokenizers\"\n+                kwargs[\"files_loaded\"] = files_loaded\n+                return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, vocab=vocab, **kwargs)\n+        except Exception:\n+            pass\n+\n+    # If all methods failed, raise an error\n+    raise ValueError(\n+        f\"Could not load tokenizer from {pretrained_model_name_or_path} using tokenizers backend. \"\n+        \"No tokenizer.json, tekken.json, vocab.json+merges.txt, vocab.txt, or compatible SentencePiece model found.\"\n+    )\n+\n+\n+def _try_load_tokenizer_with_fallbacks(tokenizer_class, pretrained_model_name_or_path, inputs, kwargs):\n+    \"\"\"\n+    Try to load a tokenizer with backend selection.\n+\n+    This function routes to the appropriate backend based on the 'backend' parameter:\n+    - \"tokenizers\" (default): Uses HuggingFace tokenizers library backend\n+    - \"sentencepiece\": Uses SentencePiece backend\n+\n+    For the tokenizers backend, attempts to load with the following priority:\n+    1. If tokenizer.json exists, load directly\n+    2. If any .model file (SPM) exists, try extracting vocab and merges\n+    3. If vocab.json and merges.txt exist, load with those\n+    4. Fallback to SentencePieceBackend if available\n+\n+    Args:\n+        tokenizer_class: The tokenizer class to instantiate (can be None)\n+        pretrained_model_name_or_path: Path or model id\n+        inputs: Additional positional arguments for tokenizer init\n+        kwargs: Additional keyword arguments (may include 'backend' parameter, defaults to \"tokenizers\")\n+\n+    Returns:\n+        An instantiated tokenizer object\n+\n+    Raises:\n+        ValueError: If no tokenizer could be loaded\n+    \"\"\"\n+    # Extract the backend parameter - default to \"tokenizers\" to prioritize tokenizers backend\n+    backend = kwargs.pop(\"backend\", \"tokenizers\")\n+\n+    # Validate backend parameter\n+    if backend not in [\"sentencepiece\", \"tokenizers\"]:\n+        logger.warning(\n+            f\"Invalid backend '{backend}' specified. Valid options are 'tokenizers' or 'sentencepiece'. \"\n+            \"Defaulting to 'tokenizers' backend.\"\n+        )\n+        backend = \"tokenizers\"\n+\n+    # Route to SentencePiece backend if requested\n+    if backend == \"sentencepiece\":\n+        if SentencePieceBackend is None:\n+            raise ValueError(\n+                \"SentencePiece backend was requested but sentencepiece is not installed. \"\n+                \"Please install it with: pip install sentencepiece\"\n+            )\n+        logger.info(\"Loading tokenizer with SentencePiece backend\")\n+        # Track files loaded for SentencePiece backend\n+        spm_file = _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n+        files_loaded = [spm_file] if spm_file else []\n+        kwargs[\"backend\"] = \"sentencepiece\"\n+        kwargs[\"files_loaded\"] = files_loaded\n+        # Resolve the SPM file path and pass it as vocab_file\n+        if spm_file is not None:\n+            resolved_vocab_file = cached_file(\n+                pretrained_model_name_or_path,\n+                spm_file,\n+                cache_dir=kwargs.get(\"cache_dir\"),\n+                force_download=kwargs.get(\"force_download\", False),\n+                proxies=kwargs.get(\"proxies\"),\n+                token=kwargs.get(\"token\"),\n+                revision=kwargs.get(\"revision\"),\n+                local_files_only=kwargs.get(\"local_files_only\", False),\n+                subfolder=kwargs.get(\"subfolder\", \"\"),\n+            )\n+            kwargs[\"vocab_file\"] = resolved_vocab_file\n+        if isinstance(tokenizer_class, type) and issubclass(tokenizer_class, SentencePieceBackend):\n+            logger.info(\"Loading tokenizer with SentencePiece backend using tokenizer class\")\n+            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+        return SentencePieceBackend.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+\n+    # Route to tokenizers backend (default)\n+    if backend == \"tokenizers\":\n+        if tokenizer_class is not None:\n+            # Check if tokenizer_class inherits from PreTrainedTokenizer (but not from TokenizersBackend/SentencePieceBackend)\n+            # These are edge cases with custom logic (e.g., BioGptTokenizer with Moses tokenization)\n+            from ...tokenization_python import PreTrainedTokenizer\n+\n+            # Build list of backend classes to check against\n+            backend_classes = [TokenizersBackend] if TokenizersBackend else []\n+            if SentencePieceBackend:\n+                backend_classes.append(SentencePieceBackend)\n+\n+            # Check if it's a custom PreTrainedTokenizer (not a backend class)\n+            is_custom_pre_trained = (\n+                isinstance(tokenizer_class, type)\n+                and issubclass(tokenizer_class, PreTrainedTokenizer)\n+                and not any(issubclass(tokenizer_class, bc) for bc in backend_classes)\n+                and tokenizer_class.__name__ not in (\"PythonBackend\", \"PreTrainedTokenizer\")\n+            )\n+\n+            # Check if it's a completely custom tokenizer (not PreTrainedTokenizer, not backend class)\n+            # e.g., MistralCommonBackend which has its own from_pretrained logic\n+            inherits_from_backend = isinstance(tokenizer_class, type) and any(\n+                bc and issubclass(tokenizer_class, bc) for bc in backend_classes\n+            )\n+            is_completely_custom = (\n+                isinstance(tokenizer_class, type)\n+                and not issubclass(tokenizer_class, PythonBackend)\n+                and not inherits_from_backend\n+            )\n+\n+            if is_custom_pre_trained:\n+                logger.info(\"Loading tokenizer with custom PreTrainedTokenizer backend (edge case)\")\n+                # Track the backend type for custom tokenizers\n+                kwargs[\"backend\"] = \"custom\"\n+                kwargs[\"files_loaded\"] = []  # Custom tokenizers may load various files\n+                return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+\n+            if is_completely_custom:\n+                # For completely custom tokenizers (like MistralCommonBackend), try calling from_pretrained directly\n+                logger.info(\"Loading tokenizer with custom tokenizer class (non-PreTrainedTokenizer)\")\n+                # Filter out AutoTokenizer-specific kwargs that custom tokenizers don't accept\n+                custom_kwargs = {k: v for k, v in kwargs.items() if k not in [\"backend\", \"files_loaded\"]}\n+                custom_kwargs[\"_from_auto\"] = True  # Signal that this is called from AutoTokenizer\n+                return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **custom_kwargs)\n+\n+            if TokenizersBackend is None:\n+                raise ValueError(\n+                    \"Tokenizers backend is the default but tokenizers library is not installed. \"\n+                    \"Please install it with: pip install tokenizers\"\n+                )\n+            logger.info(\"Loading tokenizer with tokenizers backend\")\n+            try:\n+                return _load_tokenizers_backend(tokenizer_class, pretrained_model_name_or_path, inputs, kwargs)\n+            except ValueError as e:\n+                # If tokenizers backend fails, try falling back to SentencePiece backend if available\n+                spm_file = _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n+                if spm_file is not None and SentencePieceBackend is not None:\n+                    logger.info(\n+                        f\"Tokenizers backend failed: {e}. \"\n+                        f\"Falling back to SentencePieceBackend since {spm_file} file was found.\"\n+                    )\n+                    files_loaded = [spm_file]\n+                    kwargs[\"backend\"] = \"sentencepiece\"\n+                    kwargs[\"files_loaded\"] = files_loaded\n+                    # Resolve the SPM file path and pass it as vocab_file\n+                    resolved_vocab_file = cached_file(\n+                        pretrained_model_name_or_path,\n+                        spm_file,\n+                        cache_dir=kwargs.get(\"cache_dir\"),\n+                        force_download=kwargs.get(\"force_download\", False),\n+                        proxies=kwargs.get(\"proxies\"),\n+                        token=kwargs.get(\"token\"),\n+                        revision=kwargs.get(\"revision\"),\n+                        local_files_only=kwargs.get(\"local_files_only\", False),\n+                        subfolder=kwargs.get(\"subfolder\", \"\"),\n+                    )\n+                    kwargs[\"vocab_file\"] = resolved_vocab_file\n+                    if tokenizer_class is not None and issubclass(tokenizer_class, SentencePieceBackend):\n+                        logger.info(\n+                            \"Falling back to SentencePiece backend using tokenizer class that inherits from SentencePieceBackend.\"\n+                        )\n+                        return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+                    return SentencePieceBackend.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+                # If no fallback available, try calling tokenizer class directly as last resort\n+                if hasattr(tokenizer_class, \"from_pretrained\"):\n+                    logger.info(\n+                        f\"Tokenizers backend failed: {e}. Trying to load tokenizer directly from tokenizer class.\"\n+                    )\n+                    # Filter out AutoTokenizer-specific kwargs that custom tokenizers don't accept\n+                    custom_kwargs = {k: v for k, v in kwargs.items() if k not in [\"backend\", \"files_loaded\"]}\n+                    custom_kwargs[\"_from_auto\"] = True  # Signal that this is called from AutoTokenizer\n+                    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **custom_kwargs)\n+                # Re-raise if no fallback options available\n+                raise\n+\n+        # If no tokenizer class but tokenizers backend requested, fall back to SentencePiece if available\n+        spm_file = _find_sentencepiece_model_file(pretrained_model_name_or_path, **kwargs)\n+        if spm_file is not None and SentencePieceBackend is not None:\n+            logger.info(\n+                f\"Tokenizers backend was requested but no tokenizer class found. \"\n+                f\"Falling back to SentencePieceBackend since {spm_file} file was found.\"\n+            )\n+            files_loaded = [spm_file]\n+            kwargs[\"backend\"] = \"sentencepiece\"\n+            kwargs[\"files_loaded\"] = files_loaded\n+            # Resolve the SPM file path and pass it as vocab_file\n+            resolved_vocab_file = cached_file(\n+                pretrained_model_name_or_path,\n+                spm_file,\n+                cache_dir=kwargs.get(\"cache_dir\"),\n+                force_download=kwargs.get(\"force_download\", False),\n+                proxies=kwargs.get(\"proxies\"),\n+                token=kwargs.get(\"token\"),\n+                revision=kwargs.get(\"revision\"),\n+                local_files_only=kwargs.get(\"local_files_only\", False),\n+                subfolder=kwargs.get(\"subfolder\", \"\"),\n+            )\n+            kwargs[\"vocab_file\"] = resolved_vocab_file\n+            if (\n+                tokenizer_class is not None\n+                and SentencePieceBackend is not None\n+                and issubclass(tokenizer_class, SentencePieceBackend)\n+            ):\n+                logger.info(\n+                    \"Falling back to SentencePiece backend using tokenizer class that inherits from SentencePieceBackend.\"\n+                )\n+                return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+            return SentencePieceBackend.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+\n+        raise ValueError(\n+            f\"Could not load tokenizer from {pretrained_model_name_or_path}. \"\n+            \"No tokenizer class could be determined and no SentencePiece model found.\"\n+        )\n+\n+\n def get_tokenizer_config(\n     pretrained_model_name_or_path: Union[str, os.PathLike[str]],\n     cache_dir: Optional[Union[str, os.PathLike[str]]] = None,\n@@ -970,7 +917,7 @@ def __init__(self):\n     @replace_list_option_in_docstrings(TOKENIZER_MAPPING_NAMES)\n     def from_pretrained(\n         cls, pretrained_model_name_or_path, *inputs, **kwargs\n-    ) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:\n+    ) -> Union[TokenizersBackend, SentencePieceBackend]:\n         r\"\"\"\n         Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.\n \n@@ -1010,12 +957,12 @@ def from_pretrained(\n             subfolder (`str`, *optional*):\n                 In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n                 facebook/rag-token-base), specify it here.\n-            use_fast (`bool`, *optional*, defaults to `True`):\n-                Use a [fast Rust-based tokenizer](https://huggingface.co/docs/tokenizers/index) if it is supported for\n-                a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer\n-                is returned instead.\n             tokenizer_type (`str`, *optional*):\n                 Tokenizer type to be loaded.\n+            backend (`str`, *optional*, defaults to `\"tokenizers\"`):\n+                Backend to use for tokenization. Valid options are:\n+                - `\"tokenizers\"`: Use the HuggingFace tokenizers library backend (default)\n+                - `\"sentencepiece\"`: Use the SentencePiece backend\n             trust_remote_code (`bool`, *optional*, defaults to `False`):\n                 Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n                 should only be set to `True` for repositories you trust and in which you have read the code, as it will\n@@ -1041,38 +988,45 @@ def from_pretrained(\n \n         >>> # Download vocabulary from huggingface.co and define model-specific arguments\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space=True)\n+\n+        >>> # Explicitly use the tokenizers backend\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\", backend=\"tokenizers\")\n+\n+        >>> # Explicitly use the sentencepiece backend\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\", backend=\"sentencepiece\")\n         ```\"\"\"\n+        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n+        if use_auth_token is not None:\n+            logger.warning(\n+                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n+                FutureWarning,\n+            )\n+            if kwargs.get(\"token\") is not None:\n+                raise ValueError(\n+                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n+                )\n+            kwargs[\"token\"] = use_auth_token\n+\n         config = kwargs.pop(\"config\", None)\n         kwargs[\"_from_auto\"] = True\n \n-        use_fast = kwargs.pop(\"use_fast\", True)\n+        # V5: Always use fast tokenizers, ignore use_fast parameter\n+        _ = kwargs.pop(\"use_fast\", None)\n         tokenizer_type = kwargs.pop(\"tokenizer_type\", None)\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         gguf_file = kwargs.get(\"gguf_file\")\n \n         # First, let's see whether the tokenizer_type is passed so that we can leverage it\n         if tokenizer_type is not None:\n-            tokenizer_class = None\n-            tokenizer_class_tuple = TOKENIZER_MAPPING_NAMES.get(tokenizer_type, None)\n+            tokenizer_class_name = TOKENIZER_MAPPING_NAMES.get(tokenizer_type, None)\n \n-            if tokenizer_class_tuple is None:\n+            if tokenizer_class_name is None:\n                 raise ValueError(\n                     f\"Passed `tokenizer_type` {tokenizer_type} does not exist. `tokenizer_type` should be one of \"\n                     f\"{', '.join(c for c in TOKENIZER_MAPPING_NAMES)}.\"\n                 )\n \n-            tokenizer_class_name, tokenizer_fast_class_name = tokenizer_class_tuple\n-\n-            if use_fast:\n-                if tokenizer_fast_class_name is not None:\n-                    tokenizer_class = tokenizer_class_from_name(tokenizer_fast_class_name)\n-                else:\n-                    logger.warning(\n-                        \"`use_fast` is set to `True` but the tokenizer class does not have a fast version. \"\n-                        \" Falling back to the slow version.\"\n-                    )\n-            if tokenizer_class is None:\n-                tokenizer_class = tokenizer_class_from_name(tokenizer_class_name)\n+            tokenizer_class = tokenizer_class_from_name(tokenizer_class_name)\n \n             if tokenizer_class is None:\n                 raise ValueError(f\"Tokenizer class {tokenizer_class_name} is not currently imported.\")\n@@ -1107,6 +1061,13 @@ def from_pretrained(\n             if hasattr(config, \"auto_map\") and \"AutoTokenizer\" in config.auto_map:\n                 tokenizer_auto_map = config.auto_map[\"AutoTokenizer\"]\n \n+        if (\n+            config_tokenizer_class is not None\n+            and config_tokenizer_class != \"PreTrainedTokenizerFast\"\n+            and \"Fast\" in config_tokenizer_class\n+        ):\n+            config_tokenizer_class = config_tokenizer_class[:-4]\n+\n         has_remote_code = tokenizer_auto_map is not None\n         has_local_code = type(config) in TOKENIZER_MAPPING or (\n             config_tokenizer_class is not None\n@@ -1116,7 +1077,8 @@ def from_pretrained(\n             )\n         )\n         if has_remote_code:\n-            if use_fast and tokenizer_auto_map[1] is not None:\n+            # V5: Always prefer fast tokenizer (index 1), fallback to slow (index 0)\n+            if tokenizer_auto_map[1] is not None:\n                 class_ref = tokenizer_auto_map[1]\n             else:\n                 class_ref = tokenizer_auto_map[0]\n@@ -1136,18 +1098,16 @@ def from_pretrained(\n                 pretrained_model_name_or_path, *inputs, trust_remote_code=trust_remote_code, **kwargs\n             )\n         elif config_tokenizer_class is not None:\n-            tokenizer_class = None\n-            if use_fast and not config_tokenizer_class.endswith(\"Fast\"):\n-                tokenizer_class_candidate = f\"{config_tokenizer_class}Fast\"\n-                tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n-            if tokenizer_class is None:\n+            fast_tokenizer_class = None\n+            if fast_tokenizer_class is None:\n                 tokenizer_class_candidate = config_tokenizer_class\n                 tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate)\n-            if tokenizer_class is None:\n-                raise ValueError(\n-                    f\"Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.\"\n-                )\n-            return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+                if tokenizer_class is None and not tokenizer_class_candidate.endswith(\"Fast\"):\n+                    tokenizer_class = tokenizer_class_from_name(tokenizer_class_candidate + \"Fast\")\n+            else:\n+                tokenizer_class = fast_tokenizer_class\n+\n+            return _try_load_tokenizer_with_fallbacks(tokenizer_class, pretrained_model_name_or_path, inputs, kwargs)\n \n         # Otherwise we have to be creative.\n         # if model is an encoder decoder, the encoder tokenizer class is used by default\n@@ -1163,67 +1123,54 @@ def from_pretrained(\n \n         model_type = config_class_to_model_type(type(config).__name__)\n         if model_type is not None:\n-            tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\n+            tokenizer_class = TOKENIZER_MAPPING[type(config)]\n \n-            if tokenizer_class_fast and (use_fast or tokenizer_class_py is None):\n-                return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n+            if tokenizer_class is not None:\n+                return _try_load_tokenizer_with_fallbacks(\n+                    tokenizer_class, pretrained_model_name_or_path, inputs, kwargs\n+                )\n             else:\n-                if tokenizer_class_py is not None:\n-                    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n-                else:\n-                    raise ValueError(\n-                        \"This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \"\n-                        \"in order to use this tokenizer.\"\n-                    )\n+                raise ValueError(\n+                    \"This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \"\n+                    \"in order to use this tokenizer.\"\n+                )\n \n         raise ValueError(\n             f\"Unrecognized configuration class {config.__class__} to build an AutoTokenizer.\\n\"\n             f\"Model type should be one of {', '.join(c.__name__ for c in TOKENIZER_MAPPING)}.\"\n         )\n \n     @staticmethod\n-    def register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False):\n+    def register(\n+        config_class, tokenizer_class=None, slow_tokenizer_class=None, fast_tokenizer_class=None, exist_ok=False\n+    ):\n         \"\"\"\n         Register a new tokenizer in this mapping.\n \n-\n         Args:\n             config_class ([`PreTrainedConfig`]):\n                 The configuration corresponding to the model to register.\n-            slow_tokenizer_class ([`PretrainedTokenizer`], *optional*):\n-                The slow tokenizer to register.\n-            fast_tokenizer_class ([`PreTrainedTokenizerFast`], *optional*):\n-                The fast tokenizer to register.\n+            tokenizer_class: The tokenizer class to register (V5 - preferred parameter).\n+            slow_tokenizer_class: (Deprecated) The slow tokenizer to register.\n+            fast_tokenizer_class: (Deprecated) The fast tokenizer to register.\n         \"\"\"\n-        if slow_tokenizer_class is None and fast_tokenizer_class is None:\n-            raise ValueError(\"You need to pass either a `slow_tokenizer_class` or a `fast_tokenizer_class\")\n-        if slow_tokenizer_class is not None and issubclass(slow_tokenizer_class, PreTrainedTokenizerFast):\n-            raise ValueError(\"You passed a fast tokenizer in the `slow_tokenizer_class`.\")\n-        if fast_tokenizer_class is not None and issubclass(fast_tokenizer_class, PreTrainedTokenizer):\n-            raise ValueError(\"You passed a slow tokenizer in the `fast_tokenizer_class`.\")\n+        if tokenizer_class is None:\n+            # Legacy: prefer fast over slow\n+            if fast_tokenizer_class is not None:\n+                tokenizer_class = fast_tokenizer_class\n+            elif slow_tokenizer_class is not None:\n+                tokenizer_class = slow_tokenizer_class\n+            else:\n+                raise ValueError(\"You need to pass a `tokenizer_class`\")\n \n-        if (\n-            slow_tokenizer_class is not None\n-            and fast_tokenizer_class is not None\n-            and issubclass(fast_tokenizer_class, PreTrainedTokenizerFast)\n-            and fast_tokenizer_class.slow_tokenizer_class != slow_tokenizer_class\n-        ):\n-            raise ValueError(\n-                \"The fast tokenizer class you are passing has a `slow_tokenizer_class` attribute that is not \"\n-                \"consistent with the slow tokenizer class you passed (fast tokenizer has \"\n-                f\"{fast_tokenizer_class.slow_tokenizer_class} and you passed {slow_tokenizer_class}. Fix one of those \"\n-                \"so they match!\"\n-            )\n+        for candidate in (slow_tokenizer_class, fast_tokenizer_class, tokenizer_class):\n+            if candidate is not None:\n+                REGISTERED_TOKENIZER_CLASSES[candidate.__name__] = candidate\n \n-        # Avoid resetting a set slow/fast tokenizer if we are passing just the other ones.\n-        if config_class in TOKENIZER_MAPPING._extra_content:\n-            existing_slow, existing_fast = TOKENIZER_MAPPING[config_class]\n-            if slow_tokenizer_class is None:\n-                slow_tokenizer_class = existing_slow\n-            if fast_tokenizer_class is None:\n-                fast_tokenizer_class = existing_fast\n+        if slow_tokenizer_class is not None and fast_tokenizer_class is not None:\n+            REGISTERED_FAST_ALIASES[slow_tokenizer_class.__name__] = fast_tokenizer_class\n \n-        TOKENIZER_MAPPING.register(config_class, (slow_tokenizer_class, fast_tokenizer_class), exist_ok=exist_ok)\n+        TOKENIZER_MAPPING.register(config_class, tokenizer_class, exist_ok=exist_ok)\n \n \n __all__ = [\"TOKENIZER_MAPPING\", \"AutoTokenizer\"]"
        },
        {
            "sha": "3cf9c34518954100c569a8c3a78e652a48b2a9f6",
            "filename": "src/transformers/models/bart/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,10 +18,9 @@\n \n \n if TYPE_CHECKING:\n+    from ..roberta.tokenization_roberta import RobertaTokenizer as BartTokenizer\n     from .configuration_bart import *\n     from .modeling_bart import *\n-    from .tokenization_bart import *\n-    from .tokenization_bart_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "208d4e39131e87d7fb1f078f0693563d9c3d4631",
            "filename": "src/transformers/models/bart/tokenization_bart.py",
            "status": "modified",
            "additions": 16,
            "deletions": 386,
            "changes": 402,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1,393 +1,23 @@\n-# coding=utf-8\n-# Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import json\n-import os\n-from functools import lru_cache\n-from typing import Optional\n-\n-import regex as re\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n-from ...utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\"}\n-\n-# See all BART models at https://huggingface.co/models?filter=bart\n-\n-\n-@lru_cache\n-def bytes_to_unicode():\n-    \"\"\"\n-    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n-    characters the bpe code barfs on.\n-\n-    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n-    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n-    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n-    tables between utf-8 bytes and unicode strings.\n-    \"\"\"\n-    bs = (\n-        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¬°\"), ord(\"¬¨\") + 1)) + list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1))\n-    )\n-    cs = bs[:]\n-    n = 0\n-    for b in range(2**8):\n-        if b not in bs:\n-            bs.append(b)\n-            cs.append(2**8 + n)\n-            n += 1\n-    cs = [chr(n) for n in cs]\n-    return dict(zip(bs, cs))\n-\n-\n-def get_pairs(word):\n-    \"\"\"\n-    Return set of symbol pairs in a word.\n-\n-    Word is represented as tuple of symbols (symbols being variable-length strings).\n-    \"\"\"\n-    pairs = set()\n-    prev_char = word[0]\n-    for char in word[1:]:\n-        pairs.add((prev_char, char))\n-        prev_char = char\n-    return pairs\n-\n-\n-class BartTokenizer(PreTrainedTokenizer):\n-    \"\"\"\n-    Constructs a BART tokenizer, which is smilar to the ROBERTa tokenizer, using byte-level Byte-Pair-Encoding.\n-\n-    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n-    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n-\n-    ```python\n-    >>> from transformers import BartTokenizer\n-\n-    >>> tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n-    >>> tokenizer(\"Hello world\")[\"input_ids\"]\n-    [0, 31414, 232, 2]\n-\n-    >>> tokenizer(\" Hello world\")[\"input_ids\"]\n-    [0, 20920, 232, 2]\n-    ```\n-\n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n-    call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n-\n-    <Tip>\n-\n-    When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n-\n-    </Tip>\n-\n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n-        errors (`str`, *optional*, defaults to `\"replace\"`):\n-            Paradigm to follow when decoding bytes to UTF-8. See\n-            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n-        bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the beginning of\n-            sequence. The token used is the `cls_token`.\n-\n-            </Tip>\n-\n-        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The end of sequence token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n-            The token used is the `sep_token`.\n-\n-            </Tip>\n-\n-        sep_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        cls_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        add_prefix_space (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n-            other word. (BART tokenizer detect beginning of words by the preceding space).\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-\n-    def __init__(\n-        self,\n-        vocab_file,\n-        merges_file,\n-        errors=\"replace\",\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        sep_token=\"</s>\",\n-        cls_token=\"<s>\",\n-        unk_token=\"<unk>\",\n-        pad_token=\"<pad>\",\n-        mask_token=\"<mask>\",\n-        add_prefix_space=False,\n-        **kwargs,\n-    ):\n-        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n-        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n-        sep_token = AddedToken(sep_token, lstrip=False, rstrip=False) if isinstance(sep_token, str) else sep_token\n-        cls_token = AddedToken(cls_token, lstrip=False, rstrip=False) if isinstance(cls_token, str) else cls_token\n-        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n-        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n-\n-        # Mask token behave like a normal word, i.e. include the space before it\n-        mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n-\n-        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n-            self.encoder = json.load(vocab_handle)\n-        self.decoder = {v: k for k, v in self.encoder.items()}\n-        self.errors = errors  # how to handle errors in decoding\n-        self.byte_encoder = bytes_to_unicode()\n-        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n-        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n-            bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\n-        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n-        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n-        self.cache = {}\n-        self.add_prefix_space = add_prefix_space\n-\n-        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n-        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n-\n-        super().__init__(\n-            errors=errors,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            cls_token=cls_token,\n-            pad_token=pad_token,\n-            mask_token=mask_token,\n-            add_prefix_space=add_prefix_space,\n-            **kwargs,\n-        )\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.encoder)\n-\n-    def get_vocab(self):\n-        return dict(self.encoder, **self.added_tokens_encoder)\n-\n-    def bpe(self, token):\n-        if token in self.cache:\n-            return self.cache[token]\n-        word = tuple(token)\n-        pairs = get_pairs(word)\n-\n-        if not pairs:\n-            return token\n-\n-        while True:\n-            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n-            if bigram not in self.bpe_ranks:\n-                break\n-            first, second = bigram\n-            new_word = []\n-            i = 0\n-            while i < len(word):\n-                try:\n-                    j = word.index(first, i)\n-                except ValueError:\n-                    new_word.extend(word[i:])\n-                    break\n-                else:\n-                    new_word.extend(word[i:j])\n-                    i = j\n-\n-                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n-                    new_word.append(first + second)\n-                    i += 2\n-                else:\n-                    new_word.append(word[i])\n-                    i += 1\n-            new_word = tuple(new_word)\n-            word = new_word\n-            if len(word) == 1:\n-                break\n-            else:\n-                pairs = get_pairs(word)\n-        word = \" \".join(word)\n-        self.cache[token] = word\n-        return word\n-\n-    def _tokenize(self, text):\n-        \"\"\"Tokenize a string.\"\"\"\n-        bpe_tokens = []\n-        for token in re.findall(self.pat, text):\n-            token = \"\".join(\n-                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n-            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n-            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n-        return bpe_tokens\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.encoder.get(token, self.encoder.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.decoder.get(index)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        text = \"\".join(tokens)\n-        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n-        return text\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n-        )\n-\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n-\n-        index = 0\n-        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(\"#version: 0.2\\n\")\n-            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n-                index += 1\n-\n-        return vocab_file, merge_file\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BART sequence has the following format:\n-\n-        - single sequence: `<s> X </s>`\n-        - pair of sequences: `<s> A </s></s> B </s>`\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. BART does not\n-        make use of token type ids, therefore a list of zeros is returned.\n+# This source code is licensed under the Apache 2.0 license found in the\n+# LICENSE file in the root directory of this source tree.\n \n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n+\"\"\"\n+Compatibility shims for BART tokenizers in v5.\n \n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n+In v5 we consolidate on the tokenizers-library backend and remove separate\n+\"slow\" vs \"fast\" implementations. BART uses the same byte-level BPE\n+tokenizer as RoBERTa, so we expose `BartTokenizer` and `BartTokenizerFast`\n+as aliases to `RobertaTokenizer` to preserve the public API expected by\n+existing code and tests.\n+\"\"\"\n \n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n+from ..roberta.tokenization_roberta import RobertaTokenizer as _RobertaTokenizer\n \n-    def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n-        add_prefix_space = kwargs.pop(\"add_prefix_space\", self.add_prefix_space)\n-        if (is_split_into_words or add_prefix_space) and (len(text) > 0 and not text[0].isspace()):\n-            text = \" \" + text\n-        return (text, kwargs)\n \n+# Public aliases maintained for backwards compatibility\n+BartTokenizer = _RobertaTokenizer\n+BartTokenizerFast = _RobertaTokenizer\n \n-__all__ = [\"BartTokenizer\"]\n+__all__ = [\"BartTokenizer\", \"BartTokenizerFast\"]"
        },
        {
            "sha": "88b002f595299f8d16b605c0ce4fd59330e5c4c4",
            "filename": "src/transformers/models/bart/tokenization_bart_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 271,
            "changes": 271,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,271 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 The Facebook AI Research Team Authors and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import json\n-from typing import Optional\n-\n-from tokenizers import processors\n-\n-from ...tokenization_utils_base import AddedToken, BatchEncoding\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_bart import BartTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-# See all BART models at https://huggingface.co/models?filter=bart\n-\n-\n-class BartTokenizerFast(PreTrainedTokenizerFast):\n-    r\"\"\"\n-    Construct a \"fast\" BART tokenizer (backed by HuggingFace's *tokenizers* library), derived from the GPT-2 tokenizer,\n-    using byte-level Byte-Pair-Encoding.\n-\n-    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n-    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n-\n-    ```python\n-    >>> from transformers import BartTokenizerFast\n-\n-    >>> tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-base\")\n-    >>> tokenizer(\"Hello world\")[\"input_ids\"]\n-    [0, 31414, 232, 2]\n-\n-    >>> tokenizer(\" Hello world\")[\"input_ids\"]\n-    [0, 20920, 232, 2]\n-    ```\n-\n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n-    call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n-\n-    <Tip>\n-\n-    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n-\n-    </Tip>\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n-        errors (`str`, *optional*, defaults to `\"replace\"`):\n-            Paradigm to follow when decoding bytes to UTF-8. See\n-            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n-        bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the beginning of\n-            sequence. The token used is the `cls_token`.\n-\n-            </Tip>\n-\n-        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The end of sequence token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n-            The token used is the `sep_token`.\n-\n-            </Tip>\n-\n-        sep_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        cls_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        add_prefix_space (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n-            other word. (BART tokenizer detect beginning of words by the preceding space).\n-        trim_offsets (`bool`, *optional*, defaults to `True`):\n-            Whether the post processing step should trim offsets to avoid including whitespaces.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = BartTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        errors=\"replace\",\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        sep_token=\"</s>\",\n-        cls_token=\"<s>\",\n-        unk_token=\"<unk>\",\n-        pad_token=\"<pad>\",\n-        mask_token=\"<mask>\",\n-        add_prefix_space=False,\n-        trim_offsets=True,\n-        **kwargs,\n-    ):\n-        # we have to specify that this tokens is special otherwise adding it will reset the normalized flag to `False` in `add_special_tokens`\n-        mask_token = (\n-            AddedToken(mask_token, lstrip=True, normalized=True, special=True)\n-            if isinstance(mask_token, str)\n-            else mask_token\n-        )\n-        super().__init__(\n-            vocab_file,\n-            merges_file,\n-            tokenizer_file=tokenizer_file,\n-            errors=errors,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            sep_token=sep_token,\n-            cls_token=cls_token,\n-            unk_token=unk_token,\n-            pad_token=pad_token,\n-            mask_token=mask_token,\n-            add_prefix_space=add_prefix_space,\n-            trim_offsets=trim_offsets,\n-            **kwargs,\n-        )\n-\n-        # the pre_tokenizer is already updated in the GPT2TokenizerFast `__init__`\n-        tokenizer_component = \"post_processor\"\n-        tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)\n-        if tokenizer_component_instance:\n-            state = json.loads(tokenizer_component_instance.__getstate__())\n-\n-            # The lists 'sep' and 'cls' must be cased in tuples for the object `post_processor_class`\n-            if \"sep\" in state:\n-                state[\"sep\"] = tuple(state[\"sep\"])\n-            if \"cls\" in state:\n-                state[\"cls\"] = tuple(state[\"cls\"])\n-\n-            changes_to_apply = False\n-\n-            if state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-                state[\"add_prefix_space\"] = add_prefix_space\n-                changes_to_apply = True\n-\n-            if state.get(\"trim_offsets\", trim_offsets) != trim_offsets:\n-                state[\"trim_offsets\"] = trim_offsets\n-                changes_to_apply = True\n-\n-            if changes_to_apply:\n-                component_class = getattr(processors, state.pop(\"type\"))\n-                new_value = component_class(**state)\n-                setattr(self.backend_tokenizer, tokenizer_component, new_value)\n-\n-    @property\n-    def mask_token(self) -> str:\n-        \"\"\"\n-        `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not\n-        having been set.\n-\n-        BART tokenizer has a special mask token to be usable in the fill-mask pipeline. The mask token will greedily\n-        comprise the space before the *<mask>*.\n-        \"\"\"\n-        if self._mask_token is None:\n-            if self.verbose:\n-                logger.error(\"Using mask_token, but it is not set yet.\")\n-            return None\n-        return str(self._mask_token)\n-\n-    @mask_token.setter\n-    def mask_token(self, value):\n-        \"\"\"\n-        Overriding the default behavior of the mask token to have it eat the space before it.\n-\n-        This is needed to preserve backward compatibility with all the previously used models based on Bart.\n-        \"\"\"\n-        # Mask token behave like a normal word, i.e. include the space before it\n-        # So we set lstrip to True\n-        value = AddedToken(value, lstrip=True, rstrip=False) if isinstance(value, str) else value\n-        self._mask_token = value\n-\n-    def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-\n-        if is_split_into_words and not self.add_prefix_space:\n-            raise ValueError(\n-                f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-                \"to use it with pretokenized inputs.\"\n-            )\n-\n-        return super()._batch_encode_plus(*args, **kwargs)\n-\n-    def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-\n-        if is_split_into_words and not self.add_prefix_space:\n-            raise ValueError(\n-                f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-                \"to use it with pretokenized inputs.\"\n-            )\n-\n-        return super()._encode_plus(*args, **kwargs)\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        output = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n-        if token_ids_1 is None:\n-            return output\n-\n-        return output + [self.eos_token_id] + token_ids_1 + [self.eos_token_id]\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. BART does not\n-        make use of token type ids, therefore a list of zeros is returned.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n-\n-\n-__all__ = [\"BartTokenizerFast\"]"
        },
        {
            "sha": "c9e11571fc6d49eb75f8a1779d522093af269a00",
            "filename": "src/transformers/models/barthez/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbarthez%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbarthez%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .tokenization_barthez import *\n-    from .tokenization_barthez_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "77d0b65d15b8ef0aef628f6a57e74b4ec233d55b",
            "filename": "src/transformers/models/barthez/tokenization_barthez.py",
            "status": "modified",
            "additions": 55,
            "deletions": 194,
            "changes": 249,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,40 +14,31 @@\n # limitations under the License\n \"\"\"Tokenization classes for the BARThez model.\"\"\"\n \n-import os\n-from shutil import copyfile\n-from typing import Any, Optional\n+from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers\n+from tokenizers.models import Unigram\n \n-import sentencepiece as spm\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_python import AddedToken\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n-from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n-VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\"}\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\", \"tokenizer_file\": \"tokenizer.json\"}\n \n \n SPIECE_UNDERLINE = \"‚ñÅ\"\n \n-# TODO this class is useless. This is the most standard sentencpiece model. Let's find which one is closest and nuke this.\n-\n \n-@requires(backends=(\"sentencepiece\",))\n-class BarthezTokenizer(PreTrainedTokenizer):\n+class BarthezTokenizer(TokenizersBackend):\n     \"\"\"\n-    Adapted from [`CamembertTokenizer`] and [`BartTokenizer`]. Construct a BARThez tokenizer. Based on\n+    Adapted from [`CamembertTokenizer`] and [`BartTokenizer`]. Construct a \"fast\" BARThez tokenizer. Based on\n     [SentencePiece](https://github.com/google/sentencepiece).\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n+    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n         bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n             The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n \n@@ -83,209 +74,79 @@ class BarthezTokenizer(PreTrainedTokenizer):\n         mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        sp_model_kwargs (`dict`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n-\n-    Attributes:\n-        sp_model (`SentencePieceProcessor`):\n-            The *SentencePiece* processor that is used for every conversion (string, tokens and IDs).\n+        vocab_file (`str`, *optional*):\n+            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n+            contains the vocabulary necessary to instantiate a tokenizer.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        add_prefix_space (`bool`, *optional*, defaults to `True`):\n+            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n+            other word.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    slow_tokenizer_class = None\n \n     def __init__(\n         self,\n-        vocab_file,\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         sep_token=\"</s>\",\n         cls_token=\"<s>\",\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n+        vocab_file=None,\n+        vocab=None,\n+        add_prefix_space=True,\n         **kwargs,\n-    ) -> None:\n-        # Mask token behave like a normal word, i.e. include the space before it. Will have normalized=False by default this way\n-        mask_token = AddedToken(mask_token, lstrip=True, special=True) if isinstance(mask_token, str) else mask_token\n+    ):\n+        # Mask token behave like a normal word, i.e. include the space before it\n+        mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n+        self.add_prefix_space = add_prefix_space\n+        self.vocab_file = vocab_file\n \n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n+        if vocab is not None:\n+            self._vocab = vocab\n+        else:\n+            self._vocab = [\n+                (str(pad_token), 0.0),\n+                (str(unk_token), 0.0),\n+                (str(cls_token), 0.0),\n+                (str(sep_token), 0.0),\n+                (str(mask_token), 0.0),\n+            ]\n+\n+        self._tokenizer = Tokenizer(Unigram(self._vocab, unk_id=3, byte_fallback=False))\n+\n+        self._tokenizer.normalizer = normalizers.Sequence(\n+            [\n+                normalizers.Replace(\"\\n\", \" \"),\n+                normalizers.Replace(\"\\r\", \" \"),\n+                normalizers.Replace(\"\\t\", \" \"),\n+                normalizers.Replace(Regex(r\" {2,}\"), \" \"),\n+                normalizers.NFC(),\n+                normalizers.Strip(left=False, right=True),\n+            ]\n+        )\n+        prepend_scheme = \"always\" if add_prefix_space else \"never\"\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme)\n+        self._tokenizer.decoder = decoders.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme)\n \n-        self.vocab_file = vocab_file\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(str(vocab_file))\n+        tokenizer_object = self._tokenizer\n         super().__init__(\n+            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             unk_token=unk_token,\n             sep_token=sep_token,\n             cls_token=cls_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n-            sp_model_kwargs=self.sp_model_kwargs,\n+            add_prefix_space=add_prefix_space,\n             **kwargs,\n         )\n \n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BARThez sequence has the following format:\n-\n-        - single sequence: `<s> X </s>`\n-        - pair of sequences: `<s> A </s></s> B </s>`\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.sp_model)\n-\n-    def get_vocab(self):\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n-\n-    def _tokenize(self, text: str) -> list[str]:\n-        return self.sp_model.encode(text, out_type=str)\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.sp_model.PieceToId(token)\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.sp_model.IdToPiece(index)\n-\n-    # Copied from transformers.models.albert.tokenization_albert.AlbertTokenizer.convert_tokens_to_string\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        current_sub_tokens = []\n-        out_string = \"\"\n-        prev_is_special = False\n-        for token in tokens:\n-            # make sure that special tokens are not decoded using sentencepiece model\n-            if token in self.all_special_tokens:\n-                if not prev_is_special:\n-                    out_string += \" \"\n-                out_string += self.sp_model.decode(current_sub_tokens) + token\n-                prev_is_special = True\n-                current_sub_tokens = []\n-            else:\n-                current_sub_tokens.append(token)\n-                prev_is_special = False\n-        out_string += self.sp_model.decode(current_sub_tokens)\n-        return out_string.strip()\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(self.vocab_file)\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n-\n-        return (out_vocab_file,)\n-\n \n __all__ = [\"BarthezTokenizer\"]"
        },
        {
            "sha": "64050ca8848f57c25d272e0bc31a3f878040e14c",
            "filename": "src/transformers/models/barthez/tokenization_barthez_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 193,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,193 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 Ecole Polytechnique and the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License\n-\"\"\"Tokenization classes for the BARThez model.\"\"\"\n-\n-import os\n-from shutil import copyfile\n-from typing import Optional\n-\n-from ...tokenization_utils import AddedToken\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import is_sentencepiece_available, logging\n-\n-\n-if is_sentencepiece_available():\n-    from .tokenization_barthez import BarthezTokenizer\n-else:\n-    BarthezTokenizer = None\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-SPIECE_UNDERLINE = \"‚ñÅ\"\n-\n-\n-class BarthezTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Adapted from [`CamembertTokenizer`] and [`BartTokenizer`]. Construct a \"fast\" BARThez tokenizer. Based on\n-    [SentencePiece](https://github.com/google/sentencepiece).\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the beginning of\n-            sequence. The token used is the `cls_token`.\n-\n-            </Tip>\n-\n-        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The end of sequence token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n-            The token used is the `sep_token`.\n-\n-            </Tip>\n-\n-        sep_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        cls_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        additional_special_tokens (`list[str]`, *optional*, defaults to `[\"<s>NOTUSED\", \"</s>NOTUSED\"]`):\n-            Additional special tokens used by the tokenizer.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = BarthezTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        sep_token=\"</s>\",\n-        cls_token=\"<s>\",\n-        unk_token=\"<unk>\",\n-        pad_token=\"<pad>\",\n-        mask_token=\"<mask>\",\n-        **kwargs,\n-    ):\n-        # Mask token behave like a normal word, i.e. include the space before it\n-        mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n-\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            cls_token=cls_token,\n-            pad_token=pad_token,\n-            mask_token=mask_token,\n-            **kwargs,\n-        )\n-\n-        self.vocab_file = vocab_file\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BARThez sequence has the following format:\n-\n-        - single sequence: `<s> X </s>`\n-        - pair of sequences: `<s> A </s></s> B </s>`\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not self.can_save_slow_tokenizer:\n-            raise ValueError(\n-                \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \"\n-                \"tokenizer.\"\n-            )\n-\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-\n-        return (out_vocab_file,)\n-\n-\n-__all__ = [\"BarthezTokenizerFast\"]"
        },
        {
            "sha": "a557c1352aa55f656dc3e17ff2ec273867040de9",
            "filename": "src/transformers/models/bartpho/tokenization_bartpho.py",
            "status": "modified",
            "additions": 46,
            "deletions": 40,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,22 +18,19 @@\n from shutil import copyfile\n from typing import Any, Optional\n \n-import sentencepiece as spm\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_python import AddedToken\n+from ...tokenization_utils_sentencepiece import SentencePieceBackend\n from ...utils import logging\n from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n-SPIECE_UNDERLINE = \"‚ñÅ\"\n-\n VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\", \"monolingual_vocab_file\": \"dict.txt\"}\n \n \n @requires(backends=(\"sentencepiece\",))\n-class BartphoTokenizer(PreTrainedTokenizer):\n+class BartphoTokenizer(SentencePieceBackend):\n     \"\"\"\n     Adapted from [`XLMRobertaTokenizer`]. Based on [SentencePiece](https://github.com/google/sentencepiece).\n \n@@ -105,6 +102,7 @@ class BartphoTokenizer(PreTrainedTokenizer):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    is_fast = False\n \n     def __init__(\n         self,\n@@ -123,15 +121,9 @@ def __init__(\n         # Mask token behave like a normal word, i.e. include the space before it\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n \n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-\n-        self.vocab_file = vocab_file\n         self.monolingual_vocab_file = monolingual_vocab_file\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(str(vocab_file))\n \n         # Load the reduced vocab\n-\n         # Keep order of special tokens for backward compatibility\n         self.fairseq_tokens_to_ids = {}\n         cnt = 0\n@@ -148,33 +140,23 @@ def __init__(\n \n         self.fairseq_ids_to_tokens = {v: k for k, v in self.fairseq_tokens_to_ids.items()}\n \n+        # Prepare sp_model_kwargs for parent class\n+        if sp_model_kwargs is not None:\n+            kwargs[\"sp_model_kwargs\"] = sp_model_kwargs\n+\n+        # Call parent init (which will load sp_model)\n         super().__init__(\n+            vocab_file=vocab_file,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             unk_token=unk_token,\n             sep_token=sep_token,\n             cls_token=cls_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n-            sp_model_kwargs=self.sp_model_kwargs,\n             **kwargs,\n         )\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        state[\"sp_model_proto\"] = self.sp_model.serialized_model_proto()\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n+        self._align_added_tokens_with_fairseq_vocab()\n \n     def build_inputs_with_special_tokens(\n         self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n@@ -257,31 +239,55 @@ def create_token_type_ids_from_sequences(\n \n     @property\n     def vocab_size(self):\n+        \"\"\"Override to return fairseq vocab size instead of sp_model vocab size\"\"\"\n         return len(self.fairseq_ids_to_tokens)\n \n     def get_vocab(self):\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n-        vocab.update(self.added_tokens_encoder)\n+        \"\"\"Override to use fairseq vocabulary\"\"\"\n+        vocab = dict(self.fairseq_tokens_to_ids)\n+        if hasattr(self, \"_added_tokens_encoder\"):\n+            for token, idx in self._added_tokens_encoder.items():\n+                if token not in vocab:\n+                    vocab[token] = idx\n         return vocab\n \n-    def _tokenize(self, text: str) -> list[str]:\n-        return self.sp_model.encode(text, out_type=str)\n-\n     def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n+        \"\"\"Converts a token (str) in an id using the fairseq vocab.\"\"\"\n         if token in self.fairseq_tokens_to_ids:\n             return self.fairseq_tokens_to_ids[token]\n         else:\n             return self.unk_token_id\n \n+    def _convert_token_to_id_with_added_voc(self, token):\n+        \"\"\"Override to use fairseq vocab instead of sp_model vocab.\"\"\"\n+        if token is None:\n+            return None\n+\n+        if token in self._added_tokens_encoder:\n+            return self._added_tokens_encoder[token]\n+        return self._convert_token_to_id(token)\n+\n     def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n+        \"\"\"Converts an index (integer) in a token (str) using the fairseq vocab.\"\"\"\n         return self.fairseq_ids_to_tokens[index]\n \n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n-        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n-        return out_string\n+    def _align_added_tokens_with_fairseq_vocab(self):\n+        \"\"\"\n+        The slow tokenizer base class populates `_added_tokens_*` using SentencePiece ids. Remap those entries so that\n+        every token present in the reduced fairseq dictionary uses the same ids everywhere, otherwise conversions and\n+        special-token setters observe two different vocabularies.\n+        \"\"\"\n+        if not hasattr(self, \"_added_tokens_decoder\") or not hasattr(self, \"_added_tokens_encoder\"):\n+            return\n+\n+        remapped_decoder: dict[int, AddedToken] = {}\n+        for original_id, token_obj in self._added_tokens_decoder.items():\n+            token = token_obj.content\n+            new_id = self.fairseq_tokens_to_ids.get(token, original_id)\n+            remapped_decoder[new_id] = token_obj\n+\n+        self._added_tokens_decoder = remapped_decoder\n+        self._added_tokens_encoder = {token.content: idx for idx, token in remapped_decoder.items()}\n \n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):"
        },
        {
            "sha": "54a46312ad9153f2d8c3bf1a3029a3254efab791",
            "filename": "src/transformers/models/bert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,6 @@\n     from .configuration_bert import *\n     from .modeling_bert import *\n     from .tokenization_bert import *\n-    from .tokenization_bert_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "b1edde12d5c2bc7031f6543c41118ad80af86a41",
            "filename": "src/transformers/models/bert/tokenization_bert.py",
            "status": "modified",
            "additions": 64,
            "deletions": 387,
            "changes": 451,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -15,17 +15,18 @@\n \"\"\"Tokenization classes for Bert.\"\"\"\n \n import collections\n-import os\n-import unicodedata\n from typing import Optional\n \n-from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n+from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from tokenizers.models import WordPiece\n+\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n \n \n def load_vocab(vocab_file):\n@@ -39,32 +40,18 @@ def load_vocab(vocab_file):\n     return vocab\n \n \n-def whitespace_tokenize(text):\n-    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n-    text = text.strip()\n-    if not text:\n-        return []\n-    tokens = text.split()\n-    return tokens\n-\n-\n-class BertTokenizer(PreTrainedTokenizer):\n+class BertTokenizer(TokenizersBackend):\n     r\"\"\"\n-    Construct a BERT tokenizer. Based on WordPiece.\n+    Construct a BERT tokenizer (backed by HuggingFace's tokenizers library). Based on WordPiece.\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the main methods. Users should refer to\n     this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`):\n+        vocab_file (`str`, *optional*):\n             File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n+        do_lower_case (`bool`, *optional*, defaults to `False`):\n             Whether or not to lowercase the input when tokenizing.\n-        do_basic_tokenize (`bool`, *optional*, defaults to `True`):\n-            Whether or not to do basic tokenization before WordPiece.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n         unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n@@ -82,397 +69,87 @@ class BertTokenizer(PreTrainedTokenizer):\n             modeling. This is the token which the model will try to predict.\n         tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n             Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original BERT).\n-        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n-            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n-            extra spaces.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n+    model_input_names = [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n+    slow_tokenizer_class = None\n \n     def __init__(\n         self,\n-        vocab_file,\n-        do_lower_case=True,\n-        do_basic_tokenize=True,\n-        never_split=None,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        clean_up_tokenization_spaces=True,\n+        vocab_file: Optional[str] = None,\n+        do_lower_case: bool = False,\n+        unk_token: str = \"[UNK]\",\n+        sep_token: str = \"[SEP]\",\n+        pad_token: str = \"[PAD]\",\n+        cls_token: str = \"[CLS]\",\n+        mask_token: str = \"[MASK]\",\n+        tokenize_chinese_chars: bool = True,\n+        strip_accents: Optional[bool] = None,\n+        vocab: Optional[dict] = None,\n         **kwargs,\n     ):\n-        if not os.path.isfile(vocab_file):\n-            raise ValueError(\n-                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n-                \" model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n-            )\n-        self.vocab = load_vocab(vocab_file)\n-        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n-        self.do_basic_tokenize = do_basic_tokenize\n-        if do_basic_tokenize:\n-            self.basic_tokenizer = BasicTokenizer(\n-                do_lower_case=do_lower_case,\n-                never_split=never_split,\n-                tokenize_chinese_chars=tokenize_chinese_chars,\n-                strip_accents=strip_accents,\n+        self.do_lower_case = do_lower_case\n+        self.tokenize_chinese_chars = tokenize_chinese_chars\n+        self.strip_accents = strip_accents\n+\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n             )\n+        else:\n+            self._vocab = {\n+                str(pad_token): 0,\n+                str(unk_token): 1,\n+                str(cls_token): 2,\n+                str(sep_token): 3,\n+                str(mask_token): 4,\n+            }\n+\n+        self._tokenizer = Tokenizer(WordPiece(self._vocab, unk_token=str(unk_token)))\n+\n+        self._tokenizer.normalizer = normalizers.BertNormalizer(\n+            clean_text=True,\n+            handle_chinese_chars=tokenize_chinese_chars,\n+            strip_accents=strip_accents,\n+            lowercase=do_lower_case,\n+        )\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n+        self._tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n \n-        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=str(unk_token))\n+        tokenizer_object = self._tokenizer\n \n         super().__init__(\n+            tokenizer_object=tokenizer_object,\n             do_lower_case=do_lower_case,\n-            do_basic_tokenize=do_basic_tokenize,\n-            never_split=never_split,\n             unk_token=unk_token,\n             sep_token=sep_token,\n             pad_token=pad_token,\n             cls_token=cls_token,\n             mask_token=mask_token,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n \n-    @property\n-    def do_lower_case(self):\n-        return self.basic_tokenizer.do_lower_case\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.vocab)\n-\n-    def get_vocab(self):\n-        return dict(self.vocab, **self.added_tokens_encoder)\n-\n-    def _tokenize(self, text, split_special_tokens=False):\n-        split_tokens = []\n-        if self.do_basic_tokenize:\n-            for token in self.basic_tokenizer.tokenize(\n-                text, never_split=self.all_special_tokens if not split_special_tokens else None\n-            ):\n-                # If the token is part of the never_split set\n-                if token in self.basic_tokenizer.never_split:\n-                    split_tokens.append(token)\n-                else:\n-                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n-        else:\n-            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n-        return split_tokens\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.vocab.get(token, self.vocab.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.ids_to_tokens.get(index, self.unk_token)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n-        return out_string\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        index = 0\n-        if os.path.isdir(save_directory):\n-            vocab_file = os.path.join(\n-                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-            )\n-        else:\n-            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n-            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n-                        \" Please check that the vocabulary is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(token + \"\\n\")\n-                index += 1\n-        return (vocab_file,)\n-\n-\n-class BasicTokenizer:\n-    \"\"\"\n-    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n-\n-    Args:\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        do_split_on_punc (`bool`, *optional*, defaults to `True`):\n-            In some instances we want to skip the basic punctuation splitting so that later tokenization can capture\n-            the full context of the words, such as contractions.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        do_lower_case=True,\n-        never_split=None,\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        do_split_on_punc=True,\n-    ):\n-        if never_split is None:\n-            never_split = []\n-        self.do_lower_case = do_lower_case\n-        self.never_split = set(never_split)\n-        self.tokenize_chinese_chars = tokenize_chinese_chars\n-        self.strip_accents = strip_accents\n-        self.do_split_on_punc = do_split_on_punc\n-\n-    def tokenize(self, text, never_split=None):\n-        \"\"\"\n-        Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n-\n-        Args:\n-            never_split (`List[str]`, *optional*)\n-                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n-                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n-        \"\"\"\n-        # union() returns a new set by concatenating the two sets.\n-        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n-        text = self._clean_text(text)\n+        cls_token_id = self.cls_token_id if self.cls_token_id is not None else 2\n+        sep_token_id = self.sep_token_id if self.sep_token_id is not None else 3\n \n-        # This was added on November 1st, 2018 for the multilingual and Chinese\n-        # models. This is also applied to the English models now, but it doesn't\n-        # matter since the English models were not trained on any Chinese data\n-        # and generally don't have any Chinese data in them (there are Chinese\n-        # characters in the vocabulary because Wikipedia does have some Chinese\n-        # words in the English Wikipedia.).\n-        if self.tokenize_chinese_chars:\n-            text = self._tokenize_chinese_chars(text)\n-        # prevents treating the same character with different unicode codepoints as different characters\n-        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n-        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n-        split_tokens = []\n-        for token in orig_tokens:\n-            if token not in never_split:\n-                if self.do_lower_case:\n-                    token = token.lower()\n-                    if self.strip_accents is not False:\n-                        token = self._run_strip_accents(token)\n-                elif self.strip_accents:\n-                    token = self._run_strip_accents(token)\n-            split_tokens.extend(self._run_split_on_punc(token, never_split))\n-\n-        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n-        return output_tokens\n-\n-    def _run_strip_accents(self, text):\n-        \"\"\"Strips accents from a piece of text.\"\"\"\n-        text = unicodedata.normalize(\"NFD\", text)\n-        output = []\n-        for char in text:\n-            cat = unicodedata.category(char)\n-            if cat == \"Mn\":\n-                continue\n-            output.append(char)\n-        return \"\".join(output)\n-\n-    def _run_split_on_punc(self, text, never_split=None):\n-        \"\"\"Splits punctuation on a piece of text.\"\"\"\n-        if not self.do_split_on_punc or (never_split is not None and text in never_split):\n-            return [text]\n-        chars = list(text)\n-        i = 0\n-        start_new_word = True\n-        output = []\n-        while i < len(chars):\n-            char = chars[i]\n-            if _is_punctuation(char):\n-                output.append([char])\n-                start_new_word = True\n-            else:\n-                if start_new_word:\n-                    output.append([])\n-                start_new_word = False\n-                output[-1].append(char)\n-            i += 1\n-\n-        return [\"\".join(x) for x in output]\n-\n-    def _tokenize_chinese_chars(self, text):\n-        \"\"\"Adds whitespace around any CJK character.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if self._is_chinese_char(cp):\n-                output.append(\" \")\n-                output.append(char)\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-    def _is_chinese_char(self, cp):\n-        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n-        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n-        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n-        #\n-        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n-        # despite its name. The modern Korean Hangul alphabet is a different block,\n-        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n-        # space-separated words, so they are not treated specially and handled\n-        # like the all of the other languages.\n-        if (\n-            (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n-            or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n-        ):\n-            return True\n-\n-        return False\n-\n-    def _clean_text(self, text):\n-        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if cp == 0 or cp == 0xFFFD or _is_control(char):\n-                continue\n-            if _is_whitespace(char):\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-\n-class WordpieceTokenizer:\n-    \"\"\"Runs WordPiece tokenization.\"\"\"\n-\n-    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n-        self.vocab = vocab\n-        self.unk_token = unk_token\n-        self.max_input_chars_per_word = max_input_chars_per_word\n-\n-    def tokenize(self, text):\n-        \"\"\"\n-        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n-        tokenization using the given vocabulary.\n-\n-        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n-\n-        Args:\n-            text: A single token or whitespace separated tokens. This should have\n-                already been passed through *BasicTokenizer*.\n-\n-        Returns:\n-            A list of wordpiece tokens.\n-        \"\"\"\n-\n-        output_tokens = []\n-        for token in whitespace_tokenize(text):\n-            chars = list(token)\n-            if len(chars) > self.max_input_chars_per_word:\n-                output_tokens.append(self.unk_token)\n-                continue\n-\n-            is_bad = False\n-            start = 0\n-            sub_tokens = []\n-            while start < len(chars):\n-                end = len(chars)\n-                cur_substr = None\n-                while start < end:\n-                    substr = \"\".join(chars[start:end])\n-                    if start > 0:\n-                        substr = \"##\" + substr\n-                    if substr in self.vocab:\n-                        cur_substr = substr\n-                        break\n-                    end -= 1\n-                if cur_substr is None:\n-                    is_bad = True\n-                    break\n-                sub_tokens.append(cur_substr)\n-                start = end\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"{str(self.cls_token)}:0 $A:0 {str(self.sep_token)}:0\",\n+            pair=f\"{str(self.cls_token)}:0 $A:0 {str(self.sep_token)}:0 $B:1 {str(self.sep_token)}:1\",\n+            special_tokens=[\n+                (str(self.cls_token), cls_token_id),\n+                (str(self.sep_token), sep_token_id),\n+            ],\n+        )\n \n-            if is_bad:\n-                output_tokens.append(self.unk_token)\n-            else:\n-                output_tokens.extend(sub_tokens)\n-        return output_tokens\n \n+__all__ = [\"BertTokenizer\"]\n \n-__all__ = [\"BasicTokenizer\", \"BertTokenizer\", \"WordpieceTokenizer\"]\n+BertTokenizerFast = BertTokenizer"
        },
        {
            "sha": "2cdc6129881be3f88a32ab1ae653c44db2049ece",
            "filename": "src/transformers/models/bert/tokenization_bert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 146,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,146 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Tokenization classes for Bert.\"\"\"\n-\n-import json\n-from typing import Optional\n-\n-from tokenizers import normalizers\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_bert import BertTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class BertTokenizerFast(PreTrainedTokenizerFast):\n-    r\"\"\"\n-    Construct a \"fast\" BERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        clean_text (`bool`, *optional*, defaults to `True`):\n-            Whether or not to clean the text before tokenization by removing any control characters and replacing all\n-            whitespaces by the classic one.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n-            issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n-            The prefix for subwords.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = BertTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=True,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            **kwargs,\n-        )\n-\n-        normalizer_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n-        if (\n-            normalizer_state.get(\"lowercase\", do_lower_case) != do_lower_case\n-            or normalizer_state.get(\"strip_accents\", strip_accents) != strip_accents\n-            or normalizer_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n-        ):\n-            normalizer_class = getattr(normalizers, normalizer_state.pop(\"type\"))\n-            normalizer_state[\"lowercase\"] = do_lower_case\n-            normalizer_state[\"strip_accents\"] = strip_accents\n-            normalizer_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n-            self.backend_tokenizer.normalizer = normalizer_class(**normalizer_state)\n-\n-        self.do_lower_case = do_lower_case\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-\n-        if token_ids_1 is not None:\n-            output += token_ids_1 + [self.sep_token_id]\n-\n-        return output\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"BertTokenizerFast\"]"
        },
        {
            "sha": "ebb733fd2da1ead9d390373507c1ac7a870e7e88",
            "filename": "src/transformers/models/bert/tokenization_bert_legacy.py",
            "status": "renamed",
            "additions": 9,
            "deletions": 13,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_legacy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_legacy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_legacy.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1,5 +1,5 @@\n # coding=utf-8\n-# Copyright 2020 The Google AI Team, Stanford University and The HuggingFace Inc. team.\n+# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -12,13 +12,14 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\"\"\"Tokenization classes for Bert.\"\"\"\n \n import collections\n import os\n import unicodedata\n from typing import Optional\n \n-from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n+from ...tokenization_python import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n from ...utils import logging\n \n \n@@ -27,7 +28,6 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n \n \n-# Copied from transformers.models.bert.tokenization_bert.load_vocab\n def load_vocab(vocab_file):\n     \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n     vocab = collections.OrderedDict()\n@@ -39,7 +39,6 @@ def load_vocab(vocab_file):\n     return vocab\n \n \n-# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize\n def whitespace_tokenize(text):\n     \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n     text = text.strip()\n@@ -49,10 +48,9 @@ def whitespace_tokenize(text):\n     return tokens\n \n \n-# Copied from transformers.models.bert.tokenization_bert.BertTokenizer with Bert->Electra,BERT->Electra\n-class ElectraTokenizer(PreTrainedTokenizer):\n+class BertTokenizerLegacy(PreTrainedTokenizer):\n     r\"\"\"\n-    Construct a Electra tokenizer. Based on WordPiece.\n+    Construct a BERT tokenizer. Based on WordPiece.\n \n     This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n     this superclass for more information regarding those methods.\n@@ -89,7 +87,7 @@ class ElectraTokenizer(PreTrainedTokenizer):\n             [issue](https://github.com/huggingface/transformers/issues/328)).\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original Electra).\n+            value for `lowercase` (as in the original BERT).\n         clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n             Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n             extra spaces.\n@@ -116,7 +114,7 @@ def __init__(\n         if not os.path.isfile(vocab_file):\n             raise ValueError(\n                 f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n-                \" model use `tokenizer = ElectraTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n+                \" model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n             )\n         self.vocab = load_vocab(vocab_file)\n         self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n@@ -190,7 +188,7 @@ def build_inputs_with_special_tokens(\n     ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A Electra sequence has the following format:\n+        adding special tokens. A BERT sequence has the following format:\n \n         - single sequence: `[CLS] X [SEP]`\n         - pair of sequences: `[CLS] A [SEP] B [SEP]`\n@@ -259,7 +257,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n         return (vocab_file,)\n \n \n-# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer\n class BasicTokenizer:\n     \"\"\"\n     Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n@@ -421,7 +418,6 @@ def _clean_text(self, text):\n         return \"\".join(output)\n \n \n-# Copied from transformers.models.bert.tokenization_bert.WordpieceTokenizer\n class WordpieceTokenizer:\n     \"\"\"Runs WordPiece tokenization.\"\"\"\n \n@@ -479,4 +475,4 @@ def tokenize(self, text):\n         return output_tokens\n \n \n-__all__ = [\"ElectraTokenizer\"]\n+__all__ = [\"BasicTokenizer\", \"BertTokenizerLegacy\", \"WordpieceTokenizer\"]",
            "previous_filename": "src/transformers/models/electra/tokenization_electra.py"
        },
        {
            "sha": "eb04da9761ee583fc7c9de5e397edd8be38416c4",
            "filename": "src/transformers/models/bert_generation/tokenization_bert_generation.py",
            "status": "modified",
            "additions": 6,
            "deletions": 80,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,13 +14,9 @@\n # limitations under the License.\n \"\"\"Tokenization class for model BertGeneration.\"\"\"\n \n-import os\n-from shutil import copyfile\n from typing import Any, Optional\n \n-import sentencepiece as spm\n-\n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_utils_sentencepiece import SentencePieceBackend\n from ...utils import logging\n from ...utils.import_utils import requires\n \n@@ -31,7 +27,7 @@\n \n \n @requires(backends=(\"sentencepiece\",))\n-class BertGenerationTokenizer(PreTrainedTokenizer):\n+class BertGenerationTokenizer(SentencePieceBackend):\n     \"\"\"\n     Construct a BertGeneration tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).\n \n@@ -75,6 +71,7 @@ class BertGenerationTokenizer(PreTrainedTokenizer):\n     vocab_files_names = VOCAB_FILES_NAMES\n     prefix_tokens: list[int] = []\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    is_fast = False\n \n     def __init__(\n         self,\n@@ -89,89 +86,18 @@ def __init__(\n     ) -> None:\n         self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n \n-        self.vocab_file = vocab_file\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(vocab_file)\n-\n-        # Add extra_ids to the special token list\n+        # Call parent init (which will load sp_model)\n         super().__init__(\n+            vocab_file=vocab_file,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             unk_token=unk_token,\n             pad_token=pad_token,\n             sep_token=sep_token,\n             sp_model_kwargs=self.sp_model_kwargs,\n+            special_tokens_pattern=\"none\",\n             **kwargs,\n         )\n \n-    @property\n-    def vocab_size(self):\n-        return self.sp_model.get_piece_size()\n-\n-    def get_vocab(self):\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(self.vocab_file)\n-\n-    def _tokenize(self, text: str) -> list[str]:\n-        \"\"\"Take as input a string and return a list of strings (tokens) for words/sub-words\"\"\"\n-        return self.sp_model.encode(text, out_type=str)\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.sp_model.piece_to_id(token)\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        token = self.sp_model.IdToPiece(index)\n-        return token\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        current_sub_tokens = []\n-        out_string = \"\"\n-        for token in tokens:\n-            # make sure that special tokens are not decoded using sentencepiece model\n-            if token in self.all_special_tokens:\n-                out_string += self.sp_model.decode(current_sub_tokens) + token\n-                current_sub_tokens = []\n-            else:\n-                current_sub_tokens.append(token)\n-        out_string += self.sp_model.decode(current_sub_tokens)\n-        return out_string.strip()\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n-\n-        return (out_vocab_file,)\n-\n \n __all__ = [\"BertGenerationTokenizer\"]"
        },
        {
            "sha": "8c816afbbcc1e1753fbeb471e0b0059cb040cdc8",
            "filename": "src/transformers/models/bert_japanese/tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 11,
            "deletions": 61,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,7 +20,7 @@\n import unicodedata\n from typing import Any, Optional\n \n-from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n+from ...tokenization_python import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n from ...utils import is_sentencepiece_available, is_sudachi_projection_available, logging\n \n \n@@ -36,7 +36,6 @@\n SPIECE_UNDERLINE = \"‚ñÅ\"\n \n \n-# Copied from transformers.models.bert.tokenization_bert.load_vocab\n def load_vocab(vocab_file):\n     \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n     vocab = collections.OrderedDict()\n@@ -48,7 +47,6 @@ def load_vocab(vocab_file):\n     return vocab\n \n \n-# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize\n def whitespace_tokenize(text):\n     \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n     text = text.strip()\n@@ -181,6 +179,9 @@ def __init__(\n             mecab_kwargs=mecab_kwargs,\n             sudachi_kwargs=sudachi_kwargs,\n             jumanpp_kwargs=jumanpp_kwargs,\n+            token_type_ids_pattern=\"bert_style\",\n+            token_type_ids_include_special_tokens=True,\n+            special_tokens_pattern=\"cls_sep\",\n             **kwargs,\n         )\n \n@@ -233,7 +234,13 @@ def get_vocab(self):\n             vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n             vocab.update(self.added_tokens_encoder)\n             return vocab\n-        return dict(self.vocab, **self.added_tokens_encoder)\n+        # base vocab\n+        vocab = dict(self.vocab)\n+        # + added_tokens_encoder (only for tokens not in base vocab)\n+        for token, index in self.added_tokens_encoder.items():\n+            if token not in self.vocab:\n+                vocab[token] = index\n+        return vocab\n \n     def _convert_token_to_id(self, token):\n         \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n@@ -254,61 +261,6 @@ def convert_tokens_to_string(self, tokens):\n         out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n         return out_string\n \n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.build_inputs_with_special_tokens\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.get_special_tokens_mask\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if os.path.isdir(save_directory):\n             if self.subword_tokenizer_type == \"sentencepiece\":\n@@ -660,7 +612,6 @@ def tokenize(self, text):\n         return output_tokens\n \n \n-# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer\n class BasicTokenizer:\n     \"\"\"\n     Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n@@ -822,7 +773,6 @@ def _clean_text(self, text):\n         return \"\".join(output)\n \n \n-# Copied from transformers.models.bert.tokenization_bert.WordpieceTokenizer\n class WordpieceTokenizer:\n     \"\"\"Runs WordPiece tokenization.\"\"\"\n "
        },
        {
            "sha": "821b06c56b93269116d8503614d9c15387247416",
            "filename": "src/transformers/models/bertweet/tokenization_bertweet.py",
            "status": "modified",
            "additions": 35,
            "deletions": 104,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,12 +18,11 @@\n import html\n import os\n import re\n-from shutil import copyfile\n from typing import Optional\n \n import regex\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n \n \n@@ -161,87 +160,13 @@ def __init__(\n             unk_token=unk_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n+            # Configure patterns instead of overriding methods\n+            token_type_ids_pattern=\"all_zeros\",  # BERTweet doesn't use token type IDs\n+            token_type_ids_include_special_tokens=True,\n+            special_tokens_pattern=\"cls_double_sep\",  # <s> X </s></s> Y </s>\n             **kwargs,\n         )\n \n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BERTweet sequence has the following format:\n-\n-        - single sequence: `<s> X </s>`\n-        - pair of sequences: `<s> A </s></s> B </s>`\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does\n-        not make use of token type ids, therefore a list of zeros is returned.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n-\n     @property\n     def vocab_size(self):\n         return len(self.encoder)\n@@ -370,35 +295,42 @@ def convert_tokens_to_string(self, tokens):\n         out_string = \" \".join(tokens).replace(\"@@ \", \"\").strip()\n         return out_string\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        out_merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n-\n-        if os.path.abspath(self.merges_file) != os.path.abspath(out_merge_file):\n-            copyfile(self.merges_file, out_merge_file)\n-\n-        return out_vocab_file, out_merge_file\n-\n     # def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n     #     filtered_tokens = ' '.join(self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens))\n     #     tokens_generated_so_far = re.sub('(@@ )', '', string=filtered_tokens)\n     #     tokens_generated_so_far = re.sub('(@@ ?$)', '', string=tokens_generated_so_far)\n     #     return ''.join(tokens_generated_so_far)\n \n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str, ...]:\n+        \"\"\"\n+        Save the vocabulary and merges files to a directory.\n+        \"\"\"\n+        if not os.path.isdir(save_directory):\n+            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n+            return ()\n+\n+        vocab_files_names = getattr(self, \"vocab_files_names\", {})\n+        prefix = f\"{filename_prefix}-\" if filename_prefix else \"\"\n+\n+        # Save vocabulary in the format expected by add_from_file: <token> <id>\n+        # Exclude special tokens (IDs 0-3) as they are added in __init__ before add_from_file\n+        vocab_file = os.path.join(save_directory, prefix + vocab_files_names.get(\"vocab_file\", \"vocab.txt\"))\n+        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n+            for token, token_id in sorted(self.encoder.items(), key=lambda kv: kv[1]):\n+                # Only save tokens with ID >= 4, as IDs 0-3 are reserved for special tokens\n+                if token_id >= 4:\n+                    f.write(f\"{token} {token_id}\\n\")\n+\n+        # Save BPE merges\n+        merge_file = os.path.join(save_directory, prefix + vocab_files_names.get(\"merges_file\", \"bpe.codes\"))\n+        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n+            writer.writelines(\n+                \" \".join(bpe_tokens) + \"\\n\"\n+                for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1])\n+            )\n+\n+        return (vocab_file, merge_file)\n+\n     def add_from_file(self, f):\n         \"\"\"\n         Loads a pre-existing dictionary from a text file and adds its symbols to this instance.\n@@ -622,7 +554,6 @@ def _replace_html_entities(text, keep=(), remove_illegal=True, encoding=\"utf-8\")\n         remove_illegal (bool):\n             If `True`, entities that can't be converted are removed. Otherwise, entities that can't be converted are\n             kept \"as is\".\n-\n     Returns: A unicode string with the entities removed.\n \n     See https://github.com/scrapy/w3lib/blob/master/w3lib/html.py"
        },
        {
            "sha": "52bcbffcff97867e9ab497b78ea47d5213d59575",
            "filename": "src/transformers/models/big_bird/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbig_bird%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbig_bird%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,6 @@\n     from .configuration_big_bird import *\n     from .modeling_big_bird import *\n     from .tokenization_big_bird import *\n-    from .tokenization_big_bird_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "37bc4d6ea013c7a56ea4fd4643c52a4e05033f1a",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird.py",
            "status": "modified",
            "additions": 94,
            "deletions": 219,
            "changes": 313,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1,5 +1,5 @@\n # coding=utf-8\n-# Copyright 2021 Google Research and The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -12,44 +12,49 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"Tokenization classes for BigBird.\"\"\"\n+\"\"\"Tokenization classes for Big Bird model.\"\"\"\n \n-import os\n-import re\n-from shutil import copyfile\n-from typing import Any, Optional\n+from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from tokenizers.models import Unigram\n \n-import sentencepiece as spm\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_python import AddedToken\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n-from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\", \"tokenizer_file\": \"tokenizer.json\"}\n \n-VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n \n+SPIECE_UNDERLINE = \"‚ñÅ\"\n \n-@requires(backends=(\"sentencepiece\",))\n-class BigBirdTokenizer(PreTrainedTokenizer):\n-    \"\"\"\n-    Construct a BigBird tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n+class BigBirdTokenizer(TokenizersBackend):\n+    \"\"\"\n+    Construct a \"fast\" BigBird tokenizer (backed by HuggingFace's *tokenizers* library). Based on\n+    [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models). This\n+    tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should refer to\n+    this superclass for more information regarding those methods\n \n     Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n         unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n         bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The begin of sequence token.\n+            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n+\n+            <Tip>\n+\n+            When building a sequence using special tokens, this is not the token that is used for the beginning of\n+            sequence. The token used is the `cls_token`.\n+\n+            </Tip>\n+\n         eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The end of sequence token.\n+            The end of sequence token. .. note:: When building a sequence using special tokens, this is not the token\n+            that is used for the end of sequence. The token used is the `sep_token`.\n         pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n             The token used for padding, for example when batching sequences of different lengths.\n         sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n@@ -62,21 +67,14 @@ class BigBirdTokenizer(PreTrainedTokenizer):\n         cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n             The classifier token which is used when doing sequence classification (classification of the whole sequence\n             instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        sp_model_kwargs (`dict`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n+        add_prefix_space (`bool`, *optional*, defaults to `True`):\n+            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n+            other word.\n+        vocab_file (`str`, *optional*):\n+            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n+            contains the vocabulary necessary to instantiate a tokenizer.\n+        tokenizer_file (`str`, *optional*):\n+            Path to a tokenizers JSON file containing the serialization of a tokenizer.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -85,219 +83,96 @@ class BigBirdTokenizer(PreTrainedTokenizer):\n \n     def __init__(\n         self,\n-        vocab_file,\n+        vocab=None,\n         unk_token=\"<unk>\",\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         pad_token=\"<pad>\",\n         sep_token=\"[SEP]\",\n         mask_token=\"[MASK]\",\n         cls_token=\"[CLS]\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n+        add_prefix_space=True,\n+        vocab_file=None,\n+        tokenizer_file=None,\n         **kwargs,\n-    ) -> None:\n+    ):\n         bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n         eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n         unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n         pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n         cls_token = AddedToken(cls_token, lstrip=False, rstrip=False) if isinstance(cls_token, str) else cls_token\n         sep_token = AddedToken(sep_token, lstrip=False, rstrip=False) if isinstance(sep_token, str) else sep_token\n-\n-        # Mask token behave like a normal word, i.e. include the space before it\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n \n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-\n+        self.add_prefix_space = add_prefix_space\n         self.vocab_file = vocab_file\n \n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(vocab_file)\n+        # Convert vocab to list of (token, score) tuples\n+        if vocab is None:\n+            vocab_scores = [(str(pad_token), 0.0), (str(eos_token), 0.0), (str(bos_token), 0.0)]\n+        elif isinstance(vocab, dict):\n+            vocab_scores = [(str(token), float(score)) for token, score in vocab.items()]\n+        elif isinstance(vocab, list) and len(vocab) > 0:\n+            if isinstance(vocab[0], (tuple, list)):\n+                vocab_scores = [(str(token), float(score)) for token, score in vocab]\n+            else:\n+                vocab_scores = [(str(token), 0.0) for token in vocab]\n+        else:\n+            vocab_scores = [(str(pad_token), 0.0), (str(eos_token), 0.0), (str(bos_token), 0.0)]\n+\n+        # Find unk_id in vocab\n+        unk_token_content = str(unk_token)\n+        unk_id = next((idx for idx, (token, _) in enumerate(vocab_scores) if token == unk_token_content), None)\n+        if unk_id is None:\n+            unk_id = min(len(vocab_scores), 100)\n+            if len(vocab_scores) > 100:\n+                vocab_scores.insert(100, (unk_token_content, 0.0))\n+            else:\n+                vocab_scores.append((unk_token_content, 0.0))\n+\n+        # Ensure cls_token and sep_token are in vocab\n+        cls_token_str = str(cls_token)\n+        sep_token_str = str(sep_token)\n+        cls_token_id = next((idx for idx, (token, _) in enumerate(vocab_scores) if token == cls_token_str), None)\n+        sep_token_id = next((idx for idx, (token, _) in enumerate(vocab_scores) if token == sep_token_str), None)\n+\n+        if cls_token_id is None:\n+            cls_token_id = len(vocab_scores)\n+            vocab_scores.append((cls_token_str, 0.0))\n+        if sep_token_id is None:\n+            sep_token_id = len(vocab_scores)\n+            vocab_scores.append((sep_token_str, 0.0))\n+\n+        self._tokenizer = Tokenizer(Unigram(vocab_scores, unk_id=unk_id, byte_fallback=False))\n+        self._tokenizer.normalizer = normalizers.Sequence(\n+            [normalizers.Strip(left=False, right=True), normalizers.Replace(Regex(r\" {2,}\"), SPIECE_UNDERLINE)]\n+        )\n+\n+        prepend_scheme = \"always\" if add_prefix_space else \"never\"\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(\n+            replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme, split=True\n+        )\n+        self._tokenizer.decoder = decoders.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme, split=True)\n \n         super().__init__(\n+            tokenizer_object=self._tokenizer,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             unk_token=unk_token,\n             pad_token=pad_token,\n-            sep_token=sep_token,\n             mask_token=mask_token,\n             cls_token=cls_token,\n-            sp_model_kwargs=self.sp_model_kwargs,\n+            sep_token=sep_token,\n             **kwargs,\n         )\n \n-    @property\n-    def vocab_size(self):\n-        return self.sp_model.get_piece_size()\n-\n-    def get_vocab(self):\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n+        self.init_kwargs[\"add_prefix_space\"] = add_prefix_space\n \n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(self.vocab_file)\n-\n-    def _tokenize(self, text: str) -> list[str]:\n-        \"\"\"Take as input a string and return a list of strings (tokens) for words/sub-words\"\"\"\n-        return self.sp_model.encode(text, out_type=str)\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.sp_model.piece_to_id(token)\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        token = self.sp_model.IdToPiece(index)\n-        return token\n-\n-    # Copied from transformers.models.albert.tokenization_albert.AlbertTokenizer.convert_tokens_to_string\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        current_sub_tokens = []\n-        out_string = \"\"\n-        prev_is_special = False\n-        for token in tokens:\n-            # make sure that special tokens are not decoded using sentencepiece model\n-            if token in self.all_special_tokens:\n-                if not prev_is_special:\n-                    out_string += \" \"\n-                out_string += self.sp_model.decode(current_sub_tokens) + token\n-                prev_is_special = True\n-                current_sub_tokens = []\n-            else:\n-                current_sub_tokens.append(token)\n-                prev_is_special = False\n-        out_string += self.sp_model.decode(current_sub_tokens)\n-        return out_string.strip()\n-\n-    def _decode(\n-        self,\n-        token_ids: list[int],\n-        skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: Optional[bool] = None,\n-        spaces_between_special_tokens: bool = True,\n-        **kwargs,\n-    ) -> str:\n-        self._decode_use_source_tokenizer = kwargs.pop(\"use_source_tokenizer\", False)\n-\n-        filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\n-\n-        # To avoid mixing byte-level and unicode for byte-level BPT\n-        # we need to build string separately for added tokens and byte-level tokens\n-        # cf. https://github.com/huggingface/transformers/issues/1133\n-        sub_texts = []\n-        current_sub_text = []\n-        for token in filtered_tokens:\n-            if skip_special_tokens and token in self.all_special_ids:\n-                continue\n-            if token in self.added_tokens_encoder:\n-                if current_sub_text:\n-                    sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n-                    current_sub_text = []\n-                sub_texts.append(token)\n-            else:\n-                current_sub_text.append(token)\n-        if current_sub_text:\n-            sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n-\n-        # Mimic the behavior of the Rust tokenizer:\n-        # No space before [MASK] and [SEP]\n-        if spaces_between_special_tokens:\n-            text = re.sub(r\" (\\[(MASK|SEP)\\])\", r\"\\1\", \" \".join(sub_texts))\n-        else:\n-            text = \"\".join(sub_texts)\n-\n-        clean_up_tokenization_spaces = (\n-            clean_up_tokenization_spaces\n-            if clean_up_tokenization_spaces is not None\n-            else self.clean_up_tokenization_spaces\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"{cls_token_str}:0 $A:0 {sep_token_str}:0\",\n+            pair=f\"{cls_token_str}:0 $A:0 {sep_token_str}:0 $B:1 {sep_token_str}:1\",\n+            special_tokens=[(cls_token_str, cls_token_id), (sep_token_str, sep_token_id)],\n         )\n-        if clean_up_tokenization_spaces:\n-            clean_text = self.clean_up_tokenization(text)\n-            return clean_text\n-        else:\n-            return text\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n-\n-        return (out_vocab_file,)\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A Big Bird sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n \n \n __all__ = [\"BigBirdTokenizer\"]"
        },
        {
            "sha": "6148585a40b10385098c69714f650e1f42c22b4f",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 198,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,198 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for Big Bird model.\"\"\"\n-\n-import os\n-from shutil import copyfile\n-from typing import Optional\n-\n-from ...tokenization_utils import AddedToken\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import is_sentencepiece_available, logging\n-\n-\n-if is_sentencepiece_available():\n-    from .tokenization_big_bird import BigBirdTokenizer\n-else:\n-    BigBirdTokenizer = None\n-\n-logger = logging.get_logger(__name__)\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-SPIECE_UNDERLINE = \"‚ñÅ\"\n-\n-\n-class BigBirdTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" BigBird tokenizer (backed by HuggingFace's *tokenizers* library). Based on\n-    [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models). This\n-    tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods\n-\n-    Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the beginning of\n-            sequence. The token used is the `cls_token`.\n-\n-            </Tip>\n-\n-        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The end of sequence token. .. note:: When building a sequence using special tokens, this is not the token\n-            that is used for the end of sequence. The token used is the `sep_token`.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = BigBirdTokenizer\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    prefix_tokens: list[int] = []\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        unk_token=\"<unk>\",\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        pad_token=\"<pad>\",\n-        sep_token=\"[SEP]\",\n-        mask_token=\"[MASK]\",\n-        cls_token=\"[CLS]\",\n-        **kwargs,\n-    ):\n-        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n-        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n-        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n-        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n-        cls_token = AddedToken(cls_token, lstrip=False, rstrip=False) if isinstance(cls_token, str) else cls_token\n-        sep_token = AddedToken(sep_token, lstrip=False, rstrip=False) if isinstance(sep_token, str) else sep_token\n-\n-        # Mask token behave like a normal word, i.e. include the space before it\n-        mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n-\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            **kwargs,\n-        )\n-\n-        self.vocab_file = vocab_file\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An BigBird sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return cls + token_ids_0 + sep\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of ids.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Set to True if the token list is already formatted with special tokens for the model\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            if token_ids_1 is not None:\n-                raise ValueError(\n-                    \"You should not supply a second sequence if the provided sequence of \"\n-                    \"ids is already formatted with special tokens for the model.\"\n-                )\n-            return [1 if x in [self.sep_token_id, self.cls_token_id] else 0 for x in token_ids_0]\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not self.can_save_slow_tokenizer:\n-            raise ValueError(\n-                \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \"\n-                \"tokenizer.\"\n-            )\n-\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-\n-        return (out_vocab_file,)\n-\n-\n-__all__ = [\"BigBirdTokenizerFast\"]"
        },
        {
            "sha": "28108dba6624db021f8d45f3a23d108290c9ae5c",
            "filename": "src/transformers/models/biogpt/tokenization_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,7 +18,7 @@\n import os\n from typing import Optional\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n \n "
        },
        {
            "sha": "69beb03a4583078aaca1713ea66912f016af1a06",
            "filename": "src/transformers/models/blenderbot/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fblenderbot%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fblenderbot%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,6 @@\n     from .configuration_blenderbot import *\n     from .modeling_blenderbot import *\n     from .tokenization_blenderbot import *\n-    from .tokenization_blenderbot_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "ddec6a48872aebd78c19ca21a0bcbd151ceb2ffa",
            "filename": "src/transformers/models/blenderbot/tokenization_blenderbot.py",
            "status": "modified",
            "additions": 60,
            "deletions": 283,
            "changes": 343,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,14 +14,11 @@\n # limitations under the License.\n \"\"\"Tokenization class for Blenderbot.\"\"\"\n \n-import json\n-import os\n-from functools import lru_cache\n-from typing import Optional\n+from tokenizers import Tokenizer, decoders, pre_tokenizers, processors\n+from tokenizers.models import BPE\n \n-import regex as re\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_utils_base import AddedToken\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n \n@@ -35,61 +32,20 @@\n }\n \n \n-@lru_cache\n-# Copied from transformers.models.roberta.tokenization_roberta.bytes_to_unicode\n-def bytes_to_unicode():\n-    \"\"\"\n-    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n-    characters the bpe code barfs on.\n-\n-    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n-    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n-    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n-    tables between utf-8 bytes and unicode strings.\n-    \"\"\"\n-    bs = (\n-        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¬°\"), ord(\"¬¨\") + 1)) + list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1))\n-    )\n-    cs = bs[:]\n-    n = 0\n-    for b in range(2**8):\n-        if b not in bs:\n-            bs.append(b)\n-            cs.append(2**8 + n)\n-            n += 1\n-    cs = [chr(n) for n in cs]\n-    return dict(zip(bs, cs))\n-\n-\n-# Copied from transformers.models.roberta.tokenization_roberta.get_pairs\n-def get_pairs(word):\n+class BlenderbotTokenizer(TokenizersBackend):\n     \"\"\"\n-    Return set of symbol pairs in a word.\n-\n-    Word is represented as tuple of symbols (symbols being variable-length strings).\n-    \"\"\"\n-    pairs = set()\n-    prev_char = word[0]\n-    for char in word[1:]:\n-        pairs.add((prev_char, char))\n-        prev_char = char\n-    return pairs\n-\n-\n-class BlenderbotTokenizer(PreTrainedTokenizer):\n-    \"\"\"\n-    Constructs a Blenderbot tokenizer, derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding.\n+    Construct a \"fast\" Blenderbot tokenizer (backed by HuggingFace's *tokenizers* library), derived from the GPT-2\n+    tokenizer, using byte-level Byte-Pair-Encoding.\n \n     This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n     be encoded differently whether it is at the beginning of the sentence (without space) or not:\n \n     ```python\n-    >>> from transformers import BlenderbotTokenizer\n+    >>> from transformers import BlenderbotTokenizerFast\n \n-    >>> tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-3B\")\n-    >>> tokenizer.add_prefix_space = False\n+    >>> tokenizer = BlenderbotTokenizerFast.from_pretrained(\"facebook/blenderbot-3B\")\n     >>> tokenizer(\"Hello world\")[\"input_ids\"]\n-    [47, 921, 86, 1085, 2]\n+    [6950, 1085, 2]\n \n     >>> tokenizer(\" Hello world\")[\"input_ids\"]\n     [6950, 1085, 2]\n@@ -100,21 +56,14 @@ class BlenderbotTokenizer(PreTrainedTokenizer):\n \n     <Tip>\n \n-    When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n+    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n \n     </Tip>\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n+    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n-        errors (`str`, *optional*, defaults to `\"replace\"`):\n-            Paradigm to follow when decoding bytes to UTF-8. See\n-            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n         bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n             The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n \n@@ -150,261 +99,89 @@ class BlenderbotTokenizer(PreTrainedTokenizer):\n         mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        add_prefix_space (`bool`, *optional*, defaults to `False`):\n+        add_prefix_space (`bool`, *optional*, defaults to `True`):\n             Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n             other word. (Blenderbot tokenizer detect beginning of words by the preceding space).\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        merges (`list`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from merges_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n \n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.__init__ with Roberta->Blenderbot, RoBERTa->Blenderbot\n     def __init__(\n         self,\n-        vocab_file,\n-        merges_file,\n-        errors=\"replace\",\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         sep_token=\"</s>\",\n         cls_token=\"<s>\",\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n-        add_prefix_space=False,\n+        add_prefix_space=True,\n+        vocab=None,\n+        merges=None,\n         **kwargs,\n     ):\n-        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n-        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n-        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n-        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n-        sep_token = AddedToken(sep_token, lstrip=False, rstrip=False) if isinstance(sep_token, str) else sep_token\n-        cls_token = AddedToken(cls_token, lstrip=False, rstrip=False) if isinstance(cls_token, str) else cls_token\n-\n-        # Mask token behave like a normal word, i.e. include the space before it\n+        self.add_prefix_space = add_prefix_space\n         mask_token = (\n             AddedToken(mask_token, lstrip=True, rstrip=False, normalized=False)\n             if isinstance(mask_token, str)\n             else mask_token\n         )\n \n-        # these special tokens are not part of the vocab.json, let's add them in the correct order\n-\n-        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n-            self.encoder = json.load(vocab_handle)\n-        self.decoder = {v: k for k, v in self.encoder.items()}\n-        self.errors = errors  # how to handle errors in decoding\n-        self.byte_encoder = bytes_to_unicode()\n-        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n-        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n-            bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\n-        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n-        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n-        self.cache = {}\n-        self.add_prefix_space = add_prefix_space\n+        if vocab is not None and merges is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n+            )\n+            self._merges = merges\n+        else:\n+            # Initialize with minimal vocab\n+            self._vocab = {\n+                str(bos_token): 0,\n+                str(pad_token): 1,\n+                str(eos_token): 2,\n+                str(unk_token): 3,\n+                str(mask_token): 4,\n+            }\n+            self._merges = []\n+\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=self._vocab,\n+                merges=self._merges,\n+                dropout=None,\n+                continuing_subword_prefix=\"\",\n+                end_of_word_suffix=\"\",\n+                fuse_unk=False,\n+            )\n+        )\n \n-        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n-        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n+        self._tokenizer.decoder = decoders.ByteLevel()\n+        self._tokenizer.post_processor = processors.RobertaProcessing(\n+            sep=(str(eos_token), self._vocab.get(str(eos_token), 2)),\n+            cls=(str(bos_token), self._vocab.get(str(bos_token), 0)),\n+            add_prefix_space=add_prefix_space,\n+            trim_offsets=True,\n+        )\n+\n+        tokenizer_object = self._tokenizer\n \n         super().__init__(\n-            errors=errors,\n+            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n-            unk_token=unk_token,\n             sep_token=sep_token,\n             cls_token=cls_token,\n+            unk_token=unk_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n             add_prefix_space=add_prefix_space,\n             **kwargs,\n         )\n \n-    @property\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.vocab_size with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def vocab_size(self):\n-        return len(self.encoder)\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.get_vocab with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def get_vocab(self):\n-        vocab = dict(self.encoder).copy()\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.bpe with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def bpe(self, token):\n-        if token in self.cache:\n-            return self.cache[token]\n-        word = tuple(token)\n-        pairs = get_pairs(word)\n-\n-        if not pairs:\n-            return token\n-\n-        while True:\n-            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n-            if bigram not in self.bpe_ranks:\n-                break\n-            first, second = bigram\n-            new_word = []\n-            i = 0\n-            while i < len(word):\n-                try:\n-                    j = word.index(first, i)\n-                except ValueError:\n-                    new_word.extend(word[i:])\n-                    break\n-                else:\n-                    new_word.extend(word[i:j])\n-                    i = j\n-\n-                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n-                    new_word.append(first + second)\n-                    i += 2\n-                else:\n-                    new_word.append(word[i])\n-                    i += 1\n-            new_word = tuple(new_word)\n-            word = new_word\n-            if len(word) == 1:\n-                break\n-            else:\n-                pairs = get_pairs(word)\n-        word = \" \".join(word)\n-        self.cache[token] = word\n-        return word\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer._tokenize with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def _tokenize(self, text):\n-        \"\"\"Tokenize a string.\"\"\"\n-        bpe_tokens = []\n-        for token in re.findall(self.pat, text):\n-            token = \"\".join(\n-                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n-            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n-            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n-        return bpe_tokens\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer._convert_token_to_id with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.encoder.get(token, self.encoder.get(self.unk_token))\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer._convert_id_to_token with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.decoder.get(index)\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.convert_tokens_to_string with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        text = \"\".join(tokens)\n-        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n-        return text\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.save_vocabulary with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n-        )\n-\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n-\n-        index = 0\n-        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(\"#version: 0.2\\n\")\n-            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n-                index += 1\n-\n-        return vocab_file, merge_file\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.get_special_tokens_mask with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.create_token_type_ids_from_sequences with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. Blenderbot does not\n-        make use of token type ids, therefore a list of zeros is returned.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.prepare_for_tokenization with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n-        add_prefix_space = kwargs.pop(\"add_prefix_space\", self.add_prefix_space)\n-        if (is_split_into_words or add_prefix_space) and (len(text) > 0 and not text[0].isspace()):\n-            text = \" \" + text\n-        return (text, kwargs)\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A Blenderbot sequence has the following format:\n-        - single sequence: ` X </s>`\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added\n-            token_ids_1 (`list[int]`, *optional*):\n-                Will be ignored\n-        Returns:\n-            `list[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        return token_ids_0 + [self.eos_token_id]\n-\n \n __all__ = [\"BlenderbotTokenizer\"]"
        },
        {
            "sha": "0b84200e02d5c7f89141686c4e3c63c8d7fefc14",
            "filename": "src/transformers/models/blenderbot/tokenization_blenderbot_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 284,
            "changes": 284,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,284 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 The Facebook Inc. and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Tokenization class for Blenderbot.\"\"\"\n-\n-import json\n-from typing import Optional\n-\n-from tokenizers import processors\n-\n-from ...tokenization_utils_base import AddedToken, BatchEncoding\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_blenderbot import BlenderbotTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-VOCAB_FILES_NAMES = {\n-    \"vocab_file\": \"vocab.json\",\n-    \"merges_file\": \"merges.txt\",\n-    \"tokenizer_config_file\": \"tokenizer_config.json\",\n-}\n-\n-\n-class BlenderbotTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" Blenderbot tokenizer (backed by HuggingFace's *tokenizers* library), derived from the GPT-2\n-    tokenizer, using byte-level Byte-Pair-Encoding.\n-\n-    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n-    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n-\n-    ```python\n-    >>> from transformers import BlenderbotTokenizerFast\n-\n-    >>> tokenizer = BlenderbotTokenizerFast.from_pretrained(\"facebook/blenderbot-3B\")\n-    >>> tokenizer(\"Hello world\")[\"input_ids\"]\n-    [6950, 1085, 2]\n-\n-    >>> tokenizer(\" Hello world\")[\"input_ids\"]\n-    [6950, 1085, 2]\n-    ```\n-\n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n-    call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n-\n-    <Tip>\n-\n-    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n-\n-    </Tip>\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n-        errors (`str`, *optional*, defaults to `\"replace\"`):\n-            Paradigm to follow when decoding bytes to UTF-8. See\n-            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n-        bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the beginning of\n-            sequence. The token used is the `cls_token`.\n-\n-            </Tip>\n-\n-        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The end of sequence token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n-            The token used is the `sep_token`.\n-\n-            </Tip>\n-\n-        sep_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        cls_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        add_prefix_space (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n-            other word. (Blenderbot tokenizer detect beginning of words by the preceding space).\n-        trim_offsets (`bool`, *optional*, defaults to `True`):\n-            Whether the post processing step should trim offsets to avoid including whitespaces.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = BlenderbotTokenizer\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.__init__ with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        errors=\"replace\",\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        sep_token=\"</s>\",\n-        cls_token=\"<s>\",\n-        unk_token=\"<unk>\",\n-        pad_token=\"<pad>\",\n-        mask_token=\"<mask>\",\n-        add_prefix_space=False,\n-        trim_offsets=True,\n-        **kwargs,\n-    ):\n-        mask_token = (\n-            AddedToken(mask_token, lstrip=True, rstrip=False, normalized=False)\n-            if isinstance(mask_token, str)\n-            else mask_token\n-        )\n-        super().__init__(\n-            vocab_file,\n-            merges_file,\n-            tokenizer_file=tokenizer_file,\n-            errors=errors,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            sep_token=sep_token,\n-            cls_token=cls_token,\n-            unk_token=unk_token,\n-            pad_token=pad_token,\n-            mask_token=mask_token,\n-            add_prefix_space=add_prefix_space,\n-            trim_offsets=trim_offsets,\n-            **kwargs,\n-        )\n-\n-        tokenizer_component = \"post_processor\"\n-        tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)\n-        if tokenizer_component_instance:\n-            state = json.loads(tokenizer_component_instance.__getstate__())\n-\n-            # The lists 'sep' and 'cls' must be cased in tuples for the object `post_processor_class`\n-            if \"sep\" in state:\n-                state[\"sep\"] = tuple(state[\"sep\"])\n-            if \"cls\" in state:\n-                state[\"cls\"] = tuple(state[\"cls\"])\n-\n-            changes_to_apply = False\n-\n-            if state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-                state[\"add_prefix_space\"] = add_prefix_space\n-                changes_to_apply = True\n-\n-            if state.get(\"trim_offsets\", trim_offsets) != trim_offsets:\n-                state[\"trim_offsets\"] = trim_offsets\n-                changes_to_apply = True\n-\n-            if changes_to_apply:\n-                component_class = getattr(processors, state.pop(\"type\"))\n-                new_value = component_class(**state)\n-                setattr(self.backend_tokenizer, tokenizer_component, new_value)\n-\n-    @property\n-    # Copied from transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.mask_token with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def mask_token(self) -> str:\n-        \"\"\"\n-        `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not\n-        having been set.\n-\n-        Blenderbot tokenizer has a special mask token to be usable in the fill-mask pipeline. The mask token will greedily\n-        comprise the space before the *<mask>*.\n-        \"\"\"\n-        if self._mask_token is None:\n-            if self.verbose:\n-                logger.error(\"Using mask_token, but it is not set yet.\")\n-            return None\n-        return str(self._mask_token)\n-\n-    @mask_token.setter\n-    def mask_token(self, value):\n-        \"\"\"\n-        Overriding the default behavior of the mask token to have it eat the space before it.\n-\n-        This is needed to preserve backward compatibility with all the previously used models based on Roberta.\n-        \"\"\"\n-        # Mask token behave like a normal word, i.e. include the space before it\n-        # So we set lstrip to True\n-        value = AddedToken(value, lstrip=True, rstrip=False) if isinstance(value, str) else value\n-        self._mask_token = value\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast._batch_encode_plus with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-        assert self.add_prefix_space or not is_split_into_words, (\n-            f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-            \"to use it with pretokenized inputs.\"\n-        )\n-\n-        return super()._batch_encode_plus(*args, **kwargs)\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast._encode_plus with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-\n-        assert self.add_prefix_space or not is_split_into_words, (\n-            f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-            \"to use it with pretokenized inputs.\"\n-        )\n-\n-        return super()._encode_plus(*args, **kwargs)\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.save_vocabulary with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-    # Copied from transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.create_token_type_ids_from_sequences with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. Blenderbot does not\n-        make use of token type ids, therefore a list of zeros is returned.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A Blenderbot sequence has the following format:\n-        - single sequence: ` X </s>`\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added\n-            token_ids_1 (`list[int]`, *optional*):\n-                Will be ignored\n-        Returns:\n-            `list[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        return token_ids_0 + [self.eos_token_id]\n-\n-\n-__all__ = [\"BlenderbotTokenizerFast\"]"
        },
        {
            "sha": "c075e7727e4307e84a8686c901ee8387381d9fe9",
            "filename": "src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,7 +20,7 @@\n \n import regex as re\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n \n \n@@ -96,7 +96,9 @@ def __init__(\n         merges = [tuple(merge.split()) for merge in merges]\n         self.bpe_ranks = dict(zip(merges, range(len(merges))))\n         self.cache = {}\n+\n         super().__init__(unk_token=unk_token, bos_token=bos_token, eos_token=eos_token, pad_token=pad_token, **kwargs)\n+        self.special_tokens_pattern = None\n \n     @property\n     def vocab_size(self) -> int:"
        },
        {
            "sha": "7d905dbbc5b29013823fff3d2232a50c353401ca",
            "filename": "src/transformers/models/blenderbot_small/tokenization_blenderbot_small_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 103,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,103 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021, The Facebook, Inc. and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast tokenization class for BlenderbotSmall.\"\"\"\n-\n-from typing import Optional\n-\n-from tokenizers import ByteLevelBPETokenizer\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_blenderbot_small import BlenderbotSmallTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\n-    \"vocab_file\": \"vocab.json\",\n-    \"merges_file\": \"merges.txt\",\n-    \"tokenizer_config_file\": \"tokenizer_config.json\",\n-}\n-\n-\n-class BlenderbotSmallTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" BlenderbotSmall tokenizer (backed by HuggingFace's *tokenizers* library).\n-\n-    Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = BlenderbotSmallTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        unk_token=\"<|endoftext|>\",\n-        bos_token=\"<|endoftext|>\",\n-        eos_token=\"<|endoftext|>\",\n-        add_prefix_space=False,\n-        trim_offsets=True,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            ByteLevelBPETokenizer(\n-                vocab=vocab_file,\n-                merges=merges_file,\n-                add_prefix_space=add_prefix_space,\n-                trim_offsets=trim_offsets,\n-            ),\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            **kwargs,\n-        )\n-        self.add_prefix_space = add_prefix_space\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        output = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n-        if token_ids_1 is None:\n-            return output\n-\n-        return output + [self.eos_token_id] + token_ids_1 + [self.eos_token_id]\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. BlenderbotSmall\n-        does not make use of token type ids, therefore a list of zeros is returned.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n-\n-\n-__all__ = [\"BlenderbotSmallTokenizerFast\"]"
        },
        {
            "sha": "ba39d13cedcb6ce497397c40fc85159f0bb0af16",
            "filename": "src/transformers/models/bloom/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbloom%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbloom%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,7 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_bloom import *\n     from .modeling_bloom import *\n-    from .tokenization_bloom_fast import *\n+    from .tokenization_bloom import *\n else:\n     import sys\n "
        },
        {
            "sha": "04ed9d701da43d2f141cf997fd59ff055676e888",
            "filename": "src/transformers/models/bloom/tokenization_bloom_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 146,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,146 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for Bloom.\"\"\"\n-\n-from typing import Optional\n-\n-from ...tokenization_utils_base import BatchEncoding\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class BloomTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" Bloom tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n-    Byte-Pair-Encoding.\n-\n-    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n-    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n-\n-    ```python\n-    >>> from transformers import BloomTokenizerFast\n-\n-    >>> tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom\")\n-    >>> tokenizer(\"Hello world\")[\"input_ids\"]\n-    [59414, 8876]\n-\n-    >>> tokenizer(\" Hello world\")[\"input_ids\"]\n-    [86153, 8876]\n-    ```\n-\n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n-    the model was not pretrained this way, it might yield a decrease in performance.\n-\n-    <Tip>\n-\n-    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n-\n-    </Tip>\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n-        errors (`str`, *optional*, defaults to `\"replace\"`):\n-            Paradigm to follow when decoding bytes to UTF-8. See\n-            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n-        unk_token (`str`, *optional*, defaults to `<|endoftext|>`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        bos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n-            The beginning of sequence token.\n-        eos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n-            The end of sequence token.\n-        add_prefix_space (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n-            other word. (Bloom tokenizer detect beginning of words by the preceding space).\n-        trim_offsets (`bool`, *optional*, defaults to `True`):\n-            Whether or not the post-processing step should trim offsets to avoid including whitespaces.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = None\n-    # No `max_model_input_sizes` as BLOOM uses ALiBi positional embeddings\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        unk_token=\"<unk>\",\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        pad_token=\"<pad>\",\n-        add_prefix_space=False,\n-        clean_up_tokenization_spaces=False,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file=vocab_file,\n-            merges_file=merges_file,\n-            tokenizer_file=tokenizer_file,\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            pad_token=pad_token,\n-            add_prefix_space=add_prefix_space,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            **kwargs,\n-        )\n-        # This is a `tokenizers.pre_tokenizers.Sequence`\n-        for pre_tokenizer in self.backend_tokenizer.pre_tokenizer:\n-            if hasattr(pre_tokenizer, \"add_prefix_space\"):\n-                pre_tokenizer.add_prefix_space = add_prefix_space\n-        self.backend_tokenizer.decoder.add_prefix_space = add_prefix_space\n-\n-        self.add_prefix_space = add_prefix_space\n-\n-    def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-        if not (self.add_prefix_space or not is_split_into_words):\n-            raise Exception(\n-                f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True to use it with\"\n-                \" pretokenized inputs.\"\n-            )\n-\n-        return super()._batch_encode_plus(*args, **kwargs)\n-\n-    def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-\n-        if not (self.add_prefix_space or not is_split_into_words):\n-            raise Exception(\n-                f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True to use it with\"\n-                \" pretokenized inputs.\"\n-            )\n-\n-        return super()._encode_plus(*args, **kwargs)\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"BloomTokenizerFast\"]"
        },
        {
            "sha": "68a187b22fb5c0aeec0bc49a9e338df7c8df2902",
            "filename": "src/transformers/models/byt5/tokenization_byt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbyt5%2Ftokenization_byt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fbyt5%2Ftokenization_byt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbyt5%2Ftokenization_byt5.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -17,7 +17,7 @@\n import warnings\n from typing import Optional\n \n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_python import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n \n "
        },
        {
            "sha": "070e8311ae6ff33169a88b589ca9c027c40c274f",
            "filename": "src/transformers/models/camembert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcamembert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcamembert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,6 @@\n     from .configuration_camembert import *\n     from .modeling_camembert import *\n     from .tokenization_camembert import *\n-    from .tokenization_camembert_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "fb0a2af1b7f7ef4ccf60d04cef912884e3197045",
            "filename": "src/transformers/models/camembert/tokenization_camembert.py",
            "status": "modified",
            "additions": 72,
            "deletions": 222,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,38 +14,32 @@\n # limitations under the License\n \"\"\"Tokenization classes for Camembert model.\"\"\"\n \n-import os\n-from shutil import copyfile\n-from typing import Any, Optional\n+from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from tokenizers.models import Unigram\n \n-import sentencepiece as spm\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_python import AddedToken\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n-from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n-VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\"}\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\", \"tokenizer_file\": \"tokenizer.json\"}\n \n \n SPIECE_UNDERLINE = \"‚ñÅ\"\n \n \n-@requires(backends=(\"sentencepiece\",))\n-class CamembertTokenizer(PreTrainedTokenizer):\n+class CamembertTokenizer(TokenizersBackend):\n     \"\"\"\n-    Adapted from [`RobertaTokenizer`] and [`XLNetTokenizer`]. Construct a CamemBERT tokenizer. Based on\n-    [SentencePiece](https://github.com/google/sentencepiece).\n+    Construct a \"fast\" CamemBERT tokenizer (backed by HuggingFace's *tokenizers* library). Adapted from\n+    [`RobertaTokenizer`] and [`XLNetTokenizer`]. Based on\n+    [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n+    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n         bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n             The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n \n@@ -81,243 +75,99 @@ class CamembertTokenizer(PreTrainedTokenizer):\n         mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        additional_special_tokens (`list[str]`, *optional*, defaults to `['<s>NOTUSED', '</s>NOTUSED', '<unk>NOTUSED']`):\n+        additional_special_tokens (`list[str]`, *optional*, defaults to `[\"<s>NOTUSED\", \"</s>NOTUSED\"]`):\n             Additional special tokens used by the tokenizer.\n-        sp_model_kwargs (`dict`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n-\n-    Attributes:\n-        sp_model (`SentencePieceProcessor`):\n-            The *SentencePiece* processor that is used for every conversion (string, tokens and IDs).\n+        add_prefix_space (`bool`, *optional*, defaults to `True`):\n+            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n+            other word.\n+        vocab_file (`str`, *optional*):\n+            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n+            contains the vocabulary necessary to instantiate a tokenizer.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    slow_tokenizer_class = None\n \n     def __init__(\n         self,\n-        vocab_file,\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         sep_token=\"</s>\",\n         cls_token=\"<s>\",\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n-        additional_special_tokens=[\"<s>NOTUSED\", \"</s>NOTUSED\", \"<unk>NOTUSED\"],\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n+        additional_special_tokens=None,\n+        add_prefix_space=True,\n+        vocab_file=None,\n+        vocab=None,\n         **kwargs,\n-    ) -> None:\n-        # Mask token behave like a normal word, i.e. include the space before it\n-        mask_token = (\n-            AddedToken(mask_token, lstrip=True, rstrip=False, normalized=False, special=True)\n-            if isinstance(mask_token, str)\n-            else mask_token\n-        )\n-\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(str(vocab_file))\n+    ):\n         self.vocab_file = vocab_file\n+        self.add_prefix_space = add_prefix_space\n+\n+        mask_token = AddedToken(mask_token, lstrip=True, special=True) if isinstance(mask_token, str) else mask_token\n+\n+        if additional_special_tokens is None:\n+            additional_special_tokens = [\"<s>NOTUSED\", \"</s>NOTUSED\", \"<unk>NOTUSED\"]\n+\n+        if vocab is not None and isinstance(vocab, list):\n+            self._vocab = list(vocab)\n+            unk_index = next(i for i, (tok, _) in enumerate(self._vocab) if tok == str(unk_token))\n+            self._tokenizer = Tokenizer(Unigram(self._vocab, unk_id=unk_index, byte_fallback=False))\n+        else:\n+            self._vocab = [\n+                (\"<s>NOTUSED\", 0.0),\n+                (str(pad_token), 0.0),\n+                (\"</s>NOTUSED\", 0.0),\n+                (str(unk_token), 0.0),\n+                (\"<unk>NOTUSED\", -100),\n+                (str(mask_token), 0.0),\n+            ]\n+            self._tokenizer = Tokenizer(Unigram(self._vocab, unk_id=3, byte_fallback=False))\n+\n+        self._tokenizer.normalizer = normalizers.Sequence(\n+            [\n+                normalizers.Replace(\"\\n\", \" \"),\n+                normalizers.Replace(\"\\r\", \" \"),\n+                normalizers.Replace(\"\\t\", \" \"),\n+                normalizers.Strip(left=False, right=True),\n+                normalizers.Replace(Regex(\" {2,}\"), \"‚ñÅ\"),\n+            ]\n+        )\n \n-        # HACK: These tokens were added by the author for an obscure reason as they were already part of the\n-        # sentencepiece vocabulary (this is the case for <s> and </s> and <unk>).\n-        # In this case it is recommended to properly set the tokens by hand.\n-        self._added_tokens_decoder = {\n-            0: AddedToken(\"<s>NOTUSED\", special=True),\n-            1: AddedToken(pad_token, special=True) if isinstance(pad_token, str) else pad_token,\n-            2: AddedToken(\"</s>NOTUSED\", special=True),\n-            3: AddedToken(unk_token, special=True) if isinstance(unk_token, str) else unk_token,\n-            4: AddedToken(\"<unk>NOTUSED\", special=True),\n-        }\n-\n-        self.fairseq_offset = 4  # 3 tokens are newly added, but the offset starts from 4\n+        prepend_scheme = \"always\" if add_prefix_space else \"never\"\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme)\n+        self._tokenizer.decoder = decoders.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme)\n \n-        # legacy: camemebert is a particular case were we have to make sure `\"<unk>NOTUSED\"` is here\n-        if \"added_tokens_decoder\" in kwargs:\n-            # this is the only class that requires this unfortunately.....\n-            # the reason is that the fast version has a whole.\n-            kwargs[\"added_tokens_decoder\"].update(self._added_tokens_decoder)\n+        tokenizer_object = self._tokenizer\n \n         super().__init__(\n+            tokenizer_object=tokenizer_object,\n             bos_token=bos_token,\n             eos_token=eos_token,\n-            unk_token=unk_token,\n             sep_token=sep_token,\n             cls_token=cls_token,\n+            unk_token=unk_token,\n             pad_token=pad_token,\n             mask_token=mask_token,\n             additional_special_tokens=additional_special_tokens,\n-            sp_model_kwargs=self.sp_model_kwargs,\n+            add_prefix_space=add_prefix_space,\n             **kwargs,\n         )\n \n-    @property\n-    def vocab_size(self):\n-        # The length of the vocabulary without added tokens is len(self.sp_model) but the added tokens are added at the beginning.\n-        return len(self.sp_model)\n-\n-    def get_vocab(self):\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size + self.fairseq_offset)}\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n-\n-    def _tokenize(self, text: str) -> list[str]:\n-        return self.sp_model.encode(text, out_type=str)\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        # specific to camembert, both 3 and 4 point to the unk token.\n-        if self.sp_model.PieceToId(token) == 0:\n-            # Convert sentence piece unk token to fairseq unk token index\n-            return self.unk_token_id\n-        return self.fairseq_offset + self.sp_model.PieceToId(token)\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.sp_model.IdToPiece(index - self.fairseq_offset)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        # TODO decode outputs do not match between fast and slow\n-        current_sub_tokens = []\n-        out_string = \"\"\n-        prev_is_special = False\n-        for token in tokens:\n-            # make sure that special tokens are not decoded using sentencepiece model\n-            if token in self.all_special_tokens:\n-                if not prev_is_special:\n-                    out_string += \" \"\n-                out_string += self.sp_model.decode(current_sub_tokens) + token\n-                prev_is_special = True\n-                current_sub_tokens = []\n-            else:\n-                current_sub_tokens.append(token)\n-                prev_is_special = False\n-        out_string += self.sp_model.decode(current_sub_tokens)\n-        return out_string.strip()\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(self.vocab_file)\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n+        # always adds BOS/EOS with \"</s> </s>\" separator for pairs\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"{self.bos_token} $A {self.eos_token}\",\n+            pair=f\"{self.bos_token} $A {self.eos_token} {self.eos_token} $B {self.eos_token}\",\n+            special_tokens=[\n+                (self.bos_token, self.bos_token_id),\n+                (self.eos_token, self.eos_token_id),\n+            ],\n         )\n \n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n-\n-        return (out_vocab_file,)\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An CamemBERT sequence has the following format:\n-\n-        - single sequence: `<s> X </s>`\n-        - pair of sequences: `<s> A </s></s> B </s>`\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like\n-        RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n-\n \n __all__ = [\"CamembertTokenizer\"]"
        },
        {
            "sha": "423058ed959aeb3e7122fb3730a6d0f1f57b5982",
            "filename": "src/transformers/models/camembert/tokenization_camembert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 197,
            "changes": 197,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,197 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License\n-\"\"\"Fast tokenization classes for Camembert model.\"\"\"\n-\n-import os\n-from shutil import copyfile\n-from typing import Optional\n-\n-from ...tokenization_utils import AddedToken\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import is_sentencepiece_available, logging\n-\n-\n-if is_sentencepiece_available():\n-    from .tokenization_camembert import CamembertTokenizer\n-else:\n-    CamembertTokenizer = None\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"sentencepiece.bpe.model\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-SPIECE_UNDERLINE = \"‚ñÅ\"\n-\n-\n-class CamembertTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" CamemBERT tokenizer (backed by HuggingFace's *tokenizers* library). Adapted from\n-    [`RobertaTokenizer`] and [`XLNetTokenizer`]. Based on\n-    [BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models).\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the beginning of\n-            sequence. The token used is the `cls_token`.\n-\n-            </Tip>\n-\n-        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The end of sequence token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n-            The token used is the `sep_token`.\n-\n-            </Tip>\n-\n-        sep_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        cls_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        additional_special_tokens (`list[str]`, *optional*, defaults to `[\"<s>NOTUSED\", \"</s>NOTUSED\"]`):\n-            Additional special tokens used by the tokenizer.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = CamembertTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        sep_token=\"</s>\",\n-        cls_token=\"<s>\",\n-        unk_token=\"<unk>\",\n-        pad_token=\"<pad>\",\n-        mask_token=\"<mask>\",\n-        additional_special_tokens=[\"<s>NOTUSED\", \"</s>NOTUSED\", \"<unk>NOTUSED\"],\n-        **kwargs,\n-    ):\n-        # Mask token behave like a normal word, i.e. include the space before it. Will have normalized = False\n-        mask_token = AddedToken(mask_token, lstrip=True, special=True) if isinstance(mask_token, str) else mask_token\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            sep_token=sep_token,\n-            cls_token=cls_token,\n-            unk_token=unk_token,\n-            pad_token=pad_token,\n-            mask_token=mask_token,\n-            additional_special_tokens=additional_special_tokens,\n-            **kwargs,\n-        )\n-\n-        self.vocab_file = vocab_file\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An CamemBERT sequence has the following format:\n-\n-        - single sequence: `<s> X </s>`\n-        - pair of sequences: `<s> A </s></s> B </s>`\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like\n-        RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(cls + token_ids_0 + sep) * [0]\n-        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not self.can_save_slow_tokenizer:\n-            raise ValueError(\n-                \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \"\n-                \"tokenizer.\"\n-            )\n-\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-\n-        return (out_vocab_file,)\n-\n-\n-__all__ = [\"CamembertTokenizerFast\"]"
        },
        {
            "sha": "090041a7f84fa534cb648875687171dc38f867be",
            "filename": "src/transformers/models/canine/tokenization_canine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 62,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,9 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for CANINE.\"\"\"\n \n-from typing import Optional\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_python import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n \n \n@@ -112,6 +110,9 @@ def __init__(\n             mask_token=mask_token,\n             add_prefix_space=add_prefix_space,\n             model_max_length=model_max_length,\n+            token_type_ids_pattern=\"all_zeros\",\n+            token_type_ids_include_special_tokens=True,\n+            special_tokens_pattern=\"cls_sep\",\n             **kwargs,\n         )\n \n@@ -150,64 +151,5 @@ def _convert_id_to_token(self, index: int) -> str:\n     def convert_tokens_to_string(self, tokens):\n         return \"\".join(tokens)\n \n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A CANINE sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-\n-        result = cls + token_ids_0 + sep\n-        if token_ids_1 is not None:\n-            result += token_ids_1 + sep\n-        return result\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        result = [1] + ([0] * len(token_ids_0)) + [1]\n-        if token_ids_1 is not None:\n-            result += ([0] * len(token_ids_1)) + [1]\n-        return result\n-\n-    # CanineTokenizer has no vocab file\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None):\n-        return ()\n-\n \n __all__ = [\"CanineTokenizer\"]"
        },
        {
            "sha": "b899e69bc8f2a7c4c43a47627bb989589791a12a",
            "filename": "src/transformers/models/clip/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fclip%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fclip%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -25,7 +25,6 @@\n     from .modeling_clip import *\n     from .processing_clip import *\n     from .tokenization_clip import *\n-    from .tokenization_clip_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "dc6995b2aa95493f824250cabd8eab4b5510707f",
            "filename": "src/transformers/models/clip/tokenization_clip.py",
            "status": "modified",
            "additions": 98,
            "deletions": 450,
            "changes": 548,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,258 +14,29 @@\n # limitations under the License.\n \"\"\"Tokenization classes for CLIP.\"\"\"\n \n-import json\n-import os\n-import unicodedata\n-from functools import lru_cache\n from typing import Optional\n \n-import regex as re\n+from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from tokenizers.models import BPE\n \n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n-VOCAB_FILES_NAMES = {\n-    \"vocab_file\": \"vocab.json\",\n-    \"merges_file\": \"merges.txt\",\n-}\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n \n \n-@lru_cache\n-def bytes_to_unicode():\n+class CLIPTokenizer(TokenizersBackend):\n     \"\"\"\n-    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n-    characters the bpe code barfs on.\n+    Construct a CLIP tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n+    Byte-Pair-Encoding.\n \n-    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n-    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n-    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n-    tables between utf-8 bytes and unicode strings.\n-    \"\"\"\n-    bs = (\n-        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¬°\"), ord(\"¬¨\") + 1)) + list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1))\n-    )\n-    cs = bs[:]\n-    n = 0\n-    for b in range(2**8):\n-        if b not in bs:\n-            bs.append(b)\n-            cs.append(2**8 + n)\n-            n += 1\n-    cs = [chr(n) for n in cs]\n-    return dict(zip(bs, cs))\n-\n-\n-def get_pairs(word):\n-    \"\"\"\n-    Return set of symbol pairs in a word.\n-\n-    Word is represented as tuple of symbols (symbols being variable-length strings).\n-    \"\"\"\n-    pairs = set()\n-    prev_char = word[0]\n-    for char in word[1:]:\n-        pairs.add((prev_char, char))\n-        prev_char = char\n-    return pairs\n-\n-\n-def whitespace_clean(text):\n-    text = re.sub(r\"\\s+\", \" \", text)\n-    text = text.strip()\n-    return text\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize\n-def whitespace_tokenize(text):\n-    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n-    text = text.strip()\n-    if not text:\n-        return []\n-    tokens = text.split()\n-    return tokens\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer\n-class BasicTokenizer:\n-    \"\"\"\n-    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n \n     Args:\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        do_split_on_punc (`bool`, *optional*, defaults to `True`):\n-            In some instances we want to skip the basic punctuation splitting so that later tokenization can capture\n-            the full context of the words, such as contractions.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        do_lower_case=True,\n-        never_split=None,\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        do_split_on_punc=True,\n-    ):\n-        if never_split is None:\n-            never_split = []\n-        self.do_lower_case = do_lower_case\n-        self.never_split = set(never_split)\n-        self.tokenize_chinese_chars = tokenize_chinese_chars\n-        self.strip_accents = strip_accents\n-        self.do_split_on_punc = do_split_on_punc\n-\n-    def tokenize(self, text, never_split=None):\n-        \"\"\"\n-        Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n-\n-        Args:\n-            never_split (`List[str]`, *optional*)\n-                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n-                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n-        \"\"\"\n-        # union() returns a new set by concatenating the two sets.\n-        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n-        text = self._clean_text(text)\n-\n-        # This was added on November 1st, 2018 for the multilingual and Chinese\n-        # models. This is also applied to the English models now, but it doesn't\n-        # matter since the English models were not trained on any Chinese data\n-        # and generally don't have any Chinese data in them (there are Chinese\n-        # characters in the vocabulary because Wikipedia does have some Chinese\n-        # words in the English Wikipedia.).\n-        if self.tokenize_chinese_chars:\n-            text = self._tokenize_chinese_chars(text)\n-        # prevents treating the same character with different unicode codepoints as different characters\n-        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n-        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n-        split_tokens = []\n-        for token in orig_tokens:\n-            if token not in never_split:\n-                if self.do_lower_case:\n-                    token = token.lower()\n-                    if self.strip_accents is not False:\n-                        token = self._run_strip_accents(token)\n-                elif self.strip_accents:\n-                    token = self._run_strip_accents(token)\n-            split_tokens.extend(self._run_split_on_punc(token, never_split))\n-\n-        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n-        return output_tokens\n-\n-    def _run_strip_accents(self, text):\n-        \"\"\"Strips accents from a piece of text.\"\"\"\n-        text = unicodedata.normalize(\"NFD\", text)\n-        output = []\n-        for char in text:\n-            cat = unicodedata.category(char)\n-            if cat == \"Mn\":\n-                continue\n-            output.append(char)\n-        return \"\".join(output)\n-\n-    def _run_split_on_punc(self, text, never_split=None):\n-        \"\"\"Splits punctuation on a piece of text.\"\"\"\n-        if not self.do_split_on_punc or (never_split is not None and text in never_split):\n-            return [text]\n-        chars = list(text)\n-        i = 0\n-        start_new_word = True\n-        output = []\n-        while i < len(chars):\n-            char = chars[i]\n-            if _is_punctuation(char):\n-                output.append([char])\n-                start_new_word = True\n-            else:\n-                if start_new_word:\n-                    output.append([])\n-                start_new_word = False\n-                output[-1].append(char)\n-            i += 1\n-\n-        return [\"\".join(x) for x in output]\n-\n-    def _tokenize_chinese_chars(self, text):\n-        \"\"\"Adds whitespace around any CJK character.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if self._is_chinese_char(cp):\n-                output.append(\" \")\n-                output.append(char)\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-    def _is_chinese_char(self, cp):\n-        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n-        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n-        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n-        #\n-        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n-        # despite its name. The modern Korean Hangul alphabet is a different block,\n-        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n-        # space-separated words, so they are not treated specially and handled\n-        # like the all of the other languages.\n-        if (\n-            (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n-            or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n-        ):\n-            return True\n-\n-        return False\n-\n-    def _clean_text(self, text):\n-        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if cp == 0 or cp == 0xFFFD or _is_control(char):\n-                continue\n-            if _is_whitespace(char):\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-\n-class CLIPTokenizer(PreTrainedTokenizer):\n-    \"\"\"\n-    Construct a CLIP tokenizer. Based on byte-level Byte-Pair-Encoding.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n-        errors (`str`, *optional*, defaults to `\"replace\"`):\n-            Paradigm to follow when decoding bytes to UTF-8. See\n-            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n         unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n@@ -275,245 +46,122 @@ class CLIPTokenizer(PreTrainedTokenizer):\n             The end of sequence token.\n         pad_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n             The token used for padding, for example when batching sequences of different lengths.\n+        vocab (`dict`, *optional*):\n+            Vocabulary dict to use for the tokenizer.\n+        merges (`list`, *optional*):\n+            Merges list to use for the BPE tokenizer.\n+        vocab_file (`str`, *optional*):\n+            Path to the vocabulary file.\n+        merges_file (`str`, *optional*):\n+            Path to the merges file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    slow_tokenizer_class = None\n \n     def __init__(\n         self,\n-        vocab_file,\n-        merges_file,\n-        errors=\"replace\",\n-        unk_token=\"<|endoftext|>\",\n-        bos_token=\"<|startoftext|>\",\n-        eos_token=\"<|endoftext|>\",\n-        pad_token=\"<|endoftext|>\",  # hack to enable padding\n+        unk_token: str = \"<|endoftext|>\",\n+        bos_token: str = \"<|startoftext|>\",\n+        eos_token: str = \"<|endoftext|>\",\n+        pad_token: str = \"<|endoftext|>\",\n+        vocab: Optional[dict] = None,\n+        merges: Optional[list] = None,\n+        vocab_file: Optional[str] = None,\n+        merges_file: Optional[str] = None,\n         **kwargs,\n     ):\n-        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n-        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n-        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n-        try:\n-            import ftfy\n+        self.vocab_file = vocab_file\n+        self.merges_file = merges_file\n+\n+        if vocab is not None:\n+            _vocab = {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n+        else:\n+            _vocab = {\n+                str(bos_token): 0,\n+                str(eos_token): 1,\n+                str(pad_token): 2,\n+            }\n+\n+        if merges is not None:\n+            _merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n+        else:\n+            _merges = []\n+\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=_vocab,\n+                merges=_merges,\n+                dropout=None,\n+                continuing_subword_prefix=\"\",\n+                end_of_word_suffix=\"</w>\",\n+                fuse_unk=False,\n+                unk_token=str(unk_token),\n+            )\n+        )\n+\n+        self._tokenizer.normalizer = normalizers.Sequence(\n+            [normalizers.NFC(), normalizers.Replace(Regex(r\"\\s+\"), \" \"), normalizers.Lowercase()]\n+        )\n+\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+            [\n+                pre_tokenizers.Split(\n+                    Regex(\n+                        r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\"\n+                    ),\n+                    behavior=\"removed\",\n+                    invert=True,\n+                ),\n+                pre_tokenizers.ByteLevel(add_prefix_space=False),\n+            ]\n+        )\n \n-            self.fix_text = ftfy.fix_text\n-        except ImportError:\n-            logger.info(\"ftfy or spacy is not installed using custom BasicTokenizer instead of ftfy.\")\n-            self.nlp = BasicTokenizer(strip_accents=False, do_split_on_punc=False)\n-            self.fix_text = None\n+        self._tokenizer.decoder = decoders.ByteLevel()\n \n-        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n-            self.encoder = json.load(vocab_handle)\n-        self.decoder = {v: k for k, v in self.encoder.items()}\n-        self.errors = errors  # how to handle errors in decoding\n-        self.byte_encoder = bytes_to_unicode()\n-        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n-        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n-            bpe_merges = merges_handle.read().strip().split(\"\\n\")[1 : 49152 - 256 - 2 + 1]\n-        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n-        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n-        self.cache = {\"<|startoftext|>\": \"<|startoftext|>\", \"<|endoftext|>\": \"<|endoftext|>\"}\n+        bos_token_id = _vocab.get(str(bos_token), 0)\n+        eos_token_id = _vocab.get(str(eos_token), 1)\n \n-        self.pat = re.compile(\n-            r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n-            re.IGNORECASE,\n+        self._tokenizer.post_processor = processors.RobertaProcessing(\n+            sep=(str(eos_token), eos_token_id),\n+            cls=(str(bos_token), bos_token_id),\n+            add_prefix_space=False,\n+            trim_offsets=False,\n         )\n \n+        tokenizer_object = self._tokenizer\n+\n         super().__init__(\n-            errors=errors,\n+            tokenizer_object=tokenizer_object,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             pad_token=pad_token,\n             **kwargs,\n         )\n \n-    @property\n-    def vocab_size(self):\n-        return len(self.encoder)\n-\n-    def get_vocab(self):\n-        return dict(self.encoder, **self.added_tokens_encoder)\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A CLIP sequence has the following format:\n-\n-        - single sequence: `<|startoftext|> X <|endoftext|>`\n-\n-        Pairs of sequences are not the expected use case, but they will be handled without a separator.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        bos_token = [self.bos_token_id]\n-        eos_token = [self.eos_token_id]\n-\n-        if token_ids_1 is None:\n-            return bos_token + token_ids_0 + eos_token\n-        return bos_token + token_ids_0 + eos_token + eos_token + token_ids_1 + eos_token\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n+        if hasattr(self, \"_post_init\"):\n+            self._post_init()\n \n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n+    def _post_init(self):\n+        super()._post_init()\n+        self._wrap_decode_method_backend_tokenizer()\n \n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1] + [1] + ([0] * len(token_ids_1)) + [1]\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed. CLIP does not make use of token type ids, therefore a list of\n-        zeros is returned.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        bos_token = [self.bos_token_id]\n-        eos_token = [self.eos_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(bos_token + token_ids_0 + eos_token) * [0]\n-        return len(bos_token + token_ids_0 + eos_token + eos_token + token_ids_1 + eos_token) * [0]\n-\n-    def bpe(self, token):\n-        if token in self.cache:\n-            return self.cache[token]\n-        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n-        pairs = get_pairs(word)\n-\n-        if not pairs:\n-            return token + \"</w>\"\n-\n-        while True:\n-            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n-            if bigram not in self.bpe_ranks:\n-                break\n-            first, second = bigram\n-            new_word = []\n-            i = 0\n-            while i < len(word):\n-                try:\n-                    j = word.index(first, i)\n-                except ValueError:\n-                    new_word.extend(word[i:])\n-                    break\n-                else:\n-                    new_word.extend(word[i:j])\n-                    i = j\n-\n-                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n-                    new_word.append(first + second)\n-                    i += 2\n-                else:\n-                    new_word.append(word[i])\n-                    i += 1\n-            new_word = tuple(new_word)\n-            word = new_word\n-            if len(word) == 1:\n-                break\n-            else:\n-                pairs = get_pairs(word)\n-        word = \" \".join(word)\n-        self.cache[token] = word\n-        return word\n-\n-    def _tokenize(self, text):\n-        \"\"\"Tokenize a string.\"\"\"\n-        bpe_tokens = []\n-        if self.fix_text is None:\n-            text = \" \".join(self.nlp.tokenize(text))\n-        else:\n-            text = whitespace_clean(self.fix_text(text)).lower()\n-\n-        for token in re.findall(self.pat, text):\n-            token = \"\".join(\n-                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n-            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n-            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n-        return bpe_tokens\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.encoder.get(token, self.encoder.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.decoder.get(index)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        text = \"\".join(tokens)\n-        byte_array = bytearray([self.byte_decoder[c] for c in text])\n-        text = byte_array.decode(\"utf-8\", errors=self.errors).replace(\"</w>\", \" \").strip()\n-        return text\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n-        )\n+    # Very ugly hack to enable padding to have a correct decoding see https://github.com/huggingface/tokenizers/issues/872\n+    def _wrap_decode_method_backend_tokenizer(self):\n+        orig_decode_method = self.backend_tokenizer.decode\n \n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n+        ## define this as a local variable to avoid circular reference\n+        ## See: https://github.com/huggingface/transformers/issues/30930\n+        end_of_word_suffix = self.backend_tokenizer.model.end_of_word_suffix\n \n-        index = 0\n-        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(\"#version: 0.2\\n\")\n-            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n-                index += 1\n+        def new_decode_method(*args, **kwargs):\n+            text = orig_decode_method(*args, **kwargs)\n+            text = text.replace(end_of_word_suffix, \" \").strip()\n+            return text\n \n-        return vocab_file, merge_file\n+        self.backend_tokenizer.decode = new_decode_method\n \n \n __all__ = [\"CLIPTokenizer\"]"
        },
        {
            "sha": "c859d4572df76638ab881113ff187e75dd211de0",
            "filename": "src/transformers/models/clip/tokenization_clip_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 164,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,164 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 The Open AI Team Authors and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n-\n-from typing import Optional\n-\n-from tokenizers import pre_tokenizers\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_clip import CLIPTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class CLIPTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" CLIP tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n-    Byte-Pair-Encoding.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`, *optional*):\n-            Path to the vocabulary file.\n-        merges_file (`str`, *optional*):\n-            Path to the merges file.\n-        tokenizer_file (`str`, *optional*):\n-            The path to a tokenizer file to use instead of the vocab file.\n-        unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        bos_token (`str`, *optional*, defaults to `\"<|startoftext|>\"`):\n-            The beginning of sequence token.\n-        eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The end of sequence token.\n-        pad_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = CLIPTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        unk_token=\"<|endoftext|>\",\n-        bos_token=\"<|startoftext|>\",\n-        eos_token=\"<|endoftext|>\",\n-        pad_token=\"<|endoftext|>\",  # hack to enable padding\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            merges_file,\n-            tokenizer_file=tokenizer_file,\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            pad_token=pad_token,\n-            **kwargs,\n-        )\n-\n-        if not isinstance(self.backend_tokenizer.pre_tokenizer, pre_tokenizers.Sequence):\n-            raise TypeError(\n-                \"The `backend_tokenizer` provided does not match the expected format. The CLIP tokenizer has been\"\n-                \" heavily modified from transformers version 4.17.0. You need to convert the tokenizer you are using\"\n-                \" to be compatible with this version.The easiest way to do so is\"\n-                ' `CLIPTokenizerFast.from_pretrained(\"path_to_local_folder_or_hub_repo, from_slow=True)`. If you want'\n-                \" to use your existing tokenizer, you will have to revert to a version prior to 4.17.0 of\"\n-                \" transformers.\"\n-            )\n-        self._wrap_decode_method_backend_tokenizer()\n-\n-    # Very ugly hack to enable padding to have a correct decoding see https://github.com/huggingface/tokenizers/issues/872\n-    def _wrap_decode_method_backend_tokenizer(self):\n-        orig_decode_method = self.backend_tokenizer.decode\n-\n-        ## define this as a local variable to avoid circular reference\n-        ## See: https://github.com/huggingface/transformers/issues/30930\n-        end_of_word_suffix = self.backend_tokenizer.model.end_of_word_suffix\n-\n-        def new_decode_method(*args, **kwargs):\n-            text = orig_decode_method(*args, **kwargs)\n-            text = text.replace(end_of_word_suffix, \" \").strip()\n-            return text\n-\n-        self.backend_tokenizer.decode = new_decode_method\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A CLIP sequence has the following format:\n-\n-        - single sequence: `<|startoftext|> X <|endoftext|>`\n-\n-        Pairs of sequences are not the expected use case, but they will be handled without a separator.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        bos_token = [self.bos_token_id]\n-        eos_token = [self.eos_token_id]\n-\n-        if token_ids_1 is None:\n-            return bos_token + token_ids_0 + eos_token\n-        return bos_token + token_ids_0 + eos_token + eos_token + token_ids_1 + eos_token\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed. CLIP does not make use of token type ids, therefore a list of\n-        zeros is returned.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of zeros.\n-        \"\"\"\n-        bos_token = [self.bos_token_id]\n-        eos_token = [self.eos_token_id]\n-\n-        if token_ids_1 is None:\n-            return len(bos_token + token_ids_0 + eos_token) * [0]\n-        return len(bos_token + token_ids_0 + eos_token + eos_token + token_ids_1 + eos_token) * [0]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"CLIPTokenizerFast\"]"
        },
        {
            "sha": "af7bb334c758cc1892b69d4032cac844684bf7f8",
            "filename": "src/transformers/models/clvp/tokenization_clvp.py",
            "status": "modified",
            "additions": 12,
            "deletions": 43,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,7 @@\n \n import regex as re\n \n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_python import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n from .number_normalizer import EnglishNormalizer\n \n@@ -35,7 +35,6 @@\n \n \n @lru_cache\n-# Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n@@ -60,7 +59,6 @@ def bytes_to_unicode():\n     return dict(zip(bs, cs))\n \n \n-# Copied from transformers.models.gpt2.tokenization_gpt2.get_pairs\n def get_pairs(word):\n     \"\"\"\n     Return set of symbol pairs in a word.\n@@ -160,6 +158,16 @@ def __init__(\n         self.add_eos_token = add_eos_token\n         self._normalizer = None\n \n+        # Set special_tokens_pattern based on add_bos_token and add_eos_token flags\n+        if add_bos_token and add_eos_token:\n+            kwargs[\"special_tokens_pattern\"] = \"bos_eos\"\n+        elif add_bos_token:\n+            kwargs[\"special_tokens_pattern\"] = \"bos\"\n+        elif add_eos_token:\n+            kwargs[\"special_tokens_pattern\"] = \"eos\"\n+        else:\n+            kwargs[\"special_tokens_pattern\"] = \"none\"\n+\n         with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n             self.encoder = json.load(vocab_handle)\n         self.decoder = {v: k for k, v in self.encoder.items()}\n@@ -201,7 +209,6 @@ def normalizer(self):\n     def get_vocab(self):\n         return dict(self.encoder, **self.added_tokens_encoder)\n \n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.bpe\n     def bpe(self, token):\n         if token in self.cache:\n             return self.cache[token]\n@@ -244,7 +251,6 @@ def bpe(self, token):\n         self.cache[token] = word\n         return word\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.build_inputs_with_special_tokens\n     def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n         bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n         eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n@@ -256,39 +262,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.get_special_tokens_mask\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if not self.add_bos_token:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=False\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0))\n-        return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n-\n     def _tokenize(self, text):\n         \"\"\"Tokenize a string.\"\"\"\n         bpe_tokens = []\n@@ -306,25 +279,22 @@ def _tokenize(self, text):\n \n         return bpe_tokens\n \n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id\n     def _convert_token_to_id(self, token):\n         \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n         return self.encoder.get(token, self.encoder.get(self.unk_token))\n \n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_id_to_token\n     def _convert_id_to_token(self, index):\n         \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n         return self.decoder.get(index)\n \n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string\n     def convert_tokens_to_string(self, tokens):\n         \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n         text = \"\".join(tokens)\n         text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n         return text\n \n     def clean_up_tokenization(self, text):\n-        text = \"\".join(text)\n+        text = \"\".join(text) if isinstance(text, list) else text\n         vocab_tokens = list(self.encoder.keys()) + list(self.added_tokens_encoder.keys())\n \n         text = text.replace(\"[SPACE]\", \" \") if \"[SPACE]\" in vocab_tokens else text\n@@ -333,7 +303,6 @@ def clean_up_tokenization(self, text):\n         text = text.replace(self.unk_token, \"\").replace(\"   \", \" \").replace(\"  \", \" \")\n         return text\n \n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.save_vocabulary\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")"
        },
        {
            "sha": "f37e07904132671c84071ddff742528cd7a075e2",
            "filename": "src/transformers/models/code_llama/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcode_llama%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcode_llama%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .tokenization_code_llama import *\n-    from .tokenization_code_llama_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "c2826741e82a2b54a9dd03d2d910afd2dc525411",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama.py",
            "status": "modified",
            "additions": 214,
            "deletions": 284,
            "changes": 498,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1,6 +1,5 @@\n # coding=utf-8\n-# Copyright 2023 MetaAI and the HuggingFace Inc. team. All rights reserved.\n-#\n+# Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -14,23 +13,17 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\"\"\"Tokenization classes for Code LLaMA.\"\"\"\n-\n-import os\n-from shutil import copyfile\n-from typing import Any, Optional\n \n-import sentencepiece as spm\n+from tokenizers import AddedToken, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from tokenizers.models import BPE\n \n-from ...convert_slow_tokenizer import import_protobuf\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n-from ...utils import logging, requires_backends\n-from ...utils.import_utils import requires\n+from ...tokenization_utils_base import _get_prepend_scheme, generate_merges\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\"}\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\", \"tokenizer_file\": \"tokenizer.json\"}\n \n SPIECE_UNDERLINE = \"‚ñÅ\"\n \n@@ -47,34 +40,42 @@\n # fmt: on\n \n \n-@requires(backends=(\"sentencepiece\",))\n-class CodeLlamaTokenizer(PreTrainedTokenizer):\n+class CodeLlamaTokenizer(TokenizersBackend):\n     \"\"\"\n-    Construct a CodeLlama tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as\n-    there is no padding token in the original model.\n+    Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding.\n+\n+    This uses notably ByteFallback and no normalization.\n+\n+    ```python\n+    >>> from transformers import CodeLlamaTokenizer\n+\n+    >>> tokenizer = CodeLlamaTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n+    >>> tokenizer.encode(\"Hello this is a test\")\n+    [1, 15043, 445, 338, 263, 1243]\n+    ```\n \n-    The default configuration match that of\n-    [codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/meta-llama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json)\n+    If you want to change the `bos_token` or the `eos_token`, make sure to specify them when initializing the model, or\n+    call `tokenizer.update_post_processor()` to make sure that the post-processing is correctly done (otherwise the\n+    values of the first token and final token of an encoded sequence will not be correct). For more details, checkout\n+    [post-processors] (https://huggingface.co/docs/tokenizers/api/post-processors) documentation.\n+\n+\n+    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods. The default configuration match that of\n+    [meta-llama/CodeLlama-7b-Instruct-hf](https://huggingface.co/meta-llama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json)\n     which supports prompt infilling.\n \n     Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n+        clean_up_tokenization_spaces (`str`, *optional*, defaults to `False`):\n+            Whether to cleanup spaces after decoding, cleanup consists in removing potential artifacts like extra\n+            spaces.\n         unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n         bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n             The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n         eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n             The end of sequence token.\n-\n-            <Tip>\n-\n-            When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n-            The token used is the `sep_token`.\n-\n-            </Tip>\n-\n         prefix_token (`str`, *optional*, defaults to `\"‚ñÅ<PRE>\"`):\n             Prefix token used for infilling.\n         middle_token (`str`, *optional*, defaults to `\"‚ñÅ<MID>\"`):\n@@ -85,41 +86,33 @@ class CodeLlamaTokenizer(PreTrainedTokenizer):\n             End of text token used for infilling.\n         fill_token (`str`, *optional*, defaults to `\"<FILL_ME>\"`):\n             The token used to split the input between the prefix and suffix.\n-        suffix_first (`bool`, *optional*, defaults to `False`):\n-            Whether the input prompt and suffix should be formatted with the suffix first.\n-        sp_model_kwargs (`dict`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n+        additional_special_tokens (`list[str]`, *optional*):\n+            Additional special tokens used by the tokenizer.\n         add_bos_token (`bool`, *optional*, defaults to `True`):\n             Whether to add a beginning of sequence token at the start of sequences.\n         add_eos_token (`bool`, *optional*, defaults to `False`):\n             Whether to add an end of sequence token at the end of sequences.\n-        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n-            Whether or not to clean up the tokenization spaces.\n-        additional_special_tokens (`list[str]`, *optional*):\n-            Additional special tokens used by the tokenizer.\n         use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n             Whether or not the default system prompt for Llama should be used.\n+        add_prefix_space (`bool`, *optional*):\n+            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n+            other word.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        merges (`list`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from merges_file.\n+        vocab_file (`str`, *optional*):\n+            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a .model extension) that\n+            contains the vocabulary necessary to instantiate a tokenizer.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n+    padding_side = \"left\"\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n \n     def __init__(\n         self,\n-        vocab_file,\n+        clean_up_tokenization_spaces=False,\n         unk_token=\"<unk>\",\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n@@ -128,73 +121,96 @@ def __init__(\n         suffix_token=\"‚ñÅ<SUF>\",\n         eot_token=\"‚ñÅ<EOT>\",\n         fill_token=\"<FILL_ME>\",\n-        suffix_first=False,\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n+        additional_special_tokens=None,\n         add_bos_token=True,\n         add_eos_token=False,\n-        clean_up_tokenization_spaces=False,\n-        additional_special_tokens=None,\n         use_default_system_prompt=False,\n+        add_prefix_space=None,\n+        vocab=None,\n+        merges=None,\n+        vocab_file=None,\n         **kwargs,\n     ):\n-        requires_backends(self, \"protobuf\")\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-        bos_token = AddedToken(bos_token, normalized=False, special=True) if isinstance(bos_token, str) else bos_token\n-        eos_token = AddedToken(eos_token, normalized=False, special=True) if isinstance(eos_token, str) else eos_token\n-        unk_token = AddedToken(unk_token, normalized=False, special=True) if isinstance(unk_token, str) else unk_token\n-\n+        self.add_prefix_space = add_prefix_space if add_prefix_space is not None else True\n         self.use_default_system_prompt = use_default_system_prompt\n-        # mark tokens special to skip them\n+\n         additional_special_tokens = additional_special_tokens or []\n-        for token in [prefix_token, middle_token, suffix_token, eot_token]:\n+        for token in [prefix_token, middle_token, suffix_token, eot_token, fill_token]:\n             additional_special_tokens += [token] if token is not None else []\n \n-        self.vocab_file = vocab_file\n-        self.add_bos_token = add_bos_token\n-        self.add_eos_token = add_eos_token\n-        self._prefix_token = prefix_token\n-        self._middle_token = middle_token\n-        self._suffix_token = suffix_token\n-        self._eot_token = eot_token\n-        self.fill_token = fill_token\n-        self.suffix_first = suffix_first\n-        self.sp_model = self.get_spm_processor()\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n+            )\n+        else:\n+            self._vocab = {\n+                str(unk_token): 0,\n+                str(bos_token): 1,\n+                str(eos_token): 2,\n+            }\n+\n+        filtered_vocab = {\n+            t: i for t, i in self._vocab.items() if t not in {str(eos_token), str(bos_token), str(unk_token)}\n+        }\n+        self._merges = merges if merges is not None else generate_merges(filtered_vocab)\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=self._vocab,\n+                merges=self._merges,\n+                fuse_unk=True,\n+                byte_fallback=True,\n+                dropout=None,\n+                unk_token=str(unk_token),\n+            )\n+        )\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(\n+            replacement=\"‚ñÅ\", prepend_scheme=_get_prepend_scheme(self.add_prefix_space, self), split=False\n+        )\n+\n+        self._tokenizer.decoder = decoders.Sequence(\n+            [decoders.Replace(\"‚ñÅ\", \" \"), decoders.ByteFallback(), decoders.Fuse(), decoders.Strip(content=\" \", left=1)]\n+        )\n \n         super().__init__(\n+            tokenizer_object=self._tokenizer,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n-            unk_token=unk_token,\n             add_bos_token=add_bos_token,\n             add_eos_token=add_eos_token,\n+            use_default_system_prompt=use_default_system_prompt,\n+            add_prefix_space=add_prefix_space,\n             prefix_token=prefix_token,\n             middle_token=middle_token,\n             suffix_token=suffix_token,\n             eot_token=eot_token,\n             fill_token=fill_token,\n-            sp_model_kwargs=self.sp_model_kwargs,\n-            suffix_first=suffix_first,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             additional_special_tokens=additional_special_tokens,\n-            use_default_system_prompt=use_default_system_prompt,\n             **kwargs,\n         )\n \n-    @property\n-    def unk_token_length(self):\n-        return len(self.sp_model.encode(str(self.unk_token)))\n-\n-    def get_spm_processor(self):\n-        tokenizer = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        with open(self.vocab_file, \"rb\") as f:\n-            sp_model = f.read()\n-            model_pb2 = import_protobuf()\n-            model = model_pb2.ModelProto.FromString(sp_model)\n-            normalizer_spec = model_pb2.NormalizerSpec()\n-            normalizer_spec.add_dummy_prefix = False\n-            model.normalizer_spec.MergeFrom(normalizer_spec)\n-            sp_model = model.SerializeToString()\n-            tokenizer.LoadFromSerializedProto(sp_model)\n-        return tokenizer\n+        self._add_bos_token = add_bos_token\n+        self._add_eos_token = add_eos_token\n+        self.vocab_file = vocab_file\n+\n+        self._prefix_token = prefix_token\n+        self._middle_token = middle_token\n+        self._suffix_token = suffix_token\n+        self._eot_token = eot_token\n+        self.fill_token = fill_token\n+\n+        self._post_init()\n+\n+    def _post_init(self):\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=\"first\", split=False)\n+        self._tokenizer.normalizer = None\n+\n+        # This matches LlamaTokenizer's behavior and is needed when loading from vocab/merges\n+        self.add_tokens([AddedToken(token, special=True) for token in self.all_special_tokens])\n+\n+        self.update_post_processor()\n+        super()._post_init()\n \n     @property\n     def prefix_token(self):\n@@ -226,229 +242,143 @@ def suffix_id(self):\n             return None\n         return self.convert_tokens_to_ids(self.suffix_token)\n \n-    @property\n-    def eot_token(self):\n-        return self._eot_token\n-\n     @property\n     def eot_id(self):\n         if self._eot_token is None:\n             return None\n         return self.convert_tokens_to_ids(self.eot_token)\n \n     @property\n-    def vocab_size(self):\n-        \"\"\"Returns vocab size\"\"\"\n-        return self.sp_model.get_piece_size()\n+    def eot_token(self):\n+        return self._eot_token\n+\n+    def set_infilling_processor(self, reset, suffix_first=False, add_special_tokens=True):\n+        \"\"\"\n+        Updates the normalizer to make sure the prompt format for `infilling` is respected. The infilling format is the\n+        following: if suffix_first\n+            \" <PRE> <SUF>{suf} <MID> {pre}\"\n+        else:\n+            \" <PRE> {pre} <SUF>{suf} <MID>\"\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.get_vocab\n-    def get_vocab(self):\n-        \"\"\"Returns vocab as a dict\"\"\"\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n+        If `reset` is set to `True`, the `normalizer` and `post_processor` are reset to their \"normal\" behaviour, which\n+        is to add a prefix space for the normalizer, and add a `bos_token` to the input text for the `post_processor`.\n+        \"\"\"\n+        if reset:\n+            self._tokenizer.normalizer = normalizers.Sequence(\n+                [\n+                    normalizers.Prepend(prepend=\"‚ñÅ\"),\n+                    normalizers.Replace(pattern=\" \", content=\"‚ñÅ\"),\n+                ]\n+            )\n+            self.update_post_processor()\n+            return\n \n-    def tokenize(self, prefix, suffix=None, suffix_first=False, **kwargs) -> list[int]:\n-        # add a prefix space to `prefix`\n-        if self.fill_token is not None and self.fill_token in prefix and suffix is None:\n-            prefix, suffix = prefix.split(self.fill_token)\n+        self._tokenizer.normalizer = normalizers.Replace(pattern=\" \", content=\"‚ñÅ\")\n+        pair = [self.bos_token] if self.add_bos_token and add_special_tokens else []\n+        special_tokens = [(self.bos_token, self.bos_token_id)] if self.add_bos_token and add_special_tokens else []\n+        if suffix_first:\n+            # format as \" <PRE> <SUF>{suf} <MID> {pre}\"\n+            pair += [self.prefix_token, self.suffix_token, \"$B\", self.middle_token, \"$A\"]\n+            special_tokens += [\n+                (self.prefix_token, self.prefix_id),\n+                (self.suffix_token, self.suffix_id),\n+                (self.middle_token, self.middle_id),\n+            ]\n+        else:\n+            # format as \" <PRE> {pre} <SUF>{suf} <MID>\"\n+            pair += [self.prefix_token, \"$A\", self.suffix_token, \"$B\", self.middle_token]\n+            special_tokens += [\n+                (self.prefix_token, self.prefix_id),\n+                (self.suffix_token, self.suffix_id),\n+                (self.middle_token, self.middle_id),\n+            ]\n+\n+        if self.add_eos_token and add_special_tokens:\n+            pair += [self.eos_token]\n+            special_tokens += [(self.eos_token, self.eos_token_id)]\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=\"$A\", pair=pair, special_tokens=special_tokens\n+        )\n \n-        if len(prefix) > 0:\n-            prefix = SPIECE_UNDERLINE + prefix.replace(SPIECE_UNDERLINE, \" \")\n+    def tokenize(self, text, suffix=None, suffix_first=False, **kwargs):\n+        # Handle fill_token splitting\n+        if self.fill_token is not None and self.fill_token in text and suffix is None:\n+            text, suffix = text.split(self.fill_token)\n \n+        # If no suffix, use standard tokenization\n         if suffix is None or len(suffix) < 1:\n-            tokens = super().tokenize(prefix, **kwargs)\n-            if len(tokens) > 1 and tokens[0] == SPIECE_UNDERLINE and tokens[1] in self.all_special_tokens:\n-                tokens = tokens[1:]\n-            return tokens\n-\n-        prefix_tokens = self._tokenize(prefix)  # prefix has an extra `SPIECE_UNDERLINE`\n+            return super().tokenize(text, **kwargs)\n \n+        # Check that infilling tokens are available\n         if None in (self.prefix_id, self.middle_id, self.suffix_id):\n             raise ValueError(\n                 \"The input either includes a `prefix` and a `suffix` used for the infilling task,\"\n                 f\"  or can be split on the {self.fill_token} token, creating a suffix and prefix,\"\n                 \" but the model does not support `infilling`.\"\n             )\n-        suffix_tokens = self._tokenize(suffix)  # make sure CodeLlama sp model does not mess up\n-\n-        suffix_first = suffix_first if suffix_first is not None else self.suffix_first\n-        if suffix_first:\n-            # format as \" <PRE> <SUF>{suf} <MID> {pre}\"\n-            return [self.prefix_token, self.suffix_token] + suffix_tokens + [self.middle_token] + prefix_tokens\n-        else:\n-            # format as \" <PRE> {pre} <SUF>{suf} <MID>\"\n-            return [self.prefix_token] + prefix_tokens + [self.suffix_token] + suffix_tokens + [self.middle_token]\n \n-    def _tokenize(self, text, **kwargs):\n-        \"\"\"\n-        Returns a tokenized string.\n+        # Temporarily set infilling processor\n+        self.set_infilling_processor(False, suffix_first=suffix_first, add_special_tokens=False)\n \n-        We de-activated the `add_dummy_prefix` option, thus the sentencepiece internals will always strip any\n-        SPIECE_UNDERLINE. For example: `self.sp_model.encode(f\"{SPIECE_UNDERLINE}Hey\", out_type = str)` will give\n-        `['H', 'e', 'y']` instead of `['‚ñÅHe', 'y']`. Thus we always encode `f\"{unk_token}text\"` and strip the\n-        `unk_token`. Here is an example with `unk_token = \"<unk>\"` and `unk_token_length = 4`.\n-        `self.tokenizer.sp_model.encode(\"<unk> Hey\", out_type = str)[4:]`.\n-        \"\"\"\n-        tokens = self.sp_model.encode(text, out_type=str)\n-        if not text.startswith((SPIECE_UNDERLINE, \" \")):\n-            return tokens\n-        # 1. Encode string + prefix ex: \"<unk> Hey\"\n-        tokens = self.sp_model.encode(self.unk_token + text, out_type=str)\n-        # 2. Remove self.unk_token from ['<','unk','>', '‚ñÅHey']\n-        return tokens[self.unk_token_length :] if len(tokens) >= self.unk_token_length else tokens\n-\n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer._convert_token_to_id\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.sp_model.piece_to_id(token)\n-\n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer._convert_id_to_token\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        token = self.sp_model.IdToPiece(index)\n-        return token\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        # since we manually add the prefix space, we have to remove it when decoding\n-        if tokens[0].startswith(SPIECE_UNDERLINE):\n-            tokens[0] = tokens[0][1:]\n-\n-        current_sub_tokens = []\n-        out_string = \"\"\n-        for _, token in enumerate(tokens):\n-            # make sure that special tokens are not decoded using sentencepiece model\n-            if token in self.all_special_tokens:\n-                out_string += self.sp_model.decode(current_sub_tokens) + token\n-                current_sub_tokens = []\n-            else:\n-                current_sub_tokens.append(token)\n-        out_string += self.sp_model.decode(current_sub_tokens)\n-        return out_string\n-\n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.save_vocabulary\n-    def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        \"\"\"\n-        Save the vocabulary and special tokens file to a directory.\n+        # Remove text_pair and pair from kwargs if present to avoid conflict\n+        kwargs.pop(\"text_pair\", None)\n+        kwargs.pop(\"pair\", None)\n \n-        Args:\n-            save_directory (`str`):\n-                The directory in which to save the vocabulary.\n+        # Tokenize with infilling format\n+        # The processor will handle the special token arrangement\n+        # Use pair=suffix (not text_pair) since base class tokenize expects 'pair' parameter\n+        result = super().tokenize(\" \" + text, pair=suffix, **kwargs)\n \n-        Returns:\n-            `Tuple(str)`: Paths to the files saved.\n-        \"\"\"\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n+        # Reset processor\n+        self.set_infilling_processor(True)\n \n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n+        return result\n \n-        return (out_vocab_file,)\n+    def _encode_plus(self, text, text_pair=None, suffix=None, suffix_first=False, add_special_tokens=True, **kwargs):\n+        is_infilling = False\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.build_inputs_with_special_tokens\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n-        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n+        if suffix is not None:\n+            text_pair = suffix\n+            is_infilling = True\n+        elif \"suffix\" in kwargs:\n+            text_pair = kwargs.pop(\"suffix\")\n+            is_infilling = True\n \n-        output = bos_token_id + token_ids_0 + eos_token_id\n+        if isinstance(text, str) and self.fill_token is not None and self.fill_token in text and text_pair is None:\n+            text, text_pair = text.split(self.fill_token)\n+            is_infilling = True\n \n-        if token_ids_1 is not None:\n-            output = output + bos_token_id + token_ids_1 + eos_token_id\n+        if not is_infilling:\n+            return super()._encode_plus(text, text_pair=text_pair, add_special_tokens=add_special_tokens, **kwargs)\n \n-        return output\n+        if (\n+            text_pair is None\n+            or (isinstance(text_pair, str) and len(text_pair) < 1)\n+            or (isinstance(text_pair, list) and len(text_pair) == 0)\n+        ):\n+            return super()._encode_plus(text, text_pair=text_pair, add_special_tokens=add_special_tokens, **kwargs)\n \n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.get_special_tokens_mask\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n+        if None in (self.prefix_id, self.middle_id, self.suffix_id):\n+            raise ValueError(\n+                \"The input includes a `prefix` and a `suffix` used for the infilling task,\"\n+                \" the `prefix_id, middle_id, suffix_id` must all be initialized. Current\"\n+                f\" values : {self.prefix_id, self.middle_id, self.suffix_id}\"\n             )\n \n-        bos_token_id = [1] if self.add_bos_token else []\n-        eos_token_id = [1] if self.add_eos_token else []\n-\n-        if token_ids_1 is None:\n-            return bos_token_id + ([0] * len(token_ids_0)) + eos_token_id\n-        return (\n-            bos_token_id\n-            + ([0] * len(token_ids_0))\n-            + eos_token_id\n-            + bos_token_id\n-            + ([0] * len(token_ids_1))\n-            + eos_token_id\n-        )\n-\n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.create_token_type_ids_from_sequences\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        if token_ids_1 is None, only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of ids.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n-        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n-\n-        output = [0] * len(bos_token_id + token_ids_0 + eos_token_id)\n-\n-        if token_ids_1 is not None:\n-            output += [1] * len(bos_token_id + token_ids_1 + eos_token_id)\n+        self.set_infilling_processor(False, suffix_first=suffix_first, add_special_tokens=add_special_tokens)\n+        kwargs.pop(\"text_pair\", None)\n \n-        return output\n+        if isinstance(text, str):\n+            text = \" \" + text\n+        elif isinstance(text, list):\n+            text = [\" \" + t if isinstance(t, str) else t for t in text]\n \n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        state[\"sp_model_proto\"] = self.sp_model.serialized_model_proto()\n-        return state\n+        result = super()._encode_plus(text, text_pair=text_pair, add_special_tokens=True, **kwargs)\n+        self.set_infilling_processor(True)\n+        return result\n \n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n \n+__all__ = [\"CodeLlamaTokenizer\", \"CodeLlamaTokenizerFast\"]\n \n-__all__ = [\"CodeLlamaTokenizer\"]\n+# Backward alias\n+CodeLlamaTokenizerFast = CodeLlamaTokenizer"
        },
        {
            "sha": "b3978587e7f02512a5344f9ad0a33bf86b839757",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 374,
            "changes": 374,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,374 +0,0 @@\n-# coding=utf-8\n-# Copyright 2023 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import os\n-from shutil import copyfile\n-from typing import Optional\n-\n-from tokenizers import normalizers, processors\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import is_sentencepiece_available, logging\n-\n-\n-if is_sentencepiece_available():\n-    from .tokenization_code_llama import CodeLlamaTokenizer\n-else:\n-    CodeLlamaTokenizer = None\n-\n-logger = logging.get_logger(__name__)\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-SPIECE_UNDERLINE = \"‚ñÅ\"\n-\n-\n-B_INST, E_INST = \"[INST]\", \"[/INST]\"\n-B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n-\n-# fmt: off\n-DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your \\\n-answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\\\n- that your responses are socially unbiased and positive in nature.\n-\n-If a question does not make any sense, or is not factually coherent, explain why instead of answering something not \\\n-correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n-# fmt: on\n-\n-\n-class CodeLlamaTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding.\n-\n-    This uses notably ByteFallback and no normalization.\n-\n-    ```python\n-    >>> from transformers import CodeLlamaTokenizerFast\n-\n-    >>> tokenizer = CodeLlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n-    >>> tokenizer.encode(\"Hello this is a test\")\n-    [1, 15043, 445, 338, 263, 1243]\n-    ```\n-\n-    If you want to change the `bos_token` or the `eos_token`, make sure to specify them when initializing the model, or\n-    call `tokenizer.update_post_processor()` to make sure that the post-processing is correctly done (otherwise the\n-    values of the first token and final token of an encoded sequence will not be correct). For more details, checkout\n-    [post-processors] (https://huggingface.co/docs/tokenizers/api/post-processors) documentation.\n-\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods. The default configuration match that of\n-    [meta-llama/CodeLlama-7b-Instruct-hf](https://huggingface.co/meta-llama/CodeLlama-7b-Instruct-hf/blob/main/tokenizer_config.json)\n-    which supports prompt infilling.\n-\n-    Args:\n-        vocab_file (`str`, *optional*):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a .model extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        tokenizer_file (`str`, *optional*):\n-            [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\n-            contains everything needed to load the tokenizer.\n-        clean_up_tokenization_spaces (`str`, *optional*, defaults to `False`):\n-            Whether to cleanup spaces after decoding, cleanup consists in removing potential artifacts like extra\n-            spaces.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The end of sequence token.\n-        prefix_token (`str`, *optional*, defaults to `\"‚ñÅ<PRE>\"`):\n-            Prefix token used for infilling.\n-        middle_token (`str`, *optional*, defaults to `\"‚ñÅ<MID>\"`):\n-            Middle token used for infilling.\n-        suffix_token (`str`, *optional*, defaults to `\"‚ñÅ<SUF>\"`):\n-            Suffix token used for infilling.\n-        eot_token (`str`, *optional*, defaults to `\"‚ñÅ<EOT>\"`):\n-            End of text token used for infilling.\n-        fill_token (`str`, *optional*, defaults to `\"<FILL_ME>\"`):\n-            The token used to split the input between the prefix and suffix.\n-        additional_special_tokens (`list[str]`, *optional*):\n-            Additional special tokens used by the tokenizer.\n-        add_bos_token (`bool`, *optional*, defaults to `True`):\n-            Whether to add a beginning of sequence token at the start of sequences.\n-        add_eos_token (`bool`, *optional*, defaults to `False`):\n-            Whether to add an end of sequence token at the end of sequences.\n-        use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n-            Whether or not the default system prompt for Llama should be used.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = CodeLlamaTokenizer\n-    padding_side = \"left\"\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        clean_up_tokenization_spaces=False,\n-        unk_token=\"<unk>\",\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        prefix_token=\"‚ñÅ<PRE>\",\n-        middle_token=\"‚ñÅ<MID>\",\n-        suffix_token=\"‚ñÅ<SUF>\",\n-        eot_token=\"‚ñÅ<EOT>\",\n-        fill_token=\"<FILL_ME>\",\n-        additional_special_tokens=None,\n-        add_bos_token=True,\n-        add_eos_token=False,\n-        use_default_system_prompt=False,\n-        **kwargs,\n-    ):\n-        # mark tokens special to skip them\n-        additional_special_tokens = additional_special_tokens or []\n-        for token in [prefix_token, middle_token, suffix_token, eot_token]:\n-            additional_special_tokens += [token] if token is not None else []\n-        self.use_default_system_prompt = use_default_system_prompt\n-\n-        super().__init__(\n-            vocab_file=vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            additional_special_tokens=additional_special_tokens,\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n-            prefix_token=prefix_token,\n-            middle_token=middle_token,\n-            suffix_token=suffix_token,\n-            eot_token=eot_token,\n-            fill_token=fill_token,\n-            use_default_system_prompt=use_default_system_prompt,\n-            **kwargs,\n-        )\n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = add_eos_token\n-        self.update_post_processor()\n-\n-        self.vocab_file = vocab_file\n-\n-        self._prefix_token = prefix_token\n-        self._middle_token = middle_token\n-        self._suffix_token = suffix_token\n-        self._eot_token = eot_token\n-        self.fill_token = fill_token\n-\n-    # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.update_post_processor\n-    def update_post_processor(self):\n-        \"\"\"\n-        Updates the underlying post processor with the current `bos_token` and `eos_token`.\n-        \"\"\"\n-        bos = self.bos_token\n-        bos_token_id = self.bos_token_id\n-        if bos is None and self.add_bos_token:\n-            raise ValueError(\"add_bos_token = True but bos_token = None\")\n-\n-        eos = self.eos_token\n-        eos_token_id = self.eos_token_id\n-        if eos is None and self.add_eos_token:\n-            raise ValueError(\"add_eos_token = True but eos_token = None\")\n-\n-        single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n-        pair = f\"{single}{(' ' + bos + ':1') if self.add_bos_token else ''} $B:1{(' ' + eos + ':1') if self.add_eos_token else ''}\"\n-\n-        special_tokens = []\n-        if self.add_bos_token:\n-            special_tokens.append((bos, bos_token_id))\n-        if self.add_eos_token:\n-            special_tokens.append((eos, eos_token_id))\n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=single, pair=pair, special_tokens=special_tokens\n-        )\n-\n-    @property\n-    def prefix_token(self):\n-        return self._prefix_token\n-\n-    @property\n-    def prefix_id(self):\n-        if self._prefix_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.prefix_token)\n-\n-    @property\n-    def middle_token(self):\n-        return self._middle_token\n-\n-    @property\n-    def middle_id(self):\n-        if self._middle_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.middle_token)\n-\n-    @property\n-    def suffix_token(self):\n-        return self._suffix_token\n-\n-    @property\n-    def suffix_id(self):\n-        if self._suffix_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.suffix_token)\n-\n-    @property\n-    def eot_id(self):\n-        if self._eot_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.eot_token)\n-\n-    @property\n-    def eot_token(self):\n-        return self._eot_token\n-\n-    @property\n-    def add_eos_token(self):\n-        return self._add_eos_token\n-\n-    @property\n-    def add_bos_token(self):\n-        return self._add_bos_token\n-\n-    @add_eos_token.setter\n-    def add_eos_token(self, value):\n-        self._add_eos_token = value\n-        self.update_post_processor()\n-\n-    @add_bos_token.setter\n-    def add_bos_token(self, value):\n-        self._add_bos_token = value\n-        self.update_post_processor()\n-\n-    def set_infilling_processor(self, reset, suffix_first=False, add_special_tokens=True):\n-        \"\"\"\n-        Updates the normalizer to make sure the prompt format for `infilling` is respected. The infilling format is the\n-        following: if suffix_first\n-            \" <PRE> <SUF>{suf} <MID> {pre}\"\n-        else:\n-            \" <PRE> {pre} <SUF>{suf} <MID>\"\n-\n-        If `reset` is set to `True`, the `normalizer` and `post_processor` are reset to their \"normal\" behaviour, which\n-        is to add a prefix space for the normalizer, and add a `bos_token` to the input text for the `post_processor`.\n-        \"\"\"\n-        if reset:\n-            self._tokenizer.normalizer = normalizers.Sequence(\n-                [\n-                    normalizers.Prepend(prepend=\"‚ñÅ\"),\n-                    normalizers.Replace(pattern=\" \", content=\"‚ñÅ\"),\n-                ]\n-            )\n-            self.update_post_processor()\n-            return\n-\n-        self._tokenizer.normalizer = normalizers.Replace(pattern=\" \", content=\"‚ñÅ\")\n-        pair = [self.bos_token] if self.add_bos_token and add_special_tokens else []\n-        special_tokens = [(self.bos_token, self.bos_token_id)] if self.add_bos_token and add_special_tokens else []\n-        if suffix_first:\n-            # format as \" <PRE> <SUF>{suf} <MID> {pre}\"\n-            pair += [self.prefix_token, self.suffix_token, \"$B\", self.middle_token, \"$A\"]\n-            special_tokens += [\n-                (self.prefix_token, self.prefix_id),\n-                (self.suffix_token, self.suffix_id),\n-                (self.middle_token, self.middle_id),\n-            ]\n-        else:\n-            # format as \" <PRE> {pre} <SUF>{suf} <MID>\"\n-            pair += [self.prefix_token, \"$A\", self.suffix_token, \"$B\", self.middle_token]\n-            special_tokens += [\n-                (self.prefix_token, self.prefix_id),\n-                (self.suffix_token, self.suffix_id),\n-                (self.middle_token, self.middle_id),\n-            ]\n-\n-        if self.add_eos_token and add_special_tokens:\n-            pair += [self.eos_token]\n-            special_tokens += [(self.eos_token, self.eos_token_id)]\n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=\"$A\", pair=pair, special_tokens=special_tokens\n-        )\n-\n-    def encode_plus(self, text, text_pair=None, suffix_first=False, add_special_tokens=True, **kwargs):\n-        # hack to make sure the input is pre-process but outside rust\n-        text_pair = kwargs.pop(\"suffix\", text_pair)\n-        if self.fill_token is not None and self.fill_token in text and text_pair is None:\n-            text, text_pair = text.split(self.fill_token)\n-\n-        if text_pair is None or len(text_pair) < 1:\n-            return super().encode_plus(text, text_pair, add_special_tokens=add_special_tokens, **kwargs)\n-\n-        if None in (self.prefix_id, self.middle_id, self.suffix_id):\n-            raise ValueError(\n-                \"Then input includes a `prefix` and a `suffix` used for the infilling task,\"\n-                \" the `prefix_id, middle_id, suffix_id` must all be initialized. Current\"\n-                f\" values : {self.prefix_id, self.middle_id, self.suffix_id}\"\n-            )\n-\n-        self.set_infilling_processor(False, suffix_first=suffix_first, add_special_tokens=add_special_tokens)\n-        tokens = super().encode_plus(\" \" + text, text_pair=text_pair, add_special_tokens=True, **kwargs)\n-        self.set_infilling_processor(True)\n-        return tokens\n-\n-    # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not self.can_save_slow_tokenizer:\n-            raise ValueError(\n-                \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \"\n-                \"tokenizer.\"\n-            )\n-\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-\n-        return (out_vocab_file,)\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. The special tokens depend on calling set_lang.\n-\n-        An NLLB sequence has the following format, where `X` represents the sequence:\n-\n-        - `input_ids` (for encoder) `X [eos, src_lang_code]`\n-        - `decoder_input_ids`: (for decoder) `X [eos, tgt_lang_code]`\n-\n-        BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a\n-        separator.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return self.bos_token_id + token_ids_0 + self.eos_token_id\n-        return self.bos_token_id + token_ids_0 + token_ids_1 + self.eos_token_id\n-\n-\n-__all__ = [\"CodeLlamaTokenizerFast\"]"
        },
        {
            "sha": "a06b0c4883f5e9d98469acdb0edad8f5b4b67fec",
            "filename": "src/transformers/models/codegen/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcodegen%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcodegen%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,10 +18,10 @@\n \n \n if TYPE_CHECKING:\n+    from ..gpt2.tokenization_gpt2 import GPT2Tokenizer as CodeGenTokenizerFast\n     from .configuration_codegen import *\n     from .modeling_codegen import *\n     from .tokenization_codegen import *\n-    from .tokenization_codegen_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "8bcb18d3414c030821663ad189c18d7703f6cd92",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 69,
            "deletions": 211,
            "changes": 280,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -12,75 +12,33 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"Tokenization classes for CodeGen\"\"\"\n+\"\"\"Tokenization classes for CodeGen.\"\"\"\n \n-import json\n-import os\n-from functools import lru_cache\n+import re\n from typing import TYPE_CHECKING, Optional, Union\n \n import numpy as np\n-import regex as re\n+from tokenizers import Tokenizer, decoders, pre_tokenizers, processors\n+from tokenizers.models import BPE\n \n-from ...utils import logging, to_py_obj\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n+from ...utils import is_torch_available, logging\n \n \n if TYPE_CHECKING:\n-    import torch\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+    if is_torch_available():\n+        import torch\n \n \n logger = logging.get_logger(__name__)\n \n-VOCAB_FILES_NAMES = {\n-    \"vocab_file\": \"vocab.json\",\n-    \"merges_file\": \"merges.txt\",\n-}\n-\n-\n-@lru_cache\n-def bytes_to_unicode():\n-    \"\"\"\n-    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n-    characters the bpe code barfs on.\n-\n-    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n-    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n-    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n-    tables between utf-8 bytes and unicode strings.\n-    \"\"\"\n-    bs = (\n-        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¬°\"), ord(\"¬¨\") + 1)) + list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1))\n-    )\n-    cs = bs[:]\n-    n = 0\n-    for b in range(2**8):\n-        if b not in bs:\n-            bs.append(b)\n-            cs.append(2**8 + n)\n-            n += 1\n-    cs = [chr(n) for n in cs]\n-    return dict(zip(bs, cs))\n-\n-\n-def get_pairs(word):\n-    \"\"\"\n-    Return set of symbol pairs in a word.\n-\n-    Word is represented as tuple of symbols (symbols being variable-length strings).\n-    \"\"\"\n-    pairs = set()\n-    prev_char = word[0]\n-    for char in word[1:]:\n-        pairs.add((prev_char, char))\n-        prev_char = char\n-    return pairs\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\"}\n \n \n-class CodeGenTokenizer(PreTrainedTokenizer):\n+class CodeGenTokenizer(TokenizersBackend):\n     \"\"\"\n-    Construct a CodeGen tokenizer. Based on byte-level Byte-Pair-Encoding.\n+    Construct a CodeGen tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n+    Byte-Pair-Encoding.\n \n     This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n     be encoded differently whether it is at the beginning of the sentence (without space) or not:\n@@ -96,26 +54,19 @@ class CodeGenTokenizer(PreTrainedTokenizer):\n     [18435, 995]\n     ```\n \n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n-    call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n+    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n+    the model was not pretrained this way, it might yield a decrease in performance.\n \n     <Tip>\n \n-    When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n+    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n \n     </Tip>\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n-        errors (`str`, *optional*, defaults to `\"replace\"`):\n-            Paradigm to follow when decoding bytes to UTF-8. See\n-            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n         unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n@@ -129,54 +80,75 @@ class CodeGenTokenizer(PreTrainedTokenizer):\n             Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n             other word. (CodeGen tokenizer detect beginning of words by the preceding space).\n         add_bos_token (`bool`, *optional*, defaults to `False`):\n-            Whether to add a beginning of sequence token at the start of sequences.\n+            Whether or not to add an initial beginning of sentence token to the input.\n         return_token_type_ids (`bool`, *optional*, defaults to `False`):\n             Whether to return token type IDs.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        merges (`list`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from merges_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    slow_tokenizer_class = None\n \n     def __init__(\n         self,\n-        vocab_file,\n-        merges_file,\n-        errors=\"replace\",\n         unk_token=\"<|endoftext|>\",\n         bos_token=\"<|endoftext|>\",\n         eos_token=\"<|endoftext|>\",\n         pad_token=None,\n         add_prefix_space=False,\n         add_bos_token=False,\n         return_token_type_ids=False,\n+        vocab: Optional[dict] = None,\n+        merges: Optional[list] = None,\n         **kwargs,\n     ):\n-        bos_token = AddedToken(bos_token, special=True) if isinstance(bos_token, str) else bos_token\n-        eos_token = AddedToken(eos_token, special=True) if isinstance(eos_token, str) else eos_token\n-        unk_token = AddedToken(unk_token, special=True) if isinstance(unk_token, str) else unk_token\n-        pad_token = AddedToken(pad_token, special=True) if isinstance(pad_token, str) else pad_token\n-        self.add_bos_token = add_bos_token\n         self.return_token_type_ids = return_token_type_ids\n         if self.return_token_type_ids:\n             self.model_input_names.append(\"token_type_ids\")\n \n-        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n-            self.encoder = json.load(vocab_handle)\n-        self.decoder = {v: k for k, v in self.encoder.items()}\n-        self.errors = errors  # how to handle errors in decoding\n-        self.byte_encoder = bytes_to_unicode()\n-        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n-        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n-            bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\n-        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n-        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n-        self.cache = {}\n         self.add_prefix_space = add_prefix_space\n \n-        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n-        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n+            )\n+        else:\n+            self._vocab = {}\n+\n+        if merges is not None:\n+            self._merges = merges\n+        else:\n+            self._merges = []\n+\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=self._vocab,\n+                merges=self._merges,\n+                dropout=None,\n+                continuing_subword_prefix=\"\",\n+                end_of_word_suffix=\"\",\n+                fuse_unk=False,\n+            )\n+        )\n+\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n+        self._tokenizer.decoder = decoders.ByteLevel()\n+        self._tokenizer.post_processor = processors.ByteLevel(\n+            add_prefix_space=True, use_regex=True, trim_offsets=False\n+        )\n+\n+        tokenizer_object = self._tokenizer\n+\n+        # Set these before calling super().__init__() so the base class _post_init() can use them\n+        self._add_bos_token = add_bos_token\n+        self._add_eos_token = False\n+\n         super().__init__(\n-            errors=errors,\n+            tokenizer_object=tokenizer_object,\n             unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n@@ -187,126 +159,14 @@ def __init__(\n             **kwargs,\n         )\n \n-    @property\n-    def vocab_size(self):\n-        return len(self.encoder)\n-\n-    def get_vocab(self):\n-        return dict(self.encoder, **self.added_tokens_encoder)\n-\n-    def bpe(self, token):\n-        if token in self.cache:\n-            return self.cache[token]\n-        word = tuple(token)\n-        pairs = get_pairs(word)\n-\n-        if not pairs:\n-            return token\n-\n-        while True:\n-            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n-            if bigram not in self.bpe_ranks:\n-                break\n-            first, second = bigram\n-            new_word = []\n-            i = 0\n-            while i < len(word):\n-                try:\n-                    j = word.index(first, i)\n-                except ValueError:\n-                    new_word.extend(word[i:])\n-                    break\n-                else:\n-                    new_word.extend(word[i:j])\n-                    i = j\n-\n-                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n-                    new_word.append(first + second)\n-                    i += 2\n-                else:\n-                    new_word.append(word[i])\n-                    i += 1\n-            new_word = tuple(new_word)\n-            word = new_word\n-            if len(word) == 1:\n-                break\n-            else:\n-                pairs = get_pairs(word)\n-        word = \" \".join(word)\n-        self.cache[token] = word\n-        return word\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        if self.add_bos_token:\n-            bos_token_ids = [self.bos_token_id]\n-        else:\n-            bos_token_ids = []\n-\n-        output = bos_token_ids + token_ids_0\n-\n-        if token_ids_1 is None:\n-            return output\n-\n-        return output + bos_token_ids + token_ids_1\n-\n-    def _tokenize(self, text):\n-        \"\"\"Tokenize a string.\"\"\"\n-        bpe_tokens = []\n-        for token in re.findall(self.pat, text):\n-            token = \"\".join(\n-                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n-            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n-            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n-        return bpe_tokens\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.encoder.get(token, self.encoder.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.decoder.get(index)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        text = \"\".join(tokens)\n-        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n-        return text\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n-        )\n+        self._post_init()\n \n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n-\n-        index = 0\n-        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(\"#version: 0.2\\n\")\n-            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n-                index += 1\n-\n-        return vocab_file, merge_file\n-\n-    def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n-        add_prefix_space = kwargs.pop(\"add_prefix_space\", self.add_prefix_space)\n-        if is_split_into_words or add_prefix_space:\n-            text = \" \" + text\n-        return (text, kwargs)\n+    def _post_init(self):\n+        self._tokenizer.post_processor = processors.ByteLevel(\n+            add_prefix_space=True, use_regex=True, trim_offsets=False\n+        )\n+        # Ensure base class post-init runs to register special/extra tokens, etc.\n+        super()._post_init()\n \n     def decode(\n         self,\n@@ -341,9 +201,7 @@ def decode(\n             `str`: The decoded sentence.\n         \"\"\"\n \n-        token_ids = to_py_obj(token_ids)\n-\n-        decoded_text = super()._decode(\n+        decoded_text = super().decode(\n             token_ids=token_ids,\n             skip_special_tokens=skip_special_tokens,\n             clean_up_tokenization_spaces=clean_up_tokenization_spaces,"
        },
        {
            "sha": "72c8d66c829a5ec8f2379c2af092cdefa53b6d8a",
            "filename": "src/transformers/models/codegen/tokenization_codegen_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 233,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,233 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 The Salesforce authors, The Open AI Team Authors and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n-\n-import re\n-from typing import TYPE_CHECKING, Optional, Union\n-\n-import numpy as np\n-\n-from ...utils import is_torch_available, logging\n-\n-\n-if TYPE_CHECKING:\n-    if is_torch_available():\n-        import torch\n-\n-\n-from ...tokenization_utils_base import BatchEncoding\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from .tokenization_codegen import CodeGenTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class CodeGenTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" CodeGen tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n-    Byte-Pair-Encoding.\n-\n-    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n-    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n-\n-    ```python\n-    >>> from transformers import CodeGenTokenizerFast\n-\n-    >>> tokenizer = CodeGenTokenizerFast.from_pretrained(\"Salesforce/codegen-350M-mono\")\n-    >>> tokenizer(\"Hello world\")[\"input_ids\"]\n-    [15496, 995]\n-\n-    >>> tokenizer(\" Hello world\")[\"input_ids\"]\n-    [18435, 995]\n-    ```\n-\n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n-    the model was not pretrained this way, it might yield a decrease in performance.\n-\n-    <Tip>\n-\n-    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n-\n-    </Tip>\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`, *optional*):\n-            Path to the vocabulary file.\n-        merges_file (`str`, *optional*):\n-            Path to the merges file.\n-        tokenizer_file (`str`, *optional*):\n-            Path to [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\n-            contains everything needed to load the tokenizer.\n-        unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        bos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The beginning of sequence token.\n-        eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The end of sequence token.\n-        add_prefix_space (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n-            other word. (CodeGen tokenizer detect beginning of words by the preceding space).\n-        return_token_type_ids (`bool`, *optional*, defaults to `False`):\n-            Whether to return token type IDs.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = CodeGenTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        unk_token=\"<|endoftext|>\",\n-        bos_token=\"<|endoftext|>\",\n-        eos_token=\"<|endoftext|>\",\n-        add_prefix_space=False,\n-        return_token_type_ids=False,\n-        **kwargs,\n-    ):\n-        self.return_token_type_ids = return_token_type_ids\n-        if self.return_token_type_ids:\n-            self.model_input_names.append(\"token_type_ids\")\n-\n-        super().__init__(\n-            vocab_file,\n-            merges_file,\n-            tokenizer_file=tokenizer_file,\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            add_prefix_space=add_prefix_space,\n-            return_token_type_ids=return_token_type_ids,\n-            **kwargs,\n-        )\n-\n-        if kwargs.pop(\"add_bos_token\", False):\n-            model_id = kwargs.pop(\"name_or_path\", \"\")\n-            raise ValueError(\n-                \"Currently GPT2's fast tokenizer does NOT support adding a BOS token. \"\n-                \"Instead you should use GPT2's slow tokenizer class `CodeGenTokenizer` as follows: \\n\"\n-                f\"`CodeGenTokenizer.from_pretrained('{model_id}')`\\nor\\n\"\n-                f\"`AutoTokenizer.from_pretrained('{model_id}', use_fast=False)`\\n\"\n-                \"This issue will be fixed soon, see: https://github.com/huggingface/tokenizers/pull/1005.\"\n-                \" so that the fast tokenizer works correctly.\"\n-            )\n-\n-    def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-        assert self.add_prefix_space or not is_split_into_words, (\n-            f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-            \"to use it with pretokenized inputs.\"\n-        )\n-\n-        return super()._batch_encode_plus(*args, **kwargs)\n-\n-    def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-\n-        assert self.add_prefix_space or not is_split_into_words, (\n-            f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-            \"to use it with pretokenized inputs.\"\n-        )\n-\n-        return super()._encode_plus(*args, **kwargs)\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-    def decode(\n-        self,\n-        token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n-        skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: Optional[bool] = None,\n-        truncate_before_pattern: Optional[list[str]] = None,\n-        **kwargs,\n-    ) -> str:\n-        \"\"\"\n-        Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n-        tokens and clean up tokenization spaces.\n-\n-        Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n-\n-        Args:\n-            token_ids (`Union[int, List[int], np.ndarray, torch.Tensor]`):\n-                List of tokenized input ids. Can be obtained using the `__call__` method.\n-            skip_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not to remove special tokens in the decoding.\n-            clean_up_tokenization_spaces (`bool`, *optional*):\n-                Whether or not to clean up the tokenization spaces. If `None`, will default to\n-                `self.clean_up_tokenization_spaces` (available in the `tokenizer_config`).\n-            truncate_before_pattern (`List[str]`, *optional*, defaults to `None`):\n-                A list of regular expression strings that will be used to truncate the returned string. This can be\n-                used to remove extra pieces of code (e.g. truncate if observing a comment symbol \"#\" at the beginning\n-                of a new line). An example pattern could be `[\"^#\", re.escape(\"<|endoftext|>\"), \"^'''\", \"\\n\\n\\n\"]`.\n-            kwargs (additional keyword arguments, *optional*):\n-                Will be passed to the underlying model specific decode method.\n-\n-        Returns:\n-            `str`: The decoded sentence.\n-        \"\"\"\n-\n-        decoded_text = super().decode(\n-            token_ids=token_ids,\n-            skip_special_tokens=skip_special_tokens,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            **kwargs,\n-        )\n-\n-        if truncate_before_pattern is not None and len(truncate_before_pattern) > 0:\n-            decoded_text = self.truncate(decoded_text, truncate_before_pattern)\n-\n-        return decoded_text\n-\n-    def truncate(self, completion, truncate_before_pattern):\n-        def find_re(string, pattern, start_pos):\n-            m = pattern.search(string, start_pos)\n-            return m.start() if m else -1\n-\n-        terminals = [re.compile(pattern, re.MULTILINE) for pattern in truncate_before_pattern]\n-\n-        prints = list(re.finditer(\"^print\", completion, re.MULTILINE))\n-\n-        if len(prints) > 1:\n-            completion = completion[: prints[1].start()]\n-\n-        defs = list(re.finditer(\"^def\", completion, re.MULTILINE))\n-\n-        if len(defs) > 1:\n-            completion = completion[: defs[1].start()]\n-\n-        start_pos = 0\n-\n-        terminals_pos = [\n-            pos for pos in [find_re(completion, terminal, start_pos) for terminal in terminals] if pos != -1\n-        ]\n-\n-        if len(terminals_pos) > 0:\n-            return completion[: min(terminals_pos)]\n-        else:\n-            return completion\n-\n-\n-__all__ = [\"CodeGenTokenizerFast\"]"
        },
        {
            "sha": "98c73f4cd22dde96014bb3fb8139f242d413e802",
            "filename": "src/transformers/models/cohere/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcohere%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcohere%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,7 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_cohere import *\n     from .modeling_cohere import *\n-    from .tokenization_cohere_fast import *\n+    from .tokenization_cohere import *\n else:\n     import sys\n "
        },
        {
            "sha": "0b2e8f09a6dd85a40dc5dd38892d637d208a23ff",
            "filename": "src/transformers/models/cohere/tokenization_cohere.py",
            "status": "renamed",
            "additions": 109,
            "deletions": 195,
            "changes": 304,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -13,19 +13,19 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-# This file is based on the tokenization_llama_fast.py file in transformers\n+# This file is based on the tokenization_llama.py file in transformers\n \n-from typing import Literal, Union\n+from typing import Literal, Optional, Union\n \n-from tokenizers import processors\n+from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers\n+from tokenizers.models import BPE\n \n-from ...tokenization_utils_base import BatchEncoding\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n-VOCAB_FILES_NAMES = {\"tokenizer_file\": \"tokenizer.json\"}\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n \n PRETRAINED_VOCAB_FILES_MAP = {\n     \"tokenizer_file\": {\n@@ -43,7 +43,7 @@\n # fmt: on\n \n \n-class CohereTokenizerFast(PreTrainedTokenizerFast):\n+class CohereTokenizer(TokenizersBackend):\n     \"\"\"\n     Construct a Cohere tokenizer. Based on byte-level Byte-Pair-Encoding.\n \n@@ -71,7 +71,7 @@ class CohereTokenizerFast(PreTrainedTokenizerFast):\n \n     </Tip>\n \n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the main methods. Users should\n     refer to this superclass for more information regarding those methods.\n \n     Args:\n@@ -100,6 +100,10 @@ class CohereTokenizerFast(PreTrainedTokenizerFast):\n             Whether or not the default system prompt for Cohere tokenizer should be used.\n         add_prefix_space (`bool`, *optional*, defaults to `False`):\n             Whether or not the tokenizer should automatically add a prefix space\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        merges (`list`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from merges_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -111,113 +115,104 @@ class CohereTokenizerFast(PreTrainedTokenizerFast):\n \n     def __init__(\n         self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        clean_up_tokenization_spaces=False,\n-        unk_token=\"<UNK>\",\n-        bos_token=\"<BOS_TOKEN>\",\n-        eos_token=\"<|END_OF_TURN_TOKEN|>\",\n-        add_bos_token=True,\n-        add_eos_token=False,\n-        use_default_system_prompt=False,\n-        add_prefix_space=False,\n+        errors: str = \"replace\",\n+        unk_token: str = \"<UNK>\",\n+        bos_token: str = \"<BOS_TOKEN>\",\n+        eos_token: str = \"<|END_OF_TURN_TOKEN|>\",\n+        pad_token: str = \"<PAD>\",\n+        cls_token: str = \"<CLS>\",\n+        sep_token: str = \"<SEP>\",\n+        mask_token: str = \"<MASK_TOKEN>\",\n+        add_bos_token: bool = True,\n+        add_eos_token: bool = False,\n+        use_default_system_prompt: bool = False,\n+        add_prefix_space: bool = False,\n+        vocab: Optional[dict] = None,\n+        merges: Optional[list] = None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            vocab_file=vocab_file,\n-            merges_file=merges_file,\n-            tokenizer_file=tokenizer_file,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n-            use_default_system_prompt=use_default_system_prompt,\n-            add_prefix_space=add_prefix_space,\n-            **kwargs,\n-        )\n         self._add_bos_token = add_bos_token\n         self._add_eos_token = add_eos_token\n-        self.update_post_processor()\n         self.use_default_system_prompt = use_default_system_prompt\n-        self.vocab_file = vocab_file\n+        self.add_prefix_space = add_prefix_space\n         self.grounded_generation_template = kwargs.pop(\"grounded_generation_template\", None)\n         self.tool_use_template = kwargs.pop(\"tool_use_template\", None)\n \n-        # This is a `tokenizers.pre_tokenizers.Sequence`\n-        for pre_tokenizer in self.backend_tokenizer.pre_tokenizer:\n-            if hasattr(pre_tokenizer, \"add_prefix_space\"):\n-                pre_tokenizer.add_prefix_space = add_prefix_space\n-        self.backend_tokenizer.decoder.add_prefix_space = add_prefix_space\n-\n-        self.add_prefix_space = add_prefix_space\n-\n-    def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-        if not (self.add_prefix_space or not is_split_into_words):\n-            raise Exception(\n-                f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True to use it with\"\n-                \" pretokenized inputs.\"\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n             )\n+        else:\n+            self._vocab = {\n+                str(pad_token): 0,\n+                str(unk_token): 1,\n+                str(cls_token): 2,\n+                str(sep_token): 3,\n+                str(mask_token): 4,\n+                str(bos_token): 5,\n+            }\n \n-        return super()._batch_encode_plus(*args, **kwargs)\n-\n-    def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-\n-        if not (self.add_prefix_space or not is_split_into_words):\n-            raise Exception(\n-                f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True to use it with\"\n-                \" pretokenized inputs.\"\n+        if merges is not None:\n+            self._merges = merges\n+        else:\n+            self._merges = []\n+\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=self._vocab,\n+                merges=self._merges,\n+                dropout=None,\n+                continuing_subword_prefix=\"\",\n+                end_of_word_suffix=\"\",\n+                fuse_unk=False,\n             )\n+        )\n \n-        return super()._encode_plus(*args, **kwargs)\n-\n-    def update_post_processor(self):\n-        \"\"\"\n-        Updates the underlying post processor with the current `bos_token` and `eos_token`.\n-        \"\"\"\n-        bos = self.bos_token\n-        bos_token_id = self.bos_token_id\n-        if bos is None and self.add_bos_token:\n-            raise ValueError(\"add_bos_token = True but bos_token = None\")\n-\n-        eos = self.eos_token\n-        eos_token_id = self.eos_token_id\n-        if eos is None and self.add_eos_token:\n-            raise ValueError(\"add_eos_token = True but eos_token = None\")\n-\n-        single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n-        pair = f\"{single}{(' ' + bos + ':1') if self.add_bos_token else ''} $B:1{(' ' + eos + ':1') if self.add_eos_token else ''}\"\n-\n-        special_tokens = []\n-        if self.add_bos_token:\n-            special_tokens.append((bos, bos_token_id))\n-        if self.add_eos_token:\n-            special_tokens.append((eos, eos_token_id))\n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=single, pair=pair, special_tokens=special_tokens\n+        self._tokenizer.normalizer = normalizers.NFC()\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+            [\n+                pre_tokenizers.Digits(individual_digits=True),\n+                pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space, trim_offsets=True),\n+            ]\n         )\n+        self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=add_prefix_space, trim_offsets=True)\n \n-    @property\n-    def add_eos_token(self):\n-        return self._add_eos_token\n+        tokenizer_object = self._tokenizer\n \n-    @property\n-    def add_bos_token(self):\n-        return self._add_bos_token\n+        super().__init__(\n+            tokenizer_object=tokenizer_object,\n+            errors=errors,\n+            unk_token=unk_token,\n+            bos_token=bos_token,\n+            eos_token=eos_token,\n+            pad_token=pad_token,\n+            cls_token=cls_token,\n+            sep_token=sep_token,\n+            mask_token=mask_token,\n+            add_bos_token=add_bos_token,\n+            add_eos_token=add_eos_token,\n+            use_default_system_prompt=use_default_system_prompt,\n+            add_prefix_space=add_prefix_space,\n+            **kwargs,\n+        )\n \n-    @add_eos_token.setter\n-    def add_eos_token(self, value):\n-        self._add_eos_token = value\n-        self.update_post_processor()\n+        self._post_init()\n+\n+    def _post_init(self):\n+        \"\"\"Post-initialization to ensure add_prefix_space is applied correctly.\"\"\"\n+        # Re-apply add_prefix_space setting to pre_tokenizer and decoder\n+        # This is needed because when loading from pretrained, the tokenizer.json\n+        # has these settings baked in and we need to override them\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+            [\n+                pre_tokenizers.Digits(individual_digits=True),\n+                pre_tokenizers.ByteLevel(add_prefix_space=self.add_prefix_space, trim_offsets=True),\n+            ]\n+        )\n+        self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=self.add_prefix_space, trim_offsets=True)\n \n-    @add_bos_token.setter\n-    def add_bos_token(self, value):\n-        self._add_bos_token = value\n-        self.update_post_processor()\n+        # Call parent to handle AddedToken properties\n+        super()._post_init()\n \n     def apply_tool_use_template(\n         self,\n@@ -285,73 +280,32 @@ def apply_tool_use_template(\n         Examples:\n \n         ```python\n-        >> tokenizer = CohereTokenizerFast.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n-        >> tools = [\n+        tokenizer = CohereTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n+        tools = [\n             {\n                 \"name\": \"internet_search\",\n                 \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n                 \"parameter_definitions\": {\n                     \"query\": {\n                         \"description\": \"Query to search the internet with\",\n                         \"type\": \"str\",\n-                        \"required\": True\n+                        \"required\": True,\n                     }\n-                }\n+                },\n             },\n             {\n-                \"name': \"directly_answer\",\n+                \"name\": \"directly_answer\",\n                 \"description\": \"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\",\n-                \"parameter_definitions\": {}\n-            }\n+                \"parameter_definitions\": {},\n+            },\n         ]\n-        >> conversation = [\n-            {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n+        conversation = [\n+            {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"},\n         ]\n-        >> # render the prompt, ready for user to inspect, or for input into the model:\n-        >> prompt = tokenizer.apply_tool_use_template(conversation, tools=tools, tokenize=False, add_generation_prompt=True)\n-        >> print(prompt)\n-        <BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># Safety Preamble\n-        The instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n-\n-        # System Preamble\n-        ## Basic Rules\n-        You are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n-\n-        # User Preamble\n-        ## Task and Context\n-        You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n-\n-        ## Style Guide\n-        Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n-\n-        ## Available Tools\n-        Here is a list of tools that you have available to you:\n-\n-        \\\\`\\\\`\\\\`python\n-        def internet_search(query: str) -> list[Dict]:\n-            \\\"\\\"\\\"Returns a list of relevant document snippets for a textual query retrieved from the internet\n-\n-            Args:\n-                query (str): Query to search the internet with\n-            \\\"\\\"\\\"\n-            pass\n-        \\\\`\\\\`\\\\`\n-\n-        \\\\`\\\\`\\\\`python\n-        def directly_answer() -> list[Dict]:\n-            \\\"\\\"\\\"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\n-            \\\"\\\"\\\"\n-            pass\n-        \\\\`\\\\`\\\\`<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write 'Action:' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user's last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\n-        \\\\`\\\\`\\\\`json\n-        [\n-            {\n-                \"tool_name\": title of the tool in the specification,\n-                \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\n-            }\n-        ]\\\\`\\\\`\\\\`<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n-        ```\n-        >> inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt')\n+        # Render the prompt, ready for user to inspect, or for input into the model\n+        prompt = tokenizer.apply_tool_use_template(conversation, tools=tools, tokenize=False, add_generation_prompt=True)\n+        print(prompt)\n+        >> inputs = tokenizer.encode(grounded_generation_prompt, add_special_tokens=False, return_tensors='pt')\n         >> outputs = model.generate(inputs, max_new_tokens=128)\n         >> print(tokenizer.decode(outputs[0]))\n         Action: ```json\n@@ -431,7 +385,7 @@ def apply_grounded_generation_template(\n         Examples:\n \n         ```python\n-        >> tokenizer = CohereTokenizerFast.from_pretrained('CohereForAI/c4ai-command-r-v01')\n+        >> tokenizer = CohereTokenizer.from_pretrained('CohereForAI/c4ai-command-r-v01')\n \n         >> # define documents:\n         >> documents = [\n@@ -445,38 +399,10 @@ def apply_grounded_generation_template(\n         >> # render the prompt, ready for user to inspect, or for input into the model:\n         >> grounded_generation_prompt = tokenizer.apply_grounded_generation_template(conversation, documents=documents, tokenize=False, add_generation_prompt=True)\n         >> print(grounded_generation_prompt)\n-        <BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># Safety Preamble\n-        The instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n-\n-        ## Basic Rules\n-        You are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n-\n-        # User Preamble\n-        ## Task and Context\n-        You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n-\n-        ## Style Guide\n-        Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\n-        Document: 0\n-        title: Tall penguins\n-        text: Emperor penguins are the tallest.\n-\n-        Document: 1\n-        title: Penguin habitats\n-        text: Emperor penguins only live in Antarctica.\n-        </results><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Carefully perform the following instructions, in order, starting each with a new line.\n-        Firstly, Decide which of the retrieved documents are relevant to the user's last input by writing 'Relevant Documents:' followed by comma-separated list of document numbers. If none are relevant, you should instead write 'None'.\n-        Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user's last input by writing 'Cited Documents:' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write 'None'.\n-        Thirdly, Write 'Answer:' followed by a response to the user's last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\n-        Finally, Write 'Grounded answer:' followed by a response to the user's last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'''\n-        ```\n         >> inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt')\n         >> outputs = model.generate(inputs, max_new_tokens=128)\n         >> print(tokenizer.decode(outputs[0]))\n-        Relevant Documents: 0,1\n-        Cited Documents: 0,1\n-        Answer: The Emperor Penguin is the tallest or biggest penguin in the world. It is a bird that lives only in Antarctica and grows to a height of around 122 centimetres.\n-        Grounded answer: The <co: 0>Emperor Penguin</co: 0> is the <co: 0>tallest</co: 0> or biggest penguin in the world. It is a bird that <co: 1>lives only in Antarctica</co: 1> and <co: 0>grows to a height of around 122 centimetres.</co: 0>\n+        ```\n         \"\"\"\n         return self.apply_chat_template(\n             conversation,\n@@ -486,17 +412,5 @@ def apply_grounded_generation_template(\n             **kwargs,\n         )\n \n-    # TODO ArthurZ let's rely on the template processor instead, refactor all fast tokenizers\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n-        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n-\n-        output = bos_token_id + token_ids_0 + eos_token_id\n-\n-        if token_ids_1 is not None:\n-            output = output + bos_token_id + token_ids_1 + eos_token_id\n-\n-        return output\n-\n \n-__all__ = [\"CohereTokenizerFast\"]\n+__all__ = [\"CohereTokenizer\"]",
            "previous_filename": "src/transformers/models/cohere/tokenization_cohere_fast.py"
        },
        {
            "sha": "b168f502b3de0013eb97228ae0d5f04c1cd0c8d7",
            "filename": "src/transformers/models/colqwen2/configuration_colqwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -80,6 +80,9 @@ def __init__(\n                 f\"Invalid type for `vlm_config`. Expected `PreTrainedConfig`, `dict`, or `None`, but got {type(vlm_config)}.\"\n             )\n \n+        if not hasattr(vlm_config, \"vocab_size\"):\n+            vlm_config.vocab_size = vlm_config.get_text_config().vocab_size\n+\n         self.vlm_config = vlm_config\n         self.embedding_dim = embedding_dim\n         self.initializer_range = initializer_range"
        },
        {
            "sha": "2bd6040cc69655c57e21c09c9b01017ff3e66eda",
            "filename": "src/transformers/models/convbert/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fconvbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fconvbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,10 +18,10 @@\n \n \n if TYPE_CHECKING:\n+    from ..bert.tokenization_bert import BertTokenizer as ConvBertTokenizerFast\n     from .configuration_convbert import *\n     from .modeling_convbert import *\n     from .tokenization_convbert import *\n-    from .tokenization_convbert_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "cc5d967237f74383588903dde1fd7ca5cc44edb1",
            "filename": "src/transformers/models/convbert/tokenization_convbert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 459,
            "changes": 466,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1,5 +1,5 @@\n # coding=utf-8\n-# Copyright 2018 The HuggingFace Inc. team.\n+# Copyright The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -14,470 +14,18 @@\n # limitations under the License.\n \"\"\"Tokenization classes for ConvBERT.\"\"\"\n \n-import collections\n-import os\n-import unicodedata\n-from typing import Optional\n+from ...models.bert.tokenization_bert import BertTokenizer\n \n-from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n-from ...utils import logging\n \n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.load_vocab\n-def load_vocab(vocab_file):\n-    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n-    vocab = collections.OrderedDict()\n-    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n-        tokens = reader.readlines()\n-    for index, token in enumerate(tokens):\n-        token = token.rstrip(\"\\n\")\n-        vocab[token] = index\n-    return vocab\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize\n-def whitespace_tokenize(text):\n-    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n-    text = text.strip()\n-    if not text:\n-        return []\n-    tokens = text.split()\n-    return tokens\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.BertTokenizer with bert-base-cased->YituTech/conv-bert-base, ConvBertTokenizer->BertTokenizer, BERT->ConvBERT\n-class ConvBertTokenizer(PreTrainedTokenizer):\n+class ConvBertTokenizer(BertTokenizer):\n     r\"\"\"\n-    Construct a ConvBERT tokenizer. Based on WordPiece.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        do_basic_tokenize (`bool`, *optional*, defaults to `True`):\n-            Whether or not to do basic tokenization before WordPiece.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original ConvBERT).\n-        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n-            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n-            extra spaces.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-\n-    def __init__(\n-        self,\n-        vocab_file,\n-        do_lower_case=True,\n-        do_basic_tokenize=True,\n-        never_split=None,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        clean_up_tokenization_spaces=True,\n-        **kwargs,\n-    ):\n-        if not os.path.isfile(vocab_file):\n-            raise ValueError(\n-                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n-                \" model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n-            )\n-        self.vocab = load_vocab(vocab_file)\n-        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n-        self.do_basic_tokenize = do_basic_tokenize\n-        if do_basic_tokenize:\n-            self.basic_tokenizer = BasicTokenizer(\n-                do_lower_case=do_lower_case,\n-                never_split=never_split,\n-                tokenize_chinese_chars=tokenize_chinese_chars,\n-                strip_accents=strip_accents,\n-            )\n-\n-        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=str(unk_token))\n-\n-        super().__init__(\n-            do_lower_case=do_lower_case,\n-            do_basic_tokenize=do_basic_tokenize,\n-            never_split=never_split,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            **kwargs,\n-        )\n-\n-    @property\n-    def do_lower_case(self):\n-        return self.basic_tokenizer.do_lower_case\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.vocab)\n-\n-    def get_vocab(self):\n-        return dict(self.vocab, **self.added_tokens_encoder)\n-\n-    def _tokenize(self, text, split_special_tokens=False):\n-        split_tokens = []\n-        if self.do_basic_tokenize:\n-            for token in self.basic_tokenizer.tokenize(\n-                text, never_split=self.all_special_tokens if not split_special_tokens else None\n-            ):\n-                # If the token is part of the never_split set\n-                if token in self.basic_tokenizer.never_split:\n-                    split_tokens.append(token)\n-                else:\n-                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n-        else:\n-            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n-        return split_tokens\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.vocab.get(token, self.vocab.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.ids_to_tokens.get(index, self.unk_token)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n-        return out_string\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A ConvBERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n+    Construct a ConvBERT tokenizer (backed by HuggingFace's tokenizers library). Based on WordPiece.\n \n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        index = 0\n-        if os.path.isdir(save_directory):\n-            vocab_file = os.path.join(\n-                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-            )\n-        else:\n-            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n-            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n-                        \" Please check that the vocabulary is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(token + \"\\n\")\n-                index += 1\n-        return (vocab_file,)\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer\n-class BasicTokenizer:\n-    \"\"\"\n-    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n-\n-    Args:\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        do_split_on_punc (`bool`, *optional*, defaults to `True`):\n-            In some instances we want to skip the basic punctuation splitting so that later tokenization can capture\n-            the full context of the words, such as contractions.\n+    This tokenizer inherits from [`BertTokenizer`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n     \"\"\"\n \n-    def __init__(\n-        self,\n-        do_lower_case=True,\n-        never_split=None,\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        do_split_on_punc=True,\n-    ):\n-        if never_split is None:\n-            never_split = []\n-        self.do_lower_case = do_lower_case\n-        self.never_split = set(never_split)\n-        self.tokenize_chinese_chars = tokenize_chinese_chars\n-        self.strip_accents = strip_accents\n-        self.do_split_on_punc = do_split_on_punc\n-\n-    def tokenize(self, text, never_split=None):\n-        \"\"\"\n-        Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n-\n-        Args:\n-            never_split (`List[str]`, *optional*)\n-                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n-                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n-        \"\"\"\n-        # union() returns a new set by concatenating the two sets.\n-        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n-        text = self._clean_text(text)\n-\n-        # This was added on November 1st, 2018 for the multilingual and Chinese\n-        # models. This is also applied to the English models now, but it doesn't\n-        # matter since the English models were not trained on any Chinese data\n-        # and generally don't have any Chinese data in them (there are Chinese\n-        # characters in the vocabulary because Wikipedia does have some Chinese\n-        # words in the English Wikipedia.).\n-        if self.tokenize_chinese_chars:\n-            text = self._tokenize_chinese_chars(text)\n-        # prevents treating the same character with different unicode codepoints as different characters\n-        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n-        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n-        split_tokens = []\n-        for token in orig_tokens:\n-            if token not in never_split:\n-                if self.do_lower_case:\n-                    token = token.lower()\n-                    if self.strip_accents is not False:\n-                        token = self._run_strip_accents(token)\n-                elif self.strip_accents:\n-                    token = self._run_strip_accents(token)\n-            split_tokens.extend(self._run_split_on_punc(token, never_split))\n-\n-        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n-        return output_tokens\n-\n-    def _run_strip_accents(self, text):\n-        \"\"\"Strips accents from a piece of text.\"\"\"\n-        text = unicodedata.normalize(\"NFD\", text)\n-        output = []\n-        for char in text:\n-            cat = unicodedata.category(char)\n-            if cat == \"Mn\":\n-                continue\n-            output.append(char)\n-        return \"\".join(output)\n-\n-    def _run_split_on_punc(self, text, never_split=None):\n-        \"\"\"Splits punctuation on a piece of text.\"\"\"\n-        if not self.do_split_on_punc or (never_split is not None and text in never_split):\n-            return [text]\n-        chars = list(text)\n-        i = 0\n-        start_new_word = True\n-        output = []\n-        while i < len(chars):\n-            char = chars[i]\n-            if _is_punctuation(char):\n-                output.append([char])\n-                start_new_word = True\n-            else:\n-                if start_new_word:\n-                    output.append([])\n-                start_new_word = False\n-                output[-1].append(char)\n-            i += 1\n-\n-        return [\"\".join(x) for x in output]\n-\n-    def _tokenize_chinese_chars(self, text):\n-        \"\"\"Adds whitespace around any CJK character.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if self._is_chinese_char(cp):\n-                output.append(\" \")\n-                output.append(char)\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-    def _is_chinese_char(self, cp):\n-        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n-        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n-        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n-        #\n-        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n-        # despite its name. The modern Korean Hangul alphabet is a different block,\n-        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n-        # space-separated words, so they are not treated specially and handled\n-        # like the all of the other languages.\n-        if (\n-            (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n-            or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n-        ):\n-            return True\n-\n-        return False\n-\n-    def _clean_text(self, text):\n-        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if cp == 0 or cp == 0xFFFD or _is_control(char):\n-                continue\n-            if _is_whitespace(char):\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.WordpieceTokenizer\n-class WordpieceTokenizer:\n-    \"\"\"Runs WordPiece tokenization.\"\"\"\n-\n-    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n-        self.vocab = vocab\n-        self.unk_token = unk_token\n-        self.max_input_chars_per_word = max_input_chars_per_word\n-\n-    def tokenize(self, text):\n-        \"\"\"\n-        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n-        tokenization using the given vocabulary.\n-\n-        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n-\n-        Args:\n-            text: A single token or whitespace separated tokens. This should have\n-                already been passed through *BasicTokenizer*.\n-\n-        Returns:\n-            A list of wordpiece tokens.\n-        \"\"\"\n-\n-        output_tokens = []\n-        for token in whitespace_tokenize(text):\n-            chars = list(token)\n-            if len(chars) > self.max_input_chars_per_word:\n-                output_tokens.append(self.unk_token)\n-                continue\n-\n-            is_bad = False\n-            start = 0\n-            sub_tokens = []\n-            while start < len(chars):\n-                end = len(chars)\n-                cur_substr = None\n-                while start < end:\n-                    substr = \"\".join(chars[start:end])\n-                    if start > 0:\n-                        substr = \"##\" + substr\n-                    if substr in self.vocab:\n-                        cur_substr = substr\n-                        break\n-                    end -= 1\n-                if cur_substr is None:\n-                    is_bad = True\n-                    break\n-                sub_tokens.append(cur_substr)\n-                start = end\n-\n-            if is_bad:\n-                output_tokens.append(self.unk_token)\n-            else:\n-                output_tokens.extend(sub_tokens)\n-        return output_tokens\n+    pass\n \n \n __all__ = [\"ConvBertTokenizer\"]"
        },
        {
            "sha": "e328e7490f4d2ca78a5d2005c2a00227eba33b78",
            "filename": "src/transformers/models/convbert/tokenization_convbert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 147,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,147 +0,0 @@\n-# coding=utf-8\n-# Copyright The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for ConvBERT.\"\"\"\n-\n-import json\n-from typing import Optional\n-\n-from tokenizers import normalizers\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_convbert import ConvBertTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast with bert-base-cased->YituTech/conv-bert-base, Bert->ConvBert, BERT->ConvBERT\n-class ConvBertTokenizerFast(PreTrainedTokenizerFast):\n-    r\"\"\"\n-    Construct a \"fast\" ConvBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        clean_text (`bool`, *optional*, defaults to `True`):\n-            Whether or not to clean the text before tokenization by removing any control characters and replacing all\n-            whitespaces by the classic one.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n-            issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original ConvBERT).\n-        wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n-            The prefix for subwords.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = ConvBertTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=True,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            **kwargs,\n-        )\n-\n-        normalizer_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n-        if (\n-            normalizer_state.get(\"lowercase\", do_lower_case) != do_lower_case\n-            or normalizer_state.get(\"strip_accents\", strip_accents) != strip_accents\n-            or normalizer_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n-        ):\n-            normalizer_class = getattr(normalizers, normalizer_state.pop(\"type\"))\n-            normalizer_state[\"lowercase\"] = do_lower_case\n-            normalizer_state[\"strip_accents\"] = strip_accents\n-            normalizer_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n-            self.backend_tokenizer.normalizer = normalizer_class(**normalizer_state)\n-\n-        self.do_lower_case = do_lower_case\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A ConvBERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-\n-        if token_ids_1 is not None:\n-            output += token_ids_1 + [self.sep_token_id]\n-\n-        return output\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"ConvBertTokenizerFast\"]"
        },
        {
            "sha": "16c8e646ab1b0d1f2b178515aeb825c2c293fbce",
            "filename": "src/transformers/models/cpm/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcpm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcpm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .tokenization_cpm import *\n-    from .tokenization_cpm_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "e5560e0efe4d349fb1f3a5b870401e6285b3800b",
            "filename": "src/transformers/models/cpm/tokenization_cpm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,7 @@\n \n import sentencepiece as spm\n \n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_python import AddedToken, PreTrainedTokenizer\n from ...utils import SPIECE_UNDERLINE, logging\n from ...utils.import_utils import requires\n \n@@ -157,23 +157,19 @@ def __init__(\n         self._pad_token_type_id = 3\n \n     @property\n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.vocab_size\n     def vocab_size(self):\n         return len(self.sp_model)\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.get_vocab\n     def get_vocab(self):\n         vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n         vocab.update(self.added_tokens_encoder)\n         return vocab\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.__getstate__\n     def __getstate__(self):\n         state = self.__dict__.copy()\n         state[\"sp_model\"] = None\n         return state\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.__setstate__\n     def __setstate__(self, d):\n         self.__dict__ = d\n \n@@ -184,7 +180,6 @@ def __setstate__(self, d):\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.Load(self.vocab_file)\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.preprocess_text\n     def preprocess_text(self, inputs):\n         if self.remove_space:\n             outputs = \" \".join(inputs.strip().split())\n@@ -200,7 +195,6 @@ def preprocess_text(self, inputs):\n \n         return outputs\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer._tokenize\n     def _tokenize(self, text: str) -> list[str]:\n         \"\"\"Tokenize a string.\"\"\"\n         text = self.preprocess_text(text)\n@@ -221,23 +215,19 @@ def _tokenize(self, text: str) -> list[str]:\n \n         return new_pieces\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer._convert_token_to_id\n     def _convert_token_to_id(self, token):\n         \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n         return self.sp_model.PieceToId(token)\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer._convert_id_to_token\n     def _convert_id_to_token(self, index):\n         \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n         return self.sp_model.IdToPiece(index)\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.convert_tokens_to_string\n     def convert_tokens_to_string(self, tokens):\n         \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n         out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n         return out_string\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.build_inputs_with_special_tokens\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n     ) -> list[int]:\n@@ -263,7 +253,6 @@ def build_inputs_with_special_tokens(\n             return token_ids_0 + sep + cls\n         return token_ids_0 + sep + token_ids_1 + sep + cls\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.get_special_tokens_mask\n     def get_special_tokens_mask(\n         self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n     ) -> list[int]:\n@@ -292,7 +281,6 @@ def get_special_tokens_mask(\n             return ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1, 1]\n         return ([0] * len(token_ids_0)) + [1, 1]\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.create_token_type_ids_from_sequences\n     def create_token_type_ids_from_sequences(\n         self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n     ) -> list[int]:\n@@ -323,7 +311,6 @@ def create_token_type_ids_from_sequences(\n             return len(token_ids_0 + sep) * [0] + cls_segment_id\n         return len(token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1] + cls_segment_id\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet.XLNetTokenizer.save_vocabulary\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")"
        },
        {
            "sha": "4757a520b0d9d29fd4a2545d62c690e0aa9e74a1",
            "filename": "src/transformers/models/cpm/tokenization_cpm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,7 +18,7 @@\n from shutil import copyfile\n from typing import Optional\n \n-from ...tokenization_utils_fast import AddedToken, PreTrainedTokenizerFast\n+from ...tokenization_utils_tokenizers import AddedToken, PreTrainedTokenizerFast\n from ...utils import logging\n \n \n@@ -144,7 +144,6 @@ def __init__(\n         self.jieba = rjieba\n         self.translator = str.maketrans(\" \\n\", \"\\u2582\\u2583\")\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast.build_inputs_with_special_tokens\n     def build_inputs_with_special_tokens(\n         self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n     ) -> list[int]:\n@@ -170,7 +169,6 @@ def build_inputs_with_special_tokens(\n             return token_ids_0 + sep + cls\n         return token_ids_0 + sep + token_ids_1 + sep + cls\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast.create_token_type_ids_from_sequences\n     def create_token_type_ids_from_sequences(\n         self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n     ) -> list[int]:\n@@ -201,7 +199,6 @@ def create_token_type_ids_from_sequences(\n             return len(token_ids_0 + sep) * [0] + cls_segment_id\n         return len(token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1] + cls_segment_id\n \n-    # Copied from transformers.models.xlnet.tokenization_xlnet_fast.XLNetTokenizerFast.save_vocabulary\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError("
        },
        {
            "sha": "e620be4c67e0b172566e740f3ebbaf02a31155a6",
            "filename": "src/transformers/models/cpmant/tokenization_cpmant.py",
            "status": "modified",
            "additions": 9,
            "deletions": 47,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -24,7 +24,7 @@\n if is_rjieba_available():\n     import rjieba\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n \n \n@@ -144,8 +144,16 @@ def __init__(\n             line_token=line_token,\n             space_token=space_token,\n             padding_side=padding_side,\n+            token_type_ids_pattern=\"all_zeros\",\n+            token_type_ids_include_special_tokens=True,\n+            special_tokens_pattern=\"bos\",\n             **kwargs,\n         )\n+        for special_token in [space_token, line_token]:\n+            token_id = self.added_tokens_encoder.pop(special_token, None)\n+            if token_id is not None:\n+                self._added_tokens_decoder.pop(token_id, None)\n+        self._update_total_vocab_size()\n \n     @property\n     def bod_token_id(self):\n@@ -222,51 +230,5 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                 index += 1\n         return (vocab_file,)\n \n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A CPMAnt sequence has the following format:\n-\n-        - single sequence: `[BOS] Sequence`.\n-\n-        Args:\n-            token_ids_0 (`list[int]`): The first tokenized sequence that special tokens will be added.\n-            token_ids_1 (`list[int]`): The optional second tokenized sequence that special tokens will be added.\n-\n-        Returns:\n-            `list[int]`: The model input with special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.bos_token_id] + token_ids_0\n-        return [self.bos_token_id] + token_ids_0 + [self.bos_token_id] + token_ids_1\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`): List of IDs.\n-            token_ids_1 (`list[int]`, *optional*): Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n-        return [1] + ([0] * len(token_ids_0))\n-\n \n __all__ = [\"CpmAntTokenizer\"]"
        },
        {
            "sha": "cc1d6d3e5f10a110720663e255ec1e53cc29220d",
            "filename": "src/transformers/models/ctrl/tokenization_ctrl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 33,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fctrl%2Ftokenization_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fctrl%2Ftokenization_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Ftokenization_ctrl.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -15,12 +15,10 @@\n \"\"\"Tokenization classes for Salesforce CTRL.\"\"\"\n \n import json\n-import os\n-from typing import Optional\n \n import regex as re\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n \n \n@@ -136,7 +134,14 @@ def __init__(self, vocab_file, merges_file, unk_token=\"<unk>\", **kwargs):\n         merges = [tuple(merge.split()) for merge in merges]\n         self.bpe_ranks = dict(zip(merges, range(len(merges))))\n         self.cache = {}\n-        super().__init__(unk_token=unk_token, **kwargs)\n+        self.add_bpe_version_header = True\n+        super().__init__(\n+            unk_token=unk_token,\n+            token_type_ids_pattern=\"all_zeros\",\n+            token_type_ids_include_special_tokens=True,\n+            special_tokens_pattern=\"none\",\n+            **kwargs,\n+        )\n \n     @property\n     def vocab_size(self):\n@@ -212,35 +217,6 @@ def convert_tokens_to_string(self, tokens):\n         out_string = \" \".join(tokens).replace(\"@@ \", \"\").strip()\n         return out_string\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n-        )\n-\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n-\n-        index = 0\n-        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(\"#version: 0.2\\n\")\n-            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n-                index += 1\n-\n-        return vocab_file, merge_file\n-\n     # def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n     #     filtered_tokens = ' '.join(self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens))\n     #     tokens_generated_so_far = re.sub('(@@ )', '', string=filtered_tokens)"
        },
        {
            "sha": "c7bc4d20ac9e991d7f56f88618c0b2d2a324799d",
            "filename": "src/transformers/models/deberta/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdeberta%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdeberta%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,6 @@\n     from .configuration_deberta import *\n     from .modeling_deberta import *\n     from .tokenization_deberta import *\n-    from .tokenization_deberta_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "4e554c6e0be9837ff73a75dcc9e2c6e9fc81cce0",
            "filename": "src/transformers/models/deberta/tokenization_deberta.py",
            "status": "modified",
            "additions": 87,
            "deletions": 249,
            "changes": 336,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -12,66 +12,24 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"Tokenization class for model DeBERTa.\"\"\"\n+\"\"\"Fast Tokenization class for model DeBERTa.\"\"\"\n \n-import json\n-import os\n-from typing import Optional\n+from tokenizers import AddedToken, Tokenizer, decoders, pre_tokenizers, processors\n+from tokenizers.models import BPE\n \n-import regex as re\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\"}\n-\n-\n-# Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode\n-def bytes_to_unicode():\n-    \"\"\"\n-    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n-    characters the bpe code barfs on.\n-\n-    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n-    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n-    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n-    tables between utf-8 bytes and unicode strings.\n-    \"\"\"\n-    bs = (\n-        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¬°\"), ord(\"¬¨\") + 1)) + list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1))\n-    )\n-    cs = bs[:]\n-    n = 0\n-    for b in range(2**8):\n-        if b not in bs:\n-            bs.append(b)\n-            cs.append(2**8 + n)\n-            n += 1\n-    cs = [chr(n) for n in cs]\n-    return dict(zip(bs, cs))\n-\n-\n-# Copied from transformers.models.gpt2.tokenization_gpt2.get_pairs\n-def get_pairs(word):\n-    \"\"\"\n-    Return set of symbol pairs in a word.\n-\n-    Word is represented as tuple of symbols (symbols being variable-length strings).\n-    \"\"\"\n-    pairs = set()\n-    prev_char = word[0]\n-    for char in word[1:]:\n-        pairs.add((prev_char, char))\n-        prev_char = char\n-    return pairs\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n \n \n-class DebertaTokenizer(PreTrainedTokenizer):\n+class DebertaTokenizer(TokenizersBackend):\n     \"\"\"\n-    Construct a DeBERTa tokenizer. Based on byte-level Byte-Pair-Encoding.\n+    Construct a \"fast\" DeBERTa tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n+    Byte-Pair-Encoding.\n \n     This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n     be encoded differently whether it is at the beginning of the sentence (without space) or not:\n@@ -87,23 +45,25 @@ class DebertaTokenizer(PreTrainedTokenizer):\n     [1, 20920, 232, 2]\n     ```\n \n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n-    call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n+    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n+    the model was not pretrained this way, it might yield a decrease in performance.\n \n     <Tip>\n \n-    When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n+    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n \n     </Tip>\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n+    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`):\n+        vocab_file (`str`, *optional*):\n             Path to the vocabulary file.\n-        merges_file (`str`):\n+        merges_file (`str`, *optional*):\n             Path to the merges file.\n+        tokenizer_file (`str`, *optional*):\n+            The path to a tokenizer file to use instead of the vocab file.\n         errors (`str`, *optional*, defaults to `\"replace\"`):\n             Paradigm to follow when decoding bytes to UTF-8. See\n             [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n@@ -129,18 +89,16 @@ class DebertaTokenizer(PreTrainedTokenizer):\n         add_prefix_space (`bool`, *optional*, defaults to `False`):\n             Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n             other word. (Deberta tokenizer detect beginning of words by the preceding space).\n-        add_bos_token (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial <|endoftext|> to the input. This allows to treat the leading word just as\n-            any other word.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n \n     def __init__(\n         self,\n-        vocab_file,\n-        merges_file,\n+        vocab_file=None,\n+        vocab=None,\n+        merges=None,\n         errors=\"replace\",\n         bos_token=\"[CLS]\",\n         eos_token=\"[SEP]\",\n@@ -150,37 +108,50 @@ def __init__(\n         pad_token=\"[PAD]\",\n         mask_token=\"[MASK]\",\n         add_prefix_space=False,\n-        add_bos_token=False,\n         **kwargs,\n     ):\n-        bos_token = AddedToken(bos_token, special=True) if isinstance(bos_token, str) else bos_token\n-        eos_token = AddedToken(eos_token, special=True) if isinstance(eos_token, str) else eos_token\n-        sep_token = AddedToken(sep_token, special=True) if isinstance(sep_token, str) else sep_token\n-        cls_token = AddedToken(cls_token, special=True) if isinstance(cls_token, str) else cls_token\n-        unk_token = AddedToken(unk_token, special=True) if isinstance(unk_token, str) else unk_token\n-        pad_token = AddedToken(pad_token, special=True) if isinstance(pad_token, str) else pad_token\n-\n-        # Mask token behave like a normal word, i.e. include the space before it\n-        mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n-        self.add_bos_token = add_bos_token\n-\n-        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n-            self.encoder = json.load(vocab_handle)\n-        self.decoder = {v: k for k, v in self.encoder.items()}\n-        self.errors = errors  # how to handle errors in decoding\n-        self.byte_encoder = bytes_to_unicode()\n-        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n-        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n-            bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\n-        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n-        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n-        self.cache = {}\n+        self.vocab_file = vocab_file\n         self.add_prefix_space = add_prefix_space\n \n-        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n-        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n+            )\n+        else:\n+            self._vocab = {\n+                str(unk_token): 0,\n+                str(cls_token): 1,\n+                str(sep_token): 2,\n+                str(pad_token): 3,\n+                str(mask_token): 4,\n+            }\n+\n+        if merges is not None and isinstance(merges, list) and len(merges) > 0:\n+            self._merges = [tuple(m) if isinstance(m, list) else m for m in merges]\n+        else:\n+            self._merges = []\n+\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=self._vocab,\n+                merges=self._merges,\n+                dropout=None,\n+                unk_token=None,\n+                continuing_subword_prefix=\"\",\n+                end_of_word_suffix=\"\",\n+                fuse_unk=False,\n+            )\n+        )\n+\n+        self._tokenizer.normalizer = None\n+\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n+        self._tokenizer.decoder = decoders.ByteLevel()\n+\n+        tokenizer_object = self._tokenizer\n \n         super().__init__(\n+            tokenizer_object=tokenizer_object,\n             errors=errors,\n             bos_token=bos_token,\n             eos_token=eos_token,\n@@ -190,177 +161,44 @@ def __init__(\n             pad_token=pad_token,\n             mask_token=mask_token,\n             add_prefix_space=add_prefix_space,\n-            add_bos_token=add_bos_token,\n             **kwargs,\n         )\n \n-    @property\n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.vocab_size\n-    def vocab_size(self):\n-        return len(self.encoder)\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.get_vocab\n-    def get_vocab(self):\n-        return dict(self.encoder, **self.added_tokens_encoder)\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.bpe\n-    def bpe(self, token):\n-        if token in self.cache:\n-            return self.cache[token]\n-        word = tuple(token)\n-        pairs = get_pairs(word)\n-\n-        if not pairs:\n-            return token\n-\n-        while True:\n-            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n-            if bigram not in self.bpe_ranks:\n-                break\n-            first, second = bigram\n-            new_word = []\n-            i = 0\n-            while i < len(word):\n-                try:\n-                    j = word.index(first, i)\n-                except ValueError:\n-                    new_word.extend(word[i:])\n-                    break\n-                else:\n-                    new_word.extend(word[i:j])\n-                    i = j\n-\n-                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n-                    new_word.append(first + second)\n-                    i += 2\n-                else:\n-                    new_word.append(word[i])\n-                    i += 1\n-            new_word = tuple(new_word)\n-            word = new_word\n-            if len(word) == 1:\n-                break\n-            else:\n-                pairs = get_pairs(word)\n-        word = \" \".join(word)\n-        self.cache[token] = word\n-        return word\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A DeBERTa sequence has the following format:\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"{self.cls_token} $A {self.sep_token}\",\n+            pair=f\"{self.cls_token} $A {self.sep_token} {self.sep_token} $B {self.sep_token}\",\n+            special_tokens=[\n+                (self.cls_token, self.cls_token_id),\n+                (self.sep_token, self.sep_token_id),\n+            ],\n+        )\n \n-        - single sequence: [CLS] X [SEP]\n-        - pair of sequences: [CLS] A [SEP] B [SEP]\n+        self._post_init()\n \n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n+    @property\n+    def mask_token(self) -> str:\n+        \"\"\"\n+        `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not\n+        having been set.\n \n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+        Deberta tokenizer has a special mask token to be used in the fill-mask pipeline. The mask token will greedily\n+        comprise the space before the *[MASK]*.\n         \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n+        if self._mask_token is None:\n+            if self.verbose:\n+                logger.error(\"Using mask_token, but it is not set yet.\")\n+            return None\n+        return str(self._mask_token)\n+\n+    @mask_token.setter\n+    def mask_token(self, value):\n         \"\"\"\n-        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+        Overriding the default behavior of the mask token to have it eat the space before it.\n         \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._tokenize\n-    def _tokenize(self, text):\n-        \"\"\"Tokenize a string.\"\"\"\n-        bpe_tokens = []\n-        for token in re.findall(self.pat, text):\n-            token = \"\".join(\n-                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n-            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n-            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n-        return bpe_tokens\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.encoder.get(token, self.encoder.get(self.unk_token))\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer._convert_id_to_token\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.decoder.get(index)\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        text = \"\".join(tokens)\n-        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n-        return text\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n-        )\n-\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n-\n-        index = 0\n-        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(\"#version: 0.2\\n\")\n-            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n-                index += 1\n-\n-        return vocab_file, merge_file\n-\n-    def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n-        add_prefix_space = kwargs.pop(\"add_prefix_space\", self.add_prefix_space)\n-        if (is_split_into_words or add_prefix_space) and (len(text) > 0 and not text[0].isspace()):\n-            text = \" \" + text\n-        return (text, kwargs)\n+        # Mask token behave like a normal word, i.e. include the space before it\n+        # So we set lstrip to True\n+        value = AddedToken(value, lstrip=True, rstrip=False) if isinstance(value, str) else value\n+        self._mask_token = value\n \n \n __all__ = [\"DebertaTokenizer\"]"
        },
        {
            "sha": "c2f2e6552d9dfc8c5d3d48f3c396f816f82158bc",
            "filename": "src/transformers/models/deberta/tokenization_deberta_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 209,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,209 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 Microsoft and the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Tokenization class for model DeBERTa.\"\"\"\n-\n-from typing import Optional\n-\n-from ...tokenization_utils_base import AddedToken, BatchEncoding\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_deberta import DebertaTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class DebertaTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" DeBERTa tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n-    Byte-Pair-Encoding.\n-\n-    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n-    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n-\n-    ```python\n-    >>> from transformers import DebertaTokenizerFast\n-\n-    >>> tokenizer = DebertaTokenizerFast.from_pretrained(\"microsoft/deberta-base\")\n-    >>> tokenizer(\"Hello world\")[\"input_ids\"]\n-    [1, 31414, 232, 2]\n-\n-    >>> tokenizer(\" Hello world\")[\"input_ids\"]\n-    [1, 20920, 232, 2]\n-    ```\n-\n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n-    the model was not pretrained this way, it might yield a decrease in performance.\n-\n-    <Tip>\n-\n-    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n-\n-    </Tip>\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`, *optional*):\n-            Path to the vocabulary file.\n-        merges_file (`str`, *optional*):\n-            Path to the merges file.\n-        tokenizer_file (`str`, *optional*):\n-            The path to a tokenizer file to use instead of the vocab file.\n-        errors (`str`, *optional*, defaults to `\"replace\"`):\n-            Paradigm to follow when decoding bytes to UTF-8. See\n-            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n-        bos_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The beginning of sequence token.\n-        eos_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The end of sequence token.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        add_prefix_space (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n-            other word. (Deberta tokenizer detect beginning of words by the preceding space).\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n-    slow_tokenizer_class = DebertaTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        errors=\"replace\",\n-        bos_token=\"[CLS]\",\n-        eos_token=\"[SEP]\",\n-        sep_token=\"[SEP]\",\n-        cls_token=\"[CLS]\",\n-        unk_token=\"[UNK]\",\n-        pad_token=\"[PAD]\",\n-        mask_token=\"[MASK]\",\n-        add_prefix_space=False,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            merges_file,\n-            tokenizer_file=tokenizer_file,\n-            errors=errors,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            cls_token=cls_token,\n-            pad_token=pad_token,\n-            mask_token=mask_token,\n-            add_prefix_space=add_prefix_space,\n-            **kwargs,\n-        )\n-        self.add_bos_token = kwargs.pop(\"add_bos_token\", False)\n-\n-    @property\n-    def mask_token(self) -> str:\n-        \"\"\"\n-        `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not\n-        having been set.\n-\n-        Deberta tokenizer has a special mask token to be used in the fill-mask pipeline. The mask token will greedily\n-        comprise the space before the *[MASK]*.\n-        \"\"\"\n-        if self._mask_token is None:\n-            if self.verbose:\n-                logger.error(\"Using mask_token, but it is not set yet.\")\n-            return None\n-        return str(self._mask_token)\n-\n-    @mask_token.setter\n-    def mask_token(self, value):\n-        \"\"\"\n-        Overriding the default behavior of the mask token to have it eat the space before it.\n-        \"\"\"\n-        # Mask token behave like a normal word, i.e. include the space before it\n-        # So we set lstrip to True\n-        value = AddedToken(value, lstrip=True, rstrip=False) if isinstance(value, str) else value\n-        self._mask_token = value\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A DeBERTa sequence has the following format:\n-\n-        - single sequence: [CLS] X [SEP]\n-        - pair of sequences: [CLS] A [SEP] B [SEP]\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast._batch_encode_plus\n-    def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-        assert self.add_prefix_space or not is_split_into_words, (\n-            f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-            \"to use it with pretokenized inputs.\"\n-        )\n-\n-        return super()._batch_encode_plus(*args, **kwargs)\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast._encode_plus\n-    def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-\n-        assert self.add_prefix_space or not is_split_into_words, (\n-            f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-            \"to use it with pretokenized inputs.\"\n-        )\n-\n-        return super()._encode_plus(*args, **kwargs)\n-\n-    # Copied from transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"DebertaTokenizerFast\"]"
        },
        {
            "sha": "7c8e16f4a9ffc0478ab02d8d24fe76c6014f8141",
            "filename": "src/transformers/models/deberta_v2/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,6 @@\n     from .configuration_deberta_v2 import *\n     from .modeling_deberta_v2 import *\n     from .tokenization_deberta_v2 import *\n-    from .tokenization_deberta_v2_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "0c15913d94b894984adc0c98ae1e5989b3612bd6",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2.py",
            "status": "modified",
            "additions": 100,
            "deletions": 424,
            "changes": 524,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -12,43 +12,49 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"Tokenization class for model DeBERTa.\"\"\"\n+\"\"\"Tokenization class for model DeBERTa-v2.\"\"\"\n \n-import os\n-import unicodedata\n-from typing import Any, Optional\n+from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers\n+from tokenizers.models import Unigram\n \n-import sentencepiece as sp\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n-from ...utils.import_utils import requires\n \n \n logger = logging.get_logger(__name__)\n \n+VOCAB_FILES_NAMES = {\"vocab_file\": \"spm.model\", \"tokenizer_file\": \"tokenizer.json\"}\n+\n \n-VOCAB_FILES_NAMES = {\"vocab_file\": \"spm.model\"}\n+def _get_prepend_scheme(add_prefix_space: bool) -> str:\n+    if add_prefix_space:\n+        return \"always\"\n+    else:\n+        return \"first\"\n \n \n-@requires(backends=(\"sentencepiece\",))\n-class DebertaV2Tokenizer(PreTrainedTokenizer):\n-    r\"\"\"\n-    Constructs a DeBERTa-v2 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).\n+class DebertaV2Tokenizer(TokenizersBackend):\n+    \"\"\"\n+    Construct a DeBERTa-v2 tokenizer (backed by HuggingFace's *tokenizers* library). Based on Unigram tokenization.\n+\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n+        vocab_file (`str`, *optional*):\n+            Path to the vocabulary file (SentencePiece model file). Not used directly but kept for compatibility.\n+        vocab (`list`, *optional*):\n+            List of tuples (piece, score) for the vocabulary.\n+        precompiled_charsmap (`bytes`, *optional*):\n+            Precompiled character map for normalization.\n         do_lower_case (`bool`, *optional*, defaults to `False`):\n             Whether or not to lowercase the input when tokenizing.\n-        bos_token (`string`, *optional*, defaults to `\"[CLS]\"`):\n-            The beginning of sequence token that was used during pre-training. Can be used a sequence classifier token.\n-            When building a sequence using special tokens, this is not the token that is used for the beginning of\n-            sequence. The token used is the `cls_token`.\n-        eos_token (`string`, *optional*, defaults to `\"[SEP]\"`):\n-            The end of sequence token. When building a sequence using special tokens, this is not the token that is\n-            used for the end of sequence. The token used is the `sep_token`.\n+        split_by_punct (`bool`, *optional*, defaults to `False`):\n+            Whether to split by punctuation.\n+        bos_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n+            The beginning of sequence token.\n+        eos_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n+            The end of sequence token.\n         unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n@@ -64,28 +70,20 @@ class DebertaV2Tokenizer(PreTrainedTokenizer):\n         mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        sp_model_kwargs (`dict`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n+        add_prefix_space (`bool`, *optional*, defaults to `True`):\n+            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n+            other word.\n+        unk_id (`int`, *optional*, defaults to index of `unk_token` in vocab):\n+            The ID of the unknown token in the vocabulary.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n+    model_input_names = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n \n     def __init__(\n         self,\n-        vocab_file,\n+        vocab_file=None,\n+        vocab=None,\n         do_lower_case=False,\n         split_by_punct=False,\n         bos_token=\"[CLS]\",\n@@ -95,405 +93,83 @@ def __init__(\n         pad_token=\"[PAD]\",\n         cls_token=\"[CLS]\",\n         mask_token=\"[MASK]\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n+        add_prefix_space=True,\n+        unk_id=3,\n         **kwargs,\n-    ) -> None:\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-\n-        if not os.path.isfile(vocab_file):\n-            raise ValueError(\n-                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n-                \" model use `tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n-            )\n-        self.do_lower_case = do_lower_case\n-        self.split_by_punct = split_by_punct\n-        self.vocab_file = vocab_file\n-        self._tokenizer = SPMTokenizer(\n-            vocab_file, None, split_by_punct=split_by_punct, sp_model_kwargs=self.sp_model_kwargs\n-        )\n-        unk_token = AddedToken(unk_token, normalized=True, special=True) if isinstance(unk_token, str) else unk_token\n-        super().__init__(\n-            do_lower_case=do_lower_case,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            split_by_punct=split_by_punct,\n-            sp_model_kwargs=self.sp_model_kwargs,\n-            **kwargs,\n-        )\n-        self._tokenizer.special_tokens = self.all_special_tokens\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.vocab)\n-\n-    @property\n-    def vocab(self):\n-        return self._tokenizer.vocab\n-\n-    def get_vocab(self):\n-        vocab = self.vocab.copy()\n-        vocab.update(self.get_added_vocab())\n-        return vocab\n-\n-    def _tokenize(self, text: str) -> list[str]:\n-        \"\"\"Take as input a string and return a list of strings (tokens) for words/sub-words\"\"\"\n-        if self.do_lower_case:\n-            text = text.lower()\n-        return self._tokenizer.tokenize(text)\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self._tokenizer.spm.PieceToId(token)\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self._tokenizer.spm.IdToPiece(index) if index < self.vocab_size else self.unk_token\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        return self._tokenizer.decode(tokens)\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A DeBERTa sequence has the following format:\n-\n-        - single sequence: [CLS] X [SEP]\n-        - pair of sequences: [CLS] A [SEP] B [SEP]\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n-        \"\"\"\n-        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n-        add_prefix_space = kwargs.pop(\"add_prefix_space\", False)\n-        if is_split_into_words or add_prefix_space:\n-            text = \" \" + text\n-        return (text, kwargs)\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        return self._tokenizer.save_pretrained(save_directory, filename_prefix=filename_prefix)\n-\n-\n-class SPMTokenizer:\n-    r\"\"\"\n-    Constructs a tokenizer based on [SentencePiece](https://github.com/google/sentencepiece).\n-\n-    Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        sp_model_kwargs (`dict`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n-    \"\"\"\n-\n-    def __init__(\n-        self, vocab_file, special_tokens, split_by_punct=False, sp_model_kwargs: Optional[dict[str, Any]] = None\n     ):\n-        self.split_by_punct = split_by_punct\n         self.vocab_file = vocab_file\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-        spm = sp.SentencePieceProcessor(**self.sp_model_kwargs)\n-        if not os.path.exists(vocab_file):\n-            raise FileNotFoundError(f\"{vocab_file} does not exist!\")\n-        spm.load(vocab_file)\n-        bpe_vocab_size = spm.GetPieceSize()\n-        # Token map\n-        # <unk> 0+1\n-        # <s> 1+1\n-        # </s> 2+1\n-        self.vocab = {spm.IdToPiece(i): i for i in range(bpe_vocab_size)}\n-        self.ids_to_tokens = [spm.IdToPiece(i) for i in range(bpe_vocab_size)]\n-        # self.vocab['[PAD]'] = 0\n-        # self.vocab['[CLS]'] = 1\n-        # self.vocab['[SEP]'] = 2\n-        # self.vocab['[UNK]'] = 3\n-\n-        self.spm = spm\n-        self.special_tokens = special_tokens\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"spm\"] = None\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n-\n-        self.spm = sp.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.spm.Load(self.vocab_file)\n-\n-    def tokenize(self, text):\n-        return self._encode_as_pieces(text)\n-\n-    def convert_ids_to_tokens(self, ids):\n-        tokens = []\n-        for i in ids:\n-            tokens.append(self.ids_to_tokens[i])\n-        return tokens\n+        self.do_lower_case = do_lower_case\n+        self.split_by_punct = split_by_punct\n+        self.add_prefix_space = add_prefix_space\n+\n+        if vocab is None:\n+            self._vocab = [\n+                (str(pad_token), 0.0),\n+                (str(unk_token), 0.0),\n+                (str(bos_token), 0.0),\n+                (str(eos_token), 0.0),\n+                (str(sep_token), 0.0),\n+                (str(cls_token), 0.0),\n+                (str(mask_token), 0.0),\n+            ]\n \n-    def decode(self, tokens, start=-1, end=-1, raw_text=None):\n-        if raw_text is None:\n-            current_sub_tokens = []\n-            out_string = \"\"\n-            prev_is_special = False\n-            for token in tokens:\n-                # make sure that special tokens are not decoded using sentencepiece model\n-                if token in self.special_tokens:\n-                    if not prev_is_special:\n-                        out_string += \" \"\n-                    out_string += self.spm.decode_pieces(current_sub_tokens) + token\n-                    prev_is_special = True\n-                    current_sub_tokens = []\n-                else:\n-                    current_sub_tokens.append(token)\n-                    prev_is_special = False\n-            out_string += self.spm.decode_pieces(current_sub_tokens)\n-            return out_string.strip()\n         else:\n-            words = self.split_to_words(raw_text)\n-            word_tokens = [self.tokenize(w) for w in words]\n-            token2words = [0] * len(tokens)\n-            tid = 0\n-            for i, w in enumerate(word_tokens):\n-                for k, t in enumerate(w):\n-                    token2words[tid] = i\n-                    tid += 1\n-            word_start = token2words[start]\n-            word_end = token2words[end] if end < len(tokens) else len(words)\n-            text = \"\".join(words[word_start:word_end])\n-            return text\n-\n-    # TODO add a deprecation cycle as this can have different behaviour from our API\n-    def add_special_token(self, token):\n-        if token not in self.special_tokens:\n-            self.special_tokens.append(token)\n-            if token not in self.vocab:\n-                self.vocab[token] = len(self.vocab) - 1\n-                self.ids_to_tokens.append(token)\n-        return self.id(token)\n-\n-    def part_of_whole_word(self, token, is_bos=False):\n-        logger.warning_once(\n-            \"The `DebertaTokenizer.part_of_whole_word` method is deprecated and will be removed in `transformers==4.35`\"\n+            self._vocab = [tuple(item) if not isinstance(item, tuple) else item for item in vocab]\n+            computed_unk_id = {piece: i for i, (piece, _score) in enumerate(self._vocab)}\n+            unk_id = computed_unk_id.get(str(unk_token))\n+\n+        self._tokenizer = Tokenizer(\n+            Unigram(\n+                self._vocab,\n+                unk_id=unk_id,\n+                byte_fallback=False,\n+            )\n         )\n-        if is_bos:\n-            return True\n-        if (\n-            len(token) == 1\n-            and (_is_whitespace(list(token)[0]) or _is_control(list(token)[0]) or _is_punctuation(list(token)[0]))\n-        ) or token in self.special_tokens:\n-            return False\n-\n-        word_start = b\"\\xe2\\x96\\x81\".decode(\"utf-8\")\n-        return not token.startswith(word_start)\n-\n-    def pad(self):\n-        return \"[PAD]\"\n-\n-    def bos(self):\n-        return \"[CLS]\"\n-\n-    def eos(self):\n-        return \"[SEP]\"\n-\n-    def unk(self):\n-        return \"[UNK]\"\n-\n-    def mask(self):\n-        return \"[MASK]\"\n \n-    def sym(self, id):\n-        return self.ids_to_tokens[id]\n-\n-    def id(self, sym):\n-        logger.warning_once(\n-            \"The `DebertaTokenizer.id` method is deprecated and will be removed in `transformers==4.35`\"\n+        list_normalizers = []\n+        if do_lower_case:\n+            list_normalizers.append(normalizers.Lowercase())\n+\n+        list_normalizers.extend(\n+            [\n+                normalizers.Replace(\"\\n\", \" \"),\n+                normalizers.Replace(\"\\r\", \" \"),\n+                normalizers.Replace(\"\\t\", \" \"),\n+                normalizers.Replace(Regex(r\" {2,}\"), \" \"),\n+                normalizers.NFC(),\n+                normalizers.Strip(left=False, right=True),\n+            ]\n         )\n-        return self.vocab.get(sym, 1)\n-\n-    def _encode_as_pieces(self, text):\n-        text = convert_to_unicode(text)\n-        if self.split_by_punct:\n-            words = self._run_split_on_punc(text)\n-            pieces = [self.spm.encode(w, out_type=str) for w in words]\n-            return [p for w in pieces for p in w]\n-        else:\n-            return self.spm.encode(text, out_type=str)\n-\n-    def split_to_words(self, text):\n-        pieces = self._encode_as_pieces(text)\n-        word_start = b\"\\xe2\\x96\\x81\".decode(\"utf-8\")\n-        words = []\n-        offset = 0\n-        prev_end = 0\n-        for i, p in enumerate(pieces):\n-            if p.startswith(word_start):\n-                if offset > prev_end:\n-                    words.append(text[prev_end:offset])\n-                prev_end = offset\n-                w = p.replace(word_start, \"\")\n-            else:\n-                w = p\n-            try:\n-                s = text.index(w, offset)\n-                pn = \"\"\n-                k = i + 1\n-                while k < len(pieces):\n-                    pn = pieces[k].replace(word_start, \"\")\n-                    if len(pn) > 0:\n-                        break\n-                    k += 1\n-\n-                if len(pn) > 0 and pn in text[offset:s]:\n-                    offset = offset + 1\n-                else:\n-                    offset = s + len(w)\n-            except Exception:\n-                offset = offset + 1\n-\n-        if prev_end < offset:\n-            words.append(text[prev_end:offset])\n-\n-        return words\n+        self._tokenizer.normalizer = normalizers.Sequence(list_normalizers)\n \n-    def _run_split_on_punc(self, text):\n-        \"\"\"Splits punctuation on a piece of text.\"\"\"\n-        chars = list(text)\n-        i = 0\n-        start_new_word = True\n-        output = []\n-        while i < len(chars):\n-            char = chars[i]\n-            if _is_punctuation(char):\n-                output.append([char])\n-                start_new_word = True\n-            else:\n-                if start_new_word:\n-                    output.append([])\n-                start_new_word = False\n-                output[-1].append(char)\n-            i += 1\n+        list_pretokenizers = []\n+        if split_by_punct:\n+            list_pretokenizers.append(pre_tokenizers.Punctuation(behavior=\"isolated\"))\n \n-        return [\"\".join(x) for x in output]\n+        prepend_scheme = _get_prepend_scheme(add_prefix_space)\n+        list_pretokenizers.append(pre_tokenizers.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme))\n \n-    def save_pretrained(self, path: str, filename_prefix: Optional[str] = None):\n-        filename = VOCAB_FILES_NAMES[list(VOCAB_FILES_NAMES.keys())[0]]\n-        if filename_prefix is not None:\n-            filename = filename_prefix + \"-\" + filename\n-        full_path = os.path.join(path, filename)\n-        with open(full_path, \"wb\") as fs:\n-            fs.write(self.spm.serialized_model_proto())\n-        return (full_path,)\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(list_pretokenizers)\n \n+        self._tokenizer.decoder = decoders.Metaspace(replacement=\"‚ñÅ\", prepend_scheme=prepend_scheme)\n \n-def _is_whitespace(char):\n-    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n-    # \\t, \\n, and \\r are technically control characters but we treat them\n-    # as whitespace since they are generally considered as such.\n-    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n-        return True\n-    cat = unicodedata.category(char)\n-    if cat == \"Zs\":\n-        return True\n-    return False\n+        tokenizer_object = self._tokenizer\n \n-\n-def _is_control(char):\n-    \"\"\"Checks whether `chars` is a control character.\"\"\"\n-    # These are technically control characters but we count them as whitespace\n-    # characters.\n-    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n-        return False\n-    cat = unicodedata.category(char)\n-    if cat.startswith(\"C\"):\n-        return True\n-    return False\n-\n-\n-def _is_punctuation(char):\n-    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n-    cp = ord(char)\n-    # We treat all non-letter/number ASCII as punctuation.\n-    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n-    # Punctuation class but we treat them as punctuation anyways, for\n-    # consistency.\n-    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n-        return True\n-    cat = unicodedata.category(char)\n-    if cat.startswith(\"P\"):\n-        return True\n-    return False\n-\n-\n-def convert_to_unicode(text):\n-    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n-    if isinstance(text, str):\n-        return text\n-    elif isinstance(text, bytes):\n-        return text.decode(\"utf-8\", \"ignore\")\n-    else:\n-        raise TypeError(f\"Unsupported string type: {type(text)}\")\n+        super().__init__(\n+            tokenizer_object=tokenizer_object,\n+            bos_token=bos_token,\n+            eos_token=eos_token,\n+            unk_token=unk_token,\n+            sep_token=sep_token,\n+            cls_token=cls_token,\n+            pad_token=pad_token,\n+            mask_token=mask_token,\n+            unk_id=unk_id,\n+            do_lower_case=do_lower_case,\n+            split_by_punct=split_by_punct,\n+            add_prefix_space=add_prefix_space,\n+            **kwargs,\n+        )\n \n \n __all__ = [\"DebertaV2Tokenizer\"]"
        },
        {
            "sha": "1bad9684dd84a5c33fe8584ebb113e5bc3461e83",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 192,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,192 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 Microsoft and the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Tokenization class for model DeBERTa.\"\"\"\n-\n-import os\n-from shutil import copyfile\n-from typing import Optional\n-\n-from ...file_utils import is_sentencepiece_available\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-\n-\n-if is_sentencepiece_available():\n-    from .tokenization_deberta_v2 import DebertaV2Tokenizer\n-else:\n-    DebertaV2Tokenizer = None\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"spm.model\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class DebertaV2TokenizerFast(PreTrainedTokenizerFast):\n-    r\"\"\"\n-    Constructs a DeBERTa-v2 fast tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).\n-\n-    Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        do_lower_case (`bool`, *optional*, defaults to `False`):\n-            Whether or not to lowercase the input when tokenizing.\n-        bos_token (`string`, *optional*, defaults to `\"[CLS]\"`):\n-            The beginning of sequence token that was used during pre-training. Can be used a sequence classifier token.\n-            When building a sequence using special tokens, this is not the token that is used for the beginning of\n-            sequence. The token used is the `cls_token`.\n-        eos_token (`string`, *optional*, defaults to `\"[SEP]\"`):\n-            The end of sequence token. When building a sequence using special tokens, this is not the token that is\n-            used for the end of sequence. The token used is the `sep_token`.\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        sp_model_kwargs (`dict`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = DebertaV2Tokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=False,\n-        split_by_punct=False,\n-        bos_token=\"[CLS]\",\n-        eos_token=\"[SEP]\",\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            split_by_punct=split_by_punct,\n-            **kwargs,\n-        )\n-\n-        self.do_lower_case = do_lower_case\n-        self.split_by_punct = split_by_punct\n-        self.vocab_file = vocab_file\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A DeBERTa sequence has the following format:\n-\n-        - single sequence: [CLS] X [SEP]\n-        - pair of sequences: [CLS] A [SEP] B [SEP]\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n-        \"\"\"\n-        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not self.can_save_slow_tokenizer:\n-            raise ValueError(\n-                \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \"\n-                \"tokenizer.\"\n-            )\n-\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-\n-        return (out_vocab_file,)\n-\n-\n-__all__ = [\"DebertaV2TokenizerFast\"]"
        },
        {
            "sha": "6bc56788979106427513c51489a2ff16e7ffee92",
            "filename": "src/transformers/models/dia/tokenization_dia.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdia%2Ftokenization_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdia%2Ftokenization_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Ftokenization_dia.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -16,7 +16,7 @@\n \n from typing import Optional\n \n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_python import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n \n \n@@ -63,6 +63,10 @@ def __init__(\n             unk_token=unk_token,\n             pad_token=pad_token,\n             max_length=max_length,\n+            offset=offset,\n+            token_type_ids_pattern=\"all_zeros\",\n+            token_type_ids_include_special_tokens=True,\n+            special_tokens_pattern=\"none\",\n             **kwargs,\n         )\n \n@@ -110,9 +114,5 @@ def convert_tokens_to_string(self, tokens: list[str]) -> str:\n         string = bstring.decode(\"utf-8\", errors=\"ignore\")\n         return string\n \n-    # No vocab file\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        return ()\n-\n \n __all__ = [\"DiaTokenizer\"]"
        },
        {
            "sha": "0036fba6ed7fef887b89d518c23277e8d550e93d",
            "filename": "src/transformers/models/distilbert/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdistilbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdistilbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,8 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_distilbert import *\n     from .modeling_distilbert import *\n-    from .tokenization_distilbert import *\n-    from .tokenization_distilbert_fast import *\n+    from .tokenization_distilbert import DistilBertTokenizer, DistilBertTokenizerFast\n else:\n     import sys\n "
        },
        {
            "sha": "de3271c8cbc733ca5f5c78fcb4941498c996d395",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 468,
            "changes": 474,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,479 +14,17 @@\n # limitations under the License.\n \"\"\"Tokenization classes for DistilBERT.\"\"\"\n \n-import collections\n-import os\n-import unicodedata\n-from typing import Optional\n+from ...models.bert.tokenization_bert import BertTokenizer\n \n-from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n-from ...utils import logging\n \n+VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n \n-logger = logging.get_logger(__name__)\n \n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.load_vocab\n-def load_vocab(vocab_file):\n-    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n-    vocab = collections.OrderedDict()\n-    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n-        tokens = reader.readlines()\n-    for index, token in enumerate(tokens):\n-        token = token.rstrip(\"\\n\")\n-        vocab[token] = index\n-    return vocab\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize\n-def whitespace_tokenize(text):\n-    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n-    text = text.strip()\n-    if not text:\n-        return []\n-    tokens = text.split()\n-    return tokens\n-\n-\n-class DistilBertTokenizer(PreTrainedTokenizer):\n-    r\"\"\"\n-    Construct a DistilBERT tokenizer. Based on WordPiece.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        do_basic_tokenize (`bool`, *optional*, defaults to `True`):\n-            Whether or not to do basic tokenization before WordPiece.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n-            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n-            extra spaces.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n+class DistilBertTokenizer(BertTokenizer):\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n \n-    def __init__(\n-        self,\n-        vocab_file,\n-        do_lower_case=True,\n-        do_basic_tokenize=True,\n-        never_split=None,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        clean_up_tokenization_spaces=True,\n-        **kwargs,\n-    ):\n-        if not os.path.isfile(vocab_file):\n-            raise ValueError(\n-                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n-                \" model use `tokenizer = DistilBertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n-            )\n-        self.vocab = load_vocab(vocab_file)\n-        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n-        self.do_basic_tokenize = do_basic_tokenize\n-        if do_basic_tokenize:\n-            self.basic_tokenizer = BasicTokenizer(\n-                do_lower_case=do_lower_case,\n-                never_split=never_split,\n-                tokenize_chinese_chars=tokenize_chinese_chars,\n-                strip_accents=strip_accents,\n-            )\n-        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=str(unk_token))\n-\n-        super().__init__(\n-            do_lower_case=do_lower_case,\n-            do_basic_tokenize=do_basic_tokenize,\n-            never_split=never_split,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            **kwargs,\n-        )\n-\n-    @property\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.do_lower_case\n-    def do_lower_case(self):\n-        return self.basic_tokenizer.do_lower_case\n-\n-    @property\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.vocab_size\n-    def vocab_size(self):\n-        return len(self.vocab)\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.get_vocab\n-    def get_vocab(self):\n-        return dict(self.vocab, **self.added_tokens_encoder)\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer._tokenize\n-    def _tokenize(self, text, split_special_tokens=False):\n-        split_tokens = []\n-        if self.do_basic_tokenize:\n-            for token in self.basic_tokenizer.tokenize(\n-                text, never_split=self.all_special_tokens if not split_special_tokens else None\n-            ):\n-                # If the token is part of the never_split set\n-                if token in self.basic_tokenizer.never_split:\n-                    split_tokens.append(token)\n-                else:\n-                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n-        else:\n-            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n-        return split_tokens\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer._convert_token_to_id\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.vocab.get(token, self.vocab.get(self.unk_token))\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer._convert_id_to_token\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.ids_to_tokens.get(index, self.unk_token)\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.convert_tokens_to_string\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n-        return out_string\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.build_inputs_with_special_tokens\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.get_special_tokens_mask\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        index = 0\n-        if os.path.isdir(save_directory):\n-            vocab_file = os.path.join(\n-                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-            )\n-        else:\n-            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n-            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n-                        \" Please check that the vocabulary is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(token + \"\\n\")\n-                index += 1\n-        return (vocab_file,)\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer\n-class BasicTokenizer:\n-    \"\"\"\n-    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n-\n-    Args:\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        do_split_on_punc (`bool`, *optional*, defaults to `True`):\n-            In some instances we want to skip the basic punctuation splitting so that later tokenization can capture\n-            the full context of the words, such as contractions.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        do_lower_case=True,\n-        never_split=None,\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        do_split_on_punc=True,\n-    ):\n-        if never_split is None:\n-            never_split = []\n-        self.do_lower_case = do_lower_case\n-        self.never_split = set(never_split)\n-        self.tokenize_chinese_chars = tokenize_chinese_chars\n-        self.strip_accents = strip_accents\n-        self.do_split_on_punc = do_split_on_punc\n-\n-    def tokenize(self, text, never_split=None):\n-        \"\"\"\n-        Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n-\n-        Args:\n-            never_split (`List[str]`, *optional*)\n-                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n-                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n-        \"\"\"\n-        # union() returns a new set by concatenating the two sets.\n-        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n-        text = self._clean_text(text)\n-\n-        # This was added on November 1st, 2018 for the multilingual and Chinese\n-        # models. This is also applied to the English models now, but it doesn't\n-        # matter since the English models were not trained on any Chinese data\n-        # and generally don't have any Chinese data in them (there are Chinese\n-        # characters in the vocabulary because Wikipedia does have some Chinese\n-        # words in the English Wikipedia.).\n-        if self.tokenize_chinese_chars:\n-            text = self._tokenize_chinese_chars(text)\n-        # prevents treating the same character with different unicode codepoints as different characters\n-        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n-        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n-        split_tokens = []\n-        for token in orig_tokens:\n-            if token not in never_split:\n-                if self.do_lower_case:\n-                    token = token.lower()\n-                    if self.strip_accents is not False:\n-                        token = self._run_strip_accents(token)\n-                elif self.strip_accents:\n-                    token = self._run_strip_accents(token)\n-            split_tokens.extend(self._run_split_on_punc(token, never_split))\n-\n-        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n-        return output_tokens\n-\n-    def _run_strip_accents(self, text):\n-        \"\"\"Strips accents from a piece of text.\"\"\"\n-        text = unicodedata.normalize(\"NFD\", text)\n-        output = []\n-        for char in text:\n-            cat = unicodedata.category(char)\n-            if cat == \"Mn\":\n-                continue\n-            output.append(char)\n-        return \"\".join(output)\n-\n-    def _run_split_on_punc(self, text, never_split=None):\n-        \"\"\"Splits punctuation on a piece of text.\"\"\"\n-        if not self.do_split_on_punc or (never_split is not None and text in never_split):\n-            return [text]\n-        chars = list(text)\n-        i = 0\n-        start_new_word = True\n-        output = []\n-        while i < len(chars):\n-            char = chars[i]\n-            if _is_punctuation(char):\n-                output.append([char])\n-                start_new_word = True\n-            else:\n-                if start_new_word:\n-                    output.append([])\n-                start_new_word = False\n-                output[-1].append(char)\n-            i += 1\n-\n-        return [\"\".join(x) for x in output]\n-\n-    def _tokenize_chinese_chars(self, text):\n-        \"\"\"Adds whitespace around any CJK character.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if self._is_chinese_char(cp):\n-                output.append(\" \")\n-                output.append(char)\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-    def _is_chinese_char(self, cp):\n-        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n-        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n-        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n-        #\n-        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n-        # despite its name. The modern Korean Hangul alphabet is a different block,\n-        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n-        # space-separated words, so they are not treated specially and handled\n-        # like the all of the other languages.\n-        if (\n-            (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n-            or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n-        ):\n-            return True\n-\n-        return False\n-\n-    def _clean_text(self, text):\n-        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if cp == 0 or cp == 0xFFFD or _is_control(char):\n-                continue\n-            if _is_whitespace(char):\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.WordpieceTokenizer\n-class WordpieceTokenizer:\n-    \"\"\"Runs WordPiece tokenization.\"\"\"\n-\n-    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n-        self.vocab = vocab\n-        self.unk_token = unk_token\n-        self.max_input_chars_per_word = max_input_chars_per_word\n-\n-    def tokenize(self, text):\n-        \"\"\"\n-        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n-        tokenization using the given vocabulary.\n-\n-        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n-\n-        Args:\n-            text: A single token or whitespace separated tokens. This should have\n-                already been passed through *BasicTokenizer*.\n-\n-        Returns:\n-            A list of wordpiece tokens.\n-        \"\"\"\n-\n-        output_tokens = []\n-        for token in whitespace_tokenize(text):\n-            chars = list(token)\n-            if len(chars) > self.max_input_chars_per_word:\n-                output_tokens.append(self.unk_token)\n-                continue\n-\n-            is_bad = False\n-            start = 0\n-            sub_tokens = []\n-            while start < len(chars):\n-                end = len(chars)\n-                cur_substr = None\n-                while start < end:\n-                    substr = \"\".join(chars[start:end])\n-                    if start > 0:\n-                        substr = \"##\" + substr\n-                    if substr in self.vocab:\n-                        cur_substr = substr\n-                        break\n-                    end -= 1\n-                if cur_substr is None:\n-                    is_bad = True\n-                    break\n-                sub_tokens.append(cur_substr)\n-                start = end\n-\n-            if is_bad:\n-                output_tokens.append(self.unk_token)\n-            else:\n-                output_tokens.extend(sub_tokens)\n-        return output_tokens\n \n+# DistilBertTokenizerFast is an alias for DistilBertTokenizer (since BertTokenizer is already a fast tokenizer)\n+DistilBertTokenizerFast = DistilBertTokenizer\n \n-__all__ = [\"DistilBertTokenizer\"]\n+__all__ = [\"DistilBertTokenizer\", \"DistilBertTokenizerFast\"]"
        },
        {
            "sha": "c174804dc530c863b14c3ab6e56c18117d3fa4c2",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 149,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,149 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for DistilBERT.\"\"\"\n-\n-import json\n-from typing import Optional\n-\n-from tokenizers import normalizers\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_distilbert import DistilBertTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class DistilBertTokenizerFast(PreTrainedTokenizerFast):\n-    r\"\"\"\n-    Construct a \"fast\" DistilBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        clean_text (`bool`, *optional*, defaults to `True`):\n-            Whether or not to clean the text before tokenization by removing any control characters and replacing all\n-            whitespaces by the classic one.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n-            issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n-            The prefix for subwords.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = DistilBertTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=True,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            **kwargs,\n-        )\n-\n-        normalizer_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n-        if (\n-            normalizer_state.get(\"lowercase\", do_lower_case) != do_lower_case\n-            or normalizer_state.get(\"strip_accents\", strip_accents) != strip_accents\n-            or normalizer_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n-        ):\n-            normalizer_class = getattr(normalizers, normalizer_state.pop(\"type\"))\n-            normalizer_state[\"lowercase\"] = do_lower_case\n-            normalizer_state[\"strip_accents\"] = strip_accents\n-            normalizer_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n-            self.backend_tokenizer.normalizer = normalizer_class(**normalizer_state)\n-\n-        self.do_lower_case = do_lower_case\n-\n-    # Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.build_inputs_with_special_tokens\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-\n-        if token_ids_1 is not None:\n-            output += token_ids_1 + [self.sep_token_id]\n-\n-        return output\n-\n-    # Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"DistilBertTokenizerFast\"]"
        },
        {
            "sha": "968a579267d201fbee1726065c098c9ba0e5ed23",
            "filename": "src/transformers/models/dpr/tokenization_dpr_fast.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -19,7 +19,7 @@\n \n from ...tokenization_utils_base import BatchEncoding\n from ...utils import TensorType, add_end_docstrings, add_start_docstrings, logging\n-from ..bert.tokenization_bert_fast import BertTokenizerFast\n+from ..bert.tokenization_bert import BertTokenizer\n from .tokenization_dpr import DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer, DPRReaderTokenizer\n \n \n@@ -28,28 +28,28 @@\n VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n \n \n-class DPRContextEncoderTokenizerFast(BertTokenizerFast):\n+class DPRContextEncoderTokenizerFast(BertTokenizer):\n     r\"\"\"\n     Construct a \"fast\" DPRContextEncoder tokenizer (backed by HuggingFace's *tokenizers* library).\n \n-    [`DPRContextEncoderTokenizerFast`] is identical to [`BertTokenizerFast`] and runs end-to-end tokenization:\n+    [`DPRContextEncoderTokenizerFast`] is identical to [`BertTokenizer`] and runs end-to-end tokenization:\n     punctuation splitting and wordpiece.\n \n-    Refer to superclass [`BertTokenizerFast`] for usage examples and documentation concerning parameters.\n+    Refer to superclass [`BertTokenizer`] for usage examples and documentation concerning parameters.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     slow_tokenizer_class = DPRContextEncoderTokenizer\n \n \n-class DPRQuestionEncoderTokenizerFast(BertTokenizerFast):\n+class DPRQuestionEncoderTokenizerFast(BertTokenizer):\n     r\"\"\"\n     Constructs a \"fast\" DPRQuestionEncoder tokenizer (backed by HuggingFace's *tokenizers* library).\n \n-    [`DPRQuestionEncoderTokenizerFast`] is identical to [`BertTokenizerFast`] and runs end-to-end tokenization:\n+    [`DPRQuestionEncoderTokenizerFast`] is identical to [`BertTokenizer`] and runs end-to-end tokenization:\n     punctuation splitting and wordpiece.\n \n-    Refer to superclass [`BertTokenizerFast`] for usage examples and documentation concerning parameters.\n+    Refer to superclass [`BertTokenizer`] for usage examples and documentation concerning parameters.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n@@ -300,15 +300,15 @@ def _get_best_spans(\n \n \n @add_end_docstrings(CUSTOM_DPR_READER_DOCSTRING)\n-class DPRReaderTokenizerFast(CustomDPRReaderTokenizerMixin, BertTokenizerFast):\n+class DPRReaderTokenizerFast(CustomDPRReaderTokenizerMixin, BertTokenizer):\n     r\"\"\"\n     Constructs a \"fast\" DPRReader tokenizer (backed by HuggingFace's *tokenizers* library).\n \n-    [`DPRReaderTokenizerFast`] is almost identical to [`BertTokenizerFast`] and runs end-to-end tokenization:\n+    [`DPRReaderTokenizerFast`] is almost identical to [`BertTokenizer`] and runs end-to-end tokenization:\n     punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts\n     that are combined to be fed to the [`DPRReader`] model.\n \n-    Refer to superclass [`BertTokenizerFast`] for usage examples and documentation concerning parameters.\n+    Refer to superclass [`BertTokenizer`] for usage examples and documentation concerning parameters.\n \n     \"\"\"\n "
        },
        {
            "sha": "200b5e4106a0b200a9615a63f44b0def7c7e0650",
            "filename": "src/transformers/models/electra/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Felectra%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Felectra%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,10 +18,9 @@\n \n \n if TYPE_CHECKING:\n+    from ..bert.tokenization_bert import BertTokenizer as ElectraTokenizer\n     from .configuration_electra import *\n     from .modeling_electra import *\n-    from .tokenization_electra import *\n-    from .tokenization_electra_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "db0285581ed1eea5b903a3bed573bbf6408e0167",
            "filename": "src/transformers/models/electra/tokenization_electra_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 143,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,143 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 The Google AI Team, Stanford University and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import json\n-from typing import Optional\n-\n-from tokenizers import normalizers\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from .tokenization_electra import ElectraTokenizer\n-\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast with Bert->Electra , BERT->ELECTRA\n-class ElectraTokenizerFast(PreTrainedTokenizerFast):\n-    r\"\"\"\n-    Construct a \"fast\" ELECTRA tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        clean_text (`bool`, *optional*, defaults to `True`):\n-            Whether or not to clean the text before tokenization by removing any control characters and replacing all\n-            whitespaces by the classic one.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n-            issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original ELECTRA).\n-        wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n-            The prefix for subwords.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = ElectraTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=True,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            **kwargs,\n-        )\n-\n-        normalizer_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n-        if (\n-            normalizer_state.get(\"lowercase\", do_lower_case) != do_lower_case\n-            or normalizer_state.get(\"strip_accents\", strip_accents) != strip_accents\n-            or normalizer_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n-        ):\n-            normalizer_class = getattr(normalizers, normalizer_state.pop(\"type\"))\n-            normalizer_state[\"lowercase\"] = do_lower_case\n-            normalizer_state[\"strip_accents\"] = strip_accents\n-            normalizer_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n-            self.backend_tokenizer.normalizer = normalizer_class(**normalizer_state)\n-\n-        self.do_lower_case = do_lower_case\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A ELECTRA sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-\n-        if token_ids_1 is not None:\n-            output += token_ids_1 + [self.sep_token_id]\n-\n-        return output\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"ElectraTokenizerFast\"]"
        },
        {
            "sha": "c926a0adec6ab38b09ec8f2ae2c4ada39041dee7",
            "filename": "src/transformers/models/esm/tokenization_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fesm%2Ftokenization_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fesm%2Ftokenization_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Ftokenization_esm.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -17,7 +17,7 @@\n import os\n from typing import Optional\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n \n "
        },
        {
            "sha": "7ccb482597a16c70f7dbd469d1aa72d21d89cbec",
            "filename": "src/transformers/models/evolla/processing_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -65,7 +65,7 @@ def process_proteins(self, proteins, protein_max_length=1024):\n             sa_sequence = \"\".join([s.upper() + f.lower() for s, f in zip(aa_seq, foldseek)])\n             sa_sequences.append(sa_sequence)\n \n-        sa_tokens = self.protein_tokenizer.batch_encode_plus(\n+        sa_tokens = self.protein_tokenizer(\n             sa_sequences, return_tensors=\"pt\", truncation=True, max_length=protein_max_length, padding=True\n         )\n         return sa_tokens"
        },
        {
            "sha": "9a3cb9b366f6399a8ce38b6d04ee42c0d1507616",
            "filename": "src/transformers/models/fastspeech2_conformer/tokenization_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Ftokenization_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Ftokenization_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Ftokenization_fastspeech2_conformer.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,7 +20,7 @@\n \n import regex\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging, requires_backends\n \n "
        },
        {
            "sha": "b5aba624a531bd8005e265e3c25d45b8ad4cb81b",
            "filename": "src/transformers/models/flaubert/tokenization_flaubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fflaubert%2Ftokenization_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fflaubert%2Ftokenization_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Ftokenization_flaubert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,7 +20,7 @@\n import unicodedata\n from typing import Optional\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n \n "
        },
        {
            "sha": "00785b5e50753c20f5049ce1f74a5d4878ab7499",
            "filename": "src/transformers/models/fnet/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,8 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_fnet import *\n     from .modeling_fnet import *\n-    from .tokenization_fnet import *\n-    from .tokenization_fnet_fast import *\n+    from .tokenization_fnet import FNetTokenizer, FNetTokenizerFast\n else:\n     import sys\n "
        },
        {
            "sha": "fa40ff69245bd8acee0402f1f6aee0da3c0205bd",
            "filename": "src/transformers/models/fnet/tokenization_fnet.py",
            "status": "modified",
            "additions": 16,
            "deletions": 267,
            "changes": 283,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1,5 +1,5 @@\n # coding=utf-8\n-# Copyright 2021 Google Research, Google AI, Google Brain and the HuggingFace Inc. team.\n+# Copyright 2024 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -14,43 +14,29 @@\n # limitations under the License.\n \"\"\"Tokenization classes for FNet model.\"\"\"\n \n-import os\n-import unicodedata\n-from shutil import copyfile\n-from typing import Any, Optional\n-\n-import sentencepiece as spm\n-\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n-from ...utils.import_utils import requires\n+from ..albert.tokenization_albert import AlbertTokenizer\n \n \n logger = logging.get_logger(__name__)\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\"}\n-\n \n-SPIECE_UNDERLINE = \"‚ñÅ\"\n \n-\n-@requires(backends=(\"sentencepiece\",))\n-class FNetTokenizer(PreTrainedTokenizer):\n+class FNetTokenizer(AlbertTokenizer):\n     \"\"\"\n-    Construct an FNet tokenizer. Adapted from [`AlbertTokenizer`]. Based on\n-    [SentencePiece](https://github.com/google/sentencepiece). This tokenizer inherits from [`PreTrainedTokenizer`]\n-    which contains most of the main methods. Users should refer to this superclass for more information regarding those\n-    methods.\n+    Construct an FNet tokenizer. Based on [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).\n+\n+    This tokenizer inherits from [`AlbertTokenizer`] which contains most of the main methods. Users should refer to\n+    this superclass for more information regarding those methods.\n \n     Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        do_lower_case (`bool`, *optional*, defaults to `False`):\n+        do_lower_case (`bool`, *optional*, defaults to `True`):\n             Whether or not to lowercase the input when tokenizing.\n-        remove_space (`bool`, *optional*, defaults to `True`):\n-            Whether or not to strip the text when tokenizing (removing excess spaces before and after the string).\n-        keep_accents (`bool`, *optional*, defaults to `True`):\n+        keep_accents (`bool`, *optional*, defaults to `False`):\n             Whether or not to keep accents when tokenizing.\n+        bos_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n+            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n+        eos_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n+            The end of sequence token.\n         unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n@@ -66,249 +52,12 @@ class FNetTokenizer(PreTrainedTokenizer):\n         mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        sp_model_kwargs (`dict`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n-\n-    Attributes:\n-        sp_model (`SentencePieceProcessor`):\n-            The *SentencePiece* processor that is used for every conversion (string, tokens and IDs).\n     \"\"\"\n \n-    vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"token_type_ids\"]\n \n-    def __init__(\n-        self,\n-        vocab_file,\n-        do_lower_case=False,\n-        remove_space=True,\n-        keep_accents=True,\n-        unk_token=\"<unk>\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"<pad>\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n-        **kwargs,\n-    ) -> None:\n-        # Mask token behave like a normal word, i.e. include the space before it and\n-        # is included in the raw text, there should be a match in a non-normalized sentence.\n-        mask_token = AddedToken(mask_token, lstrip=True, special=True) if isinstance(mask_token, str) else mask_token\n-        cls_token = AddedToken(cls_token, special=True) if isinstance(cls_token, str) else cls_token\n-        sep_token = AddedToken(sep_token, special=True) if isinstance(sep_token, str) else sep_token\n-        mask_token = AddedToken(mask_token, special=True) if isinstance(mask_token, str) else mask_token\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-\n-        self.do_lower_case = do_lower_case\n-        self.remove_space = remove_space\n-        self.keep_accents = keep_accents\n-        self.vocab_file = vocab_file\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(vocab_file)\n-\n-        super().__init__(\n-            do_lower_case=do_lower_case,\n-            remove_space=remove_space,\n-            keep_accents=keep_accents,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            sp_model_kwargs=self.sp_model_kwargs,\n-            **kwargs,\n-        )\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.sp_model)\n-\n-    def get_vocab(self):\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n-\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(self.vocab_file)\n-\n-    def preprocess_text(self, inputs):\n-        if self.remove_space:\n-            outputs = \" \".join(inputs.strip().split())\n-        else:\n-            outputs = inputs\n-        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n-\n-        if not self.keep_accents:\n-            outputs = unicodedata.normalize(\"NFKD\", outputs)\n-            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n-        if self.do_lower_case:\n-            outputs = outputs.lower()\n-\n-        return outputs\n-\n-    def _tokenize(self, text: str) -> list[str]:\n-        \"\"\"Tokenize a string.\"\"\"\n-        text = self.preprocess_text(text)\n-        pieces = self.sp_model.encode(text, out_type=str)\n-        new_pieces = []\n-        for piece in pieces:\n-            if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n-                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n-                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n-                    if len(cur_pieces[0]) == 1:\n-                        cur_pieces = cur_pieces[1:]\n-                    else:\n-                        cur_pieces[0] = cur_pieces[0][1:]\n-                cur_pieces.append(piece[-1])\n-                new_pieces.extend(cur_pieces)\n-            else:\n-                new_pieces.append(piece)\n-\n-        return new_pieces\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.sp_model.PieceToId(token)\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.sp_model.IdToPiece(index)\n-\n-    # Copied from transformers.models.albert.tokenization_albert.AlbertTokenizer.convert_tokens_to_string\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        current_sub_tokens = []\n-        out_string = \"\"\n-        prev_is_special = False\n-        for token in tokens:\n-            # make sure that special tokens are not decoded using sentencepiece model\n-            if token in self.all_special_tokens:\n-                if not prev_is_special:\n-                    out_string += \" \"\n-                out_string += self.sp_model.decode(current_sub_tokens) + token\n-                prev_is_special = True\n-                current_sub_tokens = []\n-            else:\n-                current_sub_tokens.append(token)\n-                prev_is_special = False\n-        out_string += self.sp_model.decode(current_sub_tokens)\n-        return out_string.strip()\n-\n-    def _decode(\n-        self,\n-        token_ids: list[int],\n-        skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: Optional[bool] = None,\n-        spaces_between_special_tokens: bool = False,\n-        **kwargs,\n-    ) -> str:\n-        text = super()._decode(\n-            token_ids=token_ids,\n-            skip_special_tokens=skip_special_tokens,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            spaces_between_special_tokens=spaces_between_special_tokens,\n-            **kwargs,\n-        )\n-        # Mimic the behavior of the Rust tokenizer:\n-        # No space after <unk>\n-        if not spaces_between_special_tokens:\n-            text = text.replace(\"<unk> \", \"<unk>\")\n-        return text\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An FNet sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return cls + token_ids_0 + sep\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n-\n-        return (out_vocab_file,)\n \n+# FNetTokenizerFast is an alias for FNetTokenizer (since AlbertTokenizer is already a fast tokenizer)\n+FNetTokenizerFast = FNetTokenizer\n \n-__all__ = [\"FNetTokenizer\"]\n+__all__ = [\"FNetTokenizer\", \"FNetTokenizerFast\"]"
        },
        {
            "sha": "4aab7997650f42bc0a7c185c5fa876a85c157a41",
            "filename": "src/transformers/models/fnet/tokenization_fnet_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 155,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,155 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 Google AI, Google Brain and the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for FNet model.\"\"\"\n-\n-import os\n-from shutil import copyfile\n-from typing import Optional\n-\n-from ...tokenization_utils import AddedToken\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import is_sentencepiece_available, logging\n-\n-\n-if is_sentencepiece_available():\n-    from .tokenization_fnet import FNetTokenizer\n-else:\n-    FNetTokenizer = None\n-\n-logger = logging.get_logger(__name__)\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"spiece.model\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-SPIECE_UNDERLINE = \"‚ñÅ\"\n-\n-\n-class FNetTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" FNetTokenizer (backed by HuggingFace's *tokenizers* library). Adapted from\n-    [`AlbertTokenizerFast`]. Based on\n-    [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models). This\n-    tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods\n-\n-    Args:\n-        vocab_file (`str`):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a *.spm* extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        do_lower_case (`bool`, *optional*, defaults to `False`):\n-            Whether or not to lowercase the input when tokenizing.\n-        remove_space (`bool`, *optional*, defaults to `True`):\n-            Whether or not to strip the text when tokenizing (removing excess spaces before and after the string).\n-        keep_accents (`bool`, *optional*, defaults to `True`):\n-            Whether or not to keep accents when tokenizing.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"token_type_ids\"]\n-    slow_tokenizer_class = FNetTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=False,\n-        remove_space=True,\n-        keep_accents=True,\n-        unk_token=\"<unk>\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"<pad>\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        **kwargs,\n-    ):\n-        # Mask token behave like a normal word, i.e. include the space before it and\n-        # is included in the raw text, there should be a match in a non-normalized sentence.\n-        mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n-        cls_token = AddedToken(cls_token, lstrip=False, rstrip=False) if isinstance(cls_token, str) else cls_token\n-        sep_token = AddedToken(sep_token, lstrip=False, rstrip=False) if isinstance(sep_token, str) else sep_token\n-\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            remove_space=remove_space,\n-            keep_accents=keep_accents,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            **kwargs,\n-        )\n-\n-        self.do_lower_case = do_lower_case\n-        self.remove_space = remove_space\n-        self.keep_accents = keep_accents\n-        self.vocab_file = vocab_file\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An FNet sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return cls + token_ids_0 + sep\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-\n-        return (out_vocab_file,)\n-\n-\n-__all__ = [\"FNetTokenizerFast\"]"
        },
        {
            "sha": "1a4215cf2ad48380f7eec1854e02794c25833f3c",
            "filename": "src/transformers/models/fsmt/tokenization_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffsmt%2Ftokenization_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffsmt%2Ftokenization_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Ftokenization_fsmt.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,7 +20,7 @@\n import unicodedata\n from typing import Optional\n \n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n \n "
        },
        {
            "sha": "e55e71fe491e7cab64dc9c046bab1d60910f34f5",
            "filename": "src/transformers/models/funnel/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffunnel%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffunnel%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -22,7 +22,6 @@\n     from .convert_funnel_original_tf_checkpoint_to_pytorch import *\n     from .modeling_funnel import *\n     from .tokenization_funnel import *\n-    from .tokenization_funnel_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "7d6e36dfa0da74c138fed98829cdf5a61e4de0ef",
            "filename": "src/transformers/models/funnel/tokenization_funnel.py",
            "status": "modified",
            "additions": 80,
            "deletions": 448,
            "changes": 528,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,12 +14,12 @@\n # limitations under the License.\n \"\"\"Tokenization class for Funnel Transformer.\"\"\"\n \n-import collections\n-import os\n-import unicodedata\n from typing import Optional\n \n-from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n+from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from tokenizers.models import WordPiece\n+\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n \n@@ -41,45 +41,18 @@\n ]\n \n \n-# Copied from transformers.models.bert.tokenization_bert.load_vocab\n-def load_vocab(vocab_file):\n-    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n-    vocab = collections.OrderedDict()\n-    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n-        tokens = reader.readlines()\n-    for index, token in enumerate(tokens):\n-        token = token.rstrip(\"\\n\")\n-        vocab[token] = index\n-    return vocab\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize\n-def whitespace_tokenize(text):\n-    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n-    text = text.strip()\n-    if not text:\n-        return []\n-    tokens = text.split()\n-    return tokens\n-\n-\n-class FunnelTokenizer(PreTrainedTokenizer):\n+class FunnelTokenizer(TokenizersBackend):\n     r\"\"\"\n-    Construct a Funnel Transformer tokenizer. Based on WordPiece.\n+    Construct a Funnel Transformer tokenizer (backed by HuggingFace's tokenizers library). Based on WordPiece.\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n \n     Args:\n         vocab_file (`str`):\n             File containing the vocabulary.\n         do_lower_case (`bool`, *optional*, defaults to `True`):\n             Whether or not to lowercase the input when tokenizing.\n-        do_basic_tokenize (`bool`, *optional*, defaults to `True`):\n-            Whether or not to do basic tokenization before WordPiece.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n         unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n             token instead.\n@@ -95,448 +68,107 @@ class FunnelTokenizer(PreTrainedTokenizer):\n         mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n+        clean_text (`bool`, *optional*, defaults to `True`):\n+            Whether or not to clean the text before tokenization by removing any control characters and replacing all\n+            whitespaces by the classic one.\n+        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n+            Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n+            issue](https://github.com/huggingface/transformers/issues/328)).\n+        bos_token (`str`, `optional`, defaults to `\"<s>\"`):\n             The beginning of sentence token.\n-        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n+        eos_token (`str`, `optional`, defaults to `\"</s>\"`):\n             The end of sentence token.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n         strip_accents (`bool`, *optional*):\n             Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n             value for `lowercase` (as in the original BERT).\n-        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n-            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n-            extra spaces.\n+        wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n+            The prefix for subwords.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n+    slow_tokenizer_class = None\n     cls_token_type_id: int = 2\n \n     def __init__(\n         self,\n-        vocab_file,\n-        do_lower_case=True,\n-        do_basic_tokenize=True,\n-        never_split=None,\n-        unk_token=\"<unk>\",\n-        sep_token=\"<sep>\",\n-        pad_token=\"<pad>\",\n-        cls_token=\"<cls>\",\n-        mask_token=\"<mask>\",\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        clean_up_tokenization_spaces=True,\n+        do_lower_case: bool = True,\n+        unk_token: str = \"<unk>\",\n+        sep_token: str = \"<sep>\",\n+        pad_token: str = \"<pad>\",\n+        cls_token: str = \"<cls>\",\n+        mask_token: str = \"<mask>\",\n+        bos_token: str = \"<s>\",\n+        eos_token: str = \"</s>\",\n+        clean_text: bool = True,\n+        tokenize_chinese_chars: bool = True,\n+        strip_accents: Optional[bool] = None,\n+        wordpieces_prefix: str = \"##\",\n+        vocab: Optional[dict] = None,\n+        vocab_file: Optional[str] = None,\n         **kwargs,\n     ):\n-        if not os.path.isfile(vocab_file):\n-            raise ValueError(\n-                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n-                \" model use `tokenizer = FunnelTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n-            )\n-        self.vocab = load_vocab(vocab_file)\n-        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n-        self.do_basic_tokenize = do_basic_tokenize\n-        if do_basic_tokenize:\n-            self.basic_tokenizer = BasicTokenizer(\n-                do_lower_case=do_lower_case,\n-                never_split=never_split,\n-                tokenize_chinese_chars=tokenize_chinese_chars,\n-                strip_accents=strip_accents,\n+        self.vocab_file = vocab_file\n+        self.do_lower_case = do_lower_case\n+        self.tokenize_chinese_chars = tokenize_chinese_chars\n+        self.strip_accents = strip_accents\n+        self.clean_text = clean_text\n+        self.wordpieces_prefix = wordpieces_prefix\n+\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n             )\n-        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=str(unk_token))\n+        else:\n+            self._vocab = {\n+                str(pad_token): 0,\n+                str(unk_token): 1,\n+                str(cls_token): 2,\n+                str(sep_token): 3,\n+                str(mask_token): 4,\n+                str(bos_token): 5,\n+                str(eos_token): 6,\n+            }\n+\n+        self._tokenizer = Tokenizer(WordPiece(self._vocab, unk_token=str(unk_token)))\n+\n+        self._tokenizer.normalizer = normalizers.BertNormalizer(\n+            clean_text=clean_text,\n+            handle_chinese_chars=tokenize_chinese_chars,\n+            strip_accents=strip_accents,\n+            lowercase=do_lower_case,\n+        )\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n+        self._tokenizer.decoder = decoders.WordPiece(prefix=wordpieces_prefix)\n+\n+        self._tokenizer.post_processor = processors.TemplateProcessing(\n+            single=f\"{cls_token}:2 $A:0 {sep_token}:0\",  # token_type_id is 2 for Funnel transformer\n+            pair=f\"{cls_token}:2 $A:0 {sep_token}:0 $B:1 {sep_token}:1\",\n+            special_tokens=[\n+                (str(cls_token), self._vocab.get(str(cls_token), 2)),\n+                (str(sep_token), self._vocab.get(str(sep_token), 3)),\n+            ],\n+        )\n+\n+        tokenizer_object = self._tokenizer\n \n         super().__init__(\n+            tokenizer_object=tokenizer_object,\n             do_lower_case=do_lower_case,\n-            do_basic_tokenize=do_basic_tokenize,\n-            never_split=never_split,\n             unk_token=unk_token,\n             sep_token=sep_token,\n             pad_token=pad_token,\n             cls_token=cls_token,\n             mask_token=mask_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n+            clean_text=clean_text,\n             tokenize_chinese_chars=tokenize_chinese_chars,\n             strip_accents=strip_accents,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            wordpieces_prefix=wordpieces_prefix,\n             **kwargs,\n         )\n \n-    @property\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.do_lower_case\n-    def do_lower_case(self):\n-        return self.basic_tokenizer.do_lower_case\n-\n-    @property\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.vocab_size\n-    def vocab_size(self):\n-        return len(self.vocab)\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.get_vocab\n-    def get_vocab(self):\n-        return dict(self.vocab, **self.added_tokens_encoder)\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer._tokenize\n-    def _tokenize(self, text, split_special_tokens=False):\n-        split_tokens = []\n-        if self.do_basic_tokenize:\n-            for token in self.basic_tokenizer.tokenize(\n-                text, never_split=self.all_special_tokens if not split_special_tokens else None\n-            ):\n-                # If the token is part of the never_split set\n-                if token in self.basic_tokenizer.never_split:\n-                    split_tokens.append(token)\n-                else:\n-                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n-        else:\n-            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n-        return split_tokens\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer._convert_token_to_id\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.vocab.get(token, self.vocab.get(self.unk_token))\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer._convert_id_to_token\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.ids_to_tokens.get(index, self.unk_token)\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.convert_tokens_to_string\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n-        return out_string\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.build_inputs_with_special_tokens\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A BERT sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.get_special_tokens_mask\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel\n-        Transformer sequence pair mask has the following format:\n-\n-        ```\n-        2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls) * [self.cls_token_type_id] + len(token_ids_0 + sep) * [0]\n-        return len(cls) * [self.cls_token_type_id] + len(token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n-    # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        index = 0\n-        if os.path.isdir(save_directory):\n-            vocab_file = os.path.join(\n-                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-            )\n-        else:\n-            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n-            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n-                        \" Please check that the vocabulary is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(token + \"\\n\")\n-                index += 1\n-        return (vocab_file,)\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer\n-class BasicTokenizer:\n-    \"\"\"\n-    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n-\n-    Args:\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        do_split_on_punc (`bool`, *optional*, defaults to `True`):\n-            In some instances we want to skip the basic punctuation splitting so that later tokenization can capture\n-            the full context of the words, such as contractions.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        do_lower_case=True,\n-        never_split=None,\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        do_split_on_punc=True,\n-    ):\n-        if never_split is None:\n-            never_split = []\n-        self.do_lower_case = do_lower_case\n-        self.never_split = set(never_split)\n-        self.tokenize_chinese_chars = tokenize_chinese_chars\n-        self.strip_accents = strip_accents\n-        self.do_split_on_punc = do_split_on_punc\n-\n-    def tokenize(self, text, never_split=None):\n-        \"\"\"\n-        Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n-\n-        Args:\n-            never_split (`List[str]`, *optional*)\n-                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n-                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n-        \"\"\"\n-        # union() returns a new set by concatenating the two sets.\n-        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n-        text = self._clean_text(text)\n-\n-        # This was added on November 1st, 2018 for the multilingual and Chinese\n-        # models. This is also applied to the English models now, but it doesn't\n-        # matter since the English models were not trained on any Chinese data\n-        # and generally don't have any Chinese data in them (there are Chinese\n-        # characters in the vocabulary because Wikipedia does have some Chinese\n-        # words in the English Wikipedia.).\n-        if self.tokenize_chinese_chars:\n-            text = self._tokenize_chinese_chars(text)\n-        # prevents treating the same character with different unicode codepoints as different characters\n-        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n-        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n-        split_tokens = []\n-        for token in orig_tokens:\n-            if token not in never_split:\n-                if self.do_lower_case:\n-                    token = token.lower()\n-                    if self.strip_accents is not False:\n-                        token = self._run_strip_accents(token)\n-                elif self.strip_accents:\n-                    token = self._run_strip_accents(token)\n-            split_tokens.extend(self._run_split_on_punc(token, never_split))\n-\n-        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n-        return output_tokens\n-\n-    def _run_strip_accents(self, text):\n-        \"\"\"Strips accents from a piece of text.\"\"\"\n-        text = unicodedata.normalize(\"NFD\", text)\n-        output = []\n-        for char in text:\n-            cat = unicodedata.category(char)\n-            if cat == \"Mn\":\n-                continue\n-            output.append(char)\n-        return \"\".join(output)\n-\n-    def _run_split_on_punc(self, text, never_split=None):\n-        \"\"\"Splits punctuation on a piece of text.\"\"\"\n-        if not self.do_split_on_punc or (never_split is not None and text in never_split):\n-            return [text]\n-        chars = list(text)\n-        i = 0\n-        start_new_word = True\n-        output = []\n-        while i < len(chars):\n-            char = chars[i]\n-            if _is_punctuation(char):\n-                output.append([char])\n-                start_new_word = True\n-            else:\n-                if start_new_word:\n-                    output.append([])\n-                start_new_word = False\n-                output[-1].append(char)\n-            i += 1\n-\n-        return [\"\".join(x) for x in output]\n-\n-    def _tokenize_chinese_chars(self, text):\n-        \"\"\"Adds whitespace around any CJK character.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if self._is_chinese_char(cp):\n-                output.append(\" \")\n-                output.append(char)\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-    def _is_chinese_char(self, cp):\n-        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n-        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n-        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n-        #\n-        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n-        # despite its name. The modern Korean Hangul alphabet is a different block,\n-        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n-        # space-separated words, so they are not treated specially and handled\n-        # like the all of the other languages.\n-        if (\n-            (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n-            or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n-        ):\n-            return True\n-\n-        return False\n-\n-    def _clean_text(self, text):\n-        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if cp == 0 or cp == 0xFFFD or _is_control(char):\n-                continue\n-            if _is_whitespace(char):\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.WordpieceTokenizer\n-class WordpieceTokenizer:\n-    \"\"\"Runs WordPiece tokenization.\"\"\"\n-\n-    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n-        self.vocab = vocab\n-        self.unk_token = unk_token\n-        self.max_input_chars_per_word = max_input_chars_per_word\n-\n-    def tokenize(self, text):\n-        \"\"\"\n-        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n-        tokenization using the given vocabulary.\n-\n-        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n-\n-        Args:\n-            text: A single token or whitespace separated tokens. This should have\n-                already been passed through *BasicTokenizer*.\n-\n-        Returns:\n-            A list of wordpiece tokens.\n-        \"\"\"\n-\n-        output_tokens = []\n-        for token in whitespace_tokenize(text):\n-            chars = list(token)\n-            if len(chars) > self.max_input_chars_per_word:\n-                output_tokens.append(self.unk_token)\n-                continue\n-\n-            is_bad = False\n-            start = 0\n-            sub_tokens = []\n-            while start < len(chars):\n-                end = len(chars)\n-                cur_substr = None\n-                while start < end:\n-                    substr = \"\".join(chars[start:end])\n-                    if start > 0:\n-                        substr = \"##\" + substr\n-                    if substr in self.vocab:\n-                        cur_substr = substr\n-                        break\n-                    end -= 1\n-                if cur_substr is None:\n-                    is_bad = True\n-                    break\n-                sub_tokens.append(cur_substr)\n-                start = end\n-\n-            if is_bad:\n-                output_tokens.append(self.unk_token)\n-            else:\n-                output_tokens.extend(sub_tokens)\n-        return output_tokens\n-\n \n __all__ = [\"FunnelTokenizer\"]"
        },
        {
            "sha": "eeeb6f7bf6cb0640ee04bb01737331ba4be1233b",
            "filename": "src/transformers/models/funnel/tokenization_funnel_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 203,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,203 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization class for Funnel Transformer.\"\"\"\n-\n-import json\n-from typing import Optional\n-\n-from tokenizers import normalizers\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_funnel import FunnelTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-_model_names = [\n-    \"small\",\n-    \"small-base\",\n-    \"medium\",\n-    \"medium-base\",\n-    \"intermediate\",\n-    \"intermediate-base\",\n-    \"large\",\n-    \"large-base\",\n-    \"xlarge\",\n-    \"xlarge-base\",\n-]\n-\n-\n-class FunnelTokenizerFast(PreTrainedTokenizerFast):\n-    r\"\"\"\n-    Construct a \"fast\" Funnel Transformer tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"<sep>\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"<cls>\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        clean_text (`bool`, *optional*, defaults to `True`):\n-            Whether or not to clean the text before tokenization by removing any control characters and replacing all\n-            whitespaces by the classic one.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n-            issue](https://github.com/huggingface/transformers/issues/328)).\n-        bos_token (`str`, `optional`, defaults to `\"<s>\"`):\n-            The beginning of sentence token.\n-        eos_token (`str`, `optional`, defaults to `\"</s>\"`):\n-            The end of sentence token.\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n-            The prefix for subwords.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = FunnelTokenizer\n-    cls_token_type_id: int = 2\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=True,\n-        unk_token=\"<unk>\",\n-        sep_token=\"<sep>\",\n-        pad_token=\"<pad>\",\n-        cls_token=\"<cls>\",\n-        mask_token=\"<mask>\",\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        clean_text=True,\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        wordpieces_prefix=\"##\",\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            clean_text=clean_text,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            wordpieces_prefix=wordpieces_prefix,\n-            **kwargs,\n-        )\n-\n-        normalizer_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n-        if (\n-            normalizer_state.get(\"lowercase\", do_lower_case) != do_lower_case\n-            or normalizer_state.get(\"strip_accents\", strip_accents) != strip_accents\n-            or normalizer_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n-        ):\n-            normalizer_class = getattr(normalizers, normalizer_state.pop(\"type\"))\n-            normalizer_state[\"lowercase\"] = do_lower_case\n-            normalizer_state[\"strip_accents\"] = strip_accents\n-            normalizer_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n-            self.backend_tokenizer.normalizer = normalizer_class(**normalizer_state)\n-\n-        self.do_lower_case = do_lower_case\n-\n-    # Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.build_inputs_with_special_tokens with BERT->Funnel\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A Funnel sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-\n-        if token_ids_1 is not None:\n-            output += token_ids_1 + [self.sep_token_id]\n-\n-        return output\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel\n-        Transformer sequence pair mask has the following format:\n-\n-        ```\n-        2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        sep = [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        if token_ids_1 is None:\n-            return len(cls) * [self.cls_token_type_id] + len(token_ids_0 + sep) * [0]\n-        return len(cls) * [self.cls_token_type_id] + len(token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n-\n-    # Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"FunnelTokenizerFast\"]"
        },
        {
            "sha": "cc1ae2991a68d6ff6df77386b4f605d419dbef99",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 162,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -13,9 +13,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Optional\n \n-import sentencepiece as spm\n import torch\n from torch import nn\n \n@@ -27,7 +26,6 @@\n from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import TransformersKwargs, logging\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -39,11 +37,10 @@\n     LlamaPreTrainedModel,\n     LlamaRotaryEmbedding,\n )\n-from ..llama.tokenization_llama import LlamaTokenizer\n \n \n if TYPE_CHECKING:\n-    from ...tokenization_utils_base import TextInput\n+    pass\n \n VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\"}\n \n@@ -198,162 +195,6 @@ def __init__(\n         )\n \n \n-class GemmaTokenizer(LlamaTokenizer, PreTrainedTokenizer):\n-    \"\"\"\n-    Construct a Gemma tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is\n-    no padding token in the original model.\n-\n-    Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<bos>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<eos>\"`):\n-            The end of sequence token.\n-        pad_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<pad>\"`):\n-            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n-            attention mechanisms or loss computation.\n-        sp_model_kwargs (`dict[str, Any]`, `Optional`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n-\n-        add_bos_token (`bool`, *optional*, defaults to `True`):\n-            Whether or not to add an `bos_token` at the start of sequences.\n-        add_eos_token (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an `eos_token` at the end of sequences.\n-        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n-            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n-            extra spaces.\n-        use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n-            Whether or not the default system prompt for Gemma should be used.\n-        spaces_between_special_tokens (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add spaces between special tokens.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        vocab_file,\n-        unk_token=\"<unk>\",\n-        bos_token=\"<bos>\",\n-        eos_token=\"<eos>\",\n-        pad_token=\"<pad>\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n-        add_bos_token=True,\n-        add_eos_token=False,\n-        clean_up_tokenization_spaces=False,\n-        use_default_system_prompt=False,\n-        spaces_between_special_tokens=False,\n-        **kwargs,\n-    ):\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-        bos_token = AddedToken(bos_token, normalized=False, special=True) if isinstance(bos_token, str) else bos_token\n-        eos_token = AddedToken(eos_token, normalized=False, special=True) if isinstance(eos_token, str) else eos_token\n-        unk_token = AddedToken(unk_token, normalized=False, special=True) if isinstance(unk_token, str) else unk_token\n-        pad_token = AddedToken(pad_token, normalized=False, special=True) if isinstance(pad_token, str) else pad_token\n-\n-        self.vocab_file = vocab_file\n-        self.add_bos_token = add_bos_token\n-        self.add_eos_token = add_eos_token\n-        self.use_default_system_prompt = use_default_system_prompt\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(vocab_file)\n-\n-        PreTrainedTokenizer.__init__(\n-            self,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            unk_token=unk_token,\n-            pad_token=pad_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n-            sp_model_kwargs=sp_model_kwargs,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            use_default_system_prompt=use_default_system_prompt,\n-            spaces_between_special_tokens=spaces_between_special_tokens,\n-            **kwargs,\n-        )\n-\n-    def get_spm_processor(self):\n-        raise AttributeError(\"Not needed for Gemma\")\n-\n-    def unk_token_length(self):\n-        raise AttributeError(\"Not needed for Gemma\")\n-\n-    def tokenize(self, text: \"TextInput\", **kwargs) -> list[str]:\n-        \"\"\"\n-        Args:\n-            text: TextInput\n-        Simply calls PreTrainedTokenizer's method\n-        \"\"\"\n-        return PreTrainedTokenizer.tokenize(self, text, **kwargs)\n-\n-    def _tokenize(self, text, **kwargs):\n-        \"\"\"\n-        Args:\n-            text: TextInput\n-        Returns a tokenized string. The Gemma tokenizer never adds a prefix space.\n-        \"\"\"\n-        return self.sp_model.encode(text, out_type=str)\n-\n-    def _decode(\n-        self,\n-        token_ids: list[int],\n-        skip_special_tokens: bool = False,\n-        spaces_between_special_tokens: bool = False,\n-        **kwargs,\n-    ) -> str:\n-        sub_texts = []\n-        current_sub_text = []\n-        for ids in token_ids:\n-            if skip_special_tokens and ids in self.all_special_ids:\n-                continue\n-            if ids in self._added_tokens_decoder:\n-                if current_sub_text:\n-                    sub_texts.append(self.sp_model.decode(current_sub_text))\n-                sub_texts.append(self._added_tokens_decoder[ids].content)\n-                current_sub_text = []\n-            else:\n-                current_sub_text.append(ids)\n-        if current_sub_text:\n-            sub_texts.append(self.sp_model.decode(current_sub_text))\n-\n-        if spaces_between_special_tokens:\n-            sub_texts = \" \".join(sub_texts)\n-        else:\n-            sub_texts = \"\".join(sub_texts)\n-\n-        return sub_texts.replace(SPIECE_UNDERLINE, \" \")\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        current_sub_tokens = []\n-        out_string = \"\"\n-        for token in tokens:\n-            # make sure that special tokens are not decoded using sentencepiece model\n-            if token in self._added_tokens_encoder:\n-                out_string += self.sp_model.decode(current_sub_tokens) + token\n-                current_sub_tokens = []\n-            else:\n-                current_sub_tokens.append(token)\n-        out_string += self.sp_model.decode(current_sub_tokens)\n-        return out_string\n-\n-\n class GemmaRMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -504,7 +345,6 @@ class GemmaForTokenClassification(LlamaForTokenClassification):\n \n __all__ = [\n     \"GemmaConfig\",\n-    \"GemmaTokenizer\",\n     \"GemmaModel\",\n     \"GemmaForCausalLM\",\n     \"GemmaForSequenceClassification\","
        },
        {
            "sha": "aa2388e9a9c09fbe3a88282b4ae0ba15811631f8",
            "filename": "src/transformers/models/gemma/tokenization_gemma.py",
            "status": "modified",
            "additions": 81,
            "deletions": 286,
            "changes": 367,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1,12 +1,5 @@\n-#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n-#           This file was automatically generated from src/transformers/models/gemma/modular_gemma.py.\n-#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#             the file from the modular. If any change should be done, please apply the change to the\n-#                          modular_gemma.py file directly. One of our CI enforces this.\n-#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n # coding=utf-8\n-# Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n-#\n+# Copyright 2024 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -19,317 +12,119 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import os\n-from shutil import copyfile\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import Optional\n \n-import sentencepiece as spm\n+from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers\n+from tokenizers.models import BPE\n \n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_utils_base import generate_merges\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n-from ...utils.import_utils import requires\n-\n \n-if TYPE_CHECKING:\n-    from ...tokenization_utils_base import TextInput\n \n logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\"}\n-\n-SPIECE_UNDERLINE = \"‚ñÅ\"\n+VOCAB_FILES_NAMES = {\"tokenizer_file\": \"tokenizer.json\"}\n \n \n-@requires(backends=(\"sentencepiece\",))\n-class GemmaTokenizer(PreTrainedTokenizer):\n+class GemmaTokenizer(TokenizersBackend):\n     \"\"\"\n-    Construct a Gemma tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is\n-    no padding token in the original model.\n+    Construct a fast Gemma tokenizer (backed by HuggingFace's tokenizers library).\n+\n+    This tokenizer uses a Unigram model with ByteFallback, no prefix space, and a normalizer that replaces\n+    spaces with \"‚ñÅ\".\n \n     Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<bos>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<eos>\"`):\n+        tokenizer_file (`str`, optional):\n+            A tokenizers JSON file containing the serialization of a tokenizer.\n+        unk_token (`str`, optional, defaults to \"<unk>\"):\n+            The unknown token.\n+        bos_token (`str`, optional, defaults to \"<bos>\"):\n+            The beginning of sequence token.\n+        eos_token (`str`, optional, defaults to \"<eos>\"):\n             The end of sequence token.\n-        pad_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<pad>\"`):\n-            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n-            attention mechanisms or loss computation.\n-        sp_model_kwargs (`dict[str, Any]`, `Optional`, *optional*):\n-            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n-            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n-            to set:\n-\n-            - `enable_sampling`: Enable subword regularization.\n-            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n-\n-              - `nbest_size = {0,1}`: No sampling is performed.\n-              - `nbest_size > 1`: samples from the nbest_size results.\n-              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n-                using forward-filtering-and-backward-sampling algorithm.\n-\n-            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n-              BPE-dropout.\n-\n-        add_bos_token (`bool`, *optional*, defaults to `True`):\n-            Whether or not to add an `bos_token` at the start of sequences.\n-        add_eos_token (`bool`, *optional*, defaults to `False`):\n+        pad_token (`str`, optional, defaults to \"<pad>\"):\n+            The padding token.\n+        mask_token (`str`, optional, defaults to \"<mask>\"):\n+            The mask token.\n+        add_bos_token (`bool`, optional, defaults to True):\n+            Whether or not to add a `bos_token` at the start of sequences.\n+        add_eos_token (`bool`, optional, defaults to False):\n             Whether or not to add an `eos_token` at the end of sequences.\n-        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n-            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n-            extra spaces.\n-        use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n-            Whether or not the default system prompt for Gemma should be used.\n-        spaces_between_special_tokens (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add spaces between special tokens.\n+        vocab (`dict`, optional):\n+            Custom vocabulary dict. If not provided, a minimal vocabulary is created using the special tokens.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n+    slow_tokenizer_class = None\n+    padding_side = \"left\"\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n \n     def __init__(\n         self,\n-        vocab_file,\n-        unk_token=\"<unk>\",\n-        bos_token=\"<bos>\",\n-        eos_token=\"<eos>\",\n-        pad_token=\"<pad>\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n-        add_bos_token=True,\n-        add_eos_token=False,\n-        clean_up_tokenization_spaces=False,\n-        use_default_system_prompt=False,\n-        spaces_between_special_tokens=False,\n+        unk_token: str = \"<unk>\",\n+        bos_token: str = \"<bos>\",\n+        eos_token: str = \"<eos>\",\n+        pad_token: str = \"<pad>\",\n+        mask_token: str = \"<mask>\",\n+        add_bos_token: bool = True,\n+        add_eos_token: bool = False,\n+        vocab: Optional[dict] = None,\n+        merges: Optional[list[tuple[str, str]]] = None,\n         **kwargs,\n     ):\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-        bos_token = AddedToken(bos_token, normalized=False, special=True) if isinstance(bos_token, str) else bos_token\n-        eos_token = AddedToken(eos_token, normalized=False, special=True) if isinstance(eos_token, str) else eos_token\n-        unk_token = AddedToken(unk_token, normalized=False, special=True) if isinstance(unk_token, str) else unk_token\n-        pad_token = AddedToken(pad_token, normalized=False, special=True) if isinstance(pad_token, str) else pad_token\n+        self._add_bos_token = add_bos_token\n+        self._add_eos_token = add_eos_token\n \n-        self.vocab_file = vocab_file\n-        self.add_bos_token = add_bos_token\n-        self.add_eos_token = add_eos_token\n-        self.use_default_system_prompt = use_default_system_prompt\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(vocab_file)\n+        special_tokens = {str(pad_token), str(eos_token), str(bos_token), str(unk_token)}\n+\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n+            )\n+        else:\n+            self._vocab = {\n+                str(pad_token): 0,\n+                str(eos_token): 1,\n+                str(bos_token): 2,\n+                str(unk_token): 3,\n+                str(mask_token): 4,\n+            }\n+\n+        filtered_vocab = {t: i for t, i in self._vocab.items() if t not in special_tokens}\n+        self._merges = merges if merges is not None else generate_merges(filtered_vocab)\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=self._vocab,\n+                merges=self._merges,\n+                fuse_unk=True,\n+                unk_token=str(unk_token),\n+                dropout=None,\n+                byte_fallback=True,\n+            )\n+        )\n+\n+        self._tokenizer.decoder = decoders.Sequence(\n+            [decoders.Replace(\"‚ñÅ\", \" \"), decoders.ByteFallback(), decoders.Fuse()]\n+        )\n+        self._tokenizer.normalizer = normalizers.Replace(\" \", \"‚ñÅ\")\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.Split(\" \", \"merged_with_previous\")\n+        tokenizer_object = self._tokenizer\n \n         super().__init__(\n+            tokenizer_object=tokenizer_object,\n+            unk_token=unk_token,\n             bos_token=bos_token,\n             eos_token=eos_token,\n-            unk_token=unk_token,\n             pad_token=pad_token,\n+            mask_token=mask_token,\n             add_bos_token=add_bos_token,\n             add_eos_token=add_eos_token,\n-            sp_model_kwargs=sp_model_kwargs,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            use_default_system_prompt=use_default_system_prompt,\n-            spaces_between_special_tokens=spaces_between_special_tokens,\n             **kwargs,\n         )\n \n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        state[\"sp_model_proto\"] = self.sp_model.serialized_model_proto()\n-        return state\n-\n-    def __setstate__(self, d):\n-        self.__dict__.update(d)\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n-\n-    @property\n-    def vocab_size(self):\n-        \"\"\"Returns vocab size\"\"\"\n-        return self.sp_model.get_piece_size()\n-\n-    def get_vocab(self):\n-        \"\"\"Returns vocab as a dict\"\"\"\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n-\n-    def tokenize(self, text: \"TextInput\", **kwargs) -> list[str]:\n-        \"\"\"\n-        Args:\n-            text: TextInput\n-        Simply calls PreTrainedTokenizer's method\n-        \"\"\"\n-        return super().tokenize(text, **kwargs)\n-\n-    def _tokenize(self, text, **kwargs):\n-        \"\"\"\n-        Args:\n-            text: TextInput\n-        Returns a tokenized string. The Gemma tokenizer never adds a prefix space.\n-        \"\"\"\n-        return self.sp_model.encode(text, out_type=str)\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.sp_model.piece_to_id(token)\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        token = self.sp_model.IdToPiece(index)\n-        return token\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        current_sub_tokens = []\n-        out_string = \"\"\n-        for token in tokens:\n-            # make sure that special tokens are not decoded using sentencepiece model\n-            if token in self._added_tokens_encoder:\n-                out_string += self.sp_model.decode(current_sub_tokens) + token\n-                current_sub_tokens = []\n-            else:\n-                current_sub_tokens.append(token)\n-        out_string += self.sp_model.decode(current_sub_tokens)\n-        return out_string\n-\n-    def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        \"\"\"\n-        Save the vocabulary and special tokens file to a directory.\n-\n-        Args:\n-            save_directory (`str`):\n-                The directory in which to save the vocabulary.\n-\n-        Returns:\n-            `Tuple(str)`: Paths to the files saved.\n-        \"\"\"\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n-\n-        return (out_vocab_file,)\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n-        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n-\n-        output = bos_token_id + token_ids_0 + eos_token_id\n-\n-        if token_ids_1 is not None:\n-            output = output + bos_token_id + token_ids_1 + eos_token_id\n-\n-        return output\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        bos_token_id = [1] if self.add_bos_token else []\n-        eos_token_id = [1] if self.add_eos_token else []\n-\n-        if token_ids_1 is None:\n-            return bos_token_id + ([0] * len(token_ids_0)) + eos_token_id\n-        return (\n-            bos_token_id\n-            + ([0] * len(token_ids_0))\n-            + eos_token_id\n-            + bos_token_id\n-            + ([0] * len(token_ids_1))\n-            + eos_token_id\n-        )\n-\n-    def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT\n-        sequence pair mask has the following format:\n-\n-        ```\n-        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n-        | first sequence    | second sequence |\n-        ```\n-\n-        if token_ids_1 is None, only returns the first portion of the mask (0s).\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of ids.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `list[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n-        \"\"\"\n-        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n-        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n-\n-        output = [0] * len(bos_token_id + token_ids_0 + eos_token_id)\n-\n-        if token_ids_1 is not None:\n-            output += [1] * len(bos_token_id + token_ids_1 + eos_token_id)\n-\n-        return output\n-\n-    def _decode(\n-        self,\n-        token_ids: list[int],\n-        skip_special_tokens: bool = False,\n-        spaces_between_special_tokens: bool = False,\n-        **kwargs,\n-    ) -> str:\n-        sub_texts = []\n-        current_sub_text = []\n-        for ids in token_ids:\n-            if skip_special_tokens and ids in self.all_special_ids:\n-                continue\n-            if ids in self._added_tokens_decoder:\n-                if current_sub_text:\n-                    sub_texts.append(self.sp_model.decode(current_sub_text))\n-                sub_texts.append(self._added_tokens_decoder[ids].content)\n-                current_sub_text = []\n-            else:\n-                current_sub_text.append(ids)\n-        if current_sub_text:\n-            sub_texts.append(self.sp_model.decode(current_sub_text))\n-\n-        if spaces_between_special_tokens:\n-            sub_texts = \" \".join(sub_texts)\n-        else:\n-            sub_texts = \"\".join(sub_texts)\n-\n-        return sub_texts.replace(SPIECE_UNDERLINE, \" \")\n+    def _unk_id(self) -> int:\n+        # Align with historical Gemma convention: pad, eos, bos, unk\n+        return 3\n \n \n __all__ = [\"GemmaTokenizer\"]"
        },
        {
            "sha": "9fc6e3d3593b9ff5403973af5a08d86820a0c2d3",
            "filename": "src/transformers/models/gemma/tokenization_gemma_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 195,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,195 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import os\n-from shutil import copyfile\n-from typing import Optional\n-\n-from tokenizers import processors\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import is_sentencepiece_available, logging\n-\n-\n-if is_sentencepiece_available():\n-    from .tokenization_gemma import GemmaTokenizer\n-else:\n-    GemmaTokenizer = None\n-\n-logger = logging.get_logger(__name__)\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class GemmaTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a Gemma tokenizer fast. Based on byte-level Byte-Pair-Encoding.\n-\n-    This uses notably ByteFallback and no prefix space. Normalization is applied to replace  `\" \"` with `\"‚ñÅ\"`\n-\n-    ```python\n-    >>> from transformers import GemmaTokenizerFast\n-\n-    >>> tokenizer = GemmaTokenizerFast.from_pretrained(\"hf-internal-testing/dummy-gemma\")\n-    >>> tokenizer.encode(\"Hello this is a test\")\n-    [2, 4521, 736, 603, 476, 2121]\n-    ```\n-\n-    If you want to change the `bos_token` or the `eos_token`, make sure to specify them when initializing the model, or\n-    call `tokenizer.update_post_processor()` to make sure that the post-processing is correctly done (otherwise the\n-    values of the first token and final token of an encoded sequence will not be correct). For more details, checkout\n-    [post-processors] (https://huggingface.co/docs/tokenizers/api/post-processors) documentation.\n-\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`, *optional*):\n-            [SentencePiece](https://github.com/google/sentencepiece) file (generally has a .model extension) that\n-            contains the vocabulary necessary to instantiate a tokenizer.\n-        tokenizer_file (`str`, *optional*):\n-            [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\n-            contains everything needed to load the tokenizer.\n-        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n-            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n-            extra spaces.\n-        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<bos>\"`):\n-            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n-        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<eos>\"`):\n-            The end of sequence token.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The padding token\n-        add_bos_token (`bool`, *optional*, defaults to `True`):\n-            Whether or not to add an `bos_token` at the start of sequences.\n-        add_eos_token (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an `eos_token` at the end of sequences.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = GemmaTokenizer\n-    padding_side = \"left\"\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        clean_up_tokenization_spaces=False,\n-        unk_token=\"<unk>\",\n-        bos_token=\"<bos>\",\n-        eos_token=\"<eos>\",\n-        pad_token=\"<pad>\",\n-        add_bos_token=True,\n-        add_eos_token=False,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file=vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            pad_token=pad_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n-            **kwargs,\n-        )\n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = add_eos_token\n-        self.update_post_processor()\n-        self.vocab_file = vocab_file\n-\n-    # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.update_post_processor\n-    def update_post_processor(self):\n-        \"\"\"\n-        Updates the underlying post processor with the current `bos_token` and `eos_token`.\n-        \"\"\"\n-        bos = self.bos_token\n-        bos_token_id = self.bos_token_id\n-        if bos is None and self.add_bos_token:\n-            raise ValueError(\"add_bos_token = True but bos_token = None\")\n-\n-        eos = self.eos_token\n-        eos_token_id = self.eos_token_id\n-        if eos is None and self.add_eos_token:\n-            raise ValueError(\"add_eos_token = True but eos_token = None\")\n-\n-        single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n-        pair = f\"{single}{(' ' + bos + ':1') if self.add_bos_token else ''} $B:1{(' ' + eos + ':1') if self.add_eos_token else ''}\"\n-\n-        special_tokens = []\n-        if self.add_bos_token:\n-            special_tokens.append((bos, bos_token_id))\n-        if self.add_eos_token:\n-            special_tokens.append((eos, eos_token_id))\n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=single, pair=pair, special_tokens=special_tokens\n-        )\n-\n-    @property\n-    def add_eos_token(self):\n-        return self._add_eos_token\n-\n-    @property\n-    def add_bos_token(self):\n-        return self._add_bos_token\n-\n-    @add_eos_token.setter\n-    def add_eos_token(self, value):\n-        self._add_eos_token = value\n-        self.update_post_processor()\n-\n-    @add_bos_token.setter\n-    def add_bos_token(self, value):\n-        self._add_bos_token = value\n-        self.update_post_processor()\n-\n-    # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not self.can_save_slow_tokenizer:\n-            raise ValueError(\n-                \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \"\n-                \"tokenizer.\"\n-            )\n-\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-\n-        return (out_vocab_file,)\n-\n-    # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.build_inputs_with_special_tokens\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n-        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n-\n-        output = bos_token_id + token_ids_0 + eos_token_id\n-\n-        if token_ids_1 is not None:\n-            output = output + bos_token_id + token_ids_1 + eos_token_id\n-\n-        return output\n-\n-\n-__all__ = [\"GemmaTokenizerFast\"]"
        },
        {
            "sha": "aa7130ece3bbd297f7095e1f2ac690237c671b05",
            "filename": "src/transformers/models/got_ocr2/convert_got_ocr2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -32,7 +32,7 @@\n     is_vision_available,\n )\n from transformers.convert_slow_tokenizer import TikTokenConverter\n-from transformers.tokenization_utils import AddedToken\n+from transformers.tokenization_python import AddedToken\n \n \n if is_vision_available():"
        },
        {
            "sha": "090606b7b8372de5ecb60af00d23c275b0305de4",
            "filename": "src/transformers/models/gpt2/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -21,7 +21,6 @@\n     from .configuration_gpt2 import *\n     from .modeling_gpt2 import *\n     from .tokenization_gpt2 import *\n-    from .tokenization_gpt2_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "9fe8d768e085fc69210e33e8e37903466b97339b",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2.py",
            "status": "modified",
            "additions": 49,
            "deletions": 220,
            "changes": 269,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -14,14 +14,12 @@\n # limitations under the License.\n \"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n \n-import json\n-import os\n-from functools import lru_cache\n from typing import Optional\n \n-import regex as re\n+from tokenizers import Tokenizer, decoders, pre_tokenizers\n+from tokenizers.models import BPE\n \n-from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n \n@@ -33,46 +31,7 @@\n }\n \n \n-@lru_cache\n-def bytes_to_unicode():\n-    \"\"\"\n-    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n-    characters the bpe code barfs on.\n-\n-    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n-    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n-    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n-    tables between utf-8 bytes and unicode strings.\n-    \"\"\"\n-    bs = (\n-        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¬°\"), ord(\"¬¨\") + 1)) + list(range(ord(\"¬Æ\"), ord(\"√ø\") + 1))\n-    )\n-    cs = bs[:]\n-    n = 0\n-    for b in range(2**8):\n-        if b not in bs:\n-            bs.append(b)\n-            cs.append(2**8 + n)\n-            n += 1\n-    cs = [chr(n) for n in cs]\n-    return dict(zip(bs, cs))\n-\n-\n-def get_pairs(word):\n-    \"\"\"\n-    Return set of symbol pairs in a word.\n-\n-    Word is represented as tuple of symbols (symbols being variable-length strings).\n-    \"\"\"\n-    pairs = set()\n-    prev_char = word[0]\n-    for char in word[1:]:\n-        pairs.add((prev_char, char))\n-        prev_char = char\n-    return pairs\n-\n-\n-class GPT2Tokenizer(PreTrainedTokenizer):\n+class GPT2Tokenizer(TokenizersBackend):\n     \"\"\"\n     Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n \n@@ -99,7 +58,7 @@ class GPT2Tokenizer(PreTrainedTokenizer):\n \n     </Tip>\n \n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the main methods. Users should refer to\n     this superclass for more information regarding those methods.\n \n     Args:\n@@ -125,48 +84,67 @@ class GPT2Tokenizer(PreTrainedTokenizer):\n         add_bos_token (`bool`, *optional*, defaults to `False`):\n             Whether or not to add an initial beginning of sentence token to the input. This allows to treat the leading\n             word just as any other word.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        merges (`list`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from merges_file.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    slow_tokenizer_class = None\n \n     def __init__(\n         self,\n-        vocab_file,\n-        merges_file,\n         errors=\"replace\",\n         unk_token=\"<|endoftext|>\",\n         bos_token=\"<|endoftext|>\",\n         eos_token=\"<|endoftext|>\",\n         pad_token=None,\n-        add_prefix_space=False,\n+        add_prefix_space=True,\n         add_bos_token=False,\n+        vocab: Optional[dict] = None,\n+        merges: Optional[list] = None,\n         **kwargs,\n     ):\n-        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n-        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n-        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n-        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n-\n-        self.add_bos_token = add_bos_token\n+        #  self.add_bos_token = add_bos_token\n \n-        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n-            self.encoder = json.load(vocab_handle)\n-        self.decoder = {v: k for k, v in self.encoder.items()}\n-        self.errors = errors  # how to handle errors in decoding\n-        self.byte_encoder = bytes_to_unicode()\n-        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n-        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n-            bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\n-        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n-        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n-        self.cache = {}\n         self.add_prefix_space = add_prefix_space\n \n-        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n-        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n+            )\n+        else:\n+            self._vocab = {}\n+\n+        if merges is not None:\n+            self._merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n+        else:\n+            self._merges = []\n+\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=self._vocab,\n+                merges=self._merges,\n+                dropout=None,\n+                continuing_subword_prefix=\"\",\n+                end_of_word_suffix=\"\",\n+                fuse_unk=False,\n+            )\n+        )\n+\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n+        self._tokenizer.decoder = decoders.ByteLevel()\n+\n+        tokenizer_object = self._tokenizer\n+\n+        # Set these before calling super().__init__() so the base class _post_init() can use them\n+        self._add_bos_token = add_bos_token\n+        self._add_eos_token = False\n \n         super().__init__(\n+            tokenizer_object=tokenizer_object,\n             errors=errors,\n             unk_token=unk_token,\n             bos_token=bos_token,\n@@ -177,158 +155,9 @@ def __init__(\n             **kwargs,\n         )\n \n-    @property\n-    def vocab_size(self):\n-        return len(self.encoder)\n-\n-    def get_vocab(self):\n-        return dict(self.encoder, **self.added_tokens_encoder)\n-\n-    def bpe(self, token):\n-        if token in self.cache:\n-            return self.cache[token]\n-        word = tuple(token)\n-        pairs = get_pairs(word)\n-\n-        if not pairs:\n-            return token\n-\n-        while True:\n-            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n-            if bigram not in self.bpe_ranks:\n-                break\n-            first, second = bigram\n-            new_word = []\n-            i = 0\n-            while i < len(word):\n-                try:\n-                    j = word.index(first, i)\n-                except ValueError:\n-                    new_word.extend(word[i:])\n-                    break\n-                else:\n-                    new_word.extend(word[i:j])\n-                    i = j\n-\n-                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n-                    new_word.append(first + second)\n-                    i += 2\n-                else:\n-                    new_word.append(word[i])\n-                    i += 1\n-            new_word = tuple(new_word)\n-            word = new_word\n-            if len(word) == 1:\n-                break\n-            else:\n-                pairs = get_pairs(word)\n-        word = \" \".join(word)\n-        self.cache[token] = word\n-        return word\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        if self.add_bos_token:\n-            bos_token_ids = [self.bos_token_id]\n-        else:\n-            bos_token_ids = []\n-\n-        output = bos_token_ids + token_ids_0\n-\n-        if token_ids_1 is None:\n-            return output\n-\n-        return output + bos_token_ids + token_ids_1\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if not self.add_bos_token:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=False\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0))\n-        return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n-\n-    def _tokenize(self, text):\n-        \"\"\"Tokenize a string.\"\"\"\n-        bpe_tokens = []\n-        for token in re.findall(self.pat, text):\n-            token = \"\".join(\n-                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n-            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n-            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n-        return bpe_tokens\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.encoder.get(token, self.encoder.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.decoder.get(index)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        text = \"\".join(tokens)\n-        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n-        return text\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n-        )\n-\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n-\n-        index = 0\n-        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(\"#version: 0.2\\n\")\n-            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n-                index += 1\n-\n-        return vocab_file, merge_file\n-\n-    def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n-        add_prefix_space = kwargs.pop(\"add_prefix_space\", self.add_prefix_space)\n-        if is_split_into_words or add_prefix_space:\n-            text = \" \" + text\n-        return (text, kwargs)\n+        # Call _post_init for tokenizers created directly (not from_pretrained)\n+        # For from_pretrained, this will be called again after loading the tokenizer from file\n+        self._post_init()\n \n \n __all__ = [\"GPT2Tokenizer\"]"
        },
        {
            "sha": "f81c155e864476cf49c24f91a0235c939f42d3e0",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 133,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,133 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n-\n-from typing import Optional\n-\n-from ...tokenization_utils_base import BatchEncoding\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_gpt2 import GPT2Tokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class GPT2TokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" GPT-2 tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n-    Byte-Pair-Encoding.\n-\n-    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n-    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n-\n-    ```python\n-    >>> from transformers import GPT2TokenizerFast\n-\n-    >>> tokenizer = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\")\n-    >>> tokenizer(\"Hello world\")[\"input_ids\"]\n-    [15496, 995]\n-\n-    >>> tokenizer(\" Hello world\")[\"input_ids\"]\n-    [18435, 995]\n-    ```\n-\n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n-    the model was not pretrained this way, it might yield a decrease in performance.\n-\n-    <Tip>\n-\n-    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n-\n-    </Tip>\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`, *optional*):\n-            Path to the vocabulary file.\n-        merges_file (`str`, *optional*):\n-            Path to the merges file.\n-        tokenizer_file (`str`, *optional*):\n-            Path to [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\n-            contains everything needed to load the tokenizer.\n-        unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        bos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The beginning of sequence token.\n-        eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n-            The end of sequence token.\n-        add_prefix_space (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n-            other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-    slow_tokenizer_class = GPT2Tokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        unk_token=\"<|endoftext|>\",\n-        bos_token=\"<|endoftext|>\",\n-        eos_token=\"<|endoftext|>\",\n-        add_prefix_space=False,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file=vocab_file,\n-            merges_file=merges_file,\n-            tokenizer_file=tokenizer_file,\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            add_prefix_space=add_prefix_space,\n-            **kwargs,\n-        )\n-\n-        self.add_bos_token = kwargs.pop(\"add_bos_token\", False)\n-\n-    def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-        assert self.add_prefix_space or not is_split_into_words, (\n-            f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-            \"to use it with pretokenized inputs.\"\n-        )\n-\n-        return super()._batch_encode_plus(*args, **kwargs)\n-\n-    def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n-        is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-\n-        assert self.add_prefix_space or not is_split_into_words, (\n-            f\"You need to instantiate {self.__class__.__name__} with add_prefix_space=True \"\n-            \"to use it with pretokenized inputs.\"\n-        )\n-\n-        return super()._encode_plus(*args, **kwargs)\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"GPT2TokenizerFast\"]"
        },
        {
            "sha": "91ef201d1126c45166aa8256cdc0e07de0444e56",
            "filename": "src/transformers/models/gpt_neox/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_neox%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_neox%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -20,7 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_gpt_neox import *\n     from .modeling_gpt_neox import *\n-    from .tokenization_gpt_neox_fast import *\n+    from .tokenization_gpt_neox import *\n else:\n     import sys\n "
        },
        {
            "sha": "c57fae09ac87461724d14ddac315d0279bb53084",
            "filename": "src/transformers/models/gpt_neox/tokenization_gpt_neox.py",
            "status": "added",
            "additions": 186,
            "deletions": 0,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -0,0 +1,186 @@\n+# coding=utf-8\n+# Copyright 2022 EleutherAI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Tokenization classes for GPTNeoX.\"\"\"\n+\n+from typing import Optional\n+\n+from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers\n+from tokenizers.models import BPE\n+\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n+\n+\n+class GPTNeoXTokenizer(TokenizersBackend):\n+    \"\"\"\n+    Construct a GPT-NeoX-20B tokenizer (backed by HuggingFace's tokenizers library). Based on byte-level\n+    Byte-Pair-Encoding.\n+\n+    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n+    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n+\n+    ```python\n+    >>> from transformers import GPTNeoXTokenizer\n+\n+    >>> tokenizer = GPTNeoXTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n+    >>> tokenizer(\"Hello world\")[\"input_ids\"]\n+    [15496, 995]\n+\n+    >>> tokenizer(\" Hello world\")[\"input_ids\"]\n+    [18435, 995]\n+    ```\n+\n+    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n+    the model was not pretrained this way, it might yield a decrease in performance.\n+\n+    <Tip>\n+\n+    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n+\n+    </Tip>\n+\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n+\n+    Args:\n+        vocab_file (`str`, *optional*):\n+            Path to the vocabulary file.\n+        merges_file (`str`, *optional*):\n+            Path to the merges file.\n+        tokenizer_file (`str`, *optional*):\n+            Path to a tokenizers JSON file containing the serialization of a tokenizer.\n+        errors (`str`, *optional*, defaults to `\"replace\"`):\n+            Paradigm to follow when decoding bytes to UTF-8. See\n+            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n+        unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n+            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n+            token instead.\n+        bos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n+            The beginning of sequence token.\n+        eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n+            The end of sequence token.\n+        pad_token (`str`, *optional*, defaults to `\"<|padding|>\"`):\n+            Token for padding a sequence.\n+        add_prefix_space (`bool`, *optional*, defaults to `False`):\n+            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n+            other word. (GPTNeoX tokenizer detect beginning of words by the preceding space).\n+        add_bos_token (`bool`, *optional*, defaults to `False`):\n+            Whether or not to add a `bos_token` at the start of sequences.\n+        add_eos_token (`bool`, *optional*, defaults to `False`):\n+            Whether or not to add an `eos_token` at the end of sequences.\n+        trim_offsets (`bool`, *optional*, defaults to `True`):\n+            Whether or not the post-processing step should trim offsets to avoid including whitespaces.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary. If not provided, vocabulary is loaded from vocab_file.\n+        merges (`list`, *optional*):\n+            Custom merges list. If not provided, merges are loaded from merges_file.\n+    \"\"\"\n+\n+    vocab_files_names = VOCAB_FILES_NAMES\n+    model_input_names = [\"input_ids\", \"attention_mask\"]\n+    slow_tokenizer_class = None\n+\n+    def __init__(\n+        self,\n+        errors: str = \"replace\",\n+        unk_token: str = \"<|endoftext|>\",\n+        bos_token: str = \"<|endoftext|>\",\n+        eos_token: str = \"<|endoftext|>\",\n+        pad_token: str = \"<|padding|>\",\n+        add_bos_token: bool = False,\n+        add_eos_token: bool = False,\n+        add_prefix_space: bool = False,\n+        trim_offsets: bool = True,\n+        vocab: Optional[dict] = None,\n+        merges: Optional[list] = None,\n+        **kwargs,\n+    ):\n+        self._add_bos_token = add_bos_token\n+        self._add_eos_token = add_eos_token\n+        self.add_prefix_space = add_prefix_space\n+        self.trim_offsets = trim_offsets\n+\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n+            )\n+        else:\n+            self._vocab = {\n+                str(unk_token): 0,\n+                str(pad_token): 1,\n+            }\n+\n+        if merges is not None:\n+            self._merges = merges\n+        else:\n+            self._merges = []\n+\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=self._vocab,\n+                merges=self._merges,\n+                dropout=None,\n+                continuing_subword_prefix=\"\",\n+                end_of_word_suffix=\"\",\n+                fuse_unk=False,\n+            )\n+        )\n+\n+        self._tokenizer.normalizer = normalizers.NFC()\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(\n+            add_prefix_space=add_prefix_space, trim_offsets=trim_offsets\n+        )\n+        self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=False, trim_offsets=True)\n+\n+        tokenizer_object = self._tokenizer\n+\n+        super().__init__(\n+            tokenizer_object=tokenizer_object,\n+            errors=errors,\n+            unk_token=unk_token,\n+            bos_token=bos_token,\n+            eos_token=eos_token,\n+            pad_token=pad_token,\n+            add_bos_token=add_bos_token,\n+            add_eos_token=add_eos_token,\n+            add_prefix_space=add_prefix_space,\n+            trim_offsets=trim_offsets,\n+            **kwargs,\n+        )\n+\n+        self.update_post_processor()\n+\n+    def _post_init(self):\n+        \"\"\"Post-initialization to ensure tokenizer settings are applied correctly.\"\"\"\n+        # Re-apply settings to ensure they're correct after loading from pretrained\n+        self._tokenizer.normalizer = normalizers.NFC()\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(\n+            add_prefix_space=self.add_prefix_space, trim_offsets=self.trim_offsets\n+        )\n+        self._tokenizer.decoder = decoders.ByteLevel(add_prefix_space=False, trim_offsets=True)\n+\n+        # Call parent to handle AddedToken properties\n+        super()._post_init()\n+\n+        # Update post processor with current bos/eos settings\n+        self.update_post_processor()\n+\n+\n+__all__ = [\"GPTNeoXTokenizer\"]"
        },
        {
            "sha": "a3b190a60eb1202a4b7dc7c82692edf22d72000b",
            "filename": "src/transformers/models/gpt_neox/tokenization_gpt_neox_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 224,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,224 +0,0 @@\n-# coding=utf-8\n-# Copyright 2022 EleutherAI and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization classes for GPTNeoX.\"\"\"\n-\n-from typing import Optional\n-\n-from tokenizers import processors\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class GPTNeoXTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"fast\" GPT-NeoX-20B tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n-    Byte-Pair-Encoding.\n-\n-    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n-    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n-\n-    ```python\n-    >>> from transformers import GPTNeoXTokenizerFast\n-\n-    >>> tokenizer = GPTNeoXTokenizerFast.from_pretrained(\"openai-community/gpt2\")\n-    >>> tokenizer(\"Hello world\")[\"input_ids\"]\n-    [15496, 995]\n-\n-    >>> tokenizer(\" Hello world\")[\"input_ids\"]\n-    [18435, 995]\n-    ```\n-\n-    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n-    the model was not pretrained this way, it might yield a decrease in performance.\n-\n-    <Tip>\n-\n-    When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n-\n-    </Tip>\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n-        errors (`str`, *optional*, defaults to `\"replace\"`):\n-            Paradigm to follow when decoding bytes to UTF-8. See\n-            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n-        unk_token (`str`, *optional*, defaults to `<|endoftext|>`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        bos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n-            The beginning of sequence token.\n-        eos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n-            The end of sequence token.\n-        pad_token (`str`, *optional*):\n-            Token for padding a sequence.\n-        add_prefix_space (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n-            other word. (GPTNeoX tokenizer detect beginning of words by the preceding space).\n-        add_bos_token (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add a `bos_token` at the start of sequences.\n-        add_eos_token (`bool`, *optional*, defaults to `False`):\n-            Whether or not to add an `eos_token` at the end of sequences.\n-        trim_offsets (`bool`, *optional*, defaults to `True`):\n-            Whether or not the post-processing step should trim offsets to avoid including whitespaces.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    model_input_names = [\"input_ids\", \"attention_mask\"]\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        unk_token=\"<|endoftext|>\",\n-        bos_token=\"<|endoftext|>\",\n-        eos_token=\"<|endoftext|>\",\n-        pad_token=None,\n-        add_bos_token=False,\n-        add_eos_token=False,\n-        add_prefix_space=False,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file=vocab_file,\n-            merges_file=merges_file,\n-            tokenizer_file=tokenizer_file,\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            pad_token=pad_token,\n-            add_bos_token=add_bos_token,\n-            add_eos_token=add_eos_token,\n-            add_prefix_space=add_prefix_space,\n-            **kwargs,\n-        )\n-\n-        self._add_bos_token = add_bos_token\n-        self._add_eos_token = add_eos_token\n-        self.update_post_processor()\n-\n-    @property\n-    def add_eos_token(self):\n-        return self._add_eos_token\n-\n-    @property\n-    def add_bos_token(self):\n-        return self._add_bos_token\n-\n-    @add_eos_token.setter\n-    def add_eos_token(self, value):\n-        self._add_eos_token = value\n-        self.update_post_processor()\n-\n-    @add_bos_token.setter\n-    def add_bos_token(self, value):\n-        self._add_bos_token = value\n-        self.update_post_processor()\n-\n-    # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.update_post_processor\n-    def update_post_processor(self):\n-        \"\"\"\n-        Updates the underlying post processor with the current `bos_token` and `eos_token`.\n-        \"\"\"\n-        bos = self.bos_token\n-        bos_token_id = self.bos_token_id\n-        if bos is None and self.add_bos_token:\n-            raise ValueError(\"add_bos_token = True but bos_token = None\")\n-\n-        eos = self.eos_token\n-        eos_token_id = self.eos_token_id\n-        if eos is None and self.add_eos_token:\n-            raise ValueError(\"add_eos_token = True but eos_token = None\")\n-\n-        single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n-        pair = f\"{single}{(' ' + bos + ':1') if self.add_bos_token else ''} $B:1{(' ' + eos + ':1') if self.add_eos_token else ''}\"\n-\n-        special_tokens = []\n-        if self.add_bos_token:\n-            special_tokens.append((bos, bos_token_id))\n-        if self.add_eos_token:\n-            special_tokens.append((eos, eos_token_id))\n-        self._tokenizer.post_processor = processors.TemplateProcessing(\n-            single=single, pair=pair, special_tokens=special_tokens\n-        )\n-\n-    # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.get_special_tokens_mask\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`list[int]`):\n-                List of IDs.\n-            token_ids_1 (`list[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        bos_token_id = [1] if self.add_bos_token else []\n-        eos_token_id = [1] if self.add_eos_token else []\n-\n-        if token_ids_1 is None:\n-            return bos_token_id + ([0] * len(token_ids_0)) + eos_token_id\n-        return (\n-            bos_token_id\n-            + ([0] * len(token_ids_0))\n-            + eos_token_id\n-            + bos_token_id\n-            + ([0] * len(token_ids_1))\n-            + eos_token_id\n-        )\n-\n-    # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.build_inputs_with_special_tokens\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n-        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n-\n-        output = bos_token_id + token_ids_0 + eos_token_id\n-\n-        if token_ids_1 is not None:\n-            output = output + bos_token_id + token_ids_1 + eos_token_id\n-\n-        return output\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"GPTNeoXTokenizerFast\"]"
        },
        {
            "sha": "e706850dca4e6743c5f75a662711631ef1d70cf9",
            "filename": "src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -23,7 +23,7 @@\n \n import numpy as np\n \n-from ...tokenization_utils_fast import PreTrainedTokenizer\n+from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n \n \n@@ -135,6 +135,7 @@ def __init__(\n             bos_token=bos_token,\n             eos_token=eos_token,\n             do_clean_text=do_clean_text,\n+            special_tokens_pattern=\"none\",\n             **kwargs,\n         )\n "
        },
        {
            "sha": "6c80045f251affb2a40c8bbdd098bb65eeabed7d",
            "filename": "src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -332,7 +332,6 @@ def create_safetensors_index(safetensors_index, num_shards, model_path):\n         json.dump(safetensors_index, f)\n \n \n-# Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\"\n     Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control"
        },
        {
            "sha": "ef9a90955dd376ffcb5c58681b5aa18211d23228",
            "filename": "src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 73,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Ftokenization_gpt_sw3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Ftokenization_gpt_sw3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Ftokenization_gpt_sw3.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -1,14 +1,10 @@\n \"\"\"The tokenizer used by the GPT-SW3 models.\"\"\"\n \n-import os\n import re\n import unicodedata\n-from shutil import copyfile\n from typing import Any, Optional, Union\n \n-import sentencepiece as spm\n-\n-from ...tokenization_utils import PreTrainedTokenizer\n+from ...tokenization_utils_sentencepiece import SentencePieceBackend\n from ...utils import is_torch_available, logging\n from ...utils.import_utils import requires\n \n@@ -22,7 +18,7 @@\n \n \n @requires(backends=(\"sentencepiece\",))\n-class GPTSw3Tokenizer(PreTrainedTokenizer):\n+class GPTSw3Tokenizer(SentencePieceBackend):\n     \"\"\"\n     Construct an GPTSw3 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).\n \n@@ -86,6 +82,7 @@ class GPTSw3Tokenizer(PreTrainedTokenizer):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+    is_fast = False\n \n     def __init__(\n         self,\n@@ -100,8 +97,6 @@ def __init__(\n         sp_model_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n     ) -> None:\n-        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n-\n         name_or_path = kwargs.get(\"name_or_path\")\n         if name_or_path is None:\n             logger.warning(\n@@ -123,55 +118,35 @@ def __init__(\n         self.do_lower_case = do_lower_case\n         self.remove_space = remove_space\n         self.keep_accents = keep_accents\n-        self.vocab_file = vocab_file\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(vocab_file)\n \n         # Used for whitespace normalization in input texts\n         # fmt : off\n-        self.whitespaces = {\" \", \"‚Äâ\", \"‚Ää\", \"‚ÄØ\", \"‚ÄÖ\", \"„ÄÄ\", \"‚ÄÇ\", \" \", \"‚Äà\", \"‚ÄÉ\", \"Ôøº\", \"¬Ñ\"}\n+        self.whitespaces = {\" \", \" \", \" \", \" \", \" \", \"„ÄÄ\", \" \", \" \", \" \", \" \", \"Ôøº\", \"\"}\n         # fmt : on\n \n         # Regular expression to remove non-printing characters (e.g. some unicode control chars) in preprocessing\n         self.non_printing_characters_re = re.compile(\n             f\"[{''.join(map(chr, list(range(0, 9)) + list(range(11, 32)) + list(range(127, 160)) + [160, 173, 8203]))}]\"\n         )\n \n+        # Ensure sp_model_kwargs is in kwargs for proper signature storage\n+        # Always add it even if None, parent class will handle the conversion to {}\n+        kwargs[\"sp_model_kwargs\"] = sp_model_kwargs if sp_model_kwargs is not None else {}\n+\n+        # Call parent init (which will load sp_model)\n         super().__init__(\n+            vocab_file=vocab_file,\n             do_lower_case=do_lower_case,\n             remove_space=remove_space,\n             keep_accents=keep_accents,\n             bos_token=bos_token,\n             eos_token=eos_token,\n             unk_token=unk_token,\n             pad_token=pad_token,\n-            sp_model_kwargs=self.sp_model_kwargs,\n+            special_tokens_pattern=\"none\",\n             **kwargs,\n         )\n \n-    # Copied from transformers.models.albert.tokenization_albert.AlbertTokenizer.__getstate__\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sp_model\"] = None\n-        return state\n-\n-    # Copied from transformers.models.albert.tokenization_albert.AlbertTokenizer.__setstate__\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        # for backward compatibility\n-        if not hasattr(self, \"sp_model_kwargs\"):\n-            self.sp_model_kwargs = {}\n-\n-        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n-        self.sp_model.Load(self.vocab_file)\n-\n-    @property\n-    # Copied from transformers.models.albert.tokenization_albert.AlbertTokenizer.vocab_size\n-    def vocab_size(self) -> int:\n-        return len(self.sp_model)\n-\n     def preprocess_text(self, text: str) -> str:\n         \"\"\"\n         Returns the preprocessed text. This procedure is identical to what was used when training the tokenizer.\n@@ -191,19 +166,6 @@ def _tokenize(self, text: str, **kwargs) -> list[str]:\n         text = self.preprocess_text(text)\n         return self.sp_model.encode(text, out_type=str)\n \n-    def _convert_token_to_id(self, token: str) -> int:\n-        \"\"\"Converts a token (str) to an id (int) using the vocab.\"\"\"\n-        return self.sp_model.PieceToId(token)\n-\n-    def _convert_id_to_token(self, index: int) -> str:\n-        \"\"\"Converts an index (int) to a token (str) using the vocab.\"\"\"\n-        return self.sp_model.IdToPiece(index)\n-\n-    @staticmethod\n-    def clean_up_tokenization(out_string: str) -> str:\n-        \"\"\"Returns the input string, this function is overridden to remove the default clean up.\"\"\"\n-        return out_string\n-\n     def convert_tokens_to_string(self, tokens: list[str]) -> str:\n         \"\"\"Converts a sequence of tokens (strings) to a single string. Special tokens remain intact.\"\"\"\n         current_sub_tokens = []\n@@ -226,30 +188,6 @@ def convert_tokens_to_string(self, tokens: list[str]) -> str:\n \n         return out_string\n \n-    # Copied from transformers.models.albert.tokenization_albert.AlbertTokenizer.get_vocab\n-    def get_vocab(self) -> dict[str, int]:\n-        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n-        vocab.update(self.added_tokens_encoder)\n-        return vocab\n-\n-    # Copied from transformers.models.albert.tokenization_albert.AlbertTokenizer.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        out_vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n-            copyfile(self.vocab_file, out_vocab_file)\n-        elif not os.path.isfile(self.vocab_file):\n-            with open(out_vocab_file, \"wb\") as fi:\n-                content_spiece_model = self.sp_model.serialized_model_proto()\n-                fi.write(content_spiece_model)\n-\n-        return (out_vocab_file,)\n-\n     def encode_fast(\n         self, text: Union[str, list[str]], return_tensors: Union[str, bool] = False\n     ) -> Union[list[int], list[list[int]], \"torch.Tensor\"]:"
        },
        {
            "sha": "f0af7a797ab7b6b6a5e1ad5222fafb4dc4851c3b",
            "filename": "src/transformers/models/granite_speech/processing_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,7 +18,7 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils import PreTokenizedInput, TextInput\n+from ...tokenization_python import PreTokenizedInput, TextInput\n from ...utils import is_torch_available, logging\n from ...utils.import_utils import requires_backends\n "
        },
        {
            "sha": "77c7ec616f1f0f5c85812bba8fe1c696dc0c3b9a",
            "filename": "src/transformers/models/herbert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fherbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fherbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .tokenization_herbert import *\n-    from .tokenization_herbert_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "af3c259c0a2858701d984246847678fc84c46a11",
            "filename": "src/transformers/models/herbert/tokenization_herbert.py",
            "status": "modified",
            "additions": 75,
            "deletions": 563,
            "changes": 638,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -12,606 +12,118 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import json\n-import os\n-import re\n-import unicodedata\n+\n from typing import Optional\n \n-from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n+from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from tokenizers.models import BPE\n+\n+from ...tokenization_utils_tokenizers import TokenizersBackend\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n-VOCAB_FILES_NAMES = {\n-    \"vocab_file\": \"vocab.json\",\n-    \"merges_file\": \"merges.txt\",\n-}\n-\n-\n-# Copied from transformers.models.xlm.tokenization_xlm.get_pairs\n-def get_pairs(word):\n-    \"\"\"\n-    Return set of symbol pairs in a word. word is represented as tuple of symbols (symbols being variable-length\n-    strings)\n-    \"\"\"\n-    pairs = set()\n-    prev_char = word[0]\n-    for char in word[1:]:\n-        pairs.add((prev_char, char))\n-        prev_char = char\n-    return pairs\n-\n-\n-# Copied from transformers.models.xlm.tokenization_xlm.replace_unicode_punct\n-def replace_unicode_punct(text):\n-    \"\"\"\n-    Port of https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/replace-unicode-punctuation.perl\n-    \"\"\"\n-    text = text.replace(\"Ôºå\", \",\")\n-    text = re.sub(r\"„ÄÇ\\s*\", \". \", text)\n-    text = text.replace(\"„ÄÅ\", \",\")\n-    text = text.replace(\"‚Äù\", '\"')\n-    text = text.replace(\"‚Äú\", '\"')\n-    text = text.replace(\"‚à∂\", \":\")\n-    text = text.replace(\"Ôºö\", \":\")\n-    text = text.replace(\"Ôºü\", \"?\")\n-    text = text.replace(\"„Ää\", '\"')\n-    text = text.replace(\"„Äã\", '\"')\n-    text = text.replace(\"Ôºâ\", \")\")\n-    text = text.replace(\"ÔºÅ\", \"!\")\n-    text = text.replace(\"Ôºà\", \"(\")\n-    text = text.replace(\"Ôºõ\", \";\")\n-    text = text.replace(\"Ôºë\", \"1\")\n-    text = text.replace(\"„Äç\", '\"')\n-    text = text.replace(\"„Äå\", '\"')\n-    text = text.replace(\"Ôºê\", \"0\")\n-    text = text.replace(\"Ôºì\", \"3\")\n-    text = text.replace(\"Ôºí\", \"2\")\n-    text = text.replace(\"Ôºï\", \"5\")\n-    text = text.replace(\"Ôºñ\", \"6\")\n-    text = text.replace(\"Ôºô\", \"9\")\n-    text = text.replace(\"Ôºó\", \"7\")\n-    text = text.replace(\"Ôºò\", \"8\")\n-    text = text.replace(\"Ôºî\", \"4\")\n-    text = re.sub(r\"Ôºé\\s*\", \". \", text)\n-    text = text.replace(\"ÔΩû\", \"~\")\n-    text = text.replace(\"‚Äô\", \"'\")\n-    text = text.replace(\"‚Ä¶\", \"...\")\n-    text = text.replace(\"‚îÅ\", \"-\")\n-    text = text.replace(\"„Äà\", \"<\")\n-    text = text.replace(\"„Äâ\", \">\")\n-    text = text.replace(\"„Äê\", \"[\")\n-    text = text.replace(\"„Äë\", \"]\")\n-    text = text.replace(\"ÔºÖ\", \"%\")\n-    return text\n-\n-\n-# Copied from transformers.models.xlm.tokenization_xlm.remove_non_printing_char\n-def remove_non_printing_char(text):\n-    \"\"\"\n-    Port of https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/remove-non-printing-char.perl\n-    \"\"\"\n-    output = []\n-    for char in text:\n-        cat = unicodedata.category(char)\n-        if cat.startswith(\"C\"):\n-            continue\n-        output.append(char)\n-    return \"\".join(output)\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize\n-def whitespace_tokenize(text):\n-    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n-    text = text.strip()\n-    if not text:\n-        return []\n-    tokens = text.split()\n-    return tokens\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer\n-class BasicTokenizer:\n-    \"\"\"\n-    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n+VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\"}\n \n-    Args:\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n \n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        do_split_on_punc (`bool`, *optional*, defaults to `True`):\n-            In some instances we want to skip the basic punctuation splitting so that later tokenization can capture\n-            the full context of the words, such as contractions.\n+class HerbertTokenizer(TokenizersBackend):\n     \"\"\"\n-\n-    def __init__(\n-        self,\n-        do_lower_case=True,\n-        never_split=None,\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        do_split_on_punc=True,\n-    ):\n-        if never_split is None:\n-            never_split = []\n-        self.do_lower_case = do_lower_case\n-        self.never_split = set(never_split)\n-        self.tokenize_chinese_chars = tokenize_chinese_chars\n-        self.strip_accents = strip_accents\n-        self.do_split_on_punc = do_split_on_punc\n-\n-    def tokenize(self, text, never_split=None):\n-        \"\"\"\n-        Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n-\n-        Args:\n-            never_split (`List[str]`, *optional*)\n-                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n-                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n-        \"\"\"\n-        # union() returns a new set by concatenating the two sets.\n-        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n-        text = self._clean_text(text)\n-\n-        # This was added on November 1st, 2018 for the multilingual and Chinese\n-        # models. This is also applied to the English models now, but it doesn't\n-        # matter since the English models were not trained on any Chinese data\n-        # and generally don't have any Chinese data in them (there are Chinese\n-        # characters in the vocabulary because Wikipedia does have some Chinese\n-        # words in the English Wikipedia.).\n-        if self.tokenize_chinese_chars:\n-            text = self._tokenize_chinese_chars(text)\n-        # prevents treating the same character with different unicode codepoints as different characters\n-        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n-        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n-        split_tokens = []\n-        for token in orig_tokens:\n-            if token not in never_split:\n-                if self.do_lower_case:\n-                    token = token.lower()\n-                    if self.strip_accents is not False:\n-                        token = self._run_strip_accents(token)\n-                elif self.strip_accents:\n-                    token = self._run_strip_accents(token)\n-            split_tokens.extend(self._run_split_on_punc(token, never_split))\n-\n-        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n-        return output_tokens\n-\n-    def _run_strip_accents(self, text):\n-        \"\"\"Strips accents from a piece of text.\"\"\"\n-        text = unicodedata.normalize(\"NFD\", text)\n-        output = []\n-        for char in text:\n-            cat = unicodedata.category(char)\n-            if cat == \"Mn\":\n-                continue\n-            output.append(char)\n-        return \"\".join(output)\n-\n-    def _run_split_on_punc(self, text, never_split=None):\n-        \"\"\"Splits punctuation on a piece of text.\"\"\"\n-        if not self.do_split_on_punc or (never_split is not None and text in never_split):\n-            return [text]\n-        chars = list(text)\n-        i = 0\n-        start_new_word = True\n-        output = []\n-        while i < len(chars):\n-            char = chars[i]\n-            if _is_punctuation(char):\n-                output.append([char])\n-                start_new_word = True\n-            else:\n-                if start_new_word:\n-                    output.append([])\n-                start_new_word = False\n-                output[-1].append(char)\n-            i += 1\n-\n-        return [\"\".join(x) for x in output]\n-\n-    def _tokenize_chinese_chars(self, text):\n-        \"\"\"Adds whitespace around any CJK character.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if self._is_chinese_char(cp):\n-                output.append(\" \")\n-                output.append(char)\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-    def _is_chinese_char(self, cp):\n-        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n-        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n-        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n-        #\n-        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n-        # despite its name. The modern Korean Hangul alphabet is a different block,\n-        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n-        # space-separated words, so they are not treated specially and handled\n-        # like the all of the other languages.\n-        if (\n-            (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n-            or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n-        ):\n-            return True\n-\n-        return False\n-\n-    def _clean_text(self, text):\n-        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if cp == 0 or cp == 0xFFFD or _is_control(char):\n-                continue\n-            if _is_whitespace(char):\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-\n-class HerbertTokenizer(PreTrainedTokenizer):\n-    \"\"\"\n-    Construct a BPE tokenizer for HerBERT.\n+    Construct a BPE tokenizer for HerBERT (backed by HuggingFace's tokenizers library).\n \n     Peculiarities:\n \n-    - uses BERT's pre-tokenizer: BaseTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of a\n-      punctuation character will be treated separately.\n-\n-    - Such pretokenized input is BPE subtokenized\n+    - uses BERT's pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of\n+      a punctuation character will be treated separately.\n \n-    This tokenizer inherits from [`XLMTokenizer`] which contains most of the methods. Users should refer to the\n+    This tokenizer inherits from [`TokenizersBackend`] which contains most of the methods. Users should refer to the\n     superclass for more information regarding methods.\n+\n+    Args:\n+        vocab_file (`str`):\n+            Path to the vocabulary file.\n+        merges_file (`str`):\n+            Path to the merges file.\n+        cls_token (`str`, *optional*, defaults to `\"<s>\"`):\n+            The classifier token.\n+        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n+            The unknown token.\n+        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n+            The padding token.\n+        mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n+            The mask token.\n+        sep_token (`str`, *optional*, defaults to `\"</s>\"`):\n+            The separator token.\n+        vocab (`dict`, *optional*):\n+            Custom vocabulary dictionary.\n+        merges (`list`, *optional*):\n+            Custom merges list.\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n+    slow_tokenizer_class = None\n \n     def __init__(\n         self,\n-        vocab_file,\n-        merges_file,\n-        tokenizer_file=None,\n-        cls_token=\"<s>\",\n-        unk_token=\"<unk>\",\n-        pad_token=\"<pad>\",\n-        mask_token=\"<mask>\",\n-        sep_token=\"</s>\",\n-        bos_token=\"<s>\",\n-        do_lowercase_and_remove_accent=False,\n-        additional_special_tokens=[\n-            \"<special0>\",\n-            \"<special1>\",\n-            \"<special2>\",\n-            \"<special3>\",\n-            \"<special4>\",\n-            \"<special5>\",\n-            \"<special6>\",\n-            \"<special7>\",\n-            \"<special8>\",\n-            \"<special9>\",\n-        ],\n-        lang2id=None,\n-        id2lang=None,\n+        vocab: Optional[dict] = None,\n+        merges: Optional[list] = None,\n+        cls_token: str = \"<s>\",\n+        unk_token: str = \"<unk>\",\n+        pad_token: str = \"<pad>\",\n+        mask_token: str = \"<mask>\",\n+        sep_token: str = \"</s>\",\n+        vocab_file: Optional[str] = None,\n+        merges_file: Optional[str] = None,\n         **kwargs,\n     ):\n-        try:\n-            import sacremoses\n-        except ImportError:\n-            raise ImportError(\n-                \"You need to install sacremoses to use HerbertTokenizer. \"\n-                \"See https://pypi.org/project/sacremoses/ for installation.\"\n+        if vocab is not None:\n+            self._vocab = (\n+                {token: idx for idx, (token, _score) in enumerate(vocab)} if isinstance(vocab, list) else vocab\n             )\n+        else:\n+            self._vocab = {}\n \n-        self.sm = sacremoses\n+        if merges is not None:\n+            # Convert lists to tuples if necessary (happens when loading from JSON)\n+            self._merges = [tuple(merge) if isinstance(merge, list) else merge for merge in merges]\n+        else:\n+            self._merges = []\n+\n+        self._tokenizer = Tokenizer(\n+            BPE(\n+                vocab=self._vocab,\n+                merges=self._merges,\n+                dropout=None,\n+                unk_token=str(unk_token),\n+                end_of_word_suffix=\"</w>\",\n+            )\n+        )\n \n-        # cache of sm.MosesPunctNormalizer instance\n-        self.cache_moses_punct_normalizer = {}\n-        # cache of sm.MosesTokenizer instance\n-        self.cache_moses_tokenizer = {}\n-        self.lang_with_custom_tokenizer = {\"zh\", \"th\", \"ja\"}\n-        # True for current supported model (v1.2.0), False for XLM-17 & 100\n-        self.do_lowercase_and_remove_accent = do_lowercase_and_remove_accent\n-        self.lang2id = lang2id\n-        self.id2lang = id2lang\n-        if lang2id is not None and id2lang is not None:\n-            assert len(lang2id) == len(id2lang)\n+        self._tokenizer.normalizer = normalizers.BertNormalizer(\n+            lowercase=False, strip_accents=False, clean_text=True, handle_chinese_chars=True\n+        )\n+        self._tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n+        self._tokenizer.decoder = decoders.BPEDecoder(suffix=\"</w>\")\n \n-        self.ja_word_tokenizer = None\n-        self.zh_word_tokenizer = None\n+        tokenizer_object = self._tokenizer\n \n-        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n-            self.encoder = json.load(vocab_handle)\n-        self.decoder = {v: k for k, v in self.encoder.items()}\n-        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n-            merges = merges_handle.read().split(\"\\n\")[:-1]\n-        merges = [tuple(merge.split()[:2]) for merge in merges]\n-        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n-        self.cache = {}\n+        self.vocab_file = vocab_file\n+        self.merges_file = merges_file\n \n         super().__init__(\n+            tokenizer_object=tokenizer_object,\n+            cls_token=cls_token,\n             unk_token=unk_token,\n-            bos_token=bos_token,\n-            sep_token=sep_token,\n             pad_token=pad_token,\n-            cls_token=cls_token,\n             mask_token=mask_token,\n-            additional_special_tokens=additional_special_tokens,\n-            lang2id=lang2id,\n-            id2lang=id2lang,\n-            do_lowercase_and_remove_accent=do_lowercase_and_remove_accent,\n-            tokenizer_file=None,\n+            sep_token=sep_token,\n             **kwargs,\n         )\n \n-        self.bert_pre_tokenizer = BasicTokenizer(\n-            do_lower_case=False,\n-            never_split=self.all_special_tokens,\n-            tokenize_chinese_chars=False,\n-            strip_accents=False,\n-        )\n-\n-    @property\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.do_lower_case\n-    def do_lower_case(self):\n-        return self.do_lowercase_and_remove_accent\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.moses_punct_norm\n-    def moses_punct_norm(self, text, lang):\n-        if lang not in self.cache_moses_punct_normalizer:\n-            punct_normalizer = self.sm.MosesPunctNormalizer(lang=lang)\n-            self.cache_moses_punct_normalizer[lang] = punct_normalizer\n-        else:\n-            punct_normalizer = self.cache_moses_punct_normalizer[lang]\n-        return punct_normalizer.normalize(text)\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.moses_tokenize\n-    def moses_tokenize(self, text, lang):\n-        if lang not in self.cache_moses_tokenizer:\n-            moses_tokenizer = self.sm.MosesTokenizer(lang=lang)\n-            self.cache_moses_tokenizer[lang] = moses_tokenizer\n-        else:\n-            moses_tokenizer = self.cache_moses_tokenizer[lang]\n-        return moses_tokenizer.tokenize(text, return_str=False, escape=False)\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.moses_pipeline\n-    def moses_pipeline(self, text, lang):\n-        text = replace_unicode_punct(text)\n-        text = self.moses_punct_norm(text, lang)\n-        text = remove_non_printing_char(text)\n-        return text\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.ja_tokenize\n-    def ja_tokenize(self, text):\n-        if self.ja_word_tokenizer is None:\n-            try:\n-                import Mykytea\n-\n-                self.ja_word_tokenizer = Mykytea.Mykytea(\n-                    f\"-model {os.path.expanduser('~')}/local/share/kytea/model.bin\"\n-                )\n-            except (AttributeError, ImportError):\n-                logger.error(\n-                    \"Make sure you install KyTea (https://github.com/neubig/kytea) and it's python wrapper\"\n-                    \" (https://github.com/chezou/Mykytea-python) with the following steps\"\n-                )\n-                logger.error(\"1. git clone git@github.com:neubig/kytea.git && cd kytea\")\n-                logger.error(\"2. autoreconf -i\")\n-                logger.error(\"3. ./configure --prefix=$HOME/local\")\n-                logger.error(\"4. make && make install\")\n-                logger.error(\"5. pip install kytea\")\n-                raise\n-        return list(self.ja_word_tokenizer.getWS(text))\n-\n-    @property\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.vocab_size\n-    def vocab_size(self):\n-        return len(self.encoder)\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.get_vocab\n-    def get_vocab(self):\n-        return dict(self.encoder, **self.added_tokens_encoder)\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.bpe\n-    def bpe(self, token):\n-        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n-        if token in self.cache:\n-            return self.cache[token]\n-        pairs = get_pairs(word)\n-\n-        if not pairs:\n-            return token + \"</w>\"\n-\n-        while True:\n-            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n-            if bigram not in self.bpe_ranks:\n-                break\n-            first, second = bigram\n-            new_word = []\n-            i = 0\n-            while i < len(word):\n-                try:\n-                    j = word.index(first, i)\n-                except ValueError:\n-                    new_word.extend(word[i:])\n-                    break\n-                else:\n-                    new_word.extend(word[i:j])\n-                    i = j\n-\n-                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n-                    new_word.append(first + second)\n-                    i += 2\n-                else:\n-                    new_word.append(word[i])\n-                    i += 1\n-            new_word = tuple(new_word)\n-            word = new_word\n-            if len(word) == 1:\n-                break\n-            else:\n-                pairs = get_pairs(word)\n-        word = \" \".join(word)\n-        if word == \"\\n  </w>\":\n-            word = \"\\n</w>\"\n-        self.cache[token] = word\n-        return word\n-\n-    def _tokenize(self, text):\n-        pre_tokens = self.bert_pre_tokenizer.tokenize(text)\n-\n-        split_tokens = []\n-        for token in pre_tokens:\n-            if token:\n-                split_tokens.extend(list(self.bpe(token).split(\" \")))\n-\n-        return split_tokens\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer._convert_token_to_id\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.encoder.get(token, self.encoder.get(self.unk_token))\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer._convert_id_to_token\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.decoder.get(index, self.unk_token)\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.convert_tokens_to_string\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        out_string = \"\".join(tokens).replace(\"</w>\", \" \").strip()\n-        return out_string\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.build_inputs_with_special_tokens\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An XLM sequence has the following format:\n-\n-        - single sequence: `<s> X </s>`\n-        - pair of sequences: `<s> A </s> B </s>`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-\n-        \"\"\"\n-        bos = [self.bos_token_id]\n-        sep = [self.sep_token_id]\n-\n-        if token_ids_1 is None:\n-            return bos + token_ids_0 + sep\n-        return bos + token_ids_0 + sep + token_ids_1 + sep\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.get_special_tokens_mask\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-        merge_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n+        self._tokenizer.post_processor = processors.BertProcessing(\n+            sep=(self.sep_token, 2),\n+            cls=(self.cls_token, 0),\n         )\n \n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n-\n-        index = 0\n-        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n-            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n-                        \" Please check that the tokenizer is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n-                index += 1\n-\n-        return vocab_file, merge_file\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.__getstate__\n-    def __getstate__(self):\n-        state = self.__dict__.copy()\n-        state[\"sm\"] = None\n-        return state\n-\n-    # Copied from transformers.models.xlm.tokenization_xlm.XLMTokenizer.__setstate__\n-    def __setstate__(self, d):\n-        self.__dict__ = d\n-\n-        try:\n-            import sacremoses\n-        except ImportError:\n-            raise ImportError(\n-                \"You need to install sacremoses to use XLMTokenizer. \"\n-                \"See https://pypi.org/project/sacremoses/ for installation.\"\n-            )\n-\n-        self.sm = sacremoses\n-\n \n __all__ = [\"HerbertTokenizer\"]"
        },
        {
            "sha": "fdc24e3c6a6e20a847043379f898fead1beb819f",
            "filename": "src/transformers/models/herbert/tokenization_herbert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 133,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,133 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 The Google AI Language Team Authors, Allegro.pl, Facebook Inc. and the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from typing import Optional\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_herbert import HerbertTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.json\", \"merges_file\": \"merges.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-class HerbertTokenizerFast(PreTrainedTokenizerFast):\n-    \"\"\"\n-    Construct a \"Fast\" BPE tokenizer for HerBERT (backed by HuggingFace's *tokenizers* library).\n-\n-    Peculiarities:\n-\n-    - uses BERT's pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of\n-      a punctuation character will be treated separately.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the methods. Users should refer to the\n-    superclass for more information regarding methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            Path to the vocabulary file.\n-        merges_file (`str`):\n-            Path to the merges file.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = HerbertTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        merges_file=None,\n-        tokenizer_file=None,\n-        cls_token=\"<s>\",\n-        unk_token=\"<unk>\",\n-        pad_token=\"<pad>\",\n-        mask_token=\"<mask>\",\n-        sep_token=\"</s>\",\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            merges_file,\n-            tokenizer_file=tokenizer_file,\n-            cls_token=cls_token,\n-            unk_token=unk_token,\n-            pad_token=pad_token,\n-            mask_token=mask_token,\n-            sep_token=sep_token,\n-            **kwargs,\n-        )\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. An HerBERT, like BERT sequence has the following format:\n-\n-        - single sequence: `<s> X </s>`\n-        - pair of sequences: `<s> A </s> B </s>`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        if token_ids_1 is None:\n-            return cls + token_ids_0 + sep\n-\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is None:\n-            return [1] + ([0] * len(token_ids_0)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"HerbertTokenizerFast\"]"
        },
        {
            "sha": "b21f6126b1c4c0668572906a74ef257654f901a4",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -16,6 +16,7 @@\n Processor class for IDEFICS2.\n \"\"\"\n \n+import re\n from itertools import accumulate\n from typing import TYPE_CHECKING, Optional, Union\n \n@@ -188,11 +189,14 @@ def __call__(\n                 image_str = image_str * 5\n \n             prompt_strings = []\n+            closing_fake_pattern = re.compile(rf\"{re.escape(fake_image_token)}(?=[^\\s<])\")\n             for sample in text:\n                 n_images_in_text.append(sample.count(image_token))\n                 sample = sample.replace(image_token, image_str)\n                 # Remove any double fake tokens if images are adjacent\n                 sample = sample.replace(f\"{fake_image_token}{fake_image_token}\", f\"{fake_image_token}\")\n+                # Ensure words attached directly after the closing fake token remain word-boundary aligned\n+                sample = closing_fake_pattern.sub(f\"{fake_image_token} \", sample)\n                 prompt_strings.append(sample)\n \n             text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])"
        },
        {
            "sha": "736cc7d917f6337b928d25e632c41f16414b73eb",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -22,7 +22,7 @@\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n-from ...tokenization_utils import AddedToken\n+from ...tokenization_python import AddedToken\n from ...tokenization_utils_base import BatchEncoding, TextInput\n \n "
        },
        {
            "sha": "25f2eed74107ce29dd58698a00853abd989f78c8",
            "filename": "src/transformers/models/kosmos2_5/processing_kosmos2_5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -55,8 +55,8 @@ class Kosmos2_5Processor(ProcessorMixin):\n     Args:\n         image_processor (`Kosmos2_5ImageProcessor`):\n             An instance of [`Kosmos2_5ImageProcessor`]. The image processor is a required input.\n-        tokenizer (Union[`T5TokenizerFast`, `T5Tokenizer`]):\n-            An instance of ['T5TokenizerFast`] or ['T5Tokenizer`]. The tokenizer is a required input.\n+        tokenizer (`T5Tokenizer`):\n+            An instance of ['T5Tokenizer`]. The tokenizer is a required input.\n         num_image_tokens (`int`, *optional*, defaults to 2048):\n             Number of image tokens used as a placeholder.\n     \"\"\""
        },
        {
            "sha": "5edb5482146e8f8bee62d44ab1948446edb0f0de",
            "filename": "src/transformers/models/layoutlm/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -18,10 +18,10 @@\n \n \n if TYPE_CHECKING:\n+    from ..bert.tokenization_bert import BertTokenizer as LayoutLMTokenizer\n+    from ..bert.tokenization_bert import BertTokenizer as LayoutLMTokenizerFast\n     from .configuration_layoutlm import *\n     from .modeling_layoutlm import *\n-    from .tokenization_layoutlm import *\n-    from .tokenization_layoutlm_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "4caccd691d0e5fcb64637f72d5c2860f6f096e9e",
            "filename": "src/transformers/models/layoutlm/tokenization_layoutlm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 483,
            "changes": 483,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,483 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The Microsoft Research Asia LayoutLM Team Authors.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization class for model LayoutLM.\"\"\"\n-\n-import collections\n-import os\n-import unicodedata\n-from typing import Optional\n-\n-from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n-from ...utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.load_vocab\n-def load_vocab(vocab_file):\n-    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n-    vocab = collections.OrderedDict()\n-    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n-        tokens = reader.readlines()\n-    for index, token in enumerate(tokens):\n-        token = token.rstrip(\"\\n\")\n-        vocab[token] = index\n-    return vocab\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize\n-def whitespace_tokenize(text):\n-    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n-    text = text.strip()\n-    if not text:\n-        return []\n-    tokens = text.split()\n-    return tokens\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.BertTokenizer with Bert->LayoutLM,BERT->LayoutLM\n-class LayoutLMTokenizer(PreTrainedTokenizer):\n-    r\"\"\"\n-    Construct a LayoutLM tokenizer. Based on WordPiece.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n-    this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        do_basic_tokenize (`bool`, *optional*, defaults to `True`):\n-            Whether or not to do basic tokenization before WordPiece.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original LayoutLM).\n-        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n-            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n-            extra spaces.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-\n-    def __init__(\n-        self,\n-        vocab_file,\n-        do_lower_case=True,\n-        do_basic_tokenize=True,\n-        never_split=None,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        clean_up_tokenization_spaces=True,\n-        **kwargs,\n-    ):\n-        if not os.path.isfile(vocab_file):\n-            raise ValueError(\n-                f\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\n-                \" model use `tokenizer = LayoutLMTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n-            )\n-        self.vocab = load_vocab(vocab_file)\n-        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n-        self.do_basic_tokenize = do_basic_tokenize\n-        if do_basic_tokenize:\n-            self.basic_tokenizer = BasicTokenizer(\n-                do_lower_case=do_lower_case,\n-                never_split=never_split,\n-                tokenize_chinese_chars=tokenize_chinese_chars,\n-                strip_accents=strip_accents,\n-            )\n-\n-        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=str(unk_token))\n-\n-        super().__init__(\n-            do_lower_case=do_lower_case,\n-            do_basic_tokenize=do_basic_tokenize,\n-            never_split=never_split,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-            **kwargs,\n-        )\n-\n-    @property\n-    def do_lower_case(self):\n-        return self.basic_tokenizer.do_lower_case\n-\n-    @property\n-    def vocab_size(self):\n-        return len(self.vocab)\n-\n-    def get_vocab(self):\n-        return dict(self.vocab, **self.added_tokens_encoder)\n-\n-    def _tokenize(self, text, split_special_tokens=False):\n-        split_tokens = []\n-        if self.do_basic_tokenize:\n-            for token in self.basic_tokenizer.tokenize(\n-                text, never_split=self.all_special_tokens if not split_special_tokens else None\n-            ):\n-                # If the token is part of the never_split set\n-                if token in self.basic_tokenizer.never_split:\n-                    split_tokens.append(token)\n-                else:\n-                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n-        else:\n-            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n-        return split_tokens\n-\n-    def _convert_token_to_id(self, token):\n-        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        return self.vocab.get(token, self.vocab.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index):\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        return self.ids_to_tokens.get(index, self.unk_token)\n-\n-    def convert_tokens_to_string(self, tokens):\n-        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n-        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n-        return out_string\n-\n-    def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n-    ) -> list[int]:\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A LayoutLM sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        if token_ids_1 is None:\n-            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-        cls = [self.cls_token_id]\n-        sep = [self.sep_token_id]\n-        return cls + token_ids_0 + sep + token_ids_1 + sep\n-\n-    def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n-    ) -> list[int]:\n-        \"\"\"\n-        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n-        special tokens using the tokenizer `prepare_for_model` method.\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n-                Whether or not the token list is already formatted with special tokens for the model.\n-\n-        Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n-        \"\"\"\n-\n-        if already_has_special_tokens:\n-            return super().get_special_tokens_mask(\n-                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n-            )\n-\n-        if token_ids_1 is not None:\n-            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n-        return [1] + ([0] * len(token_ids_0)) + [1]\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        index = 0\n-        if os.path.isdir(save_directory):\n-            vocab_file = os.path.join(\n-                save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-            )\n-        else:\n-            vocab_file = (filename_prefix + \"-\" if filename_prefix else \"\") + save_directory\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n-            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n-                if index != token_index:\n-                    logger.warning(\n-                        f\"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive.\"\n-                        \" Please check that the vocabulary is not corrupted!\"\n-                    )\n-                    index = token_index\n-                writer.write(token + \"\\n\")\n-                index += 1\n-        return (vocab_file,)\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer\n-class BasicTokenizer:\n-    \"\"\"\n-    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).\n-\n-    Args:\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        never_split (`Iterable`, *optional*):\n-            Collection of tokens which will never be split during tokenization. Only has an effect when\n-            `do_basic_tokenize=True`\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters.\n-\n-            This should likely be deactivated for Japanese (see this\n-            [issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original BERT).\n-        do_split_on_punc (`bool`, *optional*, defaults to `True`):\n-            In some instances we want to skip the basic punctuation splitting so that later tokenization can capture\n-            the full context of the words, such as contractions.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        do_lower_case=True,\n-        never_split=None,\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        do_split_on_punc=True,\n-    ):\n-        if never_split is None:\n-            never_split = []\n-        self.do_lower_case = do_lower_case\n-        self.never_split = set(never_split)\n-        self.tokenize_chinese_chars = tokenize_chinese_chars\n-        self.strip_accents = strip_accents\n-        self.do_split_on_punc = do_split_on_punc\n-\n-    def tokenize(self, text, never_split=None):\n-        \"\"\"\n-        Basic Tokenization of a piece of text. For sub-word tokenization, see WordPieceTokenizer.\n-\n-        Args:\n-            never_split (`List[str]`, *optional*)\n-                Kept for backward compatibility purposes. Now implemented directly at the base class level (see\n-                [`PreTrainedTokenizer.tokenize`]) List of token not to split.\n-        \"\"\"\n-        # union() returns a new set by concatenating the two sets.\n-        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n-        text = self._clean_text(text)\n-\n-        # This was added on November 1st, 2018 for the multilingual and Chinese\n-        # models. This is also applied to the English models now, but it doesn't\n-        # matter since the English models were not trained on any Chinese data\n-        # and generally don't have any Chinese data in them (there are Chinese\n-        # characters in the vocabulary because Wikipedia does have some Chinese\n-        # words in the English Wikipedia.).\n-        if self.tokenize_chinese_chars:\n-            text = self._tokenize_chinese_chars(text)\n-        # prevents treating the same character with different unicode codepoints as different characters\n-        unicode_normalized_text = unicodedata.normalize(\"NFC\", text)\n-        orig_tokens = whitespace_tokenize(unicode_normalized_text)\n-        split_tokens = []\n-        for token in orig_tokens:\n-            if token not in never_split:\n-                if self.do_lower_case:\n-                    token = token.lower()\n-                    if self.strip_accents is not False:\n-                        token = self._run_strip_accents(token)\n-                elif self.strip_accents:\n-                    token = self._run_strip_accents(token)\n-            split_tokens.extend(self._run_split_on_punc(token, never_split))\n-\n-        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n-        return output_tokens\n-\n-    def _run_strip_accents(self, text):\n-        \"\"\"Strips accents from a piece of text.\"\"\"\n-        text = unicodedata.normalize(\"NFD\", text)\n-        output = []\n-        for char in text:\n-            cat = unicodedata.category(char)\n-            if cat == \"Mn\":\n-                continue\n-            output.append(char)\n-        return \"\".join(output)\n-\n-    def _run_split_on_punc(self, text, never_split=None):\n-        \"\"\"Splits punctuation on a piece of text.\"\"\"\n-        if not self.do_split_on_punc or (never_split is not None and text in never_split):\n-            return [text]\n-        chars = list(text)\n-        i = 0\n-        start_new_word = True\n-        output = []\n-        while i < len(chars):\n-            char = chars[i]\n-            if _is_punctuation(char):\n-                output.append([char])\n-                start_new_word = True\n-            else:\n-                if start_new_word:\n-                    output.append([])\n-                start_new_word = False\n-                output[-1].append(char)\n-            i += 1\n-\n-        return [\"\".join(x) for x in output]\n-\n-    def _tokenize_chinese_chars(self, text):\n-        \"\"\"Adds whitespace around any CJK character.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if self._is_chinese_char(cp):\n-                output.append(\" \")\n-                output.append(char)\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-    def _is_chinese_char(self, cp):\n-        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n-        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n-        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n-        #\n-        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n-        # despite its name. The modern Korean Hangul alphabet is a different block,\n-        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n-        # space-separated words, so they are not treated specially and handled\n-        # like the all of the other languages.\n-        if (\n-            (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n-            or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n-        ):\n-            return True\n-\n-        return False\n-\n-    def _clean_text(self, text):\n-        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n-        output = []\n-        for char in text:\n-            cp = ord(char)\n-            if cp == 0 or cp == 0xFFFD or _is_control(char):\n-                continue\n-            if _is_whitespace(char):\n-                output.append(\" \")\n-            else:\n-                output.append(char)\n-        return \"\".join(output)\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert.WordpieceTokenizer\n-class WordpieceTokenizer:\n-    \"\"\"Runs WordPiece tokenization.\"\"\"\n-\n-    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n-        self.vocab = vocab\n-        self.unk_token = unk_token\n-        self.max_input_chars_per_word = max_input_chars_per_word\n-\n-    def tokenize(self, text):\n-        \"\"\"\n-        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n-        tokenization using the given vocabulary.\n-\n-        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n-\n-        Args:\n-            text: A single token or whitespace separated tokens. This should have\n-                already been passed through *BasicTokenizer*.\n-\n-        Returns:\n-            A list of wordpiece tokens.\n-        \"\"\"\n-\n-        output_tokens = []\n-        for token in whitespace_tokenize(text):\n-            chars = list(token)\n-            if len(chars) > self.max_input_chars_per_word:\n-                output_tokens.append(self.unk_token)\n-                continue\n-\n-            is_bad = False\n-            start = 0\n-            sub_tokens = []\n-            while start < len(chars):\n-                end = len(chars)\n-                cur_substr = None\n-                while start < end:\n-                    substr = \"\".join(chars[start:end])\n-                    if start > 0:\n-                        substr = \"##\" + substr\n-                    if substr in self.vocab:\n-                        cur_substr = substr\n-                        break\n-                    end -= 1\n-                if cur_substr is None:\n-                    is_bad = True\n-                    break\n-                sub_tokens.append(cur_substr)\n-                start = end\n-\n-            if is_bad:\n-                output_tokens.append(self.unk_token)\n-            else:\n-                output_tokens.extend(sub_tokens)\n-        return output_tokens\n-\n-\n-__all__ = [\"LayoutLMTokenizer\"]"
        },
        {
            "sha": "c7ade6e0b8cdf04f2d4f06b6191b93a8ed7ee2a6",
            "filename": "src/transformers/models/layoutlm/tokenization_layoutlm_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 147,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330",
            "patch": "@@ -1,147 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The Microsoft Research Asia LayoutLM Team Authors.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Tokenization class for model LayoutLM.\"\"\"\n-\n-import json\n-from typing import Optional\n-\n-from tokenizers import normalizers\n-\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n-from ...utils import logging\n-from .tokenization_layoutlm import LayoutLMTokenizer\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\", \"tokenizer_file\": \"tokenizer.json\"}\n-\n-\n-# Copied from transformers.models.bert.tokenization_bert_fast.BertTokenizerFast with Bert->LayoutLM,BERT->LayoutLM\n-class LayoutLMTokenizerFast(PreTrainedTokenizerFast):\n-    r\"\"\"\n-    Construct a \"fast\" LayoutLM tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n-    refer to this superclass for more information regarding those methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        do_lower_case (`bool`, *optional*, defaults to `True`):\n-            Whether or not to lowercase the input when tokenizing.\n-        unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n-            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n-            sequence classification or for a text and a question for question answering. It is also used as the last\n-            token of a sequence built with special tokens.\n-        pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n-            The classifier token which is used when doing sequence classification (classification of the whole sequence\n-            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n-        mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n-            The token used for masking values. This is the token used when training this model with masked language\n-            modeling. This is the token which the model will try to predict.\n-        clean_text (`bool`, *optional*, defaults to `True`):\n-            Whether or not to clean the text before tokenization by removing any control characters and replacing all\n-            whitespaces by the classic one.\n-        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n-            Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n-            issue](https://github.com/huggingface/transformers/issues/328)).\n-        strip_accents (`bool`, *optional*):\n-            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n-            value for `lowercase` (as in the original LayoutLM).\n-        wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n-            The prefix for subwords.\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class = LayoutLMTokenizer\n-\n-    def __init__(\n-        self,\n-        vocab_file=None,\n-        tokenizer_file=None,\n-        do_lower_case=True,\n-        unk_token=\"[UNK]\",\n-        sep_token=\"[SEP]\",\n-        pad_token=\"[PAD]\",\n-        cls_token=\"[CLS]\",\n-        mask_token=\"[MASK]\",\n-        tokenize_chinese_chars=True,\n-        strip_accents=None,\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            vocab_file,\n-            tokenizer_file=tokenizer_file,\n-            do_lower_case=do_lower_case,\n-            unk_token=unk_token,\n-            sep_token=sep_token,\n-            pad_token=pad_token,\n-            cls_token=cls_token,\n-            mask_token=mask_token,\n-            tokenize_chinese_chars=tokenize_chinese_chars,\n-            strip_accents=strip_accents,\n-            **kwargs,\n-        )\n-\n-        normalizer_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n-        if (\n-            normalizer_state.get(\"lowercase\", do_lower_case) != do_lower_case\n-            or normalizer_state.get(\"strip_accents\", strip_accents) != strip_accents\n-            or normalizer_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n-        ):\n-            normalizer_class = getattr(normalizers, normalizer_state.pop(\"type\"))\n-            normalizer_state[\"lowercase\"] = do_lower_case\n-            normalizer_state[\"strip_accents\"] = strip_accents\n-            normalizer_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n-            self.backend_tokenizer.normalizer = normalizer_class(**normalizer_state)\n-\n-        self.do_lower_case = do_lower_case\n-\n-    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n-        \"\"\"\n-        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n-        adding special tokens. A LayoutLM sequence has the following format:\n-\n-        - single sequence: `[CLS] X [SEP]`\n-        - pair of sequences: `[CLS] A [SEP] B [SEP]`\n-\n-        Args:\n-            token_ids_0 (`List[int]`):\n-                List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n-                Optional second list of IDs for sequence pairs.\n-\n-        Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n-        \"\"\"\n-        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n-\n-        if token_ids_1 is not None:\n-            output += token_ids_1 + [self.sep_token_id]\n-\n-        return output\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n-        files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n-        return tuple(files)\n-\n-\n-__all__ = [\"LayoutLMTokenizerFast\"]"
        },
        {
            "sha": "b22c0ee86d91887925cd2735aad138c6db6aefcc",
            "filename": "src/transformers/models/layoutlmv2/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "patch": "@@ -25,7 +25,6 @@\n     from .modeling_layoutlmv2 import *\n     from .processing_layoutlmv2 import *\n     from .tokenization_layoutlmv2 import *\n-    from .tokenization_layoutlmv2_fast import *\n else:\n     import sys\n "
        },
        {
            "sha": "bdf0bed1ed3f6d6d04bf9b8a0d2aa766060f113b",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "8e324ee0b8fe971b06cd102eaa9930990a1f5b99",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 789,
            "changes": 789,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "0a68bf55a15d0fcc0a44cad5c86a864910b87210",
            "filename": "src/transformers/models/layoutlmv3/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "c612d66c68d03f494cb337a432c7dabdf6c6d2ec",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 317,
            "deletions": 880,
            "changes": 1197,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "d0407638595d8a127cef02e5bf687b34663e9bfa",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 848,
            "changes": 848,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "b841f63c70b96c4010cf71cdc4fbe156daf665fb",
            "filename": "src/transformers/models/layoutxlm/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutxlm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutxlm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "09b9d1098c121c1362266a35920c9c513826c161",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 405,
            "deletions": 615,
            "changes": 1020,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "7b08a3aa5f0ef531235adb70e723ea26a321d4b3",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 814,
            "changes": 814,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "a9609d25e71c9c611e3169a67cc2a3c7a186472c",
            "filename": "src/transformers/models/led/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fled%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fled%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "d110ac30d969e1958a872e8bdc186df0822a25b5",
            "filename": "src/transformers/models/led/tokenization_led.py",
            "status": "removed",
            "additions": 0,
            "deletions": 454,
            "changes": 454,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "baea10f23516fef914d70fef87bd3d17bce19baa",
            "filename": "src/transformers/models/led/tokenization_led_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 322,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "8adb0f264faf3dd87d3141d9b9317106186e20a0",
            "filename": "src/transformers/models/lfm2_vl/processing_lfm2_vl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "8c15b58a0ae4ede69951aa1a304c93cdcd8feb28",
            "filename": "src/transformers/models/llama/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fllama%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fllama%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "ac5b7b4bae7fd80370030c8c69487485c9ed39e2",
            "filename": "src/transformers/models/llama/tokenization_llama.py",
            "status": "modified",
            "additions": 103,
            "deletions": 330,
            "changes": 433,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "212e65404e827e9d91a779e56cab5af7d2c97d34",
            "filename": "src/transformers/models/llama/tokenization_llama_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 251,
            "changes": 251,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "1db4be60ed44b41eec87a3f011433793b2e66b49",
            "filename": "src/transformers/models/longformer/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flongformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flongformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "104bdd7a9b99f5a4809557dcee97cda4873cea12",
            "filename": "src/transformers/models/longformer/tokenization_longformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 402,
            "changes": 402,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "bde6bb55fec609ae3550c5cf01b43f0dd3bc866c",
            "filename": "src/transformers/models/longformer/tokenization_longformer_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 265,
            "changes": 265,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "c1dd5be77b6f132b020131966d22f5f4e4542073",
            "filename": "src/transformers/models/luke/tokenization_luke.py",
            "status": "modified",
            "additions": 297,
            "deletions": 318,
            "changes": 615,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "e8370a12eaf67354e47ed5d734539e86c4b427b2",
            "filename": "src/transformers/models/lxmert/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flxmert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Flxmert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "dd1d7e205ea5588bec3f66307d5ae6c8412ee4a6",
            "filename": "src/transformers/models/lxmert/tokenization_lxmert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 482,
            "changes": 482,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "fcfa3263acdaaae8f59be7d33cea96f5f2a307dd",
            "filename": "src/transformers/models/lxmert/tokenization_lxmert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 143,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "5bbdd4575b7da676983d39d49bd01199224debe9",
            "filename": "src/transformers/models/m2m_100/tokenization_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fm2m_100%2Ftokenization_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fm2m_100%2Ftokenization_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Ftokenization_m2m_100.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "68f22fecffc71037e35bcd9ce755804cb6341da0",
            "filename": "src/transformers/models/marian/tokenization_marian.py",
            "status": "modified",
            "additions": 33,
            "deletions": 4,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmarian%2Ftokenization_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmarian%2Ftokenization_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Ftokenization_marian.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "ca084acef41b3140680ba900a304bae044112759",
            "filename": "src/transformers/models/markuplm/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmarkuplm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmarkuplm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "597008c3a97e02e636602bcb5809b2813db9f85a",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm.py",
            "status": "modified",
            "additions": 426,
            "deletions": 847,
            "changes": 1273,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "4033ef319ff8cbec11f6c29d075e59b741710513",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 929,
            "changes": 929,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "c55beabfc2cc5c255c09514279e15e8c6ef0c866",
            "filename": "src/transformers/models/mbart/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmbart%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmbart%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "d6753ee8fe41655f9325c84caadac2072c4f2a23",
            "filename": "src/transformers/models/mbart/tokenization_mbart.py",
            "status": "modified",
            "additions": 124,
            "deletions": 215,
            "changes": 339,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "7cf4d468c7df37524456a48960a7f0254cd8dd66",
            "filename": "src/transformers/models/mbart/tokenization_mbart_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 269,
            "changes": 269,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Ftokenization_mbart_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "e66676802277c6c929556955daac9d681842d375",
            "filename": "src/transformers/models/mbart50/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmbart50%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmbart50%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart50%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "6883bfb89b67be47c29a20b8a561a3e386680ddc",
            "filename": "src/transformers/models/mbart50/tokenization_mbart50.py",
            "status": "modified",
            "additions": 175,
            "deletions": 210,
            "changes": 385,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "985b0929f87c5516a2edd8e02c6aaaa8926d496e",
            "filename": "src/transformers/models/mbart50/tokenization_mbart50_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 258,
            "changes": 258,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart50%2Ftokenization_mbart50_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "af33ca785329fe67493ffda006ef47b5b5ed4968",
            "filename": "src/transformers/models/mgp_str/tokenization_mgp_str.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmgp_str%2Ftokenization_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmgp_str%2Ftokenization_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Ftokenization_mgp_str.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "14d7565409ccd4a08f8f6dfc8dca94662c8029f4",
            "filename": "src/transformers/models/mluke/tokenization_mluke.py",
            "status": "modified",
            "additions": 254,
            "deletions": 87,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "9548f0ea07ac2f7fa46e1e4d488372fa5a10488a",
            "filename": "src/transformers/models/mobilebert/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmobilebert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmobilebert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "6764442ce7740023152feced13f4594861870f15",
            "filename": "src/transformers/models/mobilebert/tokenization_mobilebert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 466,
            "changes": 474,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "0f97ddbbb7031c14aba617780973ba101d791307",
            "filename": "src/transformers/models/mobilebert/tokenization_mobilebert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 148,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "f76c1b3f538a1b31e633ac9e5fd8a28d9b6ddf73",
            "filename": "src/transformers/models/mpnet/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmpnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmpnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "0cd914aa3c9c919b5118904154cc0c3a0b2fc168",
            "filename": "src/transformers/models/mpnet/tokenization_mpnet.py",
            "status": "modified",
            "additions": 90,
            "deletions": 421,
            "changes": 511,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "1a470565a8452e765a509d0ffffb41dc6d6cd5e9",
            "filename": "src/transformers/models/mpnet/tokenization_mpnet_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 209,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "a3058816ff2032c41e2b0cdb6940705a7572faa0",
            "filename": "src/transformers/models/mt5/tokenization_mt5.py",
            "status": "removed",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmt5%2Ftokenization_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmt5%2Ftokenization_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Ftokenization_mt5.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "8737088cc44206bea1e2f9d1793ae49692ae35e8",
            "filename": "src/transformers/models/mt5/tokenization_mt5_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmt5%2Ftokenization_mt5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmt5%2Ftokenization_mt5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Ftokenization_mt5_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "4f0d850d308f6582e6e7620193bdc60da1f45403",
            "filename": "src/transformers/models/mvp/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmvp%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmvp%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "f6039df2dc02564378f6f83ef1770ded9000d31f",
            "filename": "src/transformers/models/mvp/tokenization_mvp.py",
            "status": "removed",
            "additions": 0,
            "deletions": 394,
            "changes": 394,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "ca0bc6b165f7687496999943d8e4293bd6458396",
            "filename": "src/transformers/models/mvp/tokenization_mvp_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 274,
            "changes": 274,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "2d8d0ef2761297fa3ba923ac6cfa01a223cc6414",
            "filename": "src/transformers/models/myt5/tokenization_myt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmyt5%2Ftokenization_myt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fmyt5%2Ftokenization_myt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmyt5%2Ftokenization_myt5.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "9fe4c4b7f9381425cf34bb6099c0d5d5f8417db6",
            "filename": "src/transformers/models/nllb/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fnllb%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fnllb%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "21e62cabfd6f74315a3c2d1bc07467fd0f81b9b3",
            "filename": "src/transformers/models/nllb/tokenization_nllb.py",
            "status": "modified",
            "additions": 151,
            "deletions": 219,
            "changes": 370,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "5300b3942b5d261e5e83be8c55f1a41da3427b09",
            "filename": "src/transformers/models/nllb/tokenization_nllb_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 327,
            "changes": 327,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb%2Ftokenization_nllb_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "0d44fcdb70ccf7700b86a5c0e25d87728417b564",
            "filename": "src/transformers/models/nougat/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fnougat%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fnougat%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "9bc66c1d01e21729445d1a3046a15098e4e165de",
            "filename": "src/transformers/models/nougat/tokenization_nougat.py",
            "status": "renamed",
            "additions": 130,
            "deletions": 22,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "previous_filename": "src/transformers/models/nougat/tokenization_nougat_fast.py"
        },
        {
            "sha": "16248677beaef3fd253b4946298eaa1aeb2ce7f7",
            "filename": "src/transformers/models/openai/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fopenai%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fopenai%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "3ad31681bd78b9d79f74ce28e6b188644efdd809",
            "filename": "src/transformers/models/openai/tokenization_openai.py",
            "status": "modified",
            "additions": 90,
            "deletions": 343,
            "changes": 433,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "83edf5eafa9468347c4b1c3b6bfab9d0e7759ec3",
            "filename": "src/transformers/models/openai/tokenization_openai_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "97eb286e1177825d289d5025b30adba3e3719c47",
            "filename": "src/transformers/models/parakeet/tokenization_parakeet_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fparakeet%2Ftokenization_parakeet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fparakeet%2Ftokenization_parakeet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Ftokenization_parakeet_fast.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "288776550986876700ee1702a34c11b388738d1f",
            "filename": "src/transformers/models/pegasus/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpegasus%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpegasus%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "cd6c7e6f5173a76e785d2dbf363936afac1ab3db",
            "filename": "src/transformers/models/pegasus/tokenization_pegasus.py",
            "status": "modified",
            "additions": 61,
            "deletions": 196,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "92a37c44ff2e302fe1e2a56849f2e91fa481ec9b",
            "filename": "src/transformers/models/pegasus/tokenization_pegasus_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 215,
            "changes": 215,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Ftokenization_pegasus_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "c5d387ca3a7a0da7f348605aaa8b004a5402b86c",
            "filename": "src/transformers/models/perceiver/tokenization_perceiver.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fperceiver%2Ftokenization_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fperceiver%2Ftokenization_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Ftokenization_perceiver.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "ec31f8e96c58e27b8fe4bed29dda5619206511c3",
            "filename": "src/transformers/models/phobert/tokenization_phobert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fphobert%2Ftokenization_phobert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fphobert%2Ftokenization_phobert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphobert%2Ftokenization_phobert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "1fe236339a7cd8bba3c32ab0a18336e7c564a2fa",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "b2cfa67ea2b8cf28c18042a7b19ac177e81f5eab",
            "filename": "src/transformers/models/plbart/tokenization_plbart.py",
            "status": "modified",
            "additions": 110,
            "deletions": 168,
            "changes": 278,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "bd3a2b8757e4269cbda4997c82e9045a66e71d4a",
            "filename": "src/transformers/models/pop2piano/processing_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "9d317fccfd1c673424400967117495fe7a6a01c3",
            "filename": "src/transformers/models/pop2piano/tokenization_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "50600632358352462a8485d66c831c46fa9f6fe4",
            "filename": "src/transformers/models/prophetnet/tokenization_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "48312e3cadf8e69a7ba387140f857d58486a8c9a",
            "filename": "src/transformers/models/qwen2/tokenization_qwen2.py",
            "status": "modified",
            "additions": 56,
            "deletions": 286,
            "changes": 342,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "dda8123c7d6b255e94c906024e644439ef3f2aea",
            "filename": "src/transformers/models/qwen2/tokenization_qwen2_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 137,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Ftokenization_qwen2_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "ab66f34f9dfd2db75226294d89a2730193bcac95",
            "filename": "src/transformers/models/rag/retrieval_rag.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "0db4f927f140da38ea852e2f73fcbc06842b1e8e",
            "filename": "src/transformers/models/reformer/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Freformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Freformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "021ab2a88074c11f954193f4183028e3dfdf822d",
            "filename": "src/transformers/models/reformer/tokenization_reformer.py",
            "status": "modified",
            "additions": 65,
            "deletions": 107,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "d68528de5872c24985b8cbbda69c9d63c026130d",
            "filename": "src/transformers/models/reformer/tokenization_reformer_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 114,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Ftokenization_reformer_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "371d23ef13a7ca20e9f3b6afa75cadad488e1fea",
            "filename": "src/transformers/models/rembert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Frembert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Frembert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "205356678a76ffbba89cfd145c05f335b74fb51d",
            "filename": "src/transformers/models/rembert/tokenization_rembert.py",
            "status": "modified",
            "additions": 108,
            "deletions": 157,
            "changes": 265,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "fb358746e6d2da1054142b1472efe50815f8abf8",
            "filename": "src/transformers/models/rembert/tokenization_rembert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 198,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Ftokenization_rembert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "2e8011675176b7e122895b6d6913065f89c2229e",
            "filename": "src/transformers/models/roberta/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froberta%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froberta%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "66eefc3a0caadbbdb0893250ad95af37ef34f541",
            "filename": "src/transformers/models/roberta/tokenization_roberta.py",
            "status": "modified",
            "additions": 70,
            "deletions": 282,
            "changes": 352,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "782c1a0311f2033515530d6db7959af8a62d0fc8",
            "filename": "src/transformers/models/roberta/tokenization_roberta_old.py",
            "status": "renamed",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta_old.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta_old.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta_old.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9",
            "previous_filename": "src/transformers/models/roberta/tokenization_roberta_fast.py"
        },
        {
            "sha": "d6099896150d198e553f785bc91fdb39e2fbbe53",
            "filename": "src/transformers/models/roc_bert/tokenization_roc_bert.py",
            "status": "modified",
            "additions": 247,
            "deletions": 14,
            "changes": 261,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "af44f4abea5e2ff1f7e537b05d70b797e9805c03",
            "filename": "src/transformers/models/roformer/tokenization_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "fb91a325419b901f2a486c066a0b1fdcd4f16b08",
            "filename": "src/transformers/models/roformer/tokenization_roformer_fast.py",
            "status": "modified",
            "additions": 21,
            "deletions": 12,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer_fast.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "27c08de501c141e44e7e101b79f228e826396ff7",
            "filename": "src/transformers/models/seamless_m4t/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "5388ec2b95d89961fa7643c84cb8e4ab04604a99",
            "filename": "src/transformers/models/seamless_m4t/tokenization_seamless_m4t.py",
            "status": "modified",
            "additions": 258,
            "deletions": 361,
            "changes": 619,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "081dcec7dd99153139f52ea42e694e43b5835ea8",
            "filename": "src/transformers/models/seamless_m4t/tokenization_seamless_m4t_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 446,
            "changes": 446,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Ftokenization_seamless_m4t_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "95007c864f110fdff431d05aaccfac5de83167cd",
            "filename": "src/transformers/models/siglip/tokenization_siglip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 33,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fsiglip%2Ftokenization_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fsiglip%2Ftokenization_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Ftokenization_siglip.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "f31a476102fe21fdca6e40deb200c02c92881984",
            "filename": "src/transformers/models/speech_to_text/tokenization_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Ftokenization_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Ftokenization_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Ftokenization_speech_to_text.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "cfc267486041278224ffc452f35b92d175bb4be8",
            "filename": "src/transformers/models/speecht5/convert_speecht5_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconvert_speecht5_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconvert_speecht5_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconvert_speecht5_original_pytorch_checkpoint_to_pytorch.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "6da12b3a3e6ba81fd27dae39e2b4e42a8ff5fda1",
            "filename": "src/transformers/models/speecht5/tokenization_speecht5.py",
            "status": "modified",
            "additions": 28,
            "deletions": 84,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fspeecht5%2Ftokenization_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fspeecht5%2Ftokenization_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Ftokenization_speecht5.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "757f063e6cee056f725757f2298b1a9f41e99907",
            "filename": "src/transformers/models/splinter/tokenization_splinter.py",
            "status": "modified",
            "additions": 106,
            "deletions": 417,
            "changes": 523,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "548d8e6c63b0dda90e66dc37be406fb45d68e2f8",
            "filename": "src/transformers/models/splinter/tokenization_splinter_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 193,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "9008fbeae8b089a2dffddacbc2d6995cb56407bc",
            "filename": "src/transformers/models/squeezebert/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fsqueezebert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fsqueezebert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "01f812a8e510e0a845170f84da90096ece444792",
            "filename": "src/transformers/models/squeezebert/tokenization_squeezebert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 464,
            "changes": 472,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "b6e460d43a6413a3971b22494af49ba8910730c6",
            "filename": "src/transformers/models/squeezebert/tokenization_squeezebert_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 147,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "5ba44f774dba82304e5dcbec0cbd6ee21fed1ef6",
            "filename": "src/transformers/models/t5/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ft5%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ft5%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "6b5b660d281a996281523de705ef498b1033f695",
            "filename": "src/transformers/models/t5/tokenization_t5.py",
            "status": "modified",
            "additions": 61,
            "deletions": 345,
            "changes": 406,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "bdba1a7928c8a9ef240aa6a6f694ca44e66e8c4a",
            "filename": "src/transformers/models/t5/tokenization_t5_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 233,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "df446826badbb4a30f2cc0b83ab2536c93cac30e",
            "filename": "src/transformers/models/tapas/tokenization_tapas.py",
            "status": "modified",
            "additions": 48,
            "deletions": 16,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "852ad5e3c81ad5979157b7783b21579b8dc22efb",
            "filename": "src/transformers/models/udop/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fudop%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fudop%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "f15bd28c69c6cdda106b3db86bf637f9c0bb29cf",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "1c13a43cce2629206f04d8d91a56a9df69ad51d0",
            "filename": "src/transformers/models/udop/tokenization_udop.py",
            "status": "modified",
            "additions": 390,
            "deletions": 827,
            "changes": 1217,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "9751f5d65ddf44928ffc14b97912249b7bbd9f95",
            "filename": "src/transformers/models/udop/tokenization_udop_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1026,
            "changes": 1026,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "3e78ed436349923073da61b0a99a9bad836ae2d0",
            "filename": "src/transformers/models/vits/tokenization_vits.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fvits%2Ftokenization_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fvits%2Ftokenization_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Ftokenization_vits.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "887794dd99b4e2f46049d591c0f92a4954e1f8a8",
            "filename": "src/transformers/models/voxtral/convert_voxtral_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "a6a28048de59f8ebcc29c0068a3857afe1f6019d",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "104cdf92a09909812ef8dc747ae75ac50403c597",
            "filename": "src/transformers/models/wav2vec2/tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 85,
            "deletions": 10,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "fc6daaeee71d043fd328affc836ad53a0f33ae73",
            "filename": "src/transformers/models/wav2vec2_bert/convert_wav2vec2_seamless_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "67eaf2239e6f11c29a03b691519723c3b30fcc94",
            "filename": "src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwav2vec2_phoneme%2Ftokenization_wav2vec2_phoneme.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwav2vec2_phoneme%2Ftokenization_wav2vec2_phoneme.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_phoneme%2Ftokenization_wav2vec2_phoneme.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "3d8f451a29632ceeda703c416830a85529bfa7e3",
            "filename": "src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "50aec31b3e9fdcf2de1daf47d278491ecee3b1f8",
            "filename": "src/transformers/models/whisper/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwhisper%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwhisper%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "889fab69af74b1b564e1f0703a3bccef5bd57f4a",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 385,
            "deletions": 345,
            "changes": 730,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "904f099243f916ef91e57d987f1285c1204efc23",
            "filename": "src/transformers/models/whisper/tokenization_whisper_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 617,
            "changes": 617,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "33e620386f469fc4f97bd52697254d69b8c54f3a",
            "filename": "src/transformers/models/xglm/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxglm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxglm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "e25ec6c16009e63cf156588d24fece8c75c522e4",
            "filename": "src/transformers/models/xglm/tokenization_xglm.py",
            "status": "modified",
            "additions": 68,
            "deletions": 236,
            "changes": 304,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "a9c8b3aac257abcd2e67bc2302b148bb943cb373",
            "filename": "src/transformers/models/xglm/tokenization_xglm_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 192,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Ftokenization_xglm_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "9616995fd0dc56d2b2addb6d3f1624d6d50d37a7",
            "filename": "src/transformers/models/xlm/tokenization_xlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxlm%2Ftokenization_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxlm%2Ftokenization_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Ftokenization_xlm.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "f06156f5556a8de9c0a0b288716c34ac327acc6d",
            "filename": "src/transformers/models/xlm_roberta/tokenization_xlm_roberta.py",
            "status": "modified",
            "additions": 75,
            "deletions": 247,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "bcdea2325fc1ed14bd1b152f06ed619582e5e65c",
            "filename": "src/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 194,
            "changes": 194,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Ftokenization_xlm_roberta_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "5734b98957c6e6424f41f839592fbf9c21bfc40f",
            "filename": "src/transformers/models/xlnet/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxlnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxlnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "7b8ffdbb24ea696f68cae1150ab15cc7fa9c3bcc",
            "filename": "src/transformers/models/xlnet/tokenization_xlnet.py",
            "status": "modified",
            "additions": 72,
            "deletions": 265,
            "changes": 337,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "56cd2a50e1b26552d60e4e41a86cd5780a766363",
            "filename": "src/transformers/models/xlnet/tokenization_xlnet_fast.py",
            "status": "removed",
            "additions": 0,
            "deletions": 230,
            "changes": 230,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c51596ec56407eabc7f8d1f021a8aeb679a330/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Ftokenization_xlnet_fast.py?ref=01c51596ec56407eabc7f8d1f021a8aeb679a330"
        },
        {
            "sha": "d13dd9bf891df7fcdad21f4c02eaf2981ddd20a5",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "a72c593a7c3fd4729b8711ab64424ecacf3255f5",
            "filename": "src/transformers/pipelines/any_to_any.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fany_to_any.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fany_to_any.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fany_to_any.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "f09c529072f8a64c220d80165665406f09b81b9f",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "76db996925feedb8782a9d3de965332710384249",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "d1b4c4855f25dd1d52db357728148b6a8986cb1f",
            "filename": "src/transformers/pipelines/deprecated/text2text_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fdeprecated%2Ftext2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fdeprecated%2Ftext2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdeprecated%2Ftext2text_generation.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "146465c28ffe92d17061f133eedf5b936650ba8f",
            "filename": "src/transformers/pipelines/question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "1e7b2bec2c0fcb484590a79b923d8ec26b29fc46",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "2f9df70e6a146ce722b71a715da06a00efe6682e",
            "filename": "src/transformers/pipelines/zero_shot_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        },
        {
            "sha": "ca15054f5e661e4bc2c936f548f6dfc3472387cf",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 40,
            "deletions": 6,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05c0e1d39082ee8b69064ed4ea9c239cd17405e9/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=05c0e1d39082ee8b69064ed4ea9c239cd17405e9"
        }
    ],
    "stats": {
        "total": 72305,
        "additions": 16908,
        "deletions": 55397
    }
}