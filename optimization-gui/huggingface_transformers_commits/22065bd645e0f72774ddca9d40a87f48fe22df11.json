{
    "author": "Cyrilvallez",
    "message": "fix derived berts `_init_weights` (#37341)\n\n* fix derived berts\n\n* more\n\n* roformer",
    "sha": "22065bd645e0f72774ddca9d40a87f48fe22df11",
    "files": [
        {
            "sha": "68abb317ee3f7ac8c16963fd87685802096c0d13",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -579,6 +579,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, AlbertMLMHead):\n+            module.bias.data.zero_()\n \n \n @dataclass"
        },
        {
            "sha": "75fd1b17168e10740cb2e90fc8357cedf12cfe8a",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -608,6 +608,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, BertGenerationOnlyLMHead):\n+            module.bias.data.zero_()\n \n \n BERT_GENERATION_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "abc5a1df440968ef24029bbdd5a83fdbfbba4749",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -1769,6 +1769,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, BigBirdLMPredictionHead):\n+            module.bias.data.zero_()\n \n \n BIG_BIRD_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "13efb4839a897c3c82183d94d0c3fcd0efbdc21c",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -242,7 +242,7 @@ class ConvBertPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n@@ -255,6 +255,11 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, SeparableConv1D):\n+            module.bias.data.zero_()\n+        elif isinstance(module, GroupedLinearLayer):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            module.bias.data.zero_()\n \n \n class SeparableConv1D(nn.Module):"
        },
        {
            "sha": "c574460513b956576edef6813c16d35207d980c5",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -539,11 +539,11 @@ def forward(\n             return (layer_output, None)\n \n \n-class DebertaEncoder(PreTrainedModel):\n+class DebertaEncoder(nn.Module):\n     \"\"\"Modified BertEncoder with relative position bias support\"\"\"\n \n     def __init__(self, config):\n-        super().__init__(config)\n+        super().__init__()\n         self.layer = nn.ModuleList([DebertaLayer(config) for _ in range(config.num_hidden_layers)])\n         self.relative_attention = getattr(config, \"relative_attention\", False)\n         if self.relative_attention:\n@@ -655,6 +655,14 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, (nn.LayerNorm, DebertaLayerNorm)):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, DisentangledSelfAttention):\n+            module.q_bias.data.zero_()\n+            module.v_bias.data.zero_()\n+        elif isinstance(module, (LegacyDebertaLMPredictionHead, DebertaLMPredictionHead)):\n+            module.bias.data.zero_()\n \n \n DEBERTA_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "576c4ce8799d9c26db5de749139f2a5c3b0725ef",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -703,7 +703,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.deberta.modeling_deberta.DebertaPreTrainedModel with Deberta->DebertaV2\n class DebertaV2PreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n@@ -727,6 +726,11 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, (LegacyDebertaV2LMPredictionHead, DebertaV2LMPredictionHead)):\n+            module.bias.data.zero_()\n \n \n DEBERTA_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "143ec53d54c1912b282114e3e791094f40c1dac3",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -955,7 +955,9 @@ def _init_weights(self, module):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Conv1d):\n@@ -971,8 +973,14 @@ def _init_weights(self, module):\n             else:\n                 nn.init.kaiming_normal_(module.weight.data)\n \n-        if isinstance(module, (nn.Linear, nn.Conv1d)) and module.bias is not None:\n-            module.bias.data.zero_()\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, HubertModel):\n+            if hasattr(module, \"masked_spec_embed\"):\n+                module.masked_spec_embed.data.uniform_()\n+        elif isinstance(module, HubertForSequenceClassification):\n+            if hasattr(module, \"layer_weights\"):\n+                module.layer_weights.data.fill_(1.0 / (self.config.num_hidden_layers + 1))\n \n     def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n         \"\"\""
        },
        {
            "sha": "b5138fdfb57e04dd9286dc7fec5437e8a59d5e10",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -147,7 +147,9 @@ def _init_weights(self, module):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Conv1d):\n@@ -163,8 +165,14 @@ def _init_weights(self, module):\n             else:\n                 nn.init.kaiming_normal_(module.weight.data)\n \n-        if isinstance(module, (nn.Linear, nn.Conv1d)) and module.bias is not None:\n-            module.bias.data.zero_()\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, HubertModel):\n+            if hasattr(module, \"masked_spec_embed\"):\n+                module.masked_spec_embed.data.uniform_()\n+        elif isinstance(module, HubertForSequenceClassification):\n+            if hasattr(module, \"layer_weights\"):\n+                module.layer_weights.data.fill_(1.0 / (self.config.num_hidden_layers + 1))\n \n     def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n         \"\"\""
        },
        {
            "sha": "1a922b1e297421cf42746967984b9e392852694b",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -651,6 +651,8 @@ def _init_weights(self, module):\n         elif isinstance(module, (IntLayerNorm, nn.LayerNorm)):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, IBertLMHead):\n+            module.bias.data.zero_()\n \n     def resize_token_embeddings(self, new_num_tokens=None):\n         raise NotImplementedError(\"`resize_token_embeddings` is not supported for I-BERT.\")"
        },
        {
            "sha": "35ae4a65164840ca4dbf93ba46fedfd05a85af5e",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -715,10 +715,12 @@ def _init_weights(self, module):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                module.bias.data.zero_()\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n-        if isinstance(module, nn.Linear) and module.bias is not None:\n+        elif isinstance(module, MegatronBertLMPredictionHead):\n             module.bias.data.zero_()\n \n "
        },
        {
            "sha": "b1d269055e65786c0d70d55eea98724951898e3c",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -704,6 +704,8 @@ def _init_weights(self, module):\n         elif isinstance(module, (nn.LayerNorm, NoNorm)):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, MobileBertLMPredictionHead):\n+            module.bias.data.zero_()\n \n \n @dataclass"
        },
        {
            "sha": "accc07f6c35bce53c435690d29018434f6553d18",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -634,6 +634,10 @@ def init_weight(module: nn.Module, std: float):\n             (ModernBertForSequenceClassification, ModernBertForTokenClassification, ModernBertForQuestionAnswering),\n         ):\n             init_weight(module.classifier, stds[\"final_out\"])\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n \n     @classmethod\n     def _autoset_attn_implementation("
        },
        {
            "sha": "f687324c5af56bf88f5ffffbf0f91cc4c78874d2",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -836,6 +836,10 @@ def init_weight(module: nn.Module, std: float):\n             (ModernBertForSequenceClassification, ModernBertForTokenClassification, ModernBertForQuestionAnswering),\n         ):\n             init_weight(module.classifier, stds[\"final_out\"])\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n \n     @classmethod\n     def _autoset_attn_implementation("
        },
        {
            "sha": "848f1c1703b8fd0f40eb2854c6ab5c634397bdcf",
            "filename": "src/transformers/models/mpnet/modeling_mpnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -63,6 +63,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, MPNetLMHead):\n+            module.bias.data.zero_()\n \n \n class MPNetEmbeddings(nn.Module):"
        },
        {
            "sha": "d13304405ab2b93a63bcb5dd4618053194ba36a0",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -272,9 +272,13 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n+        elif isinstance(module, nn.GroupNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, PoolFormerLayer):\n+            if hasattr(module, \"layer_scale_1\"):\n+                module.layer_scale_1.data.fill_(self.config.layer_scale_init_value)\n+                module.layer_scale_2.data.fill_(self.config.layer_scale_init_value)\n \n \n POOLFORMER_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "b5ca264fb73d3e28b576aa519a0c0d330baed152",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -794,6 +794,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, RoCBertLMPredictionHead):\n+            module.bias.data.zero_()\n \n \n ROC_BERT_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "7814472bb48f1c7ec7bb9eb6f23543bfd03d9966",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -699,6 +699,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, RoFormerLMPredictionHead):\n+            module.bias.data.zero_()\n \n \n ROFORMER_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "efb29035ba94233b853041d12371087603a728a8",
            "filename": "src/transformers/models/squeezebert/modeling_squeezebert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -436,9 +436,11 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, SqueezeBertLayerNorm):\n+        elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+        elif isinstance(module, SqueezeBertLMPredictionHead):\n+            module.bias.data.zero_()\n \n \n SQUEEZEBERT_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "2d81258a89c4118412f43d88fb8cf49ad67a9de6",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -523,11 +523,12 @@ def _init_weights(self, module):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                module.bias.data.zero_()\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n-        if isinstance(module, nn.Linear) and module.bias is not None:\n+        elif isinstance(module, VisualBertLMPredictionHead):\n             module.bias.data.zero_()\n \n "
        },
        {
            "sha": "d3d393f9baa146a5fd6e17a0b30d09c65d742d0d",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -734,7 +734,6 @@ class Wav2Vec2BertPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_features\"\n     supports_gradient_checkpointing = True\n \n-    # Ignore copy\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Wav2Vec2BertSelfAttention):\n@@ -760,6 +759,17 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 nn.init.uniform_(module.bias, a=-k, b=k)\n+        elif isinstance(module, Wav2Vec2BertModel):\n+            if hasattr(module, \"masked_spec_embed\"):\n+                module.masked_spec_embed.data.uniform_()\n+        elif isinstance(\n+            module,\n+            (Wav2Vec2BertForSequenceClassification, Wav2Vec2BertForAudioFrameClassification, Wav2Vec2BertForXVector),\n+        ):\n+            if hasattr(module, \"layer_weights\"):\n+                module.layer_weights.data.fill_(1.0 / (self.config.num_hidden_layers + 1))\n+        elif isinstance(module, AMSoftmaxLoss):  # noqa: F821\n+            module.weight.data.normal_()\n \n     # Ignore copy\n     def _get_feat_extract_output_lengths("
        },
        {
            "sha": "cc1df08aa23809048fd4a2a9d59f3dcdf42f1fe5",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22065bd645e0f72774ddca9d40a87f48fe22df11/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=22065bd645e0f72774ddca9d40a87f48fe22df11",
            "patch": "@@ -609,7 +609,6 @@ class Wav2Vec2BertPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_features\"\n     supports_gradient_checkpointing = True\n \n-    # Ignore copy\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Wav2Vec2BertSelfAttention):\n@@ -635,6 +634,17 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 nn.init.uniform_(module.bias, a=-k, b=k)\n+        elif isinstance(module, Wav2Vec2BertModel):\n+            if hasattr(module, \"masked_spec_embed\"):\n+                module.masked_spec_embed.data.uniform_()\n+        elif isinstance(\n+            module,\n+            (Wav2Vec2BertForSequenceClassification, Wav2Vec2BertForAudioFrameClassification, Wav2Vec2BertForXVector),\n+        ):\n+            if hasattr(module, \"layer_weights\"):\n+                module.layer_weights.data.fill_(1.0 / (self.config.num_hidden_layers + 1))\n+        elif isinstance(module, AMSoftmaxLoss):  # noqa: F821\n+            module.weight.data.normal_()\n \n     # Ignore copy\n     def _get_feat_extract_output_lengths("
        }
    ],
    "stats": {
        "total": 120,
        "additions": 103,
        "deletions": 17
    }
}