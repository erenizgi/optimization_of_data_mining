{
    "author": "vasqu",
    "message": "[`CI`] Fix copies (#42571)\n\n* fix\n\n* fix circular condition",
    "sha": "a5c061d24e31706c18670c0ae6579b5c72d545f3",
    "files": [
        {
            "sha": "577dd0653690c70cc6c4efbb5e6e3e2c1162e3a5",
            "filename": "src/transformers/models/fast_vlm/modeling_fast_vlm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 11,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5c061d24e31706c18670c0ae6579b5c72d545f3/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodeling_fast_vlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5c061d24e31706c18670c0ae6579b5c72d545f3/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodeling_fast_vlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodeling_fast_vlm.py?ref=a5c061d24e31706c18670c0ae6579b5c72d545f3",
            "patch": "@@ -59,7 +59,7 @@ def forward(self, image_features):\n class FastVlmPreTrainedModel(PreTrainedModel):\n     config: FastVlmConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n \n@@ -195,12 +195,11 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, FastVlmModelOutputWithPast]:\n         r\"\"\"\n-        vision_feature_select_strategy (`str`, *optional*):\n-            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n-\n         vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n             The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n             corresponding indices will be concatenated to form the vision features. Only -1 supported.\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n         \"\"\"\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n@@ -335,18 +334,16 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, FastVlmCausalLMOutputWithPast]:\n         r\"\"\"\n+        vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n+            The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n+            corresponding indices will be concatenated to form the vision features. Only -1 supported.\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-        vision_feature_select_strategy (`str`, *optional*):\n-            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n-\n-        vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n-            The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n-            corresponding indices will be concatenated to form the vision features. Only -1 supported.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "516a40bd48dce7061a84718f7b16060f0691de1e",
            "filename": "src/transformers/models/fast_vlm/modular_fast_vlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5c061d24e31706c18670c0ae6579b5c72d545f3/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodular_fast_vlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5c061d24e31706c18670c0ae6579b5c72d545f3/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodular_fast_vlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodular_fast_vlm.py?ref=a5c061d24e31706c18670c0ae6579b5c72d545f3",
            "patch": "@@ -204,12 +204,11 @@ def get_image_features(\n \n     def forward(self, **super_kwargs):\n         r\"\"\"\n-        vision_feature_select_strategy (`str`, *optional*):\n-            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n-\n         vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n             The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n             corresponding indices will be concatenated to form the vision features. Only -1 supported.\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n         \"\"\"\n         super().forward(**super_kwargs)\n \n@@ -224,18 +223,16 @@ class FastVlmForConditionalGeneration(LlavaForConditionalGeneration):\n \n     def forward(self, **super_kwargs):\n         r\"\"\"\n+        vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n+            The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n+            corresponding indices will be concatenated to form the vision features. Only -1 supported.\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-        vision_feature_select_strategy (`str`, *optional*):\n-            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n-\n-        vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n-            The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n-            corresponding indices will be concatenated to form the vision features. Only -1 supported.\n-\n         Example:\n \n         ```python"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 15,
        "deletions": 21
    }
}