{
    "author": "remi-or",
    "message": "Bump AMD docker (#41792)",
    "sha": "ff04520266646c717358ac117a30bf969f6ce589",
    "files": [
        {
            "sha": "ac5ec559516a53f3acf7366208807faa4061195c",
            "filename": "docker/transformers-pytorch-amd-gpu/Dockerfile",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff04520266646c717358ac117a30bf969f6ce589/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff04520266646c717358ac117a30bf969f6ce589/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile?ref=ff04520266646c717358ac117a30bf969f6ce589",
            "patch": "@@ -1,4 +1,4 @@\n-FROM rocm/pytorch:rocm6.4.1_ubuntu24.04_py3.12_pytorch_release_2.7.1\n+FROM rocm/pytorch:rocm7.0.2_ubuntu24.04_py3.12_pytorch_release_2.7.1\n LABEL maintainer=\"Hugging Face\"\n \n ARG DEBIAN_FRONTEND=noninteractive\n@@ -10,8 +10,8 @@ RUN apt update && \\\n \n RUN git lfs install\n \n-RUN python3 -m pip install --no-cache-dir --upgrade pip numpy\n-RUN python3 -m pip install --no-cache-dir --upgrade importlib-metadata setuptools ninja git+https://github.com/facebookresearch/detectron2.git pytesseract \"itsdangerous<2.1.0\"\n+RUN python3 -m pip install --no-cache-dir --upgrade pip numpy importlib-metadata setuptools wheel ninja pytesseract \"itsdangerous<2.1.0\"\n+RUN python3 -m pip install --no-cache-dir --no-build-isolation git+https://github.com/facebookresearch/detectron2.git\n \n ARG REF=main\n WORKDIR /\n@@ -39,6 +39,7 @@ RUN python3 -m pip install --no-cache-dir \"torchcodec==0.5\"\n # Install flash attention from source. Tested with commit 6387433156558135a998d5568a9d74c1778666d8\n RUN git clone https://github.com/ROCm/flash-attention/ -b tridao && \\\n     cd flash-attention && \\\n-    GPU_ARCHS=\"gfx942\" python setup.py install\n+    GPU_ARCHS=\"gfx942;gfx950\" python setup.py install  \n+# GPU_ARCHS builds for MI300, MI325 and MI355\n \n RUN python3 -m pip install --no-cache-dir einops"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}