{
    "author": "Cyrilvallez",
    "message": "Add Phi4 multimodal (#36939)\n\n* raw start\n\n* update\n\n* update\n\n* add to imports\n\n* update\n\n* up\n\n* simplify configs\n\n* clean configs\n\n* style\n\n* typos\n\n* Update convert_phi4_multimodal_weights_to_hf.py\n\n* Update convert_phi4_multimodal_weights_to_hf.py\n\n* fix\n\n* up\n\n* up\n\n* up\n\n* Update convert_phi4_multimodal_weights_to_hf.py\n\n* Update convert_phi4_multimodal_weights_to_hf.py\n\n* up\n\n* up\n\n* up\n\n* Update feature_extraction_phi4_multimodal.py\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* simplify configs\n\n* typo\n\n* cut code\n\n* typo\n\n* typo\n\n* typo\n\n* re\n\n* typo\n\n* up\n\n* up\n\n* up\n\n* add tests\n\n* fix\n\n* fix\n\n* Update test_modeling_phi4_multimodal.py\n\n* up\n\n* Update test_modeling_phi4_multimodal.py\n\n* doc\n\n* fix\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* up\n\n* simplify\n\n* up\n\n* simplify\n\n* config docstrings\n\n* cleanup\n\n* clean\n\n* typo\n\n* typo\n\n* fix\n\n* Update phi4_multimodal.md\n\n* fix\n\n* fix\n\n* Update test_modeling_phi4_multimodal.py\n\n* update\n\n* simplify reshapes and permutes\n\n* up\n\n* simplify special tokens\n\n* simplify processor a lot\n\n* Update processing_phi4_multimodal.py\n\n* Update processing_phi4_multimodal.py\n\n* switch to fast processor\n\n* image processor\n\n* Update image_processing_phi4_multimodal_fast.py\n\n* add lora extraction to converter\n\n* Update convert_phi4_multimodal_weights_to_hf.py\n\n* Update __init__.py\n\n* add AudioInput type in audio_utils\n\n* rewrite feature_extraction: support torch batched FFT\n\n* input_audio_embeds -> audio_input_features, input_image_embeds -> image_pixel_values\n\n* test update\n\n* not mono channel warning update\n\n* remove auto maps from processor\n\n* kargs dispatch in processor\n\n* simplify kwargs dispatch\n\n* simplify merging\n\n* remove default sampling rate\n\n* style\n\n* Update test_modeling_phi4_multimodal.py\n\n* update doc\n\n* doc\n\n* torch only feature extractor\n\n* make fake tokens adjustable\n\n* Update feature_extraction_phi4_multimodal.py\n\n* fix\n\n* Update processing_phi4_multimodal.py\n\n* simplify mask\n\n* last touch\n\n* fix copies\n\n* style\n\n* Update audio_utils.py\n\n* style\n\n* Update feature_extraction_phi4_multimodal.py\n\n* Update __init__.py\n\n* docstrings\n\n* copies\n\n* fix all checks\n\n* back to fix-copies\n\n* trigger CIs\n\n* Update feature_extraction_phi4_multimodal.py\n\n* improve tests with multimodal inputs\n\n* trigger CIs\n\n---------\n\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>",
    "sha": "4303d88c097d39e138f47a7946e46943d99bdfdf",
    "files": [
        {
            "sha": "bcd054113c2804a7249b6021dd39af4a855d144e",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -583,6 +583,8 @@\n         title: Phi\n       - local: model_doc/phi3\n         title: Phi-3\n+      - local: model_doc/phi4_multimodal\n+        title: Phi4 Multimodal\n       - local: model_doc/phimoe\n         title: PhiMoE\n       - local: model_doc/phobert"
        },
        {
            "sha": "f0d8bb3b4639cbe986f8ca4bf506366f0fceff6e",
            "filename": "docs/source/en/model_doc/phi4_multimodal.md",
            "status": "added",
            "additions": 149,
            "deletions": 0,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -0,0 +1,149 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+-->\n+\n+# Phi4 Multimodal\n+\n+## Overview\n+\n+Phi4 Multimodal is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, generating text outputs, and comes with 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning, direct preference optimization and RLHF (Reinforcement Learning from Human Feedback) to support precise instruction adherence and safety measures. The languages that each modal supports are the following:\n+\n+- Text: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian\n+- Vision: English\n+- Audio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese\n+\n+This model was contributed by [Cyril Vallez](https://huggingface.co/cyrilvallez). The most recent code can be\n+found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py).\n+\n+\n+## Usage tips\n+\n+`Phi4-multimodal-instruct` can be found on the [Huggingface Hub](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)\n+\n+In the following, we demonstrate how to use it for inference depending on the input modalities (text, image, audio).\n+\n+```python\n+import requests\n+import torch\n+import os\n+import io\n+from PIL import Image\n+import soundfile as sf\n+from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n+from urllib.request import urlopen\n+\n+\n+# Define model path\n+model_path = \"microsoft/Phi-4-multimodal-instruct\"\n+device = \"cuda:0\"\n+\n+# Load model and processor\n+processor = AutoProcessor.from_pretrained(model_path)\n+model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,  torch_dtype=torch.float16)\n+\n+# Optional: load the adapters (note that without them, the base model will very likely not work well)\n+model.load_adapter(model_path, adapter_name=\"speech\", device_map=device, adapter_kwargs={\"subfolder\": 'speech-lora'})\n+model.load_adapter(model_path, adapter_name=\"vision\", device_map=device, adapter_kwargs={\"subfolder\": 'vision-lora'})\n+\n+# Define prompt structure\n+user_prompt = '<|user|>'\n+assistant_prompt = '<|assistant|>'\n+prompt_suffix = '<|end|>'\n+\n+# Part 1: Image Processing\n+model.set_adapter(\"vision\") # if loaded, activate the vision adapter\n+print(\"\\n--- IMAGE PROCESSING ---\")\n+image_url = 'https://www.ilankelman.org/stopsigns/australia.jpg'\n+prompt = f'{user_prompt}<|image_1|>What is shown in this image?{prompt_suffix}{assistant_prompt}'\n+print(f'>>> Prompt\\n{prompt}')\n+\n+# Download and open image\n+image = Image.open(requests.get(image_url, stream=True).raw)\n+inputs = processor(text=prompt, images=image, return_tensors='pt').to(device)\n+\n+# Generate response\n+generate_ids = model.generate(\n+    **inputs,\n+    max_new_tokens=1000,\n+    do_sample=False,\n+)\n+generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n+response = processor.batch_decode(\n+    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)[0]\n+print(f'>>> Response\\n{response}')\n+\n+# Part 2: Audio Processing\n+model.set_adapter(\"speech\") # if loaded, activate the speech adapter\n+print(\"\\n--- AUDIO PROCESSING ---\")\n+audio_url = \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac\"\n+speech_prompt = \"Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.\"\n+prompt = f'{user_prompt}<|audio_1|>{speech_prompt}{prompt_suffix}{assistant_prompt}'\n+print(f'>>> Prompt\\n{prompt}')\n+\n+# Downlowd and open audio file\n+audio, sample_rate = sf.read(io.BytesIO(urlopen(audio_url).read()))\n+\n+# Process with the model\n+inputs = processor(text=prompt, audios=audio, sample_rate=sample_rate, return_tensors='pt').to(device)\n+\n+generate_ids = model.generate(\n+    **inputs,\n+    max_new_tokens=1000,\n+    do_sample=False,\n+)\n+generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n+response = processor.batch_decode(\n+    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)[0]\n+print(f'>>> Response\\n{response}')\n+```\n+\n+## Phi4MultimodalFeatureExtractor\n+\n+[[autodoc]] Phi4MultimodalFeatureExtractor\n+\n+## Phi4MultimodalImageProcessorFast\n+\n+[[autodoc]] Phi4MultimodalImageProcessorFast\n+\n+## Phi4MultimodalProcessor\n+\n+[[autodoc]] Phi4MultimodalProcessor\n+\n+## Phi4MultimodalAudioConfig\n+\n+[[autodoc]] Phi4MultimodalAudioConfig\n+\n+## Phi4MultimodalVisionConfig\n+\n+[[autodoc]] Phi4MultimodalVisionConfig\n+\n+## Phi4MultimodalConfig\n+\n+[[autodoc]] Phi4MultimodalConfig\n+\n+## Phi4MultimodalAudioModel\n+\n+[[autodoc]] Phi4MultimodalAudioModel\n+\n+## Phi4MultimodalVisionModel\n+\n+[[autodoc]] Phi4MultimodalVisionModel\n+\n+## Phi4MultimodalModel\n+\n+[[autodoc]] Phi4MultimodalModel\n+    - forward\n+\n+## Phi4MultimodalForCausalLM\n+\n+[[autodoc]] Phi4MultimodalForCausalLM\n+    - forward"
        },
        {
            "sha": "e8da536747d4f34216a2adff48205e4826b3eb82",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -699,6 +699,13 @@\n     \"models.persimmon\": [\"PersimmonConfig\"],\n     \"models.phi\": [\"PhiConfig\"],\n     \"models.phi3\": [\"Phi3Config\"],\n+    \"models.phi4_multimodal\": [\n+        \"Phi4MultimodalAudioConfig\",\n+        \"Phi4MultimodalConfig\",\n+        \"Phi4MultimodalFeatureExtractor\",\n+        \"Phi4MultimodalProcessor\",\n+        \"Phi4MultimodalVisionConfig\",\n+    ],\n     \"models.phimoe\": [\"PhimoeConfig\"],\n     \"models.phobert\": [\"PhobertTokenizer\"],\n     \"models.pix2struct\": [\n@@ -1348,6 +1355,7 @@\n     _import_structure[\"models.llava\"].append(\"LlavaImageProcessorFast\")\n     _import_structure[\"models.llava_next\"].append(\"LlavaNextImageProcessorFast\")\n     _import_structure[\"models.llava_onevision\"].append(\"LlavaOnevisionImageProcessorFast\")\n+    _import_structure[\"models.phi4_multimodal\"].append(\"Phi4MultimodalImageProcessorFast\")\n     _import_structure[\"models.pixtral\"].append(\"PixtralImageProcessorFast\")\n     _import_structure[\"models.qwen2_vl\"].append(\"Qwen2VLImageProcessorFast\")\n     _import_structure[\"models.rt_detr\"].append(\"RTDetrImageProcessorFast\")\n@@ -2802,6 +2810,17 @@\n             \"LlavaNextPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.phi4_multimodal\"].extend(\n+        [\n+            \"Phi4MultimodalForCausalLM\",\n+            \"Phi4MultimodalPreTrainedModel\",\n+            \"Phi4MultimodalAudioModel\",\n+            \"Phi4MultimodalAudioPreTrainedModel\",\n+            \"Phi4MultimodalModel\",\n+            \"Phi4MultimodalVisionModel\",\n+            \"Phi4MultimodalVisionPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.llava_next_video\"].extend(\n         [\n             \"LlavaNextVideoForConditionalGeneration\",\n@@ -5914,6 +5933,13 @@\n     )\n     from .models.phi import PhiConfig\n     from .models.phi3 import Phi3Config\n+    from .models.phi4_multimodal import (\n+        Phi4MultimodalAudioConfig,\n+        Phi4MultimodalConfig,\n+        Phi4MultimodalFeatureExtractor,\n+        Phi4MultimodalProcessor,\n+        Phi4MultimodalVisionConfig,\n+    )\n     from .models.phimoe import PhimoeConfig\n     from .models.phobert import PhobertTokenizer\n     from .models.pix2struct import (\n@@ -6587,6 +6613,7 @@\n         from .models.llava import LlavaImageProcessorFast\n         from .models.llava_next import LlavaNextImageProcessorFast\n         from .models.llava_onevision import LlavaOnevisionImageProcessorFast\n+        from .models.phi4_multimodal import Phi4MultimodalImageProcessorFast\n         from .models.pixtral import PixtralImageProcessorFast\n         from .models.qwen2_vl import Qwen2VLImageProcessorFast\n         from .models.rt_detr import RTDetrImageProcessorFast\n@@ -8153,6 +8180,15 @@\n             Phi3Model,\n             Phi3PreTrainedModel,\n         )\n+        from .models.phi4_multimodal import (\n+            Phi4MultimodalAudioModel,\n+            Phi4MultimodalAudioPreTrainedModel,\n+            Phi4MultimodalForCausalLM,\n+            Phi4MultimodalModel,\n+            Phi4MultimodalPreTrainedModel,\n+            Phi4MultimodalVisionModel,\n+            Phi4MultimodalVisionPreTrainedModel,\n+        )\n         from .models.phimoe import (\n             PhimoeForCausalLM,\n             PhimoeForSequenceClassification,"
        },
        {
            "sha": "5795b5d9bd1a73ace70214c1b9a1eb01e931a6b5",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -17,11 +17,16 @@\n \"\"\"\n \n import warnings\n-from typing import Optional, Union\n+from typing import List, Optional, Tuple, Union\n \n import numpy as np\n \n \n+AudioInput = Union[\n+    np.ndarray, \"torch.Tensor\", List[np.ndarray], Tuple[np.ndarray], List[\"torch.Tensor\"], Tuple[\"torch.Tensor\"]  # noqa: F821\n+]\n+\n+\n def hertz_to_mel(freq: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Union[float, np.ndarray]:\n     \"\"\"\n     Convert frequency from hertz to mels."
        },
        {
            "sha": "49ce48dd6c042e251c1014d8be9a976f0398e635",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -212,6 +212,7 @@\n     persimmon,\n     phi,\n     phi3,\n+    phi4_multimodal,\n     phimoe,\n     phobert,\n     pix2struct,"
        },
        {
            "sha": "c7ef472882ba058fc731d7caa762b789349c5335",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -235,6 +235,7 @@\n         (\"persimmon\", \"PersimmonConfig\"),\n         (\"phi\", \"PhiConfig\"),\n         (\"phi3\", \"Phi3Config\"),\n+        (\"phi4_multimodal\", \"Phi4MultimodalConfig\"),\n         (\"phimoe\", \"PhimoeConfig\"),\n         (\"pix2struct\", \"Pix2StructConfig\"),\n         (\"pixtral\", \"PixtralVisionConfig\"),\n@@ -587,6 +588,7 @@\n         (\"persimmon\", \"Persimmon\"),\n         (\"phi\", \"Phi\"),\n         (\"phi3\", \"Phi3\"),\n+        (\"phi4_multimodal\", \"Phi4Multimodal\"),\n         (\"phimoe\", \"Phimoe\"),\n         (\"phobert\", \"PhoBERT\"),\n         (\"pix2struct\", \"Pix2Struct\"),"
        },
        {
            "sha": "0b8b38bc347cb104b1593906debaa9fe2970bdf3",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -78,6 +78,7 @@\n         (\"nat\", \"ViTFeatureExtractor\"),\n         (\"owlvit\", \"OwlViTFeatureExtractor\"),\n         (\"perceiver\", \"PerceiverFeatureExtractor\"),\n+        (\"phi4_multimodal\", \"Phi4MultimodalFeatureExtractor\"),\n         (\"poolformer\", \"PoolFormerFeatureExtractor\"),\n         (\"pop2piano\", \"Pop2PianoFeatureExtractor\"),\n         (\"regnet\", \"ConvNextFeatureExtractor\"),"
        },
        {
            "sha": "77b9734189b398ddf4bc8440736075ed6f893287",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -124,6 +124,7 @@\n             (\"owlvit\", (\"OwlViTImageProcessor\",)),\n             (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"perceiver\", (\"PerceiverImageProcessor\",)),\n+            (\"phi4_multimodal\", \"Phi4MultimodalImageProcessorFast\"),\n             (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n             (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"poolformer\", (\"PoolFormerImageProcessor\",)),"
        },
        {
            "sha": "05a415741413c6198924151b8346ecdf4ab81780",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -218,6 +218,7 @@\n         (\"persimmon\", \"PersimmonModel\"),\n         (\"phi\", \"PhiModel\"),\n         (\"phi3\", \"Phi3Model\"),\n+        (\"phi4_multimodal\", \"Phi4MultimodalModel\"),\n         (\"phimoe\", \"PhimoeModel\"),\n         (\"pixtral\", \"PixtralVisionModel\"),\n         (\"plbart\", \"PLBartModel\"),\n@@ -566,6 +567,7 @@\n         (\"persimmon\", \"PersimmonForCausalLM\"),\n         (\"phi\", \"PhiForCausalLM\"),\n         (\"phi3\", \"Phi3ForCausalLM\"),\n+        (\"phi4_multimodal\", \"Phi4MultimodalForCausalLM\"),\n         (\"phimoe\", \"PhimoeForCausalLM\"),\n         (\"plbart\", \"PLBartForCausalLM\"),\n         (\"prophetnet\", \"ProphetNetForCausalLM\"),"
        },
        {
            "sha": "48081b9df8bcb8a04407b901a021659244086f5f",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -91,6 +91,7 @@\n         (\"owlv2\", \"Owlv2Processor\"),\n         (\"owlvit\", \"OwlViTProcessor\"),\n         (\"paligemma\", \"PaliGemmaProcessor\"),\n+        (\"phi4_multimodal\", \"Phi4MultimodalProcessor\"),\n         (\"pix2struct\", \"Pix2StructProcessor\"),\n         (\"pixtral\", \"PixtralProcessor\"),\n         (\"pop2piano\", \"Pop2PianoProcessor\"),"
        },
        {
            "sha": "c4e2e599f57a8f9fe23254a46ff6fda67e82f13b",
            "filename": "src/transformers/models/phi4_multimodal/__init__.py",
            "status": "added",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2F__init__.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -0,0 +1,32 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_phi4_multimodal import *\n+    from .feature_extraction_phi4_multimodal import *\n+    from .image_processing_phi4_multimodal_fast import *\n+    from .modeling_phi4_multimodal import *\n+    from .processing_phi4_multimodal import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "3f776b0b71ecfd6e57ebe8a0c2c2f320d281ffc0",
            "filename": "src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py",
            "status": "added",
            "additions": 482,
            "deletions": 0,
            "changes": 482,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -0,0 +1,482 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_phi4_multimodal.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 Microsoft and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class Phi4MultimodalVisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Phi4MultimodalVisionModel`]. It is used to instantiate a\n+    Phi4Multimodal vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the vision encoder of\n+    [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1152):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 4304):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 27):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input images.\n+        image_size (`int`, *optional*, defaults to 448):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        crop_size (`int`, *optional*, defaults to 448):\n+            Crop size for the input images.\n+        image_token_id (`int`, *optional*, defaults to 200010):\n+            The image token id.\n+        feature_layer (`int`, *optional*, defaults to -2):\n+            The index of the layer of the encoder from which to extract image features.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Phi4MultimodalVisionConfig\n+\n+    >>> # Initializing a Phi4MultimodalVisionConfig with microsoft/Phi-4-multimodal-instruct style configuration\n+    >>> configuration = Phi4MultimodalVisionConfig()\n+    ```\"\"\"\n+\n+    model_type = \"phi4_multimodal_vision\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1152,\n+        intermediate_size=4304,\n+        num_hidden_layers=27,\n+        num_attention_heads=16,\n+        num_channels=3,\n+        image_size=448,\n+        patch_size=14,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        layer_norm_eps=1e-6,\n+        attention_dropout=0.0,\n+        crop_size: int = 448,\n+        image_token_id: int = 200010,\n+        feature_layer: int = -2,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.image_size = image_size\n+        self.attention_dropout = attention_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.hidden_act = hidden_act\n+        self.crop_size = crop_size\n+        self.image_token_id = image_token_id\n+        self.feature_layer = feature_layer\n+\n+\n+class Phi4MultimodalAudioConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Phi4MultimodalAudioModel`]. It is used to instantiate a\n+    Phi4Multimodal audio encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the audio encoder of\n+    [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers.\n+        intermediate_size (`int`, *optional*, defaults to 1536):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_blocks (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        activation (`str`, *optional*, defaults to `\"swish\"`):\n+            The non-linear activation function in the MLPs.\n+        chunk_size (`int`, *optional*, defaults to -1):\n+            The chunk size to create the masks.\n+        left_chunk (`int`, *optional*, defaults to 18):\n+            The left chunk to create the masks.\n+        dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio.\n+        ext_pw_out_channel (`int`, *optional*, defaults to 1024):\n+            Number of out channels in the point-wise conv modules.\n+        depthwise_seperable_out_channel (`int`, *optional*, defaults to 1024):\n+            Number of out channels in the depth-wise separable conv modules.\n+        depthwise_multiplier (`int`, *optional*, defaults to 1):\n+            Input size multiplier for the depth-wise separable conv modules.\n+        kernel_size (`int`, *optional*, defaults to 3):\n+            Kernel size for the depth-wise separable conv modules.\n+        conv_activation (`str`, *optional*, defaults to `\"swish\"`):\n+            The non-linear activation function in the conv modules.\n+        input_size (`int`, *optional*, defaults to 80):\n+            Input size for the audio model.\n+        conv_glu_type (`str`, *optional*, defaults to `\"swish\"`):\n+            The non-linear activation function in the point-wise conv modules.\n+        time_reduction (`int`, *optional*, defaults to 8):\n+            Time reduction (subsampling factor).\n+        bias_max_distance (`int`, *optional*, defaults to 1000):\n+            Max distance for the relative attention bias module.\n+        bias_symmetric (`bool`, *optional*, defaults to `False`):\n+            Whether the relative attention bias should be symmetric or not.\n+        nemo_activation (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function in the nemo conv modules.\n+        nemo_conv_channels (`int`, *optional*, defaults to 1024):\n+            Number of channels in the nemo conv modules.\n+        downsample_rate (`int`, *optional*, defaults to 1):\n+            Downsample rate for the audio feature extractor.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        audio_token_id (`int`, *optional*, defaults to 200011):\n+            The audio token id.\n+        feature_layer (`int`, *optional*, defaults to -2):\n+            The index of the layer of the encoder from which to extract audio features.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Phi4MultimodalAudioConfig\n+\n+    >>> # Initializing a Phi4MultimodalAudioConfig with microsoft/Phi-4-multimodal-instruct style configuration\n+    >>> configuration = Phi4MultimodalAudioConfig()\n+    ```\"\"\"\n+\n+    model_type = \"phi4_multimodal_audio\"\n+\n+    def __init__(\n+        self,\n+        hidden_size: int = 1024,\n+        intermediate_size: int = 1536,\n+        num_blocks: int = 24,\n+        num_attention_heads: int = 16,\n+        activation: str = \"swish\",\n+        chunk_size: int = -1,\n+        left_chunk: int = 18,\n+        dropout_rate: float = 0.0,\n+        ext_pw_out_channel: int = 1024,\n+        depthwise_seperable_out_channel: int = 1024,\n+        depthwise_multiplier: int = 1,\n+        kernel_size: int = 3,\n+        conv_activation: str = \"swish\",\n+        input_size: int = 80,\n+        conv_glu_type: str = \"swish\",\n+        time_reduction: int = 8,\n+        bias_max_distance: int = 1000,\n+        bias_symmetric: bool = False,\n+        nemo_activation: str = \"relu\",\n+        nemo_conv_channels: int = 1024,\n+        downsample_rate: int = 1,\n+        initializer_range: float = 0.02,\n+        audio_token_id: int = 200011,\n+        feature_layer: int = -2,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.activation = activation\n+        self.chunk_size = chunk_size\n+        self.left_chunk = left_chunk\n+        self.num_blocks = num_blocks\n+        self.dropout_rate = dropout_rate\n+        self.ext_pw_out_channel = ext_pw_out_channel\n+        self.depthwise_seperable_out_channel = depthwise_seperable_out_channel\n+        self.depthwise_multiplier = depthwise_multiplier\n+        self.kernel_size = kernel_size\n+        self.conv_activation = conv_activation\n+        self.input_size = input_size\n+        self.conv_glu_type = conv_glu_type\n+        self.time_reduction = time_reduction\n+        self.bias_max_distance = bias_max_distance\n+        self.bias_symmetric = bias_symmetric\n+        self.nemo_activation = nemo_activation\n+        self.nemo_conv_channels = nemo_conv_channels\n+        self.downsample_rate = downsample_rate\n+        self.audio_token_id = audio_token_id\n+        self.initializer_range = initializer_range\n+        self.feature_layer = feature_layer\n+\n+        if time_reduction % 2 != 0:\n+            raise ValueError(\"`time_reduction` should be a multiple of 2!\")\n+        length = input_size\n+        for _ in range(int(math.log(time_reduction, 2))):\n+            length = math.floor((length - 1) / 2 + 1)\n+        self.nemo_final_size = length\n+\n+\n+class Phi4MultimodalConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Phi4MultimodalModel`]. It is used to instantiate a\n+    Phi4Multimodal model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the\n+    [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 200064):\n+            Vocabulary size of the Phi-3 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Phi3Model`].\n+        hidden_size (`int`, *optional*, defaults to 3072):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 8192):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        resid_pdrop (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for mlp outputs.\n+        embd_pdrop (`int`, *optional*, defaults to 0.0):\n+            The dropout ratio for the embeddings.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio after computing the attention scores.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon value used for the RMSNorm.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`. Whether to tie weight embeddings or not.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`dict`, *optional*):\n+            The scaling strategy for the RoPE embeddings. If `None`, no scaling is applied. If a dictionary, it must\n+            contain the following keys: `type`, `short_factor` and `long_factor`. The `type` must be `longrope` and\n+            the `short_factor` and `long_factor` must be lists of numbers with the same length as the hidden size\n+            divided by the number of attention heads divided by 2.\n+        partial_rotary_factor (`float`, *optional*, defaults to `1.0`):\n+            Percentage of the query and keys which will have rotary embedding. Must be between 0.0 and 1.0.\n+        bos_token_id (`int`, *optional*, defaults to 199999):\n+            The id of the \"beginning-of-sequence\" token.\n+        eos_token_id (`int` or `list[int]`, *optional*, defaults to `[199999, 200020]`):\n+            The id of the \"end-of-sequence\" token.\n+        pad_token_id (`int`, *optional*, defaults to 199999):\n+            The id of the padding token.\n+        original_max_position_embeddings (`int`, *optional*, defaults to 4096):\n+            The maximum sequence length that this model was trained with. This is used to determine the size of the\n+            original RoPE embeddings when using long scaling.\n+        sliding_window (`int`, *optional*):\n+            Sliding window attention window size. If `None`, no sliding window is applied.\n+        vision_config (`Phi4MultimodalVisionConfig` or `dict`, *optional*):\n+            The vision config for the underlying image embedding model. If not provided, will default to the configuration\n+            used to instantiate a model similar in architecture as\n+            [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct).\n+        audio_config (`Phi4MultimodalAudioConfig` or `dict`, *optional*):\n+            The audio config for the underlying audio embedding model. If not provided, will default to the configuration\n+            used to instantiate a model similar in architecture as\n+            [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct).\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Phi4MultimodalModel, Phi4MultimodalConfig\n+\n+    >>> # Initializing a Phi4Multimodal style configuration\n+    >>> configuration = Phi4MultimodalConfig.from_pretrained(\"microsoft/Phi-4-multimodal-instruct\")\n+\n+    >>> # Initializing a model from the configuration\n+    >>> model = Phi4MultimodalModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"phi4_multimodal\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.qkv_proj\": \"colwise_rep\",  # we need to replicate here due to the slicing of qkv\n+        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the slicing of qkv\n+        \"layers.*.mlp.gate_up_proj\": \"colwise_rep\",  # we need to replicate here due to the `chunk` operation\n+        \"layers.*.mlp.down_proj\": \"rowwise_rep\",  # we need to replicate here due to the `chunk` operation\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    sub_configs = {\"audio_config\": Phi4MultimodalAudioConfig, \"vision_config\": Phi4MultimodalVisionConfig}\n+\n+    def __init__(\n+        self,\n+        vocab_size=200064,\n+        hidden_size=3072,\n+        intermediate_size=8192,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=8,\n+        resid_pdrop=0.0,\n+        embd_pdrop=0.0,\n+        attention_dropout=0.0,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=131072,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        partial_rotary_factor=1,\n+        bos_token_id=199999,\n+        eos_token_id=[199999, 200020],\n+        pad_token_id=199999,\n+        original_max_position_embeddings=4096,\n+        sliding_window=None,\n+        vision_config=None,\n+        audio_config=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            pad_token_id=pad_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.resid_pdrop = resid_pdrop\n+        self.embd_pdrop = embd_pdrop\n+        self.attention_dropout = attention_dropout\n+        self.hidden_act = hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.original_max_position_embeddings = original_max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.partial_rotary_factor = partial_rotary_factor\n+        self._rope_scaling_adjustment()\n+        self._rope_scaling_validation()\n+        self.sliding_window = sliding_window\n+\n+        if isinstance(vision_config, dict):\n+            vision_config = Phi4MultimodalVisionConfig(**vision_config)\n+        elif vision_config is None:\n+            Phi4MultimodalVisionConfig()\n+        self.vision_config = vision_config\n+\n+        if isinstance(audio_config, dict):\n+            audio_config = Phi4MultimodalAudioConfig(**audio_config)\n+        elif vision_config is None:\n+            audio_config = Phi4MultimodalAudioConfig()\n+        self.audio_config = audio_config\n+\n+    def _rope_scaling_adjustment(self):\n+        \"\"\"\n+        Adjust the `type` of the `rope_scaling` configuration for backward compatibility.\n+        \"\"\"\n+        if self.rope_scaling is None:\n+            return\n+\n+        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n+\n+        # For backward compatibility if previous version used \"su\" or \"yarn\"\n+        if rope_scaling_type is not None and rope_scaling_type in [\"su\", \"yarn\"]:\n+            self.rope_scaling[\"type\"] = \"longrope\"\n+\n+    def _rope_scaling_validation(self):\n+        \"\"\"\n+        Validate the `rope_scaling` configuration.\n+        \"\"\"\n+        if self.rope_scaling is None:\n+            return\n+\n+        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 3:\n+            raise ValueError(\n+                \"`rope_scaling` must be a dictionary with three fields, `type`, `short_factor` and `long_factor`, \"\n+                f\"got {self.rope_scaling}\"\n+            )\n+        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n+        rope_scaling_short_factor = self.rope_scaling.get(\"short_factor\", None)\n+        rope_scaling_long_factor = self.rope_scaling.get(\"long_factor\", None)\n+        if rope_scaling_type is None or rope_scaling_type not in [\"longrope\"]:\n+            raise ValueError(f\"`rope_scaling`'s type field must be one of ['longrope'], got {rope_scaling_type}\")\n+        if not (\n+            isinstance(rope_scaling_short_factor, list)\n+            and all(isinstance(x, (int, float)) for x in rope_scaling_short_factor)\n+        ):\n+            raise ValueError(\n+                f\"`rope_scaling`'s short_factor field must be a list of numbers, got {rope_scaling_short_factor}\"\n+            )\n+        rotary_ndims = int(self.hidden_size // self.num_attention_heads * self.partial_rotary_factor)\n+        if not len(rope_scaling_short_factor) == rotary_ndims // 2:\n+            raise ValueError(\n+                f\"`rope_scaling`'s short_factor field must have length {rotary_ndims // 2}, got {len(rope_scaling_short_factor)}\"\n+            )\n+        if not (\n+            isinstance(rope_scaling_long_factor, list)\n+            and all(isinstance(x, (int, float)) for x in rope_scaling_long_factor)\n+        ):\n+            raise ValueError(\n+                f\"`rope_scaling`'s long_factor field must be a list of numbers, got {rope_scaling_long_factor}\"\n+            )\n+        if not len(rope_scaling_long_factor) == rotary_ndims // 2:\n+            raise ValueError(\n+                f\"`rope_scaling`'s long_factor field must have length {rotary_ndims // 2}, got {len(rope_scaling_long_factor)}\"\n+            )\n+\n+\n+__all__ = [\"Phi4MultimodalVisionConfig\", \"Phi4MultimodalAudioConfig\", \"Phi4MultimodalConfig\"]"
        },
        {
            "sha": "c7cae2ab007c9e7b28d51a460440edda30efdcb6",
            "filename": "src/transformers/models/phi4_multimodal/convert_phi4_multimodal_weights_to_hf.py",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconvert_phi4_multimodal_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconvert_phi4_multimodal_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconvert_phi4_multimodal_weights_to_hf.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -0,0 +1,229 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import json\n+import os\n+import re\n+\n+import torch\n+from peft import LoraConfig\n+from safetensors.torch import load_file, save_file\n+\n+from transformers import (\n+    Phi4MultimodalAudioConfig,\n+    Phi4MultimodalConfig,\n+    Phi4MultimodalForCausalLM,\n+    Phi4MultimodalProcessor,\n+    Phi4MultimodalVisionConfig,\n+)\n+\n+\n+# fmt: off\n+STATE_DICT_MAPPING = {\n+    r\"^model.embed_tokens_extend.audio_embed.encoder.encoders.(\\d+).feed_forward_(in|out).net.0.linear\": r\"model.embed_tokens_extend.audio_embed.encoder.encoders.\\1.feed_forward_\\2.gate_up_proj\",\n+    r\"^model.embed_tokens_extend.audio_embed.encoder.encoders.(\\d+).feed_forward_(in|out).net.2\": r\"model.embed_tokens_extend.audio_embed.encoder.encoders.\\1.feed_forward_\\2.down_proj\",\n+\n+    r\"^model.embed_tokens_extend.audio_embed.encoder.encoders.(\\d+).self_attn.linear_(q|k|v)\": r\"model.embed_tokens_extend.audio_embed.encoder.encoders.\\1.self_attn.\\2_proj\",\n+    r\"^model.embed_tokens_extend.audio_embed.encoder.encoders.(\\d+).self_attn.linear_out\": r\"model.embed_tokens_extend.audio_embed.encoder.encoders.\\1.self_attn.o_proj\",\n+\n+    r\"^model.embed_tokens_extend.image_embed.img_projection.0\": r\"model.embed_tokens_extend.image_embed.img_projection_up\",\n+    r\"^model.embed_tokens_extend.image_embed.img_projection.2\": r\"model.embed_tokens_extend.image_embed.img_projection_down\",\n+\n+    r\"^model.embed_tokens_extend.image_embed.glb_GN\": r\"model.embed_tokens_extend.image_embed.global_img_feature_extensor\",\n+    r\"^model.embed_tokens_extend.image_embed.sub_GN\": r\"model.embed_tokens_extend.image_embed.sub_img_feature_extensor\",\n+\n+    r\"^model.embed_tokens_extend.audio_embed.audio_projection.speech.0\": r\"model.embed_tokens_extend.audio_embed.up_proj_for_speech\",\n+    r\"^model.embed_tokens_extend.audio_embed.audio_projection.speech.2\": r\"model.embed_tokens_extend.audio_embed.down_proj_for_speech\",\n+    r\"^model.embed_tokens_extend.audio_embed.audio_projection.vision.0\": r\"model.embed_tokens_extend.audio_embed.up_proj_for_vision_speech\",\n+    r\"^model.embed_tokens_extend.audio_embed.audio_projection.vision.2\": r\"model.embed_tokens_extend.audio_embed.down_proj_for_vision_speech\",\n+}\n+# fmt: on\n+\n+\n+def map_old_key_to_new(old_key):\n+    \"\"\"Map of a key of the original state dict to the equivalent key in HF format\"\"\"\n+    for pattern, replacement in STATE_DICT_MAPPING.items():\n+        new_key, n_replace = re.subn(pattern, replacement, old_key)\n+        # Early exit of the loop\n+        if n_replace > 0:\n+            return new_key\n+\n+    # The state dict contains lora keys....\n+    if \"lora\" in old_key:\n+        return None\n+    # This extracts the original weight before adding the lora adapter\n+    if \"base_layer.\" in old_key:\n+        return old_key.replace(\"base_layer.\", \"\")\n+\n+    # not part of the key mapping, we keep the original name\n+    return old_key\n+\n+\n+def convert_state_dict(original_state_dict: dict):\n+    \"\"\"Convert a state dict file.\"\"\"\n+    new_dict = {}\n+    for old_key, tensor in original_state_dict.items():\n+        new_key = map_old_key_to_new(old_key)\n+        if new_key is not None:\n+            new_dict[new_key] = tensor\n+    return new_dict\n+\n+\n+def convert_config(original_config: dict):\n+    # Remove unused args\n+    original_config.pop(\"_name_or_path\", None)\n+    original_config.pop(\"architectures\", None)\n+    original_config.pop(\"auto_map\", None)\n+    original_config.pop(\"vision_lora\", None)\n+    original_config.pop(\"speech_lora\", None)\n+    original_config.pop(\"transformers_version\", None)\n+    original_config.pop(\"_attn_implementation\", None)\n+\n+    embd_layer = original_config.pop(\"embd_layer\")\n+    audio_embd_layer = embd_layer[\"audio_embd_layer\"]\n+    vision_embd_layer = embd_layer[\"image_embd_layer\"]\n+\n+    # Keep only some of the subdict\n+    keep_audio_embd_layer = [\"downsample_rate\"]\n+    keep_vision_embd_layer = [\"crop_size\"]\n+    audio_embd_layer = {k: v for k, v in audio_embd_layer.items() if k in keep_audio_embd_layer}\n+    vision_embd_layer = {k: v for k, v in vision_embd_layer.items() if k in keep_vision_embd_layer}\n+\n+    audio_config = original_config.pop(\"audio_processor\")[\"config\"]\n+    # remove\n+    audio_config.pop(\"activation_checkpointing\", None)\n+    audio_config.pop(\"cnn_layer_norm\", None)\n+    audio_config.pop(\"input_layer\", None)\n+    audio_config.pop(\"batch_norm\", None)\n+    audio_config.pop(\"encoder_embedding_config\", None)\n+    audio_config.pop(\"ext_pw_kernel_size\", None)\n+    audio_config.pop(\"bias_in_glu\", None)\n+    audio_config.pop(\"causal\", None)\n+    # rename\n+    audio_config[\"hidden_size\"] = audio_config.pop(\"attention_dim\")\n+    audio_config[\"num_attention_heads\"] = audio_config.pop(\"attention_heads\")\n+    audio_config[\"intermediate_size\"] = audio_config.pop(\"linear_units\")\n+    audio_config[\"nemo_conv_channels\"] = audio_config.pop(\"nemo_conv_settings\")[\"conv_channels\"]\n+    audio_config[\"bias_max_distance\"] = audio_config.pop(\"relative_attention_bias_args\")[\"t5_bias_max_distance\"]\n+    # add\n+    audio_config = {**audio_config, **audio_embd_layer}\n+\n+    # Create transformers config objects\n+    audio_config = Phi4MultimodalAudioConfig(**audio_config)\n+    vision_config = Phi4MultimodalVisionConfig(**vision_embd_layer)\n+\n+    # Add 2nd eos to config\n+    original_config[\"eos_token_id\"] = [199999, 200020]\n+\n+    new_config = Phi4MultimodalConfig(**original_config, vision_config=vision_config, audio_config=audio_config)\n+    return new_config\n+\n+\n+def read_json(path):\n+    with open(path, \"r\") as f:\n+        return json.load(f)\n+\n+\n+def convert_and_write_model(input_dir: str, output_dir: str):\n+    \"\"\"Convert the model and save it (this implicitly save the config as well).\"\"\"\n+    original_config = read_json(os.path.join(input_dir, \"config.json\"))\n+    config = convert_config(original_config)\n+\n+    full_state_dict = {}\n+    shards = [file for file in os.listdir(input_dir) if file.endswith(\".safetensors\")]\n+    for shard_file in shards:\n+        original_state_dict = load_file(os.path.join(input_dir, shard_file))\n+        new_dict = convert_state_dict(original_state_dict)\n+        full_state_dict.update(new_dict)\n+\n+    # Load weights into model and resave them\n+    with torch.device(\"meta\"):\n+        model = Phi4MultimodalForCausalLM(config)\n+    missing, unexpected = model.load_state_dict(full_state_dict, strict=False, assign=True)\n+    # The lm_head is missing because it's tied\n+    if missing != [\"lm_head.weight\"]:\n+        raise ValueError(\"Missing keys:\\n{missing}\")\n+    if len(unexpected) > 0:\n+        raise ValueError(f\"Unexpected keys:\\n{unexpected}\")\n+\n+    model.tie_weights()\n+    model.save_pretrained(output_dir)\n+\n+\n+def convert_and_save_processor(input_dir: str, output_dir: str):\n+    \"\"\"Convert the processor.\"\"\"\n+    processor = Phi4MultimodalProcessor.from_pretrained(input_dir)\n+    del processor.image_processor.auto_map\n+    del processor.audio_processor.auto_map\n+    processor.chat_template = processor.tokenizer.chat_template\n+    processor.tokenizer.extra_special_tokens = {\"image_token\": \"<|endoftext10|>\", \"audio_token\": \"<|endoftext11|>\"}\n+    processor.save_pretrained(output_dir)\n+\n+\n+def extract_adapters_data(input_dir: str, output_dir: str):\n+    \"\"\"Extract adapters data from the state dict and save weights and configs.\"\"\"\n+    speech_lora = {}\n+    vision_lora = {}\n+    shards = [file for file in os.listdir(input_dir) if file.endswith(\".safetensors\")]\n+    for shard_file in shards:\n+        original_state_dict = load_file(os.path.join(input_dir, shard_file))\n+        for k, v in original_state_dict.items():\n+            if \"lora\" in k:\n+                if \"speech\" in k:\n+                    speech_lora[k.replace(\"speech.\", \"\")] = v\n+                elif \"vision\" in k:\n+                    vision_lora[k.replace(\"vision.\", \"\")] = v\n+\n+    # Create and save the lora configs\n+    speech_lora_config = LoraConfig(\n+        r=320,\n+        lora_alpha=640,\n+        target_modules=r\"model.layers.\\d+.((self_attn.(qkv|o)_proj)|(mlp.(gate_up|down)_proj))\",\n+        lora_dropout=0.01,\n+        task_type=\"CAUSAL_LM\",\n+    )\n+    speech_lora_config.save_pretrained(os.path.join(output_dir, \"speech-lora\"))\n+    vision_lora_config = LoraConfig(\n+        r=256,\n+        lora_alpha=512,\n+        target_modules=r\"model.layers.\\d+.((self_attn.(qkv|o)_proj)|(mlp.(gate_up|down)_proj))\",\n+        lora_dropout=0.0,\n+        task_type=\"CAUSAL_LM\",\n+    )\n+    vision_lora_config.save_pretrained(os.path.join(output_dir, \"vision-lora\"))\n+\n+    save_file(speech_lora, os.path.join(output_dir, \"speech-lora\", \"adapter_model.safetensors\"))\n+    save_file(vision_lora, os.path.join(output_dir, \"vision-lora\", \"adapter_model.safetensors\"))\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"input_dir\",\n+        help=\"Location of the model folder containing the weights and configs.\",\n+    )\n+    parser.add_argument(\n+        \"output_dir\",\n+        help=\"Location to write HF model.\",\n+    )\n+    args = parser.parse_args()\n+\n+    # Convert\n+    convert_and_write_model(args.input_dir, args.output_dir)\n+    convert_and_save_processor(args.input_dir, args.output_dir)\n+    extract_adapters_data(args.input_dir, args.output_dir)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "5d29af6c8bfb35dd22703518ff41879129e4d6f9",
            "filename": "src/transformers/models/phi4_multimodal/feature_extraction_phi4_multimodal.py",
            "status": "added",
            "additions": 348,
            "deletions": 0,
            "changes": 348,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Ffeature_extraction_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Ffeature_extraction_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Ffeature_extraction_phi4_multimodal.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -0,0 +1,348 @@\n+# Copyright 2024 Microsoft and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+Processor class for Phi4Multimodal\n+\"\"\"\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...audio_utils import AudioInput\n+from ...feature_extraction_sequence_utils import SequenceFeatureExtractor\n+from ...image_processing_utils import BatchFeature\n+from ...utils import TensorType, is_torch_available, logging\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# TODO: @eustlb, remove this once #36603 is merged.\n+def speechlib_mel(sample_rate, n_fft, n_mels, fmin=None, fmax=None):\n+    \"\"\"Create a Mel filter-bank the same as SpeechLib FbankFC.\n+\n+    Args:\n+        sample_rate (int): Sample rate in Hz. number > 0 [scalar]\n+        n_fft (int): FFT size. int > 0 [scalar]\n+        n_mel (int): Mel filter size. int > 0 [scalar]\n+        fmin (float): lowest frequency (in Hz). If None use 0.0.\n+            float >= 0 [scalar]\n+        fmax: highest frequency (in Hz). If None use sample_rate / 2.\n+            float >= 0 [scalar]\n+\n+    Returns\n+        out (numpy.ndarray): Mel transform matrix\n+            [shape=(n_mels, 1 + n_fft/2)]\n+    \"\"\"\n+\n+    bank_width = int(n_fft // 2 + 1)\n+    if fmax is None:\n+        fmax = sample_rate / 2\n+    if fmin is None:\n+        fmin = 0\n+    assert fmin >= 0, \"fmin cannot be negtive\"\n+    assert fmin < fmax <= sample_rate / 2, \"fmax must be between (fmin, samplerate / 2]\"\n+\n+    def mel(f):\n+        return 1127.0 * np.log(1.0 + f / 700.0)\n+\n+    def bin2mel(fft_bin):\n+        return 1127.0 * np.log(1.0 + fft_bin * sample_rate / (n_fft * 700.0))\n+\n+    def f2bin(f):\n+        return int((f * n_fft / sample_rate) + 0.5)\n+\n+    # Spec 1: FFT bin range [f2bin(fmin) + 1, f2bin(fmax) - 1]\n+    klo = f2bin(fmin) + 1\n+    khi = f2bin(fmax)\n+\n+    khi = max(khi, klo)\n+\n+    # Spec 2: SpeechLib uses trianges in Mel space\n+    mlo = mel(fmin)\n+    mhi = mel(fmax)\n+    m_centers = np.linspace(mlo, mhi, n_mels + 2)\n+    ms = (mhi - mlo) / (n_mels + 1)\n+\n+    matrix = np.zeros((n_mels, bank_width), dtype=np.float32)\n+    for m in range(0, n_mels):\n+        left = m_centers[m]\n+        center = m_centers[m + 1]\n+        right = m_centers[m + 2]\n+        for fft_bin in range(klo, khi):\n+            mbin = bin2mel(fft_bin)\n+            if left < mbin < right:\n+                matrix[m, fft_bin] = 1.0 - abs(center - mbin) / ms\n+\n+    return matrix\n+\n+\n+class Phi4MultimodalFeatureExtractor(SequenceFeatureExtractor):\n+    model_input_names = [\"audio_input_features\", \"audio_embed_sizes\", \"audio_attention_mask\"]\n+\n+    def __init__(\n+        self,\n+        feature_size: int = 80,\n+        sampling_rate: int = 16000,\n+        hop_length: int = 160,\n+        n_fft: int = 512,\n+        win_length: int = 400,\n+        preemphasis: float = 0.97,\n+        padding_value: float = 0.0,\n+        audio_compression_rate: int = 8,\n+        audio_downsample_rate: int = 1,\n+        audio_feat_stride: int = 1,\n+        mel_min_frequency: float = 0,\n+        mel_max_frequency: float = 7690,\n+        **kwargs,\n+    ):\n+        super().__init__(feature_size=feature_size, sampling_rate=sampling_rate, padding_value=padding_value, **kwargs)\n+\n+        self.hop_length = hop_length\n+        self.n_fft = n_fft\n+        self.win_length = win_length\n+        self.preemphasis = preemphasis\n+        self.padding_value = padding_value\n+        self.audio_compression_rate = audio_compression_rate\n+        self.audio_downsample_rate = audio_downsample_rate\n+        self.audio_feat_stride = audio_feat_stride\n+\n+        # TODO: @eustlb, uncomment and remove speechlib_mel once #36603 is merged.\n+        # self.mel_filters = mel_filter_bank(\n+        #     num_frequency_bins=self.n_fft // 2 + 1,\n+        #     num_mel_filters=self.feature_size,\n+        #     min_frequency=mel_min_frequency,\n+        #     max_frequency=mel_max_frequency,\n+        #     sampling_rate=self.sampling_rate,\n+        #     triangularize_in_mel_space=True,\n+        #     mel_scale=\"kaldi\",\n+        # )\n+        self.mel_filters = speechlib_mel(\n+            self.sampling_rate, self.n_fft, self.feature_size, mel_min_frequency, mel_max_frequency\n+        ).T\n+\n+    def __call__(\n+        self,\n+        raw_speech: AudioInput,\n+        sampling_rate: Optional[int] = None,\n+        pad_to_multiple_of: Optional[int] = None,\n+        padding: Optional[str] = \"longest\",\n+        max_length: Optional[int] = None,\n+        truncation: bool = False,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_attention_mask: Optional[bool] = True,\n+        device: Optional[str] = \"cpu\",\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to featurize and prepare for the model one or several audio sequence(s). Implementation uses PyTorch for\n+        the STFT computation if available, otherwise a slower NumPy based one.\n+\n+        Args:\n+            raw_speech (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The sequence or batch of sequences to be processed. Each sequence can be a numpy array or PyTorch tensor.\n+                For batched inputs, sequences can be a list of numpy arrays or PyTorch tensors, or a single numpy array or\n+                PyTorch tensor with first dimension being the batch size.\n+            sampling_rate (`int`, *optional*):\n+                The sampling rate at which the `raw_speech` input was sampled. It is strongly recommended to pass\n+                `sampling_rate` at the forward call to prevent silent errors.\n+            pad_to_multiple_of (`int`, *optional*, defaults to None):\n+                If set will pad the sequence to a multiple of the provided value.\n+            padding (`str`, *optional*, defaults to \"longest\"):\n+                Padding strategy. Can be \"longest\" to pad to the longest sequence in the batch, or a specific length.\n+            max_length (`int`, *optional*):\n+                Maximum length of the returned list and optionally padding length.\n+            truncation (`bool`, *optional*, defaults to False):\n+                Activates truncation to cut input sequences longer than *max_length* to *max_length*.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors instead of numpy arrays. Acceptable values are:\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return Numpy `np.ndarray` objects.\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+            return_attention_mask (`bool`, *optional*, defaults to `True`):\n+                Whether to return the extracted audio input features' attention mask.\n+            device (`str`, *optional*, defaults to \"cpu\"):\n+                Specifies the device for computation of the audio features. (e.g., \"cpu\", \"cuda\")\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+                - **audio_input_features** -- Audio features extracted from the raw audio input, shape (batch_size, max_feature_length, feature_size).\n+                - **audio_lengths** -- Length of each audio sample in the batch, shape (batch_size,).\n+                - **audio_attention_mask** -- Attention mask for the audio input, shape (batch_size, max_feature_length).\n+                If `return_tensors` is not specified, the fields will be PyTorch tensors if PyTorch is available, otherwise NumPy arrays.\n+        \"\"\"\n+        if sampling_rate is not None:\n+            if sampling_rate != self.sampling_rate:\n+                raise ValueError(\n+                    f\"The model corresponding to this feature extractor: {self.__class__.__name__} was trained using a\"\n+                    f\" sampling rate of {self.sampling_rate}. Please make sure that the provided `raw_speech` input\"\n+                    f\" was sampled with {self.sampling_rate} and not {sampling_rate}.\"\n+                )\n+        else:\n+            logger.warning(\n+                f\"It is strongly recommended to pass the `sampling_rate` argument to `{self.__class__.__name__}()`. \"\n+                \"Failing to do so can result in silent errors that might be hard to debug.\"\n+            )\n+\n+        # Convert to torch tensor\n+        if isinstance(raw_speech, np.ndarray):\n+            raw_speech = torch.tensor(raw_speech)\n+        elif isinstance(raw_speech, (list, tuple)) and isinstance(raw_speech[0], np.ndarray):\n+            raw_speech = [torch.tensor(speech) for speech in raw_speech]\n+\n+        is_batched_torch = isinstance(raw_speech, torch.Tensor) and len(raw_speech.shape) > 1\n+        if is_batched_torch and len(raw_speech.shape) > 2:\n+            logger.warning(\n+                f\"Only mono-channel audio is supported for input to {self.__class__.__name__}. \"\n+                \"We will take the mean of the channels to convert to mono.\"\n+            )\n+            raw_speech = raw_speech.mean(-1)\n+\n+        is_batched_sequence = isinstance(raw_speech, (list, tuple))\n+        if is_batched_sequence:\n+            for speech in raw_speech:\n+                if len(speech.shape) > 1:\n+                    logger.warning(\n+                        f\"Only mono-channel audio is supported for input to {self.__class__.__name__}. \"\n+                        \"We will take the mean of the channels to convert to mono.\"\n+                    )\n+                    speech = speech.mean(-1)\n+\n+        if is_batched_torch or is_batched_sequence:\n+            raw_speech = [speech[:, None].to(torch.float32) for speech in raw_speech]\n+        else:\n+            raw_speech = [raw_speech[:, None].to(torch.float32)]\n+\n+        audio_lengths = [len(speech) for speech in raw_speech]\n+\n+        # convert into correct format for padding\n+        batched_speech = BatchFeature(data={\"audio_input_features\": raw_speech, \"audio_lengths\": audio_lengths})\n+        padded_inputs = self.pad(\n+            batched_speech,\n+            padding=padding,\n+            max_length=max_length,\n+            truncation=truncation,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            return_tensors=\"pt\",\n+        )\n+        input_features = padded_inputs.audio_input_features.squeeze(-1)\n+        audio_lengths = padded_inputs.audio_lengths\n+\n+        input_features = self._torch_extract_fbank_features(input_features, audio_lengths, device)\n+\n+        feature_lengths = (audio_lengths - self.win_length) // self.hop_length + 1\n+        feature_lengths = feature_lengths * self.audio_feat_stride\n+        audio_embed_sizes = self._compute_audio_embed_size(feature_lengths)\n+\n+        feature_attention_mask = (\n+            torch.arange(0, feature_lengths.max()) if is_torch_available() else np.arange(0, feature_lengths.max())\n+        )\n+        feature_attention_mask = (\n+            feature_attention_mask[None, :] < feature_lengths[:, None] if len(feature_lengths) > 1 else None\n+        )\n+\n+        data = {\n+            \"audio_input_features\": input_features,\n+            \"audio_embed_sizes\": audio_embed_sizes,\n+        }\n+        if feature_attention_mask is not None and return_attention_mask:\n+            data[\"audio_attention_mask\"] = feature_attention_mask\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    # TODO; @eustlb, move this to audio_utils in a general spectogram_batch function that handles torch and numpy\n+    def _torch_extract_fbank_features(\n+        self, waveform: \"torch.FloatTensor\", audio_lengths: \"torch.Tensor\", device: str = \"cpu\"\n+    ) -> \"torch.FloatTensor\":\n+        \"\"\"\n+        Compute the log mel-scaled spectrogram of batched waveforms using PyTorch's FFT implementation.\n+\n+        Args:\n+            waveform (torch.FloatTensor` of shape `(batch_size, max_audio_length)`):\n+                The batched waveforms.\n+            audio_lengths (`torch.Tensor` of shape `(batch_size,)`):\n+                The lengths of the waveforms along the max_audio_length dimension.\n+            device (`str`, *optional*, defaults to \"cpu\"):\n+                The device to run the computation on. (e.g., \"cpu\", \"cuda\")\n+\n+        Returns:\n+            `torch.FloatTensor` of shape `(batch_size, max_feature_length, feature_size)`:\n+                The log mel-scaled spectrogram of the batched waveforms.\n+        \"\"\"\n+        fft_window = torch.hamming_window(self.win_length, periodic=False, device=device, dtype=torch.float64)\n+\n+        # batched implementation\n+        batch_size = waveform.shape[0]\n+        frames = waveform.unfold(-1, self.win_length, self.hop_length)\n+\n+        # ---\n+        # the unbatched (and unpaded) original implementation skips last few audio values that can't be included in a frame\n+        # we need to ensure that the corresponding frames for the padded input also mask these values\n+        if batch_size > 1:\n+            frames = frames.clone()\n+            # concerned batch indices\n+            to_mask_batch_idxs = torch.arange(batch_size)[audio_lengths != audio_lengths.max()]\n+            if to_mask_batch_idxs.numel() > 0:\n+                batch_idxs_down = (audio_lengths[to_mask_batch_idxs] - self.win_length) // self.hop_length + 1\n+                batch_idxs_up = audio_lengths[to_mask_batch_idxs] // self.hop_length + 1\n+                offset_idx = batch_idxs_down.min()\n+                max_idx = batch_idxs_up.max()\n+\n+                mask = torch.arange(max_idx - offset_idx, device=device).expand(to_mask_batch_idxs.shape[0], -1)\n+                mask = ((batch_idxs_down - offset_idx).unsqueeze(1) <= mask) & (\n+                    mask < (batch_idxs_up - offset_idx).unsqueeze(1)\n+                )\n+                mask = mask.unsqueeze(-1).expand(-1, -1, self.win_length)\n+                masked_frames = frames[to_mask_batch_idxs, offset_idx:max_idx].masked_fill_(mask, 0)\n+                frames[to_mask_batch_idxs, offset_idx:max_idx] = masked_frames\n+        # ---\n+\n+        # apply pre-emphasis first order filter on fft windows\n+        frames_prev = torch.roll(frames, 1, dims=-1)\n+        frames_prev[:, :, 0] = frames_prev[:, :, 1]\n+        frames = (frames - self.preemphasis * frames_prev) * 32768\n+\n+        # apply fft\n+        S = torch.fft.rfft(fft_window * frames.view(-1, self.win_length), n=self.n_fft, dim=1)\n+        S = S.view(frames.shape[0], -1, S.shape[-1])\n+        S = S.to(torch.complex64)\n+\n+        spec = torch.abs(S)\n+        spec_power = spec**2\n+\n+        # apply triangular mel filter bank\n+        mel_filters = torch.from_numpy(self.mel_filters).to(device, torch.float32)\n+        log_spec = torch.clamp(spec_power @ mel_filters, min=1.0)\n+        log_spec = torch.log(log_spec)\n+\n+        return log_spec\n+\n+    def _compute_audio_embed_size(self, audio_frames):\n+        integer = audio_frames // self.audio_compression_rate\n+        remainder = audio_frames % self.audio_compression_rate\n+        result = integer + (remainder > 0).to(integer.dtype)\n+\n+        integer = result // self.audio_downsample_rate\n+        remainder = result % self.audio_downsample_rate\n+        result = integer + (remainder > 0).to(integer.dtype)  # qformer compression\n+\n+        return result\n+\n+\n+__all__ = [\"Phi4MultimodalFeatureExtractor\"]"
        },
        {
            "sha": "c81820ee32212775c31ba0cf5b507830e58c16d2",
            "filename": "src/transformers/models/phi4_multimodal/image_processing_phi4_multimodal_fast.py",
            "status": "added",
            "additions": 263,
            "deletions": 0,
            "changes": 263,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -0,0 +1,263 @@\n+# Copyright 2025 Microsoft and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+Processor class for Phi4Multimodal\n+\"\"\"\n+\n+import math\n+from typing import List, Optional, Union\n+\n+import torch\n+from torchvision.transforms import functional as F\n+\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+    Unpack,\n+    convert_to_rgb,\n+)\n+from ...image_utils import ImageInput, make_list_of_images, valid_images\n+from ...utils import TensorType, logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Phi4MultimodalFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    image_size: Optional[int]\n+    patch_size: Optional[int]\n+    dynamic_hd: Optional[int]\n+\n+\n+class Phi4MultimodalImageProcessorFast(BaseImageProcessorFast):\n+    r\"\"\"\n+    Constructs a Phi4Multimodal image processor.\n+    \"\"\"\n+\n+    image_size = 448\n+    patch_size = 14\n+    dynamic_hd = 36\n+    image_mean = [0.5, 0.5, 0.5]\n+    image_std = [0.5, 0.5, 0.5]\n+    valid_init_kwargs = Phi4MultimodalFastImageProcessorKwargs\n+    model_input_names = [\"image_pixel_values\", \"image_sizes\", \"image_attention_mask\"]\n+\n+    def __init__(self, **kwargs: Unpack[Phi4MultimodalFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height):\n+        best_ratio_diff = float(\"inf\")\n+        best_ratio = (1, 1)\n+        area = width * height\n+        for ratio in target_ratios:\n+            target_aspect_ratio = ratio[0] / ratio[1]\n+            ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n+            if ratio_diff < best_ratio_diff:\n+                best_ratio_diff = ratio_diff\n+                best_ratio = ratio\n+            elif ratio_diff == best_ratio_diff:\n+                if area > 0.5 * self.image_size * self.image_size * ratio[0] * ratio[1]:\n+                    best_ratio = ratio\n+        return best_ratio\n+\n+    def dynamic_preprocess(self, image, max_num=36, min_num=1):\n+        image_size = self.image_size\n+        patch_size = self.patch_size\n+        mask_size = image_size // patch_size\n+        orig_width, orig_height = image.size\n+\n+        w_crop_num = math.ceil(orig_width / float(image_size))\n+        h_crop_num = math.ceil(orig_height / float(image_size))\n+        if w_crop_num * h_crop_num > max_num:\n+            aspect_ratio = orig_width / orig_height\n+\n+            # calculate the existing image aspect ratio\n+            target_ratios = {\n+                (i, j)\n+                for n in range(min_num, max_num + 1)\n+                for i in range(1, n + 1)\n+                for j in range(1, n + 1)\n+                if i * j <= max_num and i * j >= min_num\n+            }\n+            target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n+\n+            # find the closest aspect ratio to the target\n+            target_aspect_ratio = self.find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height)\n+\n+            # calculate the target width and height\n+            target_width = image_size * target_aspect_ratio[0]\n+            target_height = image_size * target_aspect_ratio[1]\n+        else:\n+            target_width = image_size * w_crop_num\n+            target_height = image_size * h_crop_num\n+            target_aspect_ratio = (w_crop_num, h_crop_num)\n+\n+        # Calculate the ratio\n+        ratio_width = target_width / orig_width\n+        ratio_height = target_height / orig_height\n+        if ratio_width < ratio_height:\n+            new_size = (target_width, int(orig_height * ratio_width))\n+            padding_width = 0\n+            padding_height = target_height - int(orig_height * ratio_width)\n+        else:\n+            new_size = (int(orig_width * ratio_height), target_height)\n+            padding_width = target_width - int(orig_width * ratio_height)\n+            padding_height = 0\n+\n+        attention_mask = torch.ones((int(mask_size * target_aspect_ratio[1]), int(mask_size * target_aspect_ratio[0])))\n+        if padding_width >= patch_size:\n+            attention_mask[:, -math.floor(padding_width / patch_size) :] = 0\n+        if padding_height >= patch_size:\n+            attention_mask[-math.floor(padding_height / patch_size) :, :] = 0\n+\n+        if min(new_size[1], target_height) < 10 or min(new_size[0], target_width) < 10:\n+            raise ValueError(f\"the aspect ratio is very extreme {new_size}\")\n+\n+        image = F.resize(image, [new_size[1], new_size[0]])\n+        resized_img = F.pad(image, [0, 0, padding_width, padding_height], fill=[255, 255, 255])\n+\n+        return resized_img, attention_mask\n+\n+    def pad_to_max_num_crops(self, images, max_crops=5):\n+        \"\"\"\n+        images: B x 3 x H x W, B<=max_crops\n+        \"\"\"\n+        B, _, H, W = images.shape\n+        if B < max_crops:\n+            pad = torch.zeros(max_crops - B, 3, H, W, dtype=images.dtype, device=images.device)\n+            images = torch.cat([images, pad], dim=0)\n+        return images\n+\n+    def pad_mask_to_max_num_crops(self, masks, max_crops=5):\n+        B, H, W = masks.shape\n+        if B < max_crops:\n+            pad = torch.ones(max_crops - B, H, W, dtype=masks.dtype, device=masks.device)\n+            masks = torch.cat([masks, pad], dim=0)\n+        return masks\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+    ):\n+        \"\"\"\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+        \"\"\"\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+\n+        images = make_list_of_images(images)\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+        images = [convert_to_rgb(image) for image in images]\n+\n+        image_size = self.image_size\n+        patch_size = self.patch_size\n+        mask_size = image_size // patch_size\n+        imgs_and_masks = [self.dynamic_preprocess(image, max_num=self.dynamic_hd) for image in images]\n+        images, image_attention_masks = [x[0] for x in imgs_and_masks], [x[1] for x in imgs_and_masks]\n+\n+        images = [F.to_tensor(image) for image in images]\n+        hd_images = [F.normalize(image, image_mean, image_std) for image in images]\n+        global_image = [\n+            torch.nn.functional.interpolate(\n+                image.unsqueeze(0).float(),\n+                size=(image_size, image_size),\n+                mode=\"bicubic\",\n+            ).to(image.dtype)\n+            for image in hd_images\n+        ]\n+\n+        shapes = [[image.size(1), image.size(2)] for image in hd_images]\n+        mask_shapes = [[mask.size(0), mask.size(1)] for mask in image_attention_masks]\n+        global_attention_mask = [torch.ones((1, mask_size, mask_size)) for _ in hd_images]\n+\n+        hd_images_reshape = []\n+        for im, (h, w) in zip(hd_images, shapes):\n+            im = im.reshape(1, 3, h // image_size, image_size, w // image_size, image_size)\n+            im = im.permute(0, 2, 4, 1, 3, 5)\n+            im = im.reshape(-1, 3, image_size, image_size)\n+            hd_images_reshape.append(im.contiguous())\n+\n+        attention_masks_reshape = []\n+        for mask, (h, w) in zip(image_attention_masks, mask_shapes):\n+            mask = mask.reshape(h // mask_size, mask_size, w // mask_size, mask_size)\n+            mask = mask.transpose(1, 2)\n+            mask = mask.reshape(-1, mask_size, mask_size)\n+            attention_masks_reshape.append(mask.contiguous())\n+\n+        downsample_attention_masks = []\n+        for mask, (h, w) in zip(attention_masks_reshape, mask_shapes):\n+            mask = mask[:, 0::2, 0::2]\n+            mask = mask.reshape(\n+                h // mask_size, w // mask_size, mask_size // 2 + mask_size % 2, mask_size // 2 + mask_size % 2\n+            )\n+            mask = mask.transpose(1, 2)\n+            mask = mask.reshape(mask.size(0) * mask.size(1), mask.size(2) * mask.size(3))\n+            downsample_attention_masks.append(mask)\n+\n+        num_img_tokens = [\n+            256 + 1 + int(mask.sum().item()) + int(mask[:, 0].sum().item()) + 16 for mask in downsample_attention_masks\n+        ]\n+\n+        hd_images_reshape = [\n+            torch.cat([_global_image] + [_im], dim=0) for _global_image, _im in zip(global_image, hd_images_reshape)\n+        ]\n+        hd_masks_reshape = [\n+            torch.cat([_global_mask] + [_mask], dim=0)\n+            for _global_mask, _mask in zip(global_attention_mask, attention_masks_reshape)\n+        ]\n+        max_crops = max([img.size(0) for img in hd_images_reshape])\n+        image_transformed = [self.pad_to_max_num_crops(im, max_crops) for im in hd_images_reshape]\n+        image_transformed = torch.stack(image_transformed, dim=0)\n+        mask_transformed = [self.pad_mask_to_max_num_crops(mask, max_crops) for mask in hd_masks_reshape]\n+        mask_transformed = torch.stack(mask_transformed, dim=0)\n+\n+        returned_input_image_embeds = image_transformed\n+        returned_image_sizes = torch.tensor(shapes, dtype=torch.long)\n+        returned_image_attention_mask = mask_transformed\n+        returned_num_img_tokens = num_img_tokens\n+\n+        data = {\n+            \"image_pixel_values\": returned_input_image_embeds,\n+            \"image_sizes\": returned_image_sizes,\n+            \"image_attention_mask\": returned_image_attention_mask,\n+            \"num_img_tokens\": returned_num_img_tokens,\n+        }\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"Phi4MultimodalImageProcessorFast\"]"
        },
        {
            "sha": "5d44fae13117aa18827d2ac111caad1404e36e2d",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "added",
            "additions": 2316,
            "deletions": 0,
            "changes": 2316,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf"
        },
        {
            "sha": "06424941ec6bdab071f5649db9c78f4dadd7e433",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "added",
            "additions": 1851,
            "deletions": 0,
            "changes": 1851,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -0,0 +1,1851 @@\n+# Copyright 2025 Microsoft and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Callable, List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import DynamicCache\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    BaseModelOutputWithPooling,\n+    CausalLMOutputWithPast,\n+)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..phi3.configuration_phi3 import Phi3Config\n+from ..phi3.modeling_phi3 import Phi3DecoderLayer, Phi3ForCausalLM, Phi3Model, Phi3RMSNorm\n+from ..siglip.configuration_siglip import SiglipVisionConfig\n+from ..siglip.modeling_siglip import (\n+    SiglipEncoder,\n+    SiglipEncoderLayer,\n+    SiglipMLP,\n+    SiglipMultiheadAttentionPoolingHead,\n+    SiglipPreTrainedModel,\n+    SiglipVisionEmbeddings,\n+    default_flax_embed_init,\n+    lecun_normal_,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Phi4MultimodalVisionConfig(SiglipVisionConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Phi4MultimodalVisionModel`]. It is used to instantiate a\n+    Phi4Multimodal vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the vision encoder of\n+    [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1152):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 4304):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 27):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input images.\n+        image_size (`int`, *optional*, defaults to 448):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        crop_size (`int`, *optional*, defaults to 448):\n+            Crop size for the input images.\n+        image_token_id (`int`, *optional*, defaults to 200010):\n+            The image token id.\n+        feature_layer (`int`, *optional*, defaults to -2):\n+            The index of the layer of the encoder from which to extract image features.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Phi4MultimodalVisionConfig\n+\n+    >>> # Initializing a Phi4MultimodalVisionConfig with microsoft/Phi-4-multimodal-instruct style configuration\n+    >>> configuration = Phi4MultimodalVisionConfig()\n+    ```\"\"\"\n+\n+    model_type = \"phi4_multimodal_vision\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1152,\n+        intermediate_size=4304,\n+        num_hidden_layers=27,\n+        num_attention_heads=16,\n+        num_channels=3,\n+        image_size=448,\n+        patch_size=14,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        layer_norm_eps=1e-6,\n+        attention_dropout=0.0,\n+        crop_size: int = 448,\n+        image_token_id: int = 200010,\n+        feature_layer: int = -2,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_channels=num_channels,\n+            image_size=image_size,\n+            patch_size=patch_size,\n+            hidden_act=hidden_act,\n+            layer_norm_eps=layer_norm_eps,\n+            attention_dropout=attention_dropout,\n+            **kwargs,\n+        )\n+        self.crop_size = crop_size\n+        self.image_token_id = image_token_id\n+        self.feature_layer = feature_layer\n+\n+\n+class Phi4MultimodalAudioConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Phi4MultimodalAudioModel`]. It is used to instantiate a\n+    Phi4Multimodal audio encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the audio encoder of\n+    [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers.\n+        intermediate_size (`int`, *optional*, defaults to 1536):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_blocks (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        activation (`str`, *optional*, defaults to `\"swish\"`):\n+            The non-linear activation function in the MLPs.\n+        chunk_size (`int`, *optional*, defaults to -1):\n+            The chunk size to create the masks.\n+        left_chunk (`int`, *optional*, defaults to 18):\n+            The left chunk to create the masks.\n+        dropout_rate (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio.\n+        ext_pw_out_channel (`int`, *optional*, defaults to 1024):\n+            Number of out channels in the point-wise conv modules.\n+        depthwise_seperable_out_channel (`int`, *optional*, defaults to 1024):\n+            Number of out channels in the depth-wise separable conv modules.\n+        depthwise_multiplier (`int`, *optional*, defaults to 1):\n+            Input size multiplier for the depth-wise separable conv modules.\n+        kernel_size (`int`, *optional*, defaults to 3):\n+            Kernel size for the depth-wise separable conv modules.\n+        conv_activation (`str`, *optional*, defaults to `\"swish\"`):\n+            The non-linear activation function in the conv modules.\n+        input_size (`int`, *optional*, defaults to 80):\n+            Input size for the audio model.\n+        conv_glu_type (`str`, *optional*, defaults to `\"swish\"`):\n+            The non-linear activation function in the point-wise conv modules.\n+        time_reduction (`int`, *optional*, defaults to 8):\n+            Time reduction (subsampling factor).\n+        bias_max_distance (`int`, *optional*, defaults to 1000):\n+            Max distance for the relative attention bias module.\n+        bias_symmetric (`bool`, *optional*, defaults to `False`):\n+            Whether the relative attention bias should be symmetric or not.\n+        nemo_activation (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function in the nemo conv modules.\n+        nemo_conv_channels (`int`, *optional*, defaults to 1024):\n+            Number of channels in the nemo conv modules.\n+        downsample_rate (`int`, *optional*, defaults to 1):\n+            Downsample rate for the audio feature extractor.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        audio_token_id (`int`, *optional*, defaults to 200011):\n+            The audio token id.\n+        feature_layer (`int`, *optional*, defaults to -2):\n+            The index of the layer of the encoder from which to extract audio features.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Phi4MultimodalAudioConfig\n+\n+    >>> # Initializing a Phi4MultimodalAudioConfig with microsoft/Phi-4-multimodal-instruct style configuration\n+    >>> configuration = Phi4MultimodalAudioConfig()\n+    ```\"\"\"\n+\n+    model_type = \"phi4_multimodal_audio\"\n+\n+    def __init__(\n+        self,\n+        hidden_size: int = 1024,\n+        intermediate_size: int = 1536,\n+        num_blocks: int = 24,\n+        num_attention_heads: int = 16,\n+        activation: str = \"swish\",\n+        chunk_size: int = -1,\n+        left_chunk: int = 18,\n+        dropout_rate: float = 0.0,\n+        ext_pw_out_channel: int = 1024,\n+        depthwise_seperable_out_channel: int = 1024,\n+        depthwise_multiplier: int = 1,\n+        kernel_size: int = 3,\n+        conv_activation: str = \"swish\",\n+        input_size: int = 80,\n+        conv_glu_type: str = \"swish\",\n+        time_reduction: int = 8,\n+        bias_max_distance: int = 1000,\n+        bias_symmetric: bool = False,\n+        nemo_activation: str = \"relu\",\n+        nemo_conv_channels: int = 1024,\n+        downsample_rate: int = 1,\n+        initializer_range: float = 0.02,\n+        audio_token_id: int = 200011,\n+        feature_layer: int = -2,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.activation = activation\n+        self.chunk_size = chunk_size\n+        self.left_chunk = left_chunk\n+        self.num_blocks = num_blocks\n+        self.dropout_rate = dropout_rate\n+        self.ext_pw_out_channel = ext_pw_out_channel\n+        self.depthwise_seperable_out_channel = depthwise_seperable_out_channel\n+        self.depthwise_multiplier = depthwise_multiplier\n+        self.kernel_size = kernel_size\n+        self.conv_activation = conv_activation\n+        self.input_size = input_size\n+        self.conv_glu_type = conv_glu_type\n+        self.time_reduction = time_reduction\n+        self.bias_max_distance = bias_max_distance\n+        self.bias_symmetric = bias_symmetric\n+        self.nemo_activation = nemo_activation\n+        self.nemo_conv_channels = nemo_conv_channels\n+        self.downsample_rate = downsample_rate\n+        self.audio_token_id = audio_token_id\n+        self.initializer_range = initializer_range\n+        self.feature_layer = feature_layer\n+\n+        if time_reduction % 2 != 0:\n+            raise ValueError(\"`time_reduction` should be a multiple of 2!\")\n+        length = input_size\n+        for _ in range(int(math.log(time_reduction, 2))):\n+            length = math.floor((length - 1) / 2 + 1)\n+        self.nemo_final_size = length\n+\n+\n+class Phi4MultimodalConfig(Phi3Config):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Phi4MultimodalModel`]. It is used to instantiate a\n+    Phi4Multimodal model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the\n+    [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 200064):\n+            Vocabulary size of the Phi-3 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Phi3Model`].\n+        hidden_size (`int`, *optional*, defaults to 3072):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 8192):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        resid_pdrop (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for mlp outputs.\n+        embd_pdrop (`int`, *optional*, defaults to 0.0):\n+            The dropout ratio for the embeddings.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio after computing the attention scores.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon value used for the RMSNorm.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`. Whether to tie weight embeddings or not.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`dict`, *optional*):\n+            The scaling strategy for the RoPE embeddings. If `None`, no scaling is applied. If a dictionary, it must\n+            contain the following keys: `type`, `short_factor` and `long_factor`. The `type` must be `longrope` and\n+            the `short_factor` and `long_factor` must be lists of numbers with the same length as the hidden size\n+            divided by the number of attention heads divided by 2.\n+        partial_rotary_factor (`float`, *optional*, defaults to `1.0`):\n+            Percentage of the query and keys which will have rotary embedding. Must be between 0.0 and 1.0.\n+        bos_token_id (`int`, *optional*, defaults to 199999):\n+            The id of the \"beginning-of-sequence\" token.\n+        eos_token_id (`int` or `list[int]`, *optional*, defaults to `[199999, 200020]`):\n+            The id of the \"end-of-sequence\" token.\n+        pad_token_id (`int`, *optional*, defaults to 199999):\n+            The id of the padding token.\n+        original_max_position_embeddings (`int`, *optional*, defaults to 4096):\n+            The maximum sequence length that this model was trained with. This is used to determine the size of the\n+            original RoPE embeddings when using long scaling.\n+        sliding_window (`int`, *optional*):\n+            Sliding window attention window size. If `None`, no sliding window is applied.\n+        vision_config (`Phi4MultimodalVisionConfig` or `dict`, *optional*):\n+            The vision config for the underlying image embedding model. If not provided, will default to the configuration\n+            used to instantiate a model similar in architecture as\n+            [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct).\n+        audio_config (`Phi4MultimodalAudioConfig` or `dict`, *optional*):\n+            The audio config for the underlying audio embedding model. If not provided, will default to the configuration\n+            used to instantiate a model similar in architecture as\n+            [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct).\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Phi4MultimodalModel, Phi4MultimodalConfig\n+\n+    >>> # Initializing a Phi4Multimodal style configuration\n+    >>> configuration = Phi4MultimodalConfig.from_pretrained(\"microsoft/Phi-4-multimodal-instruct\")\n+\n+    >>> # Initializing a model from the configuration\n+    >>> model = Phi4MultimodalModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    sub_configs = {\"audio_config\": Phi4MultimodalAudioConfig, \"vision_config\": Phi4MultimodalVisionConfig}\n+\n+    def __init__(\n+        self,\n+        vocab_size=200064,\n+        hidden_size=3072,\n+        intermediate_size=8192,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=8,\n+        resid_pdrop=0.0,\n+        embd_pdrop=0.0,\n+        attention_dropout=0.0,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=131072,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        partial_rotary_factor=1,\n+        bos_token_id=199999,\n+        eos_token_id=[199999, 200020],\n+        pad_token_id=199999,\n+        original_max_position_embeddings=4096,\n+        sliding_window=None,\n+        vision_config=None,\n+        audio_config=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            resid_pdrop=resid_pdrop,\n+            embd_pdrop=embd_pdrop,\n+            attention_dropout=attention_dropout,\n+            hidden_act=hidden_act,\n+            max_position_embeddings=max_position_embeddings,\n+            initializer_range=initializer_range,\n+            rms_norm_eps=rms_norm_eps,\n+            use_cache=use_cache,\n+            tie_word_embeddings=tie_word_embeddings,\n+            rope_theta=rope_theta,\n+            rope_scaling=rope_scaling,\n+            partial_rotary_factor=partial_rotary_factor,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            pad_token_id=pad_token_id,\n+            original_max_position_embeddings=original_max_position_embeddings,\n+            sliding_window=sliding_window,\n+            **kwargs,\n+        )\n+\n+        if isinstance(vision_config, dict):\n+            vision_config = Phi4MultimodalVisionConfig(**vision_config)\n+        elif vision_config is None:\n+            Phi4MultimodalVisionConfig()\n+        self.vision_config = vision_config\n+\n+        if isinstance(audio_config, dict):\n+            audio_config = Phi4MultimodalAudioConfig(**audio_config)\n+        elif vision_config is None:\n+            audio_config = Phi4MultimodalAudioConfig()\n+        self.audio_config = audio_config\n+\n+\n+class Phi4MultimodalVisionMLP(SiglipMLP):\n+    pass\n+\n+\n+def simple_eager_attention_forward(\n+    module: nn.Module,\n+    query_states: torch.Tensor,\n+    key_states: torch.Tensor,\n+    value_states: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Phi4MultimodalVisionAttention(nn.Module):\n+    def __init__(self, config: Phi4MultimodalVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.is_causal = True\n+        self.attention_dropout = config.attention_dropout\n+\n+        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = simple_eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1)\n+        attn_output = self.out_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Phi4MultimodalVisionEncoderLayer(SiglipEncoderLayer):\n+    def __init__(self, config: Phi4MultimodalVisionConfig):\n+        super().__init__(config)\n+        self.self_attn = Phi4MultimodalVisionAttention(config)\n+        self.mlp = Phi4MultimodalVisionMLP(config)\n+\n+\n+class Phi4MultimodalVisionEncoder(SiglipEncoder):\n+    def __init__(self, config: Phi4MultimodalVisionConfig):\n+        super().__init__()\n+        self.layers = nn.ModuleList(\n+            [Phi4MultimodalVisionEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n+        )\n+\n+\n+class Phi4MultimodalVisionPreTrainedModel(SiglipPreTrainedModel):\n+    config_class = Phi4MultimodalVisionConfig\n+    base_model_prefix = \"phi4_vision\"\n+    supports_gradient_checkpointing = True\n+\n+    _no_split_modules = [\"Phi4MultimodalVisionEncoderLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, Phi4MultimodalVisionEmbeddings):\n+            width = (\n+                self.config.hidden_size\n+                if isinstance(self.config, Phi4MultimodalVisionConfig)\n+                else self.config.hidden_size\n+            )\n+            nn.init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n+        elif isinstance(module, nn.Embedding):\n+            default_flax_embed_init(module.weight)\n+        elif isinstance(module, Phi4MultimodalVisionAttention):\n+            nn.init.normal_(module.q_proj.weight)\n+            nn.init.normal_(module.k_proj.weight)\n+            nn.init.normal_(module.v_proj.weight)\n+            nn.init.normal_(module.out_proj.weight)\n+            nn.init.zeros_(module.q_proj.bias)\n+            nn.init.zeros_(module.k_proj.bias)\n+            nn.init.zeros_(module.v_proj.bias)\n+            nn.init.zeros_(module.out_proj.bias)\n+        elif isinstance(module, Phi4MultimodalVisionMLP):\n+            nn.init.normal_(module.fc1.weight)\n+            nn.init.normal_(module.fc2.weight)\n+            nn.init.normal_(module.fc1.bias, std=1e-6)\n+            nn.init.normal_(module.fc2.bias, std=1e-6)\n+        elif isinstance(module, Phi4MultimodalVisionMultiheadAttentionPoolingHead):\n+            nn.init.normal_(module.probe.data)\n+            nn.init.normal_(module.attention.in_proj_weight.data)\n+            nn.init.zeros_(module.attention.in_proj_bias.data)\n+        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n+            lecun_normal_(module.weight)\n+            if module.bias is not None:\n+                nn.init.zeros_(module.bias)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+class Phi4MultimodalVisionEmbeddings(SiglipVisionEmbeddings, nn.Module):\n+    def __init__(self, config: Phi4MultimodalVisionConfig):\n+        nn.Module.__init__()\n+        self.config = config\n+        self.patch_size = config.patch_size\n+        self.num_patches_per_side = config.image_size // self.patch_size\n+\n+        self.patch_embedding = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=config.hidden_size,\n+            kernel_size=self.patch_size,\n+            stride=self.patch_size,\n+            padding=\"valid\",\n+        )\n+        self.position_embedding = nn.Embedding(self.num_patches_per_side**2, config.hidden_size)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor) -> torch.Tensor:\n+        batch_size = pixel_values.size(0)\n+\n+        patch_embeds = self.patch_embedding(pixel_values)\n+        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n+\n+        max_im_h, max_im_w = pixel_values.size(2), pixel_values.size(3)\n+        max_nb_patches_h, max_nb_patches_w = max_im_h // self.patch_size, max_im_w // self.patch_size\n+        boundaries = torch.arange(1 / self.num_patches_per_side, 1.0, 1 / self.num_patches_per_side)\n+        position_ids = torch.full((batch_size, max_nb_patches_h * max_nb_patches_w), fill_value=0)\n+\n+        for batch_idx, p_attn_mask in enumerate(patch_attention_mask):\n+            nb_patches_h = p_attn_mask[:, 0].sum()\n+            nb_patches_w = p_attn_mask[0].sum()\n+\n+            fractional_coords_h = torch.arange(0, 1 - 1e-6, 1 / nb_patches_h)\n+            fractional_coords_w = torch.arange(0, 1 - 1e-6, 1 / nb_patches_w)\n+\n+            bucket_coords_h = torch.bucketize(fractional_coords_h, boundaries, right=True)\n+            bucket_coords_w = torch.bucketize(fractional_coords_w, boundaries, right=True)\n+\n+            pos_ids = (bucket_coords_h[:, None] * self.num_patches_per_side + bucket_coords_w).flatten()\n+            position_ids[batch_idx][p_attn_mask.view(-1).cpu()] = pos_ids\n+\n+        position_ids = position_ids.to(self.position_embedding.weight.device)\n+\n+        embeddings = embeddings + self.position_embedding(position_ids)\n+        return embeddings\n+\n+\n+class Phi4MultimodalVisionMultiheadAttentionPoolingHead(SiglipMultiheadAttentionPoolingHead):\n+    def __init__(self, config: Phi4MultimodalVisionConfig):\n+        super().__init__(config)\n+        self.mlp = Phi4MultimodalVisionMLP(config)\n+\n+    def forward(self, hidden_state, attention_mask):\n+        batch_size = hidden_state.shape[0]\n+        probe = self.probe.repeat(batch_size, 1, 1)\n+\n+        hidden_state = self.attention(\n+            query=probe, key=hidden_state, value=hidden_state, key_padding_mask=~attention_mask\n+        )[0]\n+\n+        residual = hidden_state\n+        hidden_state = self.layernorm(hidden_state)\n+        hidden_state = residual + self.mlp(hidden_state)\n+\n+        return hidden_state[:, 0]\n+\n+\n+class Phi4MultimodalVisionModel(Phi4MultimodalVisionPreTrainedModel):\n+    config_class = Phi4MultimodalVisionConfig\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: Phi4MultimodalVisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embeddings = Phi4MultimodalVisionEmbeddings(config)\n+        self.encoder = Phi4MultimodalVisionEncoder(config)\n+        self.post_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.head = Phi4MultimodalVisionMultiheadAttentionPoolingHead(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.embeddings.patch_embedding\n+\n+    def forward(\n+        self,\n+        pixel_values,\n+        patch_attention_mask: Optional[torch.BoolTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        batch_size = pixel_values.size(0)\n+        if patch_attention_mask is None:\n+            patch_attention_mask = torch.ones(\n+                size=(\n+                    batch_size,\n+                    pixel_values.size(2) // self.config.patch_size,\n+                    pixel_values.size(3) // self.config.patch_size,\n+                ),\n+                dtype=torch.bool,\n+                device=pixel_values.device,\n+            )\n+\n+        hidden_states = self.embeddings(pixel_values=pixel_values, patch_attention_mask=patch_attention_mask)\n+\n+        patch_attention_mask = patch_attention_mask.view(batch_size, -1)\n+        # The call to `_upad_input` in `_flash_attention_forward` is expensive\n+        # So when the `patch_attention_mask` is full of 1s (i.e. attending to the whole sequence),\n+        # avoiding passing the attention_mask, which is equivalent to attending to the full sequence\n+        if not torch.any(~patch_attention_mask):\n+            attention_mask = None\n+        else:\n+            attention_mask = (\n+                _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n+                if not self.config._attn_implementation == \"flash_attention_2\"\n+                else patch_attention_mask\n+            )\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooled_output = self.head(\n+            hidden_state=last_hidden_state,\n+            attention_mask=patch_attention_mask,\n+        )\n+\n+        if not return_dict:\n+            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class Phi4MultimodalImageEmbedding(nn.Module):\n+    \"\"\"Image embedding.\"\"\"\n+\n+    def __init__(self, config: Phi4MultimodalConfig):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = config.vision_config.feature_layer\n+        self.crop_size = config.vision_config.crop_size\n+        self.image_dim_out = config.vision_config.hidden_size\n+\n+        n_patches = config.vision_config.image_size // config.vision_config.patch_size\n+        if n_patches % 2 != 0:\n+            self.img_processor_padding = nn.ReflectionPad2d((0, 1, 0, 1))\n+            n_patches += 1\n+        self.num_img_tokens = (n_patches // 2) ** 2\n+\n+        self.drop = nn.Dropout(config.embd_pdrop)\n+        self.img_processor = Phi4MultimodalVisionModel._from_config(config.vision_config)\n+        self.image_token_compression = nn.AvgPool2d(kernel_size=2, stride=2)\n+        self.img_projection_up = nn.Linear(self.image_dim_out, config.hidden_size)\n+        self.img_projection_down = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.global_img_feature_extensor = nn.Parameter(torch.zeros([1, 1, self.image_dim_out]))\n+        self.sub_img_feature_extensor = nn.Parameter(torch.zeros([1, 1, 1, self.image_dim_out]))\n+\n+    def get_img_features(self, img_embeds: torch.FloatTensor, attention_mask=None) -> torch.FloatTensor:\n+        img_processor_output = self.img_processor(\n+            img_embeds, patch_attention_mask=attention_mask, output_hidden_states=True\n+        )\n+        img_feature = img_processor_output.hidden_states[self.layer_idx]\n+\n+        patch_feature = img_feature\n+        # reshape to 2D tensor\n+        width = int(math.sqrt(patch_feature.size(1)))\n+        patch_feature = patch_feature.view(-1, width, width, patch_feature.size(-1))\n+        # convert to NCHW\n+        patch_feature = patch_feature.permute(0, 3, 1, 2)\n+        if getattr(self, \"img_processor_padding\", None) is not None:\n+            patch_feature = self.img_processor_padding(patch_feature)\n+        patch_feature = self.image_token_compression(patch_feature)\n+        # convert to NHWC\n+        patch_feature = patch_feature.permute(0, 2, 3, 1)\n+        patch_feature = patch_feature.view(-1, patch_feature.size(1) * patch_feature.size(2), patch_feature.size(-1))\n+        return patch_feature\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.Tensor,\n+        image_pixel_values: torch.FloatTensor,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        image_attention_mask: Optional[torch.Tensor] = None,\n+    ) -> torch.FloatTensor:\n+        image_pixel_values = image_pixel_values.to(self.img_processor.embeddings.patch_embedding.weight.dtype)\n+\n+        target_device = self.img_projection_up.bias.device\n+        target_dtype = self.img_projection_up.bias.dtype\n+\n+        batch_size = image_pixel_values.shape[0]\n+\n+        img_features = self.get_img_features(\n+            image_pixel_values.flatten(0, 1),\n+            attention_mask=image_attention_mask.flatten(0, 1).to(dtype=bool, device=target_device),\n+        )\n+        base_feat_size = int(np.sqrt(img_features.shape[1]))\n+        img_features = img_features.view(batch_size, -1, base_feat_size**2, self.image_dim_out)\n+        image_sizes = image_sizes.view(-1, 2)\n+\n+        output_imgs = []\n+        for idx in range(batch_size):\n+            height, width = image_sizes[idx]\n+            height_ratio = height // self.crop_size\n+            width_ratio = width // self.crop_size\n+            area_ratio = height_ratio * width_ratio\n+\n+            global_img = img_features[idx, :1]\n+            global_img = global_img.reshape(1, base_feat_size, base_feat_size, self.image_dim_out).contiguous()\n+            temporary_extensor = self.sub_img_feature_extensor.repeat(1, base_feat_size, 1, 1)\n+            global_img = torch.cat([global_img, temporary_extensor], dim=2).reshape(1, -1, self.image_dim_out)\n+\n+            sub_img = img_features[idx, 1:]\n+            sub_img = sub_img[:area_ratio]\n+            sub_img = (\n+                sub_img.reshape(height_ratio, width_ratio, base_feat_size, base_feat_size, self.image_dim_out)\n+                .transpose(1, 2)\n+                .reshape(1, height_ratio * base_feat_size, width_ratio * base_feat_size, self.image_dim_out)\n+                .contiguous()\n+            )\n+\n+            if image_attention_mask is not None:\n+                reshaped_image_attention_mask = (\n+                    image_attention_mask[idx, 1 : area_ratio + 1, 0::2, 0::2]\n+                    .reshape(height_ratio, width_ratio, base_feat_size, base_feat_size)\n+                    .transpose(1, 2)\n+                    .reshape(1, height_ratio * base_feat_size, width_ratio * base_feat_size)\n+                )\n+                useful_height = int(reshaped_image_attention_mask[0, :, 0].sum().item())\n+                useful_width = int(reshaped_image_attention_mask[0, 0, :].sum().item())\n+                sub_img = sub_img[:, :useful_height, :useful_width]\n+                temporary_extensor = self.sub_img_feature_extensor.repeat(1, useful_height, 1, 1)\n+            else:\n+                temporary_extensor = self.sub_img_feature_extensor.repeat(1, height_ratio * base_feat_size, 1, 1)\n+\n+            sub_img = torch.cat([sub_img, temporary_extensor], dim=2).reshape(1, -1, self.image_dim_out)\n+\n+            # Merge global and sub\n+            output_imgs.append(torch.cat([sub_img, self.global_img_feature_extensor, global_img], dim=1))\n+\n+        img_set_tensor = []\n+        for output_img in output_imgs:\n+            output_img = output_img.to(device=target_device, dtype=target_dtype)\n+            img_feature_proj = self.img_projection_up(output_img)\n+            img_feature_proj = nn.functional.gelu(img_feature_proj)\n+            img_feature_proj = self.img_projection_down(img_feature_proj)\n+            img_set_tensor.append(img_feature_proj)\n+\n+        merged_img_set_tensor = torch.cat(img_set_tensor, dim=1).squeeze(0)\n+        merged_img_set_tensor = merged_img_set_tensor.to(dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n+\n+        with torch.no_grad():\n+            positions_tuple = torch.nonzero(input_ids == self.config.vision_config.image_token_id, as_tuple=True)\n+\n+        # Temporarily disable autocast to avoid issue on bf16 tensors\n+        # Ref: https://github.com/pytorch/pytorch/issues/132715\n+        with torch.autocast(device_type=inputs_embeds.device.type, enabled=False):\n+            image_embeds = inputs_embeds.index_put(\n+                indices=positions_tuple, values=merged_img_set_tensor, accumulate=False\n+            )\n+\n+        image_embeds = self.drop(image_embeds)\n+\n+        return image_embeds\n+\n+\n+########################################################## AUDIO #############################################\n+\n+\n+class Phi4MultimodalAudioMLP(nn.Module):\n+    def __init__(self, config: Phi4MultimodalAudioConfig):\n+        super().__init__()\n+        self.layer_norm = nn.LayerNorm(config.hidden_size)\n+        self.act_fn = ACT2FN[config.activation]\n+        self.gate_up_proj = nn.Linear(config.hidden_size, config.intermediate_size * 2)\n+        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size)\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.layer_norm(hidden_states)\n+        up_states = self.gate_up_proj(hidden_states)\n+        up_states, gate = up_states.chunk(2, dim=-1)\n+        up_states = up_states * self.act_fn(gate)\n+        up_states = self.dropout(up_states)\n+        hidden_states = self.down_proj(up_states)\n+        out = self.dropout(hidden_states)\n+\n+        return out\n+\n+\n+class Phi4MultimodalAudioAttention(nn.Module):\n+    def __init__(self, config: Phi4MultimodalAudioConfig):\n+        super().__init__()\n+        self.config = config\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.dropout_rate\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=True)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        **kwargs,\n+    ):\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = simple_eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, _ = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output\n+\n+\n+class Phi4MultimodalAudioDepthWiseSeperableConv1d(nn.Module):\n+    def __init__(self, config: Phi4MultimodalAudioConfig, padding: int = 0):\n+        super().__init__()\n+        self.dw_conv = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size * config.depthwise_multiplier,\n+            config.kernel_size,\n+            1,\n+            padding=padding,\n+            groups=config.hidden_size,\n+        )\n+        self.pw_conv = nn.Conv1d(\n+            config.hidden_size * config.depthwise_multiplier, config.depthwise_seperable_out_channel, 1, 1, 0\n+        )\n+\n+    def forward(self, hidden_states):\n+        return self.pw_conv(self.dw_conv(hidden_states))\n+\n+\n+class Phi4MultimodalAudioGluPointWiseConv(nn.Module):\n+    def __init__(self, config: Phi4MultimodalAudioConfig):\n+        super().__init__()\n+        self.config = config\n+        self.output_dim = config.ext_pw_out_channel\n+\n+        self.ext_pw_conv_1d = nn.Conv1d(config.hidden_size, config.ext_pw_out_channel * 2, kernel_size=1, stride=1)\n+        self.glu_act = ACT2FN[config.conv_glu_type]\n+        self.b1 = nn.Parameter(torch.zeros(1, config.ext_pw_out_channel, 1))\n+        self.b2 = nn.Parameter(torch.zeros(1, config.ext_pw_out_channel, 1))\n+\n+    def forward(self, hidden_states):\n+        # we assume the input always has the #channel (#dim) in the last dimension of the\n+        # tensor, so need to switch the dimension first for 1D-Conv case\n+        hidden_states = hidden_states.permute([0, 2, 1])\n+        hidden_states = self.ext_pw_conv_1d(hidden_states)\n+        out = hidden_states[:, 0 : self.output_dim, :] + self.b1\n+        out = out * self.glu_act(hidden_states[:, self.output_dim : self.output_dim * 2, :] + self.b2)\n+        return out.permute([0, 2, 1])\n+\n+\n+class Phi4MultimodalAudioConvModule(nn.Module):\n+    def __init__(self, config: Phi4MultimodalAudioConfig):\n+        super().__init__()\n+        self.config = config\n+        self.kernel_size = config.kernel_size\n+\n+        self.layer_norm = nn.LayerNorm(config.hidden_size)\n+        self.glu = Phi4MultimodalAudioGluPointWiseConv(config)\n+        self.dw_sep_conv_1d = Phi4MultimodalAudioDepthWiseSeperableConv1d(config, padding=config.kernel_size - 1)\n+        self.act = ACT2FN[config.conv_activation]\n+        self.ext_pw_conv_1d = nn.Conv1d(config.hidden_size, config.ext_pw_out_channel, kernel_size=1, stride=1)\n+        self.dropout = nn.Dropout(config.dropout_rate)\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = self.glu(self.layer_norm(hidden_states))\n+        hidden_states = self.dw_sep_conv_1d(hidden_states.permute([0, 2, 1]))\n+\n+        if self.kernel_size > 1:\n+            hidden_states = hidden_states[:, :, : -(self.kernel_size - 1)]\n+\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.ext_pw_conv_1d(hidden_states)\n+        out = self.dropout(hidden_states.permute([0, 2, 1]))\n+        return out\n+\n+\n+class Phi4MultimodalAudioConformerEncoderLayer(nn.Module):\n+    def __init__(self, config: Phi4MultimodalAudioConfig):\n+        super().__init__()\n+\n+        self.feed_forward_in = Phi4MultimodalAudioMLP(config)\n+        self.self_attn = Phi4MultimodalAudioAttention(config)\n+        self.conv = Phi4MultimodalAudioConvModule(config)\n+        self.feed_forward_out = Phi4MultimodalAudioMLP(config)\n+        self.layer_norm_att = nn.LayerNorm(config.hidden_size)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+    ):\n+        residual = hidden_states + 0.5 * self.feed_forward_in(hidden_states)\n+        hidden_states = self.layer_norm_att(residual)\n+\n+        hidden_states = residual + self.self_attn(hidden_states, attention_mask)\n+        hidden_states = hidden_states + self.conv(hidden_states)\n+        hidden_states = hidden_states + 0.5 * self.feed_forward_out(hidden_states)\n+\n+        out = self.layer_norm(hidden_states)\n+\n+        return out\n+\n+\n+class Phi4MultimodalAudioNemoConvSubsampling(torch.nn.Module):\n+    def __init__(self, config: Phi4MultimodalAudioConfig):\n+        super().__init__()\n+        self.subsampling_factor = config.time_reduction\n+        self.sampling_num = int(math.log(self.subsampling_factor, 2))\n+        self.act_fn = ACT2FN[config.nemo_activation]\n+        conv_channels = config.nemo_conv_channels\n+\n+        layers = [\n+            nn.Conv2d(1, conv_channels, kernel_size=3, stride=2, padding=1),\n+            self.act_fn,\n+        ]\n+        for _ in range(self.sampling_num - 1):\n+            layers.extend(\n+                [\n+                    nn.Conv2d(conv_channels, conv_channels, kernel_size=3, stride=2, padding=1, groups=conv_channels),\n+                    nn.Conv2d(conv_channels, conv_channels, kernel_size=1, stride=1, padding=0, groups=1),\n+                    self.act_fn,\n+                ]\n+            )\n+\n+        # Aggregate the layers\n+        self.conv = torch.nn.Sequential(*layers)\n+        self.out = torch.nn.Linear(conv_channels * config.nemo_final_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor]):\n+        # Unsqueeze Channel Axis\n+        hidden_states = hidden_states.unsqueeze(1)\n+        hidden_states = self.conv(hidden_states)\n+\n+        # Flatten Channel and Frequency Axes\n+        b, _, t, _ = hidden_states.size()\n+        hidden_states = self.out(hidden_states.transpose(1, 2).reshape(b, t, -1))\n+\n+        if mask is None:\n+            return hidden_states, None\n+\n+        max_audio_length = hidden_states.shape[1]\n+        feature_lens = mask.sum(1)\n+        padding_length = torch.ceil(feature_lens / self.subsampling_factor)\n+        arange_ = torch.arange(0, max_audio_length, device=hidden_states.device)\n+        pad_mask = arange_.expand(padding_length.size(0), -1) < padding_length.unsqueeze(1)\n+        return hidden_states, pad_mask.unsqueeze(1)\n+\n+\n+class Phi4MultimodalAudioRelativeAttentionBias(nn.Module):\n+    def __init__(self, config: Phi4MultimodalAudioConfig):\n+        super().__init__()\n+\n+        self.max_distance = config.bias_max_distance\n+        self.symmetric = config.bias_symmetric\n+        self.num_buckets = self.max_distance\n+        if not config.bias_symmetric:\n+            self.num_buckets *= 2\n+        self.bias_values = nn.Embedding(self.num_buckets, config.num_attention_heads)\n+\n+    def forward(self, x):\n+        # instantiate bias compatible with shape of x\n+        max_pos = x.size(1)\n+        context_position = torch.arange(max_pos, device=x.device, dtype=torch.long)[:, None]\n+        memory_position = torch.arange(max_pos, device=x.device, dtype=torch.long)[None, :]\n+        relative_position = memory_position - context_position\n+        # clipping to a maximum distance using ops that play well with ONNX export\n+        relative_position = relative_position.masked_fill(relative_position < -self.max_distance, -self.max_distance)\n+        relative_position = relative_position.masked_fill(\n+            relative_position > self.max_distance - 1, self.max_distance - 1\n+        )\n+\n+        # mapping from relative position to index in the bias parameter\n+        bias_idx = relative_position\n+        bias_idx = bias_idx.abs() if self.symmetric else bias_idx + self.num_buckets // 2\n+\n+        att_bias = self.bias_values(bias_idx)\n+        att_bias = att_bias.permute(2, 0, 1).unsqueeze(0)\n+\n+        return att_bias\n+\n+\n+class Phi4MultimodalAudioMeanVarianceNormLayer(nn.Module):\n+    def __init__(self, config: Phi4MultimodalAudioConfig):\n+        super().__init__()\n+        self.register_buffer(\"global_mean\", torch.zeros(config.input_size))\n+        self.register_buffer(\"global_invstd\", torch.ones(config.input_size))\n+\n+    def forward(self, x):\n+        return (x - self.global_mean) * self.global_invstd\n+\n+\n+class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n+    config_class = Phi4MultimodalAudioConfig\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Phi4MultimodalAudioConformerEncoderLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+class Phi4MultimodalAudioModel(Phi4MultimodalAudioPreTrainedModel):\n+    def __init__(self, config: Phi4MultimodalAudioConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.encoder_embedding = Phi4MultimodalAudioMeanVarianceNormLayer(config)\n+        self.embed = Phi4MultimodalAudioNemoConvSubsampling(config)\n+        self.relative_attention_bias_layer = Phi4MultimodalAudioRelativeAttentionBias(config)\n+        self.encoders = nn.ModuleList(\n+            [Phi4MultimodalAudioConformerEncoderLayer(config) for _ in range(config.num_blocks)]\n+        )\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def _streaming_mask(self, seq_len, batch_size, chunk_size, left_chunk):\n+        # Create mask matrix for streaming\n+        # S stores start index. if chunksize is 18, s is [0,18,36,....]\n+        chunk_start_idx = np.arange(0, seq_len, chunk_size)\n+        # avoid randomness when run evaluation or decoding\n+        if self.training and np.random.rand() > 0.5:\n+            # Either first or last chunk is not complete.\n+            # If only the last one is not complete, EOS is not effective\n+            chunk_start_idx = seq_len - chunk_start_idx\n+            chunk_start_idx = chunk_start_idx[::-1]\n+            chunk_start_idx = chunk_start_idx[:-1]\n+            chunk_start_idx = np.insert(chunk_start_idx, 0, 0)\n+\n+        enc_streaming_mask = (\n+            adaptive_enc_mask(seq_len, chunk_start_idx, left_window=left_chunk)\n+            .unsqueeze(0)\n+            .expand([batch_size, -1, -1])\n+        )\n+        return enc_streaming_mask\n+\n+    def forward_embeddings(self, hidden_states, masks):\n+        \"\"\"Forwarding the inputs through the top embedding layers\"\"\"\n+        seq_len = math.ceil(hidden_states.shape[1] / self.config.time_reduction)\n+        if seq_len <= 0:\n+            raise ValueError(\n+                f\"The squence length after time reduction is invalid: {seq_len}. Your input feature is too short.\"\n+            )\n+\n+        batch_size = hidden_states.shape[0]\n+\n+        enc_streaming_mask = self._streaming_mask(seq_len, batch_size, self.config.chunk_size, self.config.left_chunk)\n+        enc_streaming_mask = enc_streaming_mask.to(hidden_states.device)\n+\n+        hidden_states, masks = self.embed(hidden_states, masks)\n+\n+        streaming_mask = enc_streaming_mask\n+        if streaming_mask is not None and masks is not None:\n+            hs_mask = masks & streaming_mask\n+        elif masks is not None:\n+            hs_mask = masks\n+        else:\n+            hs_mask = streaming_mask\n+\n+        return hidden_states, hs_mask, masks\n+\n+    def calculate_hs_mask(self, hidden_states, device, mask):\n+        max_audio_length = hidden_states.shape[1]\n+        batch_size = hidden_states.shape[0]\n+        enc_streaming_mask = self._streaming_mask(\n+            max_audio_length, batch_size, self.config.chunk_size, self.config.left_chunk\n+        )\n+        enc_streaming_mask = enc_streaming_mask.to(device)\n+        if mask is None:\n+            return enc_streaming_mask\n+\n+        feature_lens = mask.sum(1)\n+        padding_length = feature_lens\n+        pad_mask = torch.arange(0, max_audio_length, device=device).expand(\n+            padding_length.size(0), -1\n+        ) < padding_length.unsqueeze(1)\n+        pad_mask = pad_mask.unsqueeze(1)\n+        pad_mask = pad_mask & enc_streaming_mask\n+        return pad_mask\n+\n+    def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor]):\n+        hidden_states = self.encoder_embedding(hidden_states)\n+        hidden_states, hs_mask, mask = self.forward_embeddings(hidden_states, mask)\n+\n+        unfolded = False\n+        bs, seq_len, _ = hidden_states.shape\n+        max_seq_len = 500  # maxium position for absolute positional encoding\n+        if seq_len > max_seq_len:\n+            # audio sequence is longer than max_seq_len, unfold it into chunks of max_seq_len\n+            unfolded = True\n+            # the unfold op will drop residual frames, pad it to the multiple of max_seq_len\n+            if seq_len % max_seq_len > 0:\n+                chunk_pad_size = max_seq_len - (seq_len % max_seq_len)\n+            else:\n+                chunk_pad_size = 0\n+            if chunk_pad_size > 0:\n+                hidden_states_pad = F.pad(hidden_states, (0, 0, 0, chunk_pad_size), \"constant\", 0)\n+                hidden_states = hidden_states_pad.to(hidden_states.device)\n+\n+            hidden_states = unfold_tensor(hidden_states, max_seq_len)\n+            masks_unfold = None\n+            if mask is not None:\n+                # revise hs_mask here because the previous calculated hs_mask did not consider extra pad\n+                subsampled_pad_mask = mask.squeeze(1)  # [bz, subsampled_unmask_seq_len]\n+                extra_padded_subsamlped_pad_mask = F.pad(\n+                    subsampled_pad_mask, (0, chunk_pad_size), \"constant\", False\n+                )  # extra padding to the pad mask\n+                extra_padded_subsamlped_pad_mask = extra_padded_subsamlped_pad_mask.unsqueeze(-1).float()\n+                masks_unfold = unfold_tensor(\n+                    extra_padded_subsamlped_pad_mask, max_seq_len\n+                )  # unfold the pad mask like we did to the input tensor\n+                masks_unfold = masks_unfold.squeeze(-1).bool()  # unfold op does not support bool tensor\n+            hs_mask = self.calculate_hs_mask(\n+                hidden_states, hidden_states.device, masks_unfold\n+            )  # calculate hs_mask based on the unfolded pad mask\n+\n+        relative_attention_bias = self.relative_attention_bias_layer(hidden_states)\n+        attention_mask = hs_mask.unsqueeze(1) + relative_attention_bias\n+\n+        for layer in self.encoders:\n+            if self.gradient_checkpointing and self.training:\n+                hidden_states = self._gradient_checkpointing_func(\n+                    layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                )\n+            else:\n+                hidden_states = layer(hidden_states, attention_mask)\n+\n+        if unfolded:\n+            embed_dim = hidden_states.shape[-1]\n+            hidden_states = hidden_states.reshape(bs, -1, embed_dim)\n+            # if we ever padded before unfolding, we need to remove the padding\n+            if chunk_pad_size > 0:\n+                hidden_states = hidden_states[:, :-chunk_pad_size, :]\n+\n+        return hidden_states\n+\n+\n+def unfold_tensor(tensor, max_seq_len):\n+    \"\"\"\n+    For a given tensor with shape of (N, T, D), if sequence length T is longer than max_seq_len,\n+    this function unfold it to a (NT', max_seq_len, D) where T' is T // max_seq_len.\n+    Args:\n+        tensor: N, T, D\n+    \"\"\"\n+    _, _, D = tensor.shape\n+    tensor = tensor.transpose(-1, -2)\n+    # N x D x 1 x T => N x (D x max_seq_len) x T'\n+    tensor = F.unfold(tensor[..., None, :], kernel_size=(1, max_seq_len), stride=(1, max_seq_len))\n+\n+    new_bsz, _, slen = tensor.shape\n+    tensor = tensor.view(new_bsz, -1, max_seq_len, slen)\n+    tensor = tensor.permute(0, 3, 2, 1)\n+    tensor = tensor.view(-1, max_seq_len, D).contiguous()\n+    return tensor\n+\n+\n+def adaptive_enc_mask(x_len, chunk_start_idx, left_window=0, right_window=0):\n+    \"\"\"\n+    The function is very important for Transformer Transducer Streaming mode\n+    Args:\n+        xs_len (int): sequence length\n+        chunk_start_idx (list): first idx of each chunk, such as [0,18,36,48]. It also supports adaptive chunk size [0,10,15,45]\n+        left_window (int): how many left chunks can be seen\n+        right_window (int): how many right chunks can be seen. It is used for chunk overlap model.\n+        Returns:\n+            mask (torch.Tensor): a mask tensor for streaming model\n+    \"\"\"\n+    chunk_start_idx = torch.Tensor(chunk_start_idx).long()\n+    start_pad = torch.nn.functional.pad(\n+        chunk_start_idx, (1, 0)\n+    )  # append 0 to the beginning, so it becomes [0, 0, 18, 36, 48]\n+    end_pad = torch.nn.functional.pad(\n+        chunk_start_idx, (0, 1), value=x_len\n+    )  # append x_len to the end, so it becomes [0,18,36,48, x_len]\n+    seq_range = torch.arange(0, x_len).unsqueeze(-1)\n+    idx = ((seq_range < end_pad) & (seq_range >= start_pad)).nonzero()[:, 1]\n+    seq_range_expand = torch.arange(0, x_len).unsqueeze(0).expand(x_len, -1)\n+    idx_left = idx - left_window\n+    idx_left[idx_left < 0] = 0\n+    boundary_left = start_pad[idx_left]\n+    mask_left = seq_range_expand >= boundary_left.unsqueeze(-1)\n+    idx_right = idx + right_window\n+    idx_right[idx_right > len(chunk_start_idx)] = len(chunk_start_idx)\n+    boundary_right = end_pad[idx_right]\n+    mask_right = seq_range_expand < boundary_right.unsqueeze(-1)\n+    return mask_left & mask_right\n+\n+\n+class Phi4MultimodalAudioEmbedding(nn.Module):\n+    def __init__(self, config: Phi4MultimodalConfig):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = config.audio_config.feature_layer\n+\n+        self.drop = nn.Dropout(config.embd_pdrop)\n+        self.encoder = Phi4MultimodalAudioModel._from_config(config.audio_config)\n+        self.up_proj_for_speech = nn.Linear(\n+            config.audio_config.hidden_size * config.audio_config.downsample_rate, config.hidden_size\n+        )\n+        self.down_proj_for_speech = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.up_proj_for_vision_speech = nn.Linear(\n+            config.audio_config.hidden_size * config.audio_config.downsample_rate, config.hidden_size\n+        )\n+        self.down_proj_for_vision_speech = nn.Linear(config.hidden_size, config.hidden_size)\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.Tensor,\n+        audio_input_features: torch.FloatTensor,\n+        audio_embed_sizes=None,\n+        audio_attention_mask=None,\n+        audio_projection_mode=\"speech\",\n+    ) -> torch.FloatTensor:\n+        with torch.no_grad():\n+            positions_tuple = torch.nonzero(input_ids == self.config.audio_config.audio_token_id, as_tuple=True)\n+\n+        up_proj = self.up_proj_for_speech if audio_projection_mode == \"speech\" else self.up_proj_for_vision_speech\n+        down_proj = (\n+            self.down_proj_for_speech if audio_projection_mode == \"speech\" else self.down_proj_for_vision_speech\n+        )\n+\n+        target_device = up_proj.bias.device\n+        target_dtype = up_proj.bias.dtype\n+\n+        audio_input_features = audio_input_features.to(device=target_device, dtype=target_dtype)\n+\n+        audio_encoder_hidden_states = self.encoder(audio_input_features, audio_attention_mask)\n+        audio_encoder_hidden_states = up_proj(audio_encoder_hidden_states)\n+        audio_encoder_hidden_states = nn.functional.gelu(audio_encoder_hidden_states)\n+        audio_embeds = down_proj(audio_encoder_hidden_states)\n+\n+        merged_audio_embeds = torch.cat(\n+            [audio_embeds[i, : audio_embed_sizes[i], :] for i in range(len(audio_embed_sizes))], dim=0\n+        )\n+        merged_audio_embeds = merged_audio_embeds.to(dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n+        # Temporarily disable autocast to avoid issue on bf16 tensors\n+        # Ref: https://github.com/pytorch/pytorch/issues/132715\n+        with torch.autocast(device_type=inputs_embeds.device.type, enabled=False):\n+            audio_embeds = inputs_embeds.index_put(\n+                indices=positions_tuple, values=merged_audio_embeds, accumulate=False\n+            )\n+\n+        audio_embeds = self.drop(audio_embeds)\n+\n+        return audio_embeds\n+\n+\n+#################################################### TEXT ####################################################\n+\n+\n+class Phi4MultimodalRMSNorm(Phi3RMSNorm):\n+    pass\n+\n+\n+class Phi4MultimodalDecoderLayer(Phi3DecoderLayer):\n+    pass\n+\n+\n+class Phi4MultimodalFeatureEmbedding(nn.Module):\n+    \"\"\"Image-audio embedding.\"\"\"\n+\n+    def __init__(self, config: Phi4MultimodalConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.image_token_id = config.vision_config.image_token_id\n+        self.audio_token_id = config.audio_config.audio_token_id\n+        self.image_embed = Phi4MultimodalImageEmbedding(config)\n+        self.audio_embed = Phi4MultimodalAudioEmbedding(config)\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.Tensor,\n+        image_pixel_values: Optional[torch.FloatTensor] = None,\n+        audio_input_features: Optional[torch.FloatTensor] = None,\n+        image_sizes=None,\n+        image_attention_mask=None,\n+        audio_embed_sizes=None,\n+        audio_attention_mask=None,\n+    ) -> torch.FloatTensor:\n+        with torch.no_grad():\n+            image_position_mask = (input_ids == self.config.vision_config.image_token_id).unsqueeze(-1)\n+            non_image_position_mask = ~image_position_mask\n+\n+        image_embeds = None\n+        audio_embeds = None\n+        if image_pixel_values is not None and (input_ids == self.image_token_id).any():\n+            image_embeds = self.image_embed(\n+                input_ids,\n+                inputs_embeds,\n+                image_pixel_values=image_pixel_values,\n+                image_sizes=image_sizes,\n+                image_attention_mask=image_attention_mask,\n+            )\n+        if audio_input_features is not None and (input_ids == self.audio_token_id).any():\n+            audio_projection_mode = \"vision\" if image_pixel_values is not None else \"speech\"\n+            audio_embeds = self.audio_embed(\n+                input_ids,\n+                inputs_embeds,\n+                audio_input_features=audio_input_features,\n+                audio_embed_sizes=audio_embed_sizes,\n+                audio_attention_mask=audio_attention_mask,\n+                audio_projection_mode=audio_projection_mode,\n+            )\n+\n+        # merge image and audio\n+        if image_embeds is not None and audio_embeds is not None:\n+            inputs_embeds = image_embeds * image_position_mask + audio_embeds * non_image_position_mask\n+        elif image_embeds is not None:\n+            inputs_embeds = image_embeds\n+        elif audio_embeds is not None:\n+            inputs_embeds = audio_embeds\n+\n+        return inputs_embeds\n+\n+\n+PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding indices in `input_values`. Mask values selected in `[0, 1]`:\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+            [What are attention masks?](../glossary#attention-mask)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache`)`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+            See our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        image_pixel_values (`torch.FloatTensor`, *optional*):\n+            If the input contains images, these correspond to the pixel values after transformations (as returned by\n+            the Processor)\n+        image_sizes (`torch.LongTensor`, *optional*):\n+            If the input contains images, these correspond to size of each image.\n+        image_attention_mask (`torch.LongTensor`, *optional*):\n+            Attention mask for the images.\n+        audio_input_features (`torch.FloatTensor`, *optional*):\n+            If the input contains audio samples, these correspond to the values after transformation (as returned by\n+            the Processor).\n+        audio_embed_sizes (`torch.Tensor`, *optional*):\n+            Size of the audio inputs.\n+        audio_attention_mask (`torch.Tensor, *optional*):\n+            Attention mask for the audio inputs.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+class Phi4MultimodalModel(Phi3Model, nn.Module):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Phi4MultimodalMMDecoderLayer`]\n+    Args:\n+        config: Phi4MultimodalMMConfig\n+    \"\"\"\n+\n+    def __init__(self, config: Phi4MultimodalConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.embed_dropout = nn.Dropout(config.embd_pdrop)\n+\n+        self.embed_tokens_extend = Phi4MultimodalFeatureEmbedding(config)\n+\n+        self.layers = nn.ModuleList(\n+            [Phi4MultimodalDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Phi4MultimodalRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        image_pixel_values: Optional[torch.FloatTensor] = None,\n+        image_sizes: Optional[torch.LongTensor] = None,\n+        image_attention_mask=None,\n+        audio_input_features: Optional[torch.FloatTensor] = None,\n+        audio_embed_sizes=None,\n+        audio_attention_mask=None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = self.embed_tokens_extend(\n+                input_ids,\n+                inputs_embeds,\n+                image_pixel_values=image_pixel_values,\n+                audio_input_features=audio_input_features,\n+                image_sizes=image_sizes,\n+                image_attention_mask=image_attention_mask,\n+                audio_embed_sizes=audio_embed_sizes,\n+                audio_attention_mask=audio_attention_mask,\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class Phi4MultimodalForCausalLM(Phi3ForCausalLM, nn.Module):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Phi4MultimodalModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=Phi4MultimodalConfig)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        image_pixel_values: Optional[torch.FloatTensor] = None,\n+        image_sizes: Optional[torch.LongTensor] = None,\n+        image_attention_mask=None,\n+        audio_input_features: Optional[torch.FloatTensor] = None,\n+        audio_embed_sizes=None,\n+        audio_attention_mask=None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+        Returns:\n+\n+        Example:\n+        ```python\n+        >>> from transformers import AutoTokenizer, Phi4MultimodalForCausalLM\n+        >>> model = Phi4MultimodalForCausalLM.from_pretrained(\"TBA\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"TBA\")\n+        >>> prompt = \"This is an example script .\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        'This is an example script .\\n Certainly! Below is a sample script that demonstrates a simple task, such as calculating the sum'\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            image_pixel_values=image_pixel_values,\n+            image_sizes=image_sizes,\n+            image_attention_mask=image_attention_mask,\n+            audio_input_features=audio_input_features,\n+            audio_embed_sizes=audio_embed_sizes,\n+            audio_attention_mask=audio_attention_mask,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        image_pixel_values=None,\n+        image_sizes=None,\n+        image_attention_mask=None,\n+        audio_input_features=None,\n+        audio_embed_sizes=None,\n+        audio_attention_mask=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        logits_to_keep=0,\n+        **kwargs,\n+    ):\n+        # Overwritten -- this model may need to switch between short and long rope, invalidating the cache in the\n+        # process\n+\n+        # When the first time input length reached long and short factor switching point, enforce re-compute cache\n+        # It will cause downside of slower at this single token position, however, better than current failure.\n+        if (\n+            past_key_values\n+            and self.config.rope_scaling\n+            and input_ids.shape[1] >= self.config.original_max_position_embeddings + 1\n+        ):\n+            past_length = cache_position[0]\n+            if past_length <= self.config.original_max_position_embeddings:\n+                past_key_values = None\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids=input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            image_pixel_values=image_pixel_values,\n+            image_sizes=image_sizes,\n+            image_attention_mask=image_attention_mask,\n+            audio_input_features=audio_input_features,\n+            audio_embed_sizes=audio_embed_sizes,\n+            audio_attention_mask=audio_attention_mask,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+        return model_inputs\n+\n+\n+__all__ = [\n+    \"Phi4MultimodalAudioPreTrainedModel\",\n+    \"Phi4MultimodalAudioModel\",\n+    \"Phi4MultimodalVisionPreTrainedModel\",\n+    \"Phi4MultimodalVisionModel\",\n+    \"Phi4MultimodalPreTrainedModel\",  # noqa\n+    \"Phi4MultimodalModel\",\n+    \"Phi4MultimodalForCausalLM\",\n+    \"Phi4MultimodalVisionConfig\",\n+    \"Phi4MultimodalAudioConfig\",\n+    \"Phi4MultimodalConfig\",\n+]"
        },
        {
            "sha": "d60275542f7e9c364fa8496b2493decfc962c04a",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "added",
            "additions": 194,
            "deletions": 0,
            "changes": 194,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -0,0 +1,194 @@\n+# Copyright 2025 Microsoft and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+Processor class for Phi4Multimodal\n+\"\"\"\n+\n+import re\n+from typing import List, Optional, Union\n+\n+from ...audio_utils import AudioInput\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import TextInput\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Phi4MultimodalProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"audio_kwargs\": {\n+            \"device\": \"cpu\",\n+        },\n+    }\n+\n+\n+class Phi4MultimodalProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Phi4Multimodal processor which raps an image processor, a audio processor, and a GPT tokenizer into a single processor.\n+\n+    [`Phi4MultimodalProcessor`] offers all the functionalities of [`Phi4MultimodalImageProcessorFast`] and [`GPT2Tokenizer`]. See the\n+    [`~Phi4MultimodalProcessor.__call__`] and [`~Phi4MultimodalProcessor.decode`] for more information.\n+\n+    Args:\n+        image_processor (`Phi4MultimodalImageProcessorFast`):\n+            The image processor to use for images.\n+        audio_processor (`Phi4MultimodalFeatureExtractor`):\n+            The audio processor to use for audio inputs.\n+        tokenizer (`GPT2TokenizerFast`):\n+            The tokenizer to use for text.\n+        fake_image_token_pattern (`str`, *optional*, defaults to `r\"<\\|image_\\d+\\|>\"`):\n+            The fake image token pattern.\n+        fake_audio_token_pattern (`str`, *optional*, defaults to `r\"<\\|audio_\\d+\\|>\"`):\n+            The fake audio token pattern.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"audio_processor\", \"tokenizer\"]\n+    tokenizer_class = \"GPT2TokenizerFast\"\n+    image_processor_class = \"Phi4MultimodalImageProcessorFast\"\n+    audio_processor_class = \"Phi4MultimodalFeatureExtractor\"\n+    valid_kwargs = [\"chat_template\", \"fake_image_token_pattern\", \"fake_audio_token_pattern\"]\n+\n+    def __init__(\n+        self,\n+        image_processor,\n+        audio_processor,\n+        tokenizer,\n+        fake_image_token_pattern: str = r\"<\\|image_\\d+\\|>\",\n+        fake_audio_token_pattern: str = r\"<\\|audio_\\d+\\|>\",\n+        **kwargs,\n+    ):\n+        super().__init__(image_processor, audio_processor, tokenizer, **kwargs)\n+        self.fake_image_token_pattern = fake_image_token_pattern\n+        self.fake_audio_token_pattern = fake_audio_token_pattern\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, List[TextInput]],\n+        images: Optional[ImageInput] = None,\n+        audios: Optional[AudioInput] = None,\n+        **kwargs: Unpack[ProcessingKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forards the `text`\n+        and `kwargs` arguments to GPT2Tokenizer's [`~GPT2Tokenizer.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        Phi4MultimodalImageProcessorFast's [`~Phi4MultimodalImageProcessorFast.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            audios (`List[Union[np.ndarray, torch.Tensor]]`):\n+                List of the audios to be prepared.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n+            - **input_image_embeds** -- Pixel values to be fed to a model.\n+            - **image_sizes** -- List of tuples specifying the size of each image in `input_image_embeds`.\n+            - **image_attention_mask** -- List of attention masks for each image in `input_image_embeds`.\n+            - **input_audio_embeds** -- Audio embeddings to be fed to a model.\n+            - **audio_embed_sizes** -- List of integers specifying the size of each audio in `input_audio_embeds`.\n+        \"\"\"\n+\n+        output_kwargs = self._merge_kwargs(Phi4MultimodalProcessorKwargs, self.tokenizer.init_kwargs, **kwargs)\n+        image_kwargs = output_kwargs[\"images_kwargs\"]\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+        text_kwargs = output_kwargs[\"text_kwargs\"]\n+\n+        image_inputs = self.image_processor(images, **image_kwargs) if images is not None else {}\n+        audio_inputs = self.audio_processor(audios, **audio_kwargs) if audios is not None else {}\n+\n+        # We pop here for images as we don't need it later\n+        num_img_tokens = image_inputs.pop(\"num_img_tokens\", [])\n+        audio_embed_sizes = audio_inputs.get(\"audio_embed_sizes\", [])\n+\n+        # Replace certain special tokens for compatibility\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        image_token = self.tokenizer.image_token\n+        audio_token = self.tokenizer.audio_token\n+        processed_text = [re.sub(self.fake_image_token_pattern, image_token, t) for t in text]\n+        processed_text = [re.sub(self.fake_audio_token_pattern, audio_token, t) for t in processed_text]\n+\n+        # Check that the number of special tokens is sound\n+        concatenated_prompt = \"\".join(processed_text)\n+        if concatenated_prompt.count(self.tokenizer.image_token) != len(num_img_tokens):\n+            raise ValueError(\n+                \"You should add as much image tokens `<|image_i|>` in your prompt as you pass `images` to the processor\"\n+            )\n+        if concatenated_prompt.count(self.tokenizer.audio_token) != len(audio_embed_sizes):\n+            raise ValueError(\n+                \"You should add as much audio tokens `<|audio_i|>` in your prompt as you pass `audios` to the processor\"\n+            )\n+\n+        # Add appropriate number of image/audio tokens (note that the count of replacement is dynamic)\n+        image_count_iter = iter(num_img_tokens)\n+        audio_count_iter = iter(audio_embed_sizes)\n+        processed_text = [\n+            re.sub(re.escape(image_token), lambda _: image_token * next(image_count_iter), t) for t in processed_text\n+        ]\n+        processed_text = [\n+            re.sub(re.escape(audio_token), lambda _: audio_token * next(audio_count_iter), t) for t in processed_text\n+        ]\n+\n+        text_inputs = self.tokenizer(processed_text, **text_kwargs)\n+\n+        # prepare batch feature\n+        data = {\n+            **text_inputs,\n+            **image_inputs,\n+            **audio_inputs,\n+        }\n+\n+        return BatchFeature(data=data)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to GPT2Tokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to GPT2Tokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        audio_processor_input_names = self.audio_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names + audio_processor_input_names))\n+\n+\n+__all__ = [\"Phi4MultimodalProcessor\"]"
        },
        {
            "sha": "a7051cffca81a9f419b71309b63ed13347d31d35",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -7746,6 +7746,55 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Phi4MultimodalAudioModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Phi4MultimodalAudioPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Phi4MultimodalForCausalLM(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Phi4MultimodalModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Phi4MultimodalPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Phi4MultimodalVisionModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Phi4MultimodalVisionPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class PhimoeForCausalLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "50314fc55e9ef7056d309f6809388532bb3fdf6f",
            "filename": "src/transformers/utils/dummy_torchvision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -93,6 +93,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torchvision\"])\n \n \n+class Phi4MultimodalImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"torchvision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchvision\"])\n+\n+\n class PixtralImageProcessorFast(metaclass=DummyObject):\n     _backends = [\"torchvision\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/phi4_multimodal/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/tests%2Fmodels%2Fphi4_multimodal%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/tests%2Fmodels%2Fphi4_multimodal%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2F__init__.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf"
        },
        {
            "sha": "737e712a34ff91c01aa4abc2a8d9f9b6f42edf17",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "added",
            "additions": 405,
            "deletions": 0,
            "changes": 405,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -0,0 +1,405 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import tempfile\n+import unittest\n+\n+import requests\n+from parameterized import parameterized\n+\n+from transformers import (\n+    AutoModelForCausalLM,\n+    AutoProcessor,\n+    GenerationConfig,\n+    Phi4MultimodalAudioConfig,\n+    Phi4MultimodalConfig,\n+    Phi4MultimodalForCausalLM,\n+    Phi4MultimodalModel,\n+    Phi4MultimodalVisionConfig,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import (\n+    require_soundfile,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_soundfile_available\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+if is_soundfile_available():\n+    import soundfile\n+\n+\n+class Phi4MultimodalModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        seq_length=12,\n+        image_seq_length=275,\n+        audio_seq_length=8,\n+        is_training=True,\n+        num_hidden_layers=2,\n+        vocab_size=49,\n+        hidden_size=32,\n+        intermediate_size=64,\n+        num_attention_heads=8,\n+        num_key_value_heads=4,\n+        bos_token_id=0,\n+        eos_token_id=0,\n+        pad_token_id=0,\n+        image_token_id=1,\n+        audio_token_id=2,\n+        image_size=16,\n+        audio_size=12,\n+        audio_config=Phi4MultimodalAudioConfig(\n+            num_blocks=2,\n+            hidden_size=32,\n+            num_attention_heads=8,\n+            intermediate_size=48,\n+            depthwise_seperable_out_channel=128,\n+            nemo_conv_channels=128,\n+        ),\n+        vision_config=Phi4MultimodalVisionConfig(\n+            num_hidden_layers=2,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            num_attention_heads=8,\n+            crop_size=16,\n+        ),\n+    ):\n+        self.parent = parent\n+        self.num_hidden_layers = num_hidden_layers\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.bos_token_id = bos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.eos_token_id = eos_token_id\n+        self.image_token_id = image_token_id\n+        self.audio_token_id = audio_token_id\n+        self.audio_config = audio_config\n+        self.vision_config = vision_config\n+\n+        self.is_training = is_training\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length + image_seq_length + audio_seq_length\n+        self.image_seq_length = image_seq_length\n+        self.audio_seq_length = audio_seq_length\n+        self.image_size = image_size\n+        self.audio_size = audio_size\n+        self.num_channels = 3\n+\n+    def get_config(self):\n+        return Phi4MultimodalConfig(\n+            num_hidden_layers=self.num_hidden_layers,\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            intermediate_size=self.intermediate_size,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+            vision_config=self.vision_config,\n+            audio_config=self.audio_config,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        # The shapes corresponds to the inputs for image of size 16x16\n+        image_pixel_values = floats_tensor([self.batch_size, 2, self.num_channels, self.image_size, self.image_size])\n+        image_attention_mask = torch.ones(self.batch_size, 2, 1, 1)\n+        image_sizes = torch.tensor(\n+            [[self.image_size, self.image_size]] * self.batch_size, dtype=torch.long, device=torch_device\n+        )\n+\n+        # Feature sizes returned by an audio of size 10000\n+        audio_input_features = floats_tensor([self.batch_size, 61, 80])\n+        audio_embed_sizes = torch.tensor([self.audio_seq_length] * self.batch_size, dtype=torch.long)\n+\n+        input_ids[input_ids == self.pad_token_id] = self.pad_token_id + 1  # random value but not pad token\n+        input_ids[-1, 0] = self.pad_token_id  # mask the last text token\n+        input_ids[:, -self.image_seq_length - self.audio_seq_length : -self.audio_seq_length] = self.image_token_id\n+        input_ids[:, -self.audio_seq_length :] = self.audio_token_id\n+\n+        attention_mask = torch.ones_like(input_ids)\n+        attention_mask[-1, 0] = 0  # mask the last text token\n+        config = self.get_config()\n+\n+        return (\n+            config,\n+            input_ids,\n+            attention_mask,\n+            image_pixel_values,\n+            image_attention_mask,\n+            image_sizes,\n+            audio_input_features,\n+            audio_embed_sizes,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        (\n+            config,\n+            input_ids,\n+            attention_mask,\n+            image_pixel_values,\n+            image_attention_mask,\n+            image_sizes,\n+            audio_input_features,\n+            audio_embed_sizes,\n+        ) = self.prepare_config_and_inputs()\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"image_pixel_values\": image_pixel_values,\n+            \"image_attention_mask\": image_attention_mask,\n+            \"image_sizes\": image_sizes,\n+            \"audio_input_features\": audio_input_features,\n+            \"audio_embed_sizes\": audio_embed_sizes,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_model(self, config, input_ids, attention_mask):\n+        model = Phi4MultimodalForCausalLM(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n+            logits = model(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                return_dict=True,\n+            )[\"logits\"]\n+        self.parent.assertEqual(logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+\n+@require_torch\n+class Phi4MultimodalModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `Phi4Multimodal`.\n+    \"\"\"\n+\n+    all_model_classes = (Phi4MultimodalForCausalLM, Phi4MultimodalModel) if is_torch_available() else ()\n+    test_pruning = False\n+    test_head_masking = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Phi4MultimodalModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Phi4MultimodalConfig)\n+\n+    @unittest.skip(reason=\"Unstable test\")\n+    def test_initialization(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Right padding not supported\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+    @unittest.skip(reason=\"This one tries to use right padding as well\")\n+    def test_eager_matches_fa2_generate(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Depending on input modalities, some params may not have gradients\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Depending on input modalities, some params may not have gradients\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Depending on input modalities, some params may not have gradients\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Test tries to instantiate dynamic cache with an arg\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Test is only for old attention format\")\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Static cache supported only for text-only inputs (not images or audios)\")\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Static cache supported only for text-only inputs (not images or audios)\")\n+    def test_generate_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Supported only for text-only inputs (otherwise dynamic control flows for multimodal inputs)\"\n+    )\n+    def test_generate_compilation_all_outputs(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Supported only for text-only inputs (otherwise dynamic control flows for multimodal inputs)\"\n+    )\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n+    @parameterized.expand([(\"random\",), (\"same\",)])\n+    @unittest.skip(reason=\"`image_attention_mask` has a specific shape\")\n+    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(reason=\"`image_attention_mask` has a specific shape\")\n+    def test_assisted_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(reason=\"`image_attention_mask` has a specific shape\")\n+    def test_prompt_lookup_decoding_matches_greedy_search(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Cannot unpad inputs for all modalities so easily\")\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Dynamo error\")\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n+\n+@require_torch\n+@slow\n+class Phi4MultimodalIntegrationTest(unittest.TestCase):\n+    checkpoint_path = \"microsoft/Phi-4-multimodal-instruct\"\n+    image_url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+    audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"\n+\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(self.checkpoint_path)\n+        self.generation_config = GenerationConfig(max_new_tokens=20, do_sample=False)\n+        self.user_token = \"<|user|>\"\n+        self.assistant_token = \"<|assistant|>\"\n+        self.end_token = \"<|end|>\"\n+        self.image = Image.open(requests.get(self.image_url, stream=True).raw)\n+        with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".wav\") as tmp:\n+            tmp.write(requests.get(self.audio_url, stream=True).raw.data)\n+            tmp.flush()\n+            tmp.seek(0)\n+            self.audio, self.sampling_rate = soundfile.read(tmp.name)\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+\n+    def test_text_only_generation(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n+        )\n+\n+        prompt = f\"{self.user_token}What is the answer for 1+1? Explain it.{self.end_token}{self.assistant_token}\"\n+        inputs = self.processor(prompt, images=None, return_tensors=\"pt\").to(torch_device)\n+\n+        output = model.generate(\n+            **inputs,\n+            generation_config=self.generation_config,\n+        )\n+        output = output[:, inputs[\"input_ids\"].shape[1] :]\n+        response = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+\n+        EXPECTED_RESPONSE = \"The answer for 1+1 is 2. This is because when you add one to another\"\n+\n+        self.assertEqual(response, EXPECTED_RESPONSE)\n+\n+    def test_vision_text_generation(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n+        )\n+\n+        prompt = f\"{self.user_token}<|image_1|>What is shown in this image?{self.end_token}{self.assistant_token}\"\n+        inputs = self.processor(prompt, images=self.image, return_tensors=\"pt\").to(torch_device)\n+\n+        output = model.generate(\n+            **inputs,\n+            generation_config=self.generation_config,\n+        )\n+        output = output[:, inputs[\"input_ids\"].shape[1] :]\n+        response = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+\n+        EXPECTED_RESPONSE = \"The image shows a vibrant scene at a street intersection in a city with a Chinese-influenced architectural\"\n+\n+        self.assertEqual(response, EXPECTED_RESPONSE)\n+\n+    def test_multi_image_vision_text_generation(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n+        )\n+\n+        images = []\n+        placeholder = \"\"\n+        for i in range(1, 5):\n+            url = f\"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg\"\n+            images.append(Image.open(requests.get(url, stream=True).raw))\n+            placeholder += f\"<|image_{i}|>\"\n+\n+        prompt = f\"{self.user_token}{placeholder}Summarize the deck of slides.{self.end_token}{self.assistant_token}\"\n+        inputs = self.processor(prompt, images, return_tensors=\"pt\").to(torch_device)\n+\n+        output = model.generate(\n+            **inputs,\n+            generation_config=self.generation_config,\n+        )\n+        output = output[:, inputs[\"input_ids\"].shape[1] :]\n+        response = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+\n+        EXPECTED_RESPONSE = \"The presentation provides an overview of Microsoft Azure, a cloud computing platform by Microsoft, and its various services\"\n+\n+        self.assertEqual(response, EXPECTED_RESPONSE)\n+\n+    @require_soundfile\n+    def test_audio_text_generation(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.checkpoint_path, torch_dtype=torch.float16, device_map=torch_device\n+        )\n+\n+        prompt = f\"{self.user_token}<|audio_1|>What is happening in this audio?{self.end_token}{self.assistant_token}\"\n+        inputs = self.processor(prompt, audios=self.audio, sampling_rate=self.sampling_rate, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        output = model.generate(\n+            **inputs,\n+            generation_config=self.generation_config,\n+        )\n+        output = output[:, inputs[\"input_ids\"].shape[1] :]\n+        response = self.processor.batch_decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+\n+        # Yes, it is truly the expected response... Even though the model correctly treats the audio file\n+        EXPECTED_RESPONSE = \"I'm sorry, but I can't listen to audio. However, if you describe the audio to me,\"\n+\n+        self.assertEqual(response, EXPECTED_RESPONSE)"
        },
        {
            "sha": "5721e5913cee64f9b9426735f55b259b326aa3f8",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -524,6 +524,7 @@\n     \"TimeSeriesTransformerConfig\",\n     \"TokenClassificationPipeline\",\n     \"TrOCRConfig\",\n+    \"Phi4MultimodalProcessor\",\n     \"TrainerState\",\n     \"TrainingArguments\",\n     \"TrajectoryTransformerConfig\","
        },
        {
            "sha": "488754d2e86d54b784ef0526630896b737b37e70",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4303d88c097d39e138f47a7946e46943d99bdfdf/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4303d88c097d39e138f47a7946e46943d99bdfdf/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=4303d88c097d39e138f47a7946e46943d99bdfdf",
            "patch": "@@ -89,6 +89,8 @@\n     \"SmolVLMVisionTransformer\",\n     \"AriaTextForCausalLM\",\n     \"AriaTextModel\",\n+    \"Phi4MultimodalAudioModel\",\n+    \"Phi4MultimodalVisionModel\",\n ]\n \n # Update this list for models that are not tested with a comment explaining the reason it should not be."
        }
    ],
    "stats": {
        "total": 6381,
        "additions": 6380,
        "deletions": 1
    }
}