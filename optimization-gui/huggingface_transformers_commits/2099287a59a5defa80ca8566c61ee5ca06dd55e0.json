{
    "author": "gante",
    "message": "[CI] lazy loading external datasets (#37218)",
    "sha": "2099287a59a5defa80ca8566c61ee5ca06dd55e0",
    "files": [
        {
            "sha": "cea317d0eb0801e42d2cce4484a2c237356ffa7f",
            "filename": "tests/pipelines/test_pipelines_audio_classification.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py?ref=2099287a59a5defa80ca8566c61ee5ca06dd55e0",
            "patch": "@@ -25,7 +25,6 @@\n )\n from transformers.pipelines import AudioClassificationPipeline, pipeline\n from transformers.testing_utils import (\n-    _run_pipeline_tests,\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n@@ -46,9 +45,15 @@\n class AudioClassificationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n+    _dataset = None\n \n-    if _run_pipeline_tests:\n-        _dataset = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            cls._dataset = datasets.load_dataset(\n+                \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n+            )\n \n     def get_test_pipeline(\n         self,\n@@ -99,6 +104,7 @@ def run_pipeline_test(self, audio_classifier, examples):\n \n     @require_torchaudio\n     def run_torchaudio(self, audio_classifier):\n+        self._load_dataset()\n         # test with a local file\n         audio = self._dataset[0][\"audio\"][\"array\"]\n         output = audio_classifier(audio)"
        },
        {
            "sha": "a5dcb3ef249ad25c036cfed62e74f00627991a0e",
            "filename": "tests/pipelines/test_pipelines_depth_estimation.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py?ref=2099287a59a5defa80ca8566c61ee5ca06dd55e0",
            "patch": "@@ -21,7 +21,6 @@\n from transformers import MODEL_FOR_DEPTH_ESTIMATION_MAPPING, is_torch_available, is_vision_available\n from transformers.pipelines import DepthEstimationPipeline, pipeline\n from transformers.testing_utils import (\n-    _run_pipeline_tests,\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n@@ -59,13 +58,17 @@ def hashimage(image: Image) -> str:\n @require_torch\n class DepthEstimationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_DEPTH_ESTIMATION_MAPPING\n-\n-    if _run_pipeline_tests:\n-        # we use revision=\"refs/pr/1\" until the PR is merged\n-        # https://hf.co/datasets/hf-internal-testing/fixtures_image_utils/discussions/1\n-        _dataset = datasets.load_dataset(\n-            \"hf-internal-testing/fixtures_image_utils\", split=\"test\", revision=\"refs/pr/1\"\n-        )\n+    _dataset = None\n+\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            # we use revision=\"refs/pr/1\" until the PR is merged\n+            # https://hf.co/datasets/hf-internal-testing/fixtures_image_utils/discussions/1\n+            cls._dataset = datasets.load_dataset(\n+                \"hf-internal-testing/fixtures_image_utils\", split=\"test\", revision=\"refs/pr/1\"\n+            )\n \n     def get_test_pipeline(\n         self,\n@@ -90,6 +93,7 @@ def get_test_pipeline(\n         ]\n \n     def run_pipeline_test(self, depth_estimator, examples):\n+        self._load_dataset()\n         outputs = depth_estimator(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n         self.assertEqual({\"predicted_depth\": ANY(torch.Tensor), \"depth\": ANY(Image.Image)}, outputs)\n "
        },
        {
            "sha": "17aec8bf35b27ef813578683902aacb8d24a900d",
            "filename": "tests/pipelines/test_pipelines_image_classification.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_classification.py?ref=2099287a59a5defa80ca8566c61ee5ca06dd55e0",
            "patch": "@@ -26,7 +26,6 @@\n )\n from transformers.pipelines import ImageClassificationPipeline, pipeline\n from transformers.testing_utils import (\n-    _run_pipeline_tests,\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n@@ -59,13 +58,17 @@ def open(*args, **kwargs):\n class ImageClassificationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\n     tf_model_mapping = TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\n-\n-    if _run_pipeline_tests:\n-        # we use revision=\"refs/pr/1\" until the PR is merged\n-        # https://hf.co/datasets/hf-internal-testing/fixtures_image_utils/discussions/1\n-        _dataset = datasets.load_dataset(\n-            \"hf-internal-testing/fixtures_image_utils\", split=\"test\", revision=\"refs/pr/1\"\n-        )\n+    _dataset = None\n+\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            # we use revision=\"refs/pr/1\" until the PR is merged\n+            # https://hf.co/datasets/hf-internal-testing/fixtures_image_utils/discussions/1\n+            cls._dataset = datasets.load_dataset(\n+                \"hf-internal-testing/fixtures_image_utils\", split=\"test\", revision=\"refs/pr/1\"\n+            )\n \n     def get_test_pipeline(\n         self,\n@@ -92,6 +95,7 @@ def get_test_pipeline(\n         return image_classifier, examples\n \n     def run_pipeline_test(self, image_classifier, examples):\n+        self._load_dataset()\n         outputs = image_classifier(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n \n         self.assertEqual("
        },
        {
            "sha": "62e56556ddaddf3103456875a53f05953821b481",
            "filename": "tests/pipelines/test_pipelines_image_segmentation.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py?ref=2099287a59a5defa80ca8566c61ee5ca06dd55e0",
            "patch": "@@ -37,7 +37,6 @@\n     pipeline,\n )\n from transformers.testing_utils import (\n-    _run_pipeline_tests,\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n@@ -89,13 +88,17 @@ class ImageSegmentationPipelineTests(unittest.TestCase):\n         + (MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING.items() if MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING else [])\n         + (MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING.items() if MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING else [])\n     )\n-\n-    if _run_pipeline_tests:\n-        # we use revision=\"refs/pr/1\" until the PR is merged\n-        # https://hf.co/datasets/hf-internal-testing/fixtures_image_utils/discussions/1\n-        _dataset = datasets.load_dataset(\n-            \"hf-internal-testing/fixtures_image_utils\", split=\"test\", revision=\"refs/pr/1\"\n-        )\n+    _dataset = None\n+\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            # we use revision=\"refs/pr/1\" until the PR is merged\n+            # https://hf.co/datasets/hf-internal-testing/fixtures_image_utils/discussions/1\n+            cls._dataset = datasets.load_dataset(\n+                \"hf-internal-testing/fixtures_image_utils\", split=\"test\", revision=\"refs/pr/1\"\n+            )\n \n     def get_test_pipeline(\n         self,\n@@ -120,6 +123,7 @@ def get_test_pipeline(\n         ]\n \n     def run_pipeline_test(self, image_segmenter, examples):\n+        self._load_dataset()\n         outputs = image_segmenter(\n             \"./tests/fixtures/tests_samples/COCO/000000039769.png\",\n             threshold=0.0,"
        },
        {
            "sha": "fcc50ca5b2ba1833c4567f09949cd9f2a6cc0792",
            "filename": "tests/pipelines/test_pipelines_object_detection.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_object_detection.py?ref=2099287a59a5defa80ca8566c61ee5ca06dd55e0",
            "patch": "@@ -25,8 +25,7 @@\n     is_vision_available,\n     pipeline,\n )\n-from transformers.testing_utils import (  #\n-    _run_pipeline_tests,\n+from transformers.testing_utils import (\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n@@ -57,13 +56,17 @@ def open(*args, **kwargs):\n @require_torch\n class ObjectDetectionPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_OBJECT_DETECTION_MAPPING\n-\n-    if _run_pipeline_tests:\n-        # we use revision=\"refs/pr/1\" until the PR is merged\n-        # https://hf.co/datasets/hf-internal-testing/fixtures_image_utils/discussions/1\n-        _dataset = datasets.load_dataset(\n-            \"hf-internal-testing/fixtures_image_utils\", split=\"test\", revision=\"refs/pr/1\"\n-        )\n+    _dataset = None\n+\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            # we use revision=\"refs/pr/1\" until the PR is merged\n+            # https://hf.co/datasets/hf-internal-testing/fixtures_image_utils/discussions/1\n+            cls._dataset = datasets.load_dataset(\n+                \"hf-internal-testing/fixtures_image_utils\", split=\"test\", revision=\"refs/pr/1\"\n+            )\n \n     def get_test_pipeline(\n         self,\n@@ -85,6 +88,7 @@ def get_test_pipeline(\n         return object_detector, [\"./tests/fixtures/tests_samples/COCO/000000039769.png\"]\n \n     def run_pipeline_test(self, object_detector, examples):\n+        self._load_dataset()\n         outputs = object_detector(\"./tests/fixtures/tests_samples/COCO/000000039769.png\", threshold=0.0)\n \n         self.assertGreater(len(outputs), 0)"
        },
        {
            "sha": "6dbe324ed3d0f14c1d7057e47a922ec8831a7e8e",
            "filename": "tests/pipelines/test_pipelines_video_classification.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_video_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2099287a59a5defa80ca8566c61ee5ca06dd55e0/tests%2Fpipelines%2Ftest_pipelines_video_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_video_classification.py?ref=2099287a59a5defa80ca8566c61ee5ca06dd55e0",
            "patch": "@@ -19,7 +19,6 @@\n from transformers import MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING, VideoMAEFeatureExtractor\n from transformers.pipelines import VideoClassificationPipeline, pipeline\n from transformers.testing_utils import (\n-    _run_pipeline_tests,\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n@@ -39,11 +38,15 @@\n @require_av\n class VideoClassificationPipelineTests(unittest.TestCase):\n     model_mapping = MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING\n+    example_video_filepath = None\n \n-    if _run_pipeline_tests:\n-        example_video_filepath = hf_hub_download(\n-            repo_id=\"nateraw/video-demo\", filename=\"archery.mp4\", repo_type=\"dataset\"\n-        )\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls.example_video_filepath is None:\n+            cls.example_video_filepath = hf_hub_download(\n+                repo_id=\"nateraw/video-demo\", filename=\"archery.mp4\", repo_type=\"dataset\"\n+            )\n \n     def get_test_pipeline(\n         self,\n@@ -54,6 +57,7 @@ def get_test_pipeline(\n         processor=None,\n         torch_dtype=\"float32\",\n     ):\n+        self._load_dataset()\n         video_classifier = VideoClassificationPipeline(\n             model=model,\n             tokenizer=tokenizer,"
        }
    ],
    "stats": {
        "total": 108,
        "additions": 67,
        "deletions": 41
    }
}