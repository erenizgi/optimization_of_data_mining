{
    "author": "gante",
    "message": "[causallm tester] automate pipeline mappings + bloom tests (#41318)",
    "sha": "9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
    "files": [
        {
            "sha": "8d486c0410906690be40259c89a638a9ceb8d577",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 29,
            "deletions": 4,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -144,6 +144,23 @@ def all_model_classes(self):\n             if model_class is not None\n         ]\n \n+    @property\n+    def pipeline_model_mapping(self):\n+        # This is the default pipeline mapping.\n+        mapping = {\n+            \"feature-extraction\": self.base_model_class,\n+            \"text-generation\": self.causal_lm_class,\n+        }\n+        if self.question_answering_class is not None:\n+            mapping[\"question-answering\"] = self.question_answering_class\n+        if self.sequence_classification_class is not None:\n+            mapping[\"text-classification\"] = self.sequence_classification_class\n+        if self.token_classification_class is not None:\n+            mapping[\"token-classification\"] = self.token_classification_class\n+        if self.sequence_classification_class is not None:\n+            mapping[\"zero-shot\"] = self.sequence_classification_class\n+        return mapping\n+\n     def __init__(\n         self,\n         parent,\n@@ -298,12 +315,20 @@ def setUp(self):\n             )\n         self.model_tester = self.model_tester_class(self)\n         self.config_tester = ConfigTester(self, config_class=self.model_tester.config_class)\n+\n+        if self.pipeline_model_mapping is None:\n+            # If `all_model_classes` is not the default, maybe there are more pipeline mappings to be set.\n+            if self.all_model_classes is not None:\n+                raise ValueError(\n+                    \"Testes that inherit from `CausalLMModelTest` and set `all_model_classes` must manually set \"\n+                    \"`pipeline_model_mapping`.\"\n+                )\n+            # Otherwise, we know the pipeline mapping is the default.\n+            else:\n+                self.pipeline_model_mapping = self.model_tester.pipeline_model_mapping\n+\n         if self.all_model_classes is None:\n             self.all_model_classes = self.model_tester.all_model_classes\n-        if self.pipeline_model_mapping is None:\n-            raise ValueError(\n-                \"You have inherited from CausalLMModelTest but did not set the pipeline_model_mapping attribute.\"\n-            )\n \n     def test_config(self):\n         self.config_tester.run_common_tests()"
        },
        {
            "sha": "801953b2d45237859fb22d2b16b215021369b96e",
            "filename": "tests/models/apertus/test_modeling_apertus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fapertus%2Ftest_modeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fapertus%2Ftest_modeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fapertus%2Ftest_modeling_apertus.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -34,7 +34,6 @@\n if is_torch_available():\n     from transformers import (\n         ApertusForCausalLM,\n-        ApertusForTokenClassification,\n         ApertusModel,\n     )\n \n@@ -46,15 +45,6 @@ class ApertusModelTester(CausalLMModelTester):\n \n @require_torch\n class ApertusModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": ApertusModel,\n-            \"text-generation\": ApertusForCausalLM,\n-            \"token-classification\": ApertusForTokenClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     model_tester_class = ApertusModelTester\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "ffd914882e8aafdf5b6a66898df7a18538099a4e",
            "filename": "tests/models/arcee/test_modeling_arcee.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -34,9 +34,6 @@\n     from transformers import (\n         ArceeConfig,\n         ArceeForCausalLM,\n-        ArceeForQuestionAnswering,\n-        ArceeForSequenceClassification,\n-        ArceeForTokenClassification,\n         ArceeModel,\n     )\n \n@@ -48,18 +45,6 @@ class ArceeModelTester(CausalLMModelTester):\n \n @require_torch\n class ArceeModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": ArceeModel,\n-            \"text-classification\": ArceeForSequenceClassification,\n-            \"text-generation\": ArceeForCausalLM,\n-            \"zero-shot\": ArceeForSequenceClassification,\n-            \"question-answering\": ArceeForQuestionAnswering,\n-            \"token-classification\": ArceeForTokenClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False\n     model_tester_class = ArceeModelTester\n "
        },
        {
            "sha": "6fd1c6f6276602c81076e91a0287e08ffdaca300",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 152,
            "deletions": 346,
            "changes": 498,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -16,134 +16,30 @@\n import math\n import unittest\n \n-from transformers import BloomConfig, is_torch_available\n+from transformers import is_torch_available\n from transformers.testing_utils import require_torch, require_torch_accelerator, slow, torch_device\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+from ...test_modeling_common import ids_tensor\n \n \n if is_torch_available():\n     import torch\n \n     from transformers import (\n         BloomForCausalLM,\n-        BloomForQuestionAnswering,\n-        BloomForSequenceClassification,\n-        BloomForTokenClassification,\n         BloomModel,\n         BloomTokenizerFast,\n     )\n \n \n @require_torch\n-class BloomModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=14,\n-        seq_length=7,\n-        is_training=True,\n-        use_token_type_ids=False,\n-        use_input_mask=True,\n-        use_labels=True,\n-        use_mc_token_ids=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_input_mask = use_input_mask\n-        self.use_labels = use_labels\n-        self.use_mc_token_ids = use_mc_token_ids\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_dropout_prob = attention_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.scope = None\n-        self.bos_token_id = vocab_size - 1\n-        self.eos_token_id = vocab_size - 1\n-        self.pad_token_id = vocab_size - 1\n-\n-    def get_large_model_config(self):\n-        return BloomConfig.from_pretrained(\"bigscience/bloom\")\n-\n-    def prepare_config_and_inputs(self, gradient_checkpointing=False):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        sequence_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-\n-        config = self.get_config(gradient_checkpointing=gradient_checkpointing)\n-\n-        return (config, input_ids, input_mask, sequence_labels)\n-\n-    def get_config(self, gradient_checkpointing=False, slow_but_exact=True):\n-        return BloomConfig(\n-            vocab_size=self.vocab_size,\n-            seq_length=self.seq_length,\n-            hidden_size=self.hidden_size,\n-            n_layer=self.num_hidden_layers,\n-            n_head=self.num_attention_heads,\n-            hidden_dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_dropout_prob,\n-            n_positions=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-            use_cache=True,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            num_labels=self.num_labels,\n-            gradient_checkpointing=gradient_checkpointing,\n-            slow_but_exact=slow_but_exact,\n-            dtype=\"float32\",\n-        )\n+class BloomModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        base_model_class = BloomModel\n \n-    def create_and_check_bloom_model(self, config, input_ids, input_mask, *args):\n-        model = BloomModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(len(result.past_key_values), config.n_layer)\n-\n-    def create_and_check_bloom_model_past(self, config, input_ids, input_mask, *args):\n+    def create_and_check_bloom_model_past(self, config, *args):\n+        input_ids, _, input_mask, _, _, _ = args\n         model = BloomModel(config=config)\n \n         model.to(torch_device)\n@@ -176,7 +72,8 @@ def create_and_check_bloom_model_past(self, config, input_ids, input_mask, *args\n         # test that outputs are equal for slice\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n-    def create_and_check_bloom_model_attention_mask_past(self, config, input_ids, input_mask, *args):\n+    def create_and_check_bloom_model_attention_mask_past(self, config, *args):\n+        input_ids, _, input_mask, _, _, _ = args\n         model = BloomModel(config=config)\n         model.to(torch_device)\n         model.eval()\n@@ -216,7 +113,8 @@ def create_and_check_bloom_model_attention_mask_past(self, config, input_ids, in\n         # test that outputs are equal for slice\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n-    def create_and_check_bloom_model_past_large_inputs(self, config, input_ids, input_mask, *args):\n+    def create_and_check_bloom_model_past_large_inputs(self, config, *args):\n+        input_ids, _, input_mask, _, _, _ = args\n         model = BloomModel(config=config)\n         model.to(torch_device)\n         model.eval()\n@@ -248,7 +146,8 @@ def create_and_check_bloom_model_past_large_inputs(self, config, input_ids, inpu\n         # test that outputs are equal for slice\n         self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n \n-    def create_and_check_lm_head_model(self, config, input_ids, input_mask, *args):\n+    def create_and_check_lm_head_model(self, config, *args):\n+        input_ids, _, input_mask, _, _, _ = args\n         model = BloomForCausalLM(config)\n         model.to(torch_device)\n         model.eval()\n@@ -257,36 +156,6 @@ def create_and_check_lm_head_model(self, config, input_ids, input_mask, *args):\n         self.parent.assertEqual(result.loss.shape, ())\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n \n-    def create_and_check_sequence_classification_model(self, config, input_ids, input_mask, *args):\n-        config.num_labels = self.num_labels\n-        model = BloomForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_token_classification_model(self, config, input_ids, input_mask, *args):\n-        model = BloomForTokenClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        result = model(input_ids, attention_mask=input_mask)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def create_and_check_forward_and_backwards(\n-        self, config, input_ids, input_mask, *args, gradient_checkpointing=False\n-    ):\n-        model = BloomForCausalLM(config)\n-        model.to(torch_device)\n-        if gradient_checkpointing:\n-            model.gradient_checkpointing_enable()\n-\n-        result = model(input_ids, labels=input_ids)\n-        self.parent.assertEqual(result.loss.shape, ())\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-        result.loss.backward()\n-\n     def create_and_check_bloom_weight_initialization(self, config, *args):\n         model = BloomModel(config)\n         model_std = model.config.initializer_range / math.sqrt(2 * model.config.n_layer)\n@@ -295,58 +164,14 @@ def create_and_check_bloom_weight_initialization(self, config, *args):\n                 self.parent.assertLessEqual(abs(torch.std(model.state_dict()[key]) - model_std), 0.001)\n                 self.parent.assertLessEqual(abs(torch.mean(model.state_dict()[key]) - 0.0), 0.01)\n \n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-\n-        config, input_ids, input_mask, sequence_labels = config_and_inputs\n-\n-        inputs_dict = {\"input_ids\": input_ids}\n-\n-        return config, inputs_dict\n-\n \n @require_torch\n-class BloomModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            BloomModel,\n-            BloomForCausalLM,\n-            BloomForSequenceClassification,\n-            BloomForTokenClassification,\n-            BloomForQuestionAnswering,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": BloomModel,\n-            \"question-answering\": BloomForQuestionAnswering,\n-            \"text-classification\": BloomForSequenceClassification,\n-            \"text-generation\": BloomForCausalLM,\n-            \"token-classification\": BloomForTokenClassification,\n-            \"zero-shot\": BloomForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n+class BloomModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = BloomModelTester\n     fx_compatible = True\n     test_missing_keys = False\n-\n     test_torchscript = True  # torch.autograd functions seems not to be supported\n \n-    def setUp(self):\n-        self.model_tester = BloomModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=BloomConfig, n_embd=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_bloom_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_bloom_model(*config_and_inputs)\n-\n     def test_bloom_model_past(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_bloom_model_past(*config_and_inputs)\n@@ -363,177 +188,43 @@ def test_bloom_lm_head_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_lm_head_model(*config_and_inputs)\n \n-    def test_bloom_sequence_classification_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_sequence_classification_model(*config_and_inputs)\n-\n-    def test_bloom_token_classification_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_token_classification_model(*config_and_inputs)\n-\n-    def test_bloom_gradient_checkpointing(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs, gradient_checkpointing=True)\n-\n     def test_bloom_weight_initialization(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_bloom_weight_initialization(*config_and_inputs)\n \n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"bigscience/bigscience-small-testing\"\n-        model = BloomModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @slow\n-    @require_torch_accelerator\n-    def test_simple_generation(self):\n-        # This test is a bit flaky. For some GPU architectures, pytorch sets by default allow_fp16_reduced_precision_reduction = True and some operations\n-        # do not give the same results under this configuration, especially torch.baddmm and torch.bmm. https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200\n-        # As we leave the default value (True) for allow_fp16_reduced_precision_reduction, the tests failed when running in half-precision with smaller models (560m)\n-        # Please see: https://pytorch.org/docs/stable/notes/cuda.html#reduced-precision-reduction-in-fp16-gemms\n-        # This discrepancy is observed only when using small models and seems to be stable for larger models.\n-        # Our conclusion is that these operations are flaky for small inputs but seems to be stable for larger inputs (for the functions `baddmm` and `bmm`), and therefore for larger models.\n-\n-        # Here is a summary of an ablation study of our observations\n-        # EXPECTED_OUTPUT = \"I enjoy walking with my cute dog, and I love to watch the kids play. I am a very active person, and I am a very good listener. I am a very good person, and I am a very good person. I am a\"\n-        # 560m + allow_fp16_reduced_precision_reduction = False  + torch.bmm  ==> PASS\n-        # 560m + allow_fp16_reduced_precision_reduction = False  + torch.baddm  ==> PASS\n-        # 560m + allow_fp16_reduced_precision_reduction = True  + torch.baddm  ==> PASS\n-        # 560m + allow_fp16_reduced_precision_reduction = True  + torch.bmm  ==> FAIL\n-\n-        # EXPECTED_OUTPUT = \"I enjoy walking with my cute dog, but I also enjoy hiking, biking, and swimming. I love to cook and bake. I love to cook and bake. I love to cook and bake. I love to cook and bake. I love\"\n-        # >=1b1 + allow_fp16_reduced_precision_reduction = True  + torch.baddm  ==> PASS  (for use_cache=True and use_cache=False)\n-        # >=1b1 + allow_fp16_reduced_precision_reduction = True  + torch.bmm  ==> PASS\n-        # >=1b1 + allow_fp16_reduced_precision_reduction = False  + torch.bmm  ==> PASS\n-\n-        path_560m = \"bigscience/bloom-560m\"\n-        model = BloomForCausalLM.from_pretrained(path_560m, use_cache=True, revision=\"gs555750\").to(torch_device)\n-        model = model.eval()\n-        tokenizer = BloomTokenizerFast.from_pretrained(path_560m)\n-\n-        input_sentence = \"I enjoy walking with my cute dog\"\n-        # This output has been obtained using fp32 model on the huggingface DGX workstation - NVIDIA A100 GPU\n-        EXPECTED_OUTPUT = (\n-            \"I enjoy walking with my cute dog, and I love to watch the kids play with the kids. I am a very \"\n-            \"active person, and I enjoy working out, and I am a very active person. I am a very active person, and I\"\n-        )\n-\n-        input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n-        greedy_output = model.generate(input_ids.to(torch_device), max_length=50)\n-\n-        self.assertEqual(tokenizer.decode(greedy_output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n-\n-    @slow\n-    @require_torch_accelerator\n-    def test_batch_generation(self):\n-        path_560m = \"bigscience/bloom-560m\"\n-        model = BloomForCausalLM.from_pretrained(path_560m, use_cache=True, revision=\"gs555750\").to(torch_device)\n-        model = model.eval()\n-        tokenizer = BloomTokenizerFast.from_pretrained(path_560m, padding_side=\"left\")\n-\n-        input_sentence = [\"I enjoy walking with my cute dog\", \"I enjoy walking with my cute dog\"]\n-\n-        inputs = tokenizer.batch_encode_plus(input_sentence, return_tensors=\"pt\", padding=True)\n-        input_ids = inputs[\"input_ids\"].to(torch_device)\n-        attention_mask = inputs[\"attention_mask\"]\n-        greedy_output = model.generate(input_ids, attention_mask=attention_mask, max_length=50, do_sample=False)\n-\n-        self.assertEqual(\n-            tokenizer.decode(greedy_output[0], skip_special_tokens=True),\n-            tokenizer.decode(greedy_output[1], skip_special_tokens=True),\n-        )\n-\n-    @slow\n-    @require_torch_accelerator\n-    def test_batch_generation_padding(self):\n-        path_560m = \"bigscience/bloom-560m\"\n-        model = BloomForCausalLM.from_pretrained(path_560m, use_cache=True, revision=\"gs555750\").to(torch_device)\n-        model = model.eval()\n-        tokenizer = BloomTokenizerFast.from_pretrained(path_560m, padding_side=\"left\")\n-\n-        input_sentence = [\"I enjoy walking with my cute dog\", \"Hello my name is\"]\n-        input_sentence_without_pad = \"Hello my name is\"\n-\n-        input_ids = tokenizer.batch_encode_plus(input_sentence, return_tensors=\"pt\", padding=True)\n-        input_ids_without_pad = tokenizer.encode(input_sentence_without_pad, return_tensors=\"pt\")\n-\n-        input_ids, attention_mask = input_ids[\"input_ids\"].to(torch_device), input_ids[\"attention_mask\"]\n-        greedy_output = model.generate(input_ids, attention_mask=attention_mask, max_length=50, do_sample=False)\n-        greedy_output_without_pad = model.generate(\n-            input_ids_without_pad.to(torch_device), max_length=50, do_sample=False\n-        )\n-\n-        # test token values\n-        self.assertEqual(greedy_output[-1, 3:].tolist(), greedy_output_without_pad[0, :-3].tolist())\n-\n-        # test reconstructions\n-        self.assertEqual(\n-            tokenizer.decode(greedy_output[-1, 3:], skip_special_tokens=True),\n-            tokenizer.decode(greedy_output_without_pad[0, :-3], skip_special_tokens=True),\n-        )\n-\n-    @slow\n-    @require_torch_accelerator\n-    def test_batch_generated_text(self):\n-        path_560m = \"bigscience/bloom-560m\"\n-\n-        model = BloomForCausalLM.from_pretrained(path_560m, use_cache=True, revision=\"gs555750\").to(torch_device)\n-        model = model.eval()\n-        tokenizer = BloomTokenizerFast.from_pretrained(path_560m, padding_side=\"left\")\n-\n-        input_sentences = [\n-            \"Hello what is\",\n-            \"Running a quick test with the\",\n-        ]\n-        inputs = tokenizer(input_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n-        generated_ids = model.generate(\n-            inputs[\"input_ids\"].to(torch_device), attention_mask=inputs[\"attention_mask\"], max_length=20\n-        )\n-        generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-\n-        # these generations match those of the PyTorch model\n-        EXPECTED_GENERATIONS = [\n-            \"Hello what is the best way to get the data from the server? I have tried\",\n-            \"Running a quick test with the following command:\\nsudo apt-get install python3\\nsudo apt-get install python2\",\n-        ]\n-\n-        self.assertListEqual(generated_text, EXPECTED_GENERATIONS)\n-\n     @unittest.skip(\"Bloom needs a 2D attention for alibi\")\n     def test_custom_4d_attention_mask(self):\n         pass\n \n \n @require_torch\n-class BloomEmbeddingTest(unittest.TestCase):\n-    \"\"\"\n-    The goal here is to compare the embeddings generated by the model trained\n-    using Megatron-LM with the one from the transformers library, with a small GPT2-like model\n-    to ensure that the conversion from Megatron-LM to transformers has been done successfully.\n-    The script compares the logits of the embedding layer and the transformer layers.\n-\n-    WARNING: It is expected that these logits will not have exactly the same statistics when running\n-    the code on CPU or GPU. For more info, please visit:\n-      - https://github.com/pytorch/pytorch/issues/76052#issuecomment-1103193548\n-      - https://discuss.pytorch.org/t/reproducibility-issue-between-intel-and-amd-cpus/144779/9\n-\n-\n-    You need to install tokenizers following this readme:\n-        - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles\n-\n-    Tokenizer used during training:\n-        - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles\n-\n-    # TODO change the script (or just add skip) when building the env with tokenizers 0.12.0\n-    \"\"\"\n-\n+class BloomIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         super().setUp()\n         self.path_bigscience_model = \"bigscience/bigscience-small-testing\"\n \n     @require_torch\n     def test_embeddings(self):\n+        \"\"\"\n+        The goal here is to compare the embeddings generated by the model trained\n+        using Megatron-LM with the one from the transformers library, with a small GPT2-like model\n+        to ensure that the conversion from Megatron-LM to transformers has been done successfully.\n+        The script compares the logits of the embedding layer and the transformer layers.\n+\n+        WARNING: It is expected that these logits will not have exactly the same statistics when running\n+        the code on CPU or GPU. For more info, please visit:\n+        - https://github.com/pytorch/pytorch/issues/76052#issuecomment-1103193548\n+        - https://discuss.pytorch.org/t/reproducibility-issue-between-intel-and-amd-cpus/144779/9\n+\n+\n+        You need to install tokenizers following this readme:\n+            - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles\n+\n+        Tokenizer used during training:\n+            - https://huggingface.co/bigscience-catalogue-data-dev/byte-level-bpe-tokenizer-no-norm-250k-whitespace-and-eos-regex-alpha-v3-dedup-lines-articles\n+\n+        # TODO change the script (or just add skip) when building the env with tokenizers 0.12.0\n+        \"\"\"\n         # The config in this checkpoint has `bfloat16` as `dtype` -> model in `bfloat16`\n         model = BloomForCausalLM.from_pretrained(self.path_bigscience_model, dtype=\"auto\")\n         model.eval()\n@@ -805,3 +496,118 @@ def test_logits(self):\n         output_gpu_1, output_gpu_2 = output.split(125440, dim=-1)\n         self.assertAlmostEqual(output_gpu_1.mean().item(), MEAN_LOGITS_GPU_1, places=6)\n         self.assertAlmostEqual(output_gpu_2.mean().item(), MEAN_LOGITS_GPU_2, places=6)\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_simple_generation(self):\n+        # This test is a bit flaky. For some GPU architectures, pytorch sets by default allow_fp16_reduced_precision_reduction = True and some operations\n+        # do not give the same results under this configuration, especially torch.baddmm and torch.bmm. https://pytorch.org/docs/stable/notes/numerical_accuracy.html#fp16-on-mi200\n+        # As we leave the default value (True) for allow_fp16_reduced_precision_reduction, the tests failed when running in half-precision with smaller models (560m)\n+        # Please see: https://pytorch.org/docs/stable/notes/cuda.html#reduced-precision-reduction-in-fp16-gemms\n+        # This discrepancy is observed only when using small models and seems to be stable for larger models.\n+        # Our conclusion is that these operations are flaky for small inputs but seems to be stable for larger inputs (for the functions `baddmm` and `bmm`), and therefore for larger models.\n+\n+        # Here is a summary of an ablation study of our observations\n+        # EXPECTED_OUTPUT = \"I enjoy walking with my cute dog, and I love to watch the kids play. I am a very active person, and I am a very good listener. I am a very good person, and I am a very good person. I am a\"\n+        # 560m + allow_fp16_reduced_precision_reduction = False  + torch.bmm  ==> PASS\n+        # 560m + allow_fp16_reduced_precision_reduction = False  + torch.baddm  ==> PASS\n+        # 560m + allow_fp16_reduced_precision_reduction = True  + torch.baddm  ==> PASS\n+        # 560m + allow_fp16_reduced_precision_reduction = True  + torch.bmm  ==> FAIL\n+\n+        # EXPECTED_OUTPUT = \"I enjoy walking with my cute dog, but I also enjoy hiking, biking, and swimming. I love to cook and bake. I love to cook and bake. I love to cook and bake. I love to cook and bake. I love\"\n+        # >=1b1 + allow_fp16_reduced_precision_reduction = True  + torch.baddm  ==> PASS  (for use_cache=True and use_cache=False)\n+        # >=1b1 + allow_fp16_reduced_precision_reduction = True  + torch.bmm  ==> PASS\n+        # >=1b1 + allow_fp16_reduced_precision_reduction = False  + torch.bmm  ==> PASS\n+\n+        path_560m = \"bigscience/bloom-560m\"\n+        model = BloomForCausalLM.from_pretrained(path_560m, use_cache=True, revision=\"gs555750\").to(torch_device)\n+        model = model.eval()\n+        tokenizer = BloomTokenizerFast.from_pretrained(path_560m)\n+\n+        input_sentence = \"I enjoy walking with my cute dog\"\n+        # This output has been obtained using fp32 model on the huggingface DGX workstation - NVIDIA A100 GPU\n+        EXPECTED_OUTPUT = (\n+            \"I enjoy walking with my cute dog, and I love to watch the kids play with the kids. I am a very \"\n+            \"active person, and I enjoy working out, and I am a very active person. I am a very active person, and I\"\n+        )\n+\n+        input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n+        greedy_output = model.generate(input_ids.to(torch_device), max_length=50)\n+\n+        self.assertEqual(tokenizer.decode(greedy_output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_batch_generation(self):\n+        path_560m = \"bigscience/bloom-560m\"\n+        model = BloomForCausalLM.from_pretrained(path_560m, use_cache=True, revision=\"gs555750\").to(torch_device)\n+        model = model.eval()\n+        tokenizer = BloomTokenizerFast.from_pretrained(path_560m, padding_side=\"left\")\n+\n+        input_sentence = [\"I enjoy walking with my cute dog\", \"I enjoy walking with my cute dog\"]\n+\n+        inputs = tokenizer.batch_encode_plus(input_sentence, return_tensors=\"pt\", padding=True)\n+        input_ids = inputs[\"input_ids\"].to(torch_device)\n+        attention_mask = inputs[\"attention_mask\"]\n+        greedy_output = model.generate(input_ids, attention_mask=attention_mask, max_length=50, do_sample=False)\n+\n+        self.assertEqual(\n+            tokenizer.decode(greedy_output[0], skip_special_tokens=True),\n+            tokenizer.decode(greedy_output[1], skip_special_tokens=True),\n+        )\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_batch_generation_padding(self):\n+        path_560m = \"bigscience/bloom-560m\"\n+        model = BloomForCausalLM.from_pretrained(path_560m, use_cache=True, revision=\"gs555750\").to(torch_device)\n+        model = model.eval()\n+        tokenizer = BloomTokenizerFast.from_pretrained(path_560m, padding_side=\"left\")\n+\n+        input_sentence = [\"I enjoy walking with my cute dog\", \"Hello my name is\"]\n+        input_sentence_without_pad = \"Hello my name is\"\n+\n+        input_ids = tokenizer.batch_encode_plus(input_sentence, return_tensors=\"pt\", padding=True)\n+        input_ids_without_pad = tokenizer.encode(input_sentence_without_pad, return_tensors=\"pt\")\n+\n+        input_ids, attention_mask = input_ids[\"input_ids\"].to(torch_device), input_ids[\"attention_mask\"]\n+        greedy_output = model.generate(input_ids, attention_mask=attention_mask, max_length=50, do_sample=False)\n+        greedy_output_without_pad = model.generate(\n+            input_ids_without_pad.to(torch_device), max_length=50, do_sample=False\n+        )\n+\n+        # test token values\n+        self.assertEqual(greedy_output[-1, 3:].tolist(), greedy_output_without_pad[0, :-3].tolist())\n+\n+        # test reconstructions\n+        self.assertEqual(\n+            tokenizer.decode(greedy_output[-1, 3:], skip_special_tokens=True),\n+            tokenizer.decode(greedy_output_without_pad[0, :-3], skip_special_tokens=True),\n+        )\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_batch_generated_text(self):\n+        path_560m = \"bigscience/bloom-560m\"\n+\n+        model = BloomForCausalLM.from_pretrained(path_560m, use_cache=True, revision=\"gs555750\").to(torch_device)\n+        model = model.eval()\n+        tokenizer = BloomTokenizerFast.from_pretrained(path_560m, padding_side=\"left\")\n+\n+        input_sentences = [\n+            \"Hello what is\",\n+            \"Running a quick test with the\",\n+        ]\n+        inputs = tokenizer(input_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n+        generated_ids = model.generate(\n+            inputs[\"input_ids\"].to(torch_device), attention_mask=inputs[\"attention_mask\"], max_length=20\n+        )\n+        generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        # these generations match those of the PyTorch model\n+        EXPECTED_GENERATIONS = [\n+            \"Hello what is the best way to get the data from the server? I have tried\",\n+            \"Running a quick test with the following command:\\nsudo apt-get install python3\\nsudo apt-get install python2\",\n+        ]\n+\n+        self.assertListEqual(generated_text, EXPECTED_GENERATIONS)"
        },
        {
            "sha": "562144dbe792e8adfe9d102f05b2f40bc688ee80",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -41,7 +41,6 @@\n     import torch\n \n     from transformers import BltConfig, BltForCausalLM, BltModel\n-from transformers.models.blt.modeling_blt import BltRotaryEmbedding\n \n \n class BltModelTester(CausalLMModelTester):\n@@ -170,26 +169,8 @@ def get_config(self):\n \n @require_torch\n class BltModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            BltModel,\n-            BltForCausalLM,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": BltModel,\n-            \"text-generation\": BltForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     fx_compatible = False\n     model_tester_class = BltModelTester\n-    rotary_embedding_layer = BltRotaryEmbedding  # Enables RoPE tests if set\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n@@ -245,8 +226,6 @@ def test_eager_matches_sdpa_inference(\n     @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n     def test_model_rope_scaling_from_config(self, scaling_type):\n         \"\"\"Override rope scaling from config test to handle Blt's sub-config structure.\"\"\"\n-        if self.rotary_embedding_layer is None:\n-            self.skipTest(\"Rotary embedding layer not set\")\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         short_input = ids_tensor([1, 10], config.vocab_size)\n         long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)"
        },
        {
            "sha": "6ad9ac4eee746fa04b1d4ee7768bfc5f86c4b8e8",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -86,9 +86,6 @@ def __init__(\n         self.eos_token_id = vocab_size - 1\n         self.pad_token_id = vocab_size - 1\n \n-    def get_large_model_config(self):\n-        return CodeGenConfig.from_pretrained(\"Salesforce/codegen-2B-mono\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n "
        },
        {
            "sha": "53ff6de5449fdd31785e247e57eb8f6c7763fc49",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -86,15 +86,6 @@ def config_args(self):\n \n @require_torch\n class DbrxModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (DbrxModel, DbrxForCausalLM) if is_torch_available() else ()\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": DbrxModel,\n-            \"text-generation\": DbrxForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     model_tester_class = DbrxModelTester\n \n     @slow"
        },
        {
            "sha": "c28b7da71ee7fdac258b5ee39e0373c28a418d87",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -27,7 +27,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import AutoTokenizer, DeepseekV2ForCausalLM, DeepseekV2ForSequenceClassification, DeepseekV2Model\n+    from transformers import AutoTokenizer, DeepseekV2ForCausalLM, DeepseekV2Model\n     from transformers.models.deepseek_v2.modeling_deepseek_v2 import DeepseekV2RotaryEmbedding\n \n \n@@ -54,16 +54,6 @@ def __init__(\n \n @require_torch\n class DeepseekV2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": DeepseekV2Model,\n-            \"text-classification\": DeepseekV2ForSequenceClassification,\n-            \"text-generation\": DeepseekV2ForCausalLM,\n-            \"zero-shot\": DeepseekV2ForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False\n     test_torchscript = False\n     test_all_params_have_gradient = False"
        },
        {
            "sha": "77cb4836398f49aa61202a9aa364ed715eb8b402",
            "filename": "tests/models/dots1/test_modeling_dots1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -60,23 +60,6 @@ def __init__(\n \n @require_torch\n class Dots1ModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            Dots1Model,\n-            Dots1ForCausalLM,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Dots1Model,\n-            \"text-generation\": Dots1ForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     model_tester_class = Dots1ModelTester\n \n "
        },
        {
            "sha": "b5ef2766ed06e656d915d7e384c6eedc16b2489e",
            "filename": "tests/models/ernie4_5/test_modeling_ernie4_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -45,14 +45,6 @@ class Ernie4_5ModelTester(CausalLMModelTester):\n \n @require_torch\n class Ernie4_5ModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Ernie4_5Model,\n-            \"text-generation\": Ernie4_5ForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = Ernie4_5ModelTester\n "
        },
        {
            "sha": "32743cd1960b122e6d1a2127a967b8bc4133559e",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -52,15 +52,6 @@ class Ernie4_5_MoeModelTester(CausalLMModelTester):\n \n @require_torch\n class Ernie4_5_MoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Ernie4_5_MoeModel,\n-            \"text-generation\": Ernie4_5_MoeForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     test_all_params_have_gradient = False\n     model_tester_class = Ernie4_5_MoeModelTester\n "
        },
        {
            "sha": "a2809388c9e5733438d28686ec50c83838ae8d2c",
            "filename": "tests/models/exaone4/test_modeling_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -41,9 +41,6 @@\n \n     from transformers import (\n         Exaone4ForCausalLM,\n-        Exaone4ForQuestionAnswering,\n-        Exaone4ForSequenceClassification,\n-        Exaone4ForTokenClassification,\n         Exaone4Model,\n     )\n \n@@ -55,18 +52,6 @@ class Exaone4ModelTester(CausalLMModelTester):\n \n @require_torch\n class Exaone4ModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Exaone4Model,\n-            \"question-answering\": Exaone4ForQuestionAnswering,\n-            \"text-classification\": Exaone4ForSequenceClassification,\n-            \"text-generation\": Exaone4ForCausalLM,\n-            \"zero-shot\": Exaone4ForSequenceClassification,\n-            \"token-classification\": Exaone4ForTokenClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = Exaone4ModelTester\n     model_split_percents = [0.5, 0.6]"
        },
        {
            "sha": "6c6cc8c12b4287eecbb390dd7e69b1245c428499",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -37,8 +37,6 @@\n \n     from transformers import (\n         FalconForCausalLM,\n-        FalconForSequenceClassification,\n-        FalconForTokenClassification,\n         FalconModel,\n     )\n \n@@ -55,17 +53,6 @@ def __init__(self, parent, new_decoder_architecture=True):\n @require_torch\n class FalconModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = FalconModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": FalconModel,\n-            \"text-classification\": FalconForSequenceClassification,\n-            \"token-classification\": FalconForTokenClassification,\n-            \"text-generation\": FalconForCausalLM,\n-            \"zero-shot\": FalconForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "927e83ebb1af536e0365a9b227f9fe8baafdf0c1",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -90,10 +90,6 @@ def __init__(\n         self.pad_token_id = vocab_size - 1\n         self.tie_word_embeddings = tie_word_embeddings\n \n-    # Ignore copy\n-    def get_large_model_config(self):\n-        return FalconMambaConfig.from_pretrained(\"tiiuae/falcon-mamba-7b\")\n-\n     def prepare_config_and_inputs(\n         self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n     ):"
        },
        {
            "sha": "e77d9b9f4568c1b0abc92b9cb7b0a0e5286d6b57",
            "filename": "tests/models/flex_olmo/test_modeling_flex_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -38,7 +38,6 @@\n         FlexOlmoForCausalLM,\n         FlexOlmoModel,\n     )\n-    from transformers.models.flex_olmo.modeling_flex_olmo import FlexOlmoRotaryEmbedding\n \n \n class FlexOlmoModelTester(CausalLMModelTester):\n@@ -48,20 +47,10 @@ class FlexOlmoModelTester(CausalLMModelTester):\n \n @require_torch\n class FlexOlmoModelTest(CausalLMModelTest, unittest.TestCase):\n-    all_model_classes = (FlexOlmoModel, FlexOlmoForCausalLM) if is_torch_available() else ()\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": FlexOlmoModel,\n-            \"text-generation\": FlexOlmoForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False\n     test_torchscript = False\n     test_all_params_have_gradient = False\n     model_tester_class = FlexOlmoModelTester\n-    rotary_embedding_layer = FlexOlmoRotaryEmbedding\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer"
        },
        {
            "sha": "ce9be3ac970943a661c4f9e7eaddc0f42bed8355",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -42,8 +42,6 @@\n \n     from transformers import (\n         GemmaForCausalLM,\n-        GemmaForSequenceClassification,\n-        GemmaForTokenClassification,\n         GemmaModel,\n     )\n \n@@ -56,17 +54,6 @@ class GemmaModelTester(CausalLMModelTester):\n \n @require_torch\n class GemmaModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": GemmaModel,\n-            \"text-classification\": GemmaForSequenceClassification,\n-            \"token-classification\": GemmaForTokenClassification,\n-            \"text-generation\": GemmaForCausalLM,\n-            \"zero-shot\": GemmaForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     model_tester_class = GemmaModelTester\n \n     # used in `test_torch_compile_for_training`"
        },
        {
            "sha": "8e0ee189d7ae398a332c0fa4ac0e3113ecf93a1b",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -45,9 +45,6 @@\n     import torch\n \n     from transformers import (\n-        Gemma2ForCausalLM,\n-        Gemma2ForSequenceClassification,\n-        Gemma2ForTokenClassification,\n         Gemma2Model,\n     )\n \n@@ -59,18 +56,6 @@ class Gemma2ModelTester(CausalLMModelTester):\n \n @require_torch\n class Gemma2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Gemma2Model,\n-            \"text-classification\": Gemma2ForSequenceClassification,\n-            \"token-classification\": Gemma2ForTokenClassification,\n-            \"text-generation\": Gemma2ForCausalLM,\n-            \"zero-shot\": Gemma2ForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n     model_tester_class = Gemma2ModelTester"
        },
        {
            "sha": "bc58d71a09b03bd7696c59957f842492d3f12813",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -72,15 +72,6 @@ class Gemma3TextModelTester(CausalLMModelTester):\n @require_torch\n class Gemma3TextModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Gemma3TextModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Gemma3TextModel,\n-            \"text-classification\": Gemma3TextForSequenceClassification,\n-            \"text-generation\": Gemma3ForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n "
        },
        {
            "sha": "e4d650d952f9f8cc0259b74f3f9ef4cb34fe4b87",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -323,14 +323,6 @@ def __init__(\n @require_torch\n class Gemma3nTextModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Gemma3nTextModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Gemma3nTextModel,\n-            \"text-generation\": Gemma3nForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n "
        },
        {
            "sha": "f38cd44dd9dc1a39142c5e227b070d1cd7cf6d03",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -34,9 +34,6 @@\n     import torch\n \n     from transformers import (\n-        GlmForCausalLM,\n-        GlmForSequenceClassification,\n-        GlmForTokenClassification,\n         GlmModel,\n     )\n \n@@ -49,17 +46,6 @@ class GlmModelTester(CausalLMModelTester):\n \n @require_torch\n class GlmModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": GlmModel,\n-            \"text-classification\": GlmForSequenceClassification,\n-            \"token-classification\": GlmForTokenClassification,\n-            \"text-generation\": GlmForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     model_tester_class = GlmModelTester\n \n "
        },
        {
            "sha": "2672861f542153a42223f2c62ff8c200a1145625",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -37,9 +37,6 @@\n     import torch\n \n     from transformers import (\n-        Glm4ForCausalLM,\n-        Glm4ForSequenceClassification,\n-        Glm4ForTokenClassification,\n         Glm4Model,\n     )\n \n@@ -52,17 +49,6 @@ class Glm4ModelTester(CausalLMModelTester):\n @require_torch\n class Glm4ModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Glm4ModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Glm4Model,\n-            \"text-classification\": Glm4ForSequenceClassification,\n-            \"token-classification\": Glm4ForTokenClassification,\n-            \"text-generation\": Glm4ForCausalLM,\n-            \"zero-shot\": Glm4ForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n "
        },
        {
            "sha": "66c64a86654cde84e0275ee13719b797d2f55f33",
            "filename": "tests/models/glm4_moe/test_modeling_glm4_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -58,14 +58,6 @@ def __init__(\n \n @require_torch\n class Glm4MoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Glm4MoeModel,\n-            \"text-generation\": Glm4MoeForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False\n     model_tester_class = Glm4MoeModelTester\n     # used in `test_torch_compile_for_training`. Skip as \"Dynamic control flow in MoE\""
        },
        {
            "sha": "667366461e4695936f2aea653b84836b305c78fd",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -156,6 +156,7 @@ class GPT2ModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else ()\n     )\n+    # We need to set `pipeline_model_mapping` because we overwrite `all_model_classes`\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GPT2Model,"
        },
        {
            "sha": "934b83e5a36c5d8b15eb38cd155cd5e14325fd3a",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -95,9 +95,6 @@ def __init__(\n         self.pad_token_id = vocab_size - 3\n         self.multi_query = multi_query\n \n-    def get_large_model_config(self):\n-        return GPTBigCodeConfig.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")\n-\n     def prepare_config_and_inputs(\n         self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n     ):"
        },
        {
            "sha": "93f77412cb06c1244615751fdac27833334cd59e",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -94,9 +94,6 @@ def __init__(\n         self.pad_token_id = vocab_size - 1\n         self.attention_types = attention_types\n \n-    def get_large_model_config(self):\n-        return GPTNeoConfig.from_pretrained(\"gpt-neo-125M\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n "
        },
        {
            "sha": "37a8bb57b41339c794884f4095523214c235d642",
            "filename": "tests/models/gpt_oss/test_modeling_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -45,9 +45,6 @@\n     import torch\n \n     from transformers import (\n-        GptOssForCausalLM,\n-        GptOssForSequenceClassification,\n-        GptOssForTokenClassification,\n         GptOssModel,\n     )\n \n@@ -61,17 +58,6 @@ class GptOssModelTester(CausalLMModelTester):\n \n @require_torch\n class GptOssModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": GptOssModel,\n-            \"text-classification\": GptOssForSequenceClassification,\n-            \"text-generation\": GptOssForCausalLM,\n-            \"token-classification\": GptOssForTokenClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n     model_tester_class = GptOssModelTester"
        },
        {
            "sha": "2b853a78a53785793fddd4ff4cf6e7cb5a4424c5",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -96,9 +96,6 @@ def __init__(\n         self.eos_token_id = vocab_size - 1\n         self.pad_token_id = vocab_size - 1\n \n-    def get_large_model_config(self):\n-        return GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n "
        },
        {
            "sha": "fb8348c1505bc4f4afc943057db6731d4821da72",
            "filename": "tests/models/helium/test_modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -31,9 +31,6 @@\n     import torch\n \n     from transformers import (\n-        HeliumForCausalLM,\n-        HeliumForSequenceClassification,\n-        HeliumForTokenClassification,\n         HeliumModel,\n     )\n \n@@ -45,20 +42,8 @@ class HeliumModelTester(CausalLMModelTester):\n \n @require_torch\n class HeliumModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": HeliumModel,\n-            \"text-classification\": HeliumForSequenceClassification,\n-            \"token-classification\": HeliumForTokenClassification,\n-            \"text-generation\": HeliumForCausalLM,\n-            \"zero-shot\": HeliumForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n-\n     model_tester_class = HeliumModelTester\n \n "
        },
        {
            "sha": "614606ded0b072c2d0349f178e196a895a8d7bf7",
            "filename": "tests/models/hunyuan_v1_dense/test_modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -28,8 +28,6 @@\n \n if is_torch_available():\n     from transformers import (\n-        HunYuanDenseV1ForCausalLM,\n-        HunYuanDenseV1ForSequenceClassification,\n         HunYuanDenseV1Model,\n     )\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n@@ -43,15 +41,6 @@ class HunYuanDenseV1ModelTester(CausalLMModelTester):\n @require_torch\n class HunYuanDenseV1ModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = HunYuanDenseV1ModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": HunYuanDenseV1Model,\n-            \"text-generation\": HunYuanDenseV1ForCausalLM,\n-            \"text-classification\": HunYuanDenseV1ForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n     def is_pipeline_test_to_skip(\n         self,"
        },
        {
            "sha": "0c4e4b5a44a883ba60760e52dfc64e57648ebd8f",
            "filename": "tests/models/hunyuan_v1_moe/test_modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -31,8 +31,6 @@\n     from transformers import (\n         AutoModelForCausalLM,\n         AutoTokenizer,\n-        HunYuanMoEV1ForCausalLM,\n-        HunYuanMoEV1ForSequenceClassification,\n         HunYuanMoEV1Model,\n     )\n \n@@ -48,15 +46,6 @@ class HunYuanMoEV1ModelTester(CausalLMModelTester):\n class HunYuanMoEV1ModelTest(CausalLMModelTest, unittest.TestCase):\n     test_all_params_have_gradient = False\n     model_tester_class = HunYuanMoEV1ModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": HunYuanMoEV1Model,\n-            \"text-generation\": HunYuanMoEV1ForCausalLM,\n-            \"text-classification\": HunYuanMoEV1ForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n     def is_pipeline_test_to_skip(\n         self,"
        },
        {
            "sha": "85ed7da20bbcd15e866b5e03d9d1b7b24ab45cec",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -93,9 +93,6 @@ def __init__(\n         self.num_choices = num_choices\n         self.scope = None\n \n-    def get_large_model_config(self):\n-        return ImageGPTConfig.from_pretrained(\"imagegpt\")\n-\n     def prepare_config_and_inputs(\n         self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n     ):"
        },
        {
            "sha": "9a5ed55e277bba375039f5a3d37386530ede4940",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -35,7 +35,6 @@\n \n     from transformers import (\n         JetMoeForCausalLM,\n-        JetMoeForSequenceClassification,\n         JetMoeModel,\n     )\n \n@@ -106,15 +105,6 @@ class JetMoeModelTest(CausalLMModelTest, unittest.TestCase):\n     test_disk_offload_bin = False\n     test_disk_offload_safetensors = False\n     model_tester_class = JetMoeModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": JetMoeModel,\n-            \"text-classification\": JetMoeForSequenceClassification,\n-            \"text-generation\": JetMoeForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n     @require_flash_attn\n     @require_torch_gpu"
        },
        {
            "sha": "cd0169f03bd30ce4bcc783bcd0c9dc4e784e64e9",
            "filename": "tests/models/lfm2/test_modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -49,14 +49,6 @@ def __init__(\n \n @require_torch\n class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Lfm2Model,\n-            \"text-generation\": Lfm2ForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False\n     model_tester_class = Lfm2ModelTester\n     # used in `test_torch_compile_for_training`"
        },
        {
            "sha": "7cf52692c2937eb50f90c63ccae71ea035cb3a5f",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -39,9 +39,6 @@\n \n     from transformers import (\n         LlamaForCausalLM,\n-        LlamaForQuestionAnswering,\n-        LlamaForSequenceClassification,\n-        LlamaForTokenClassification,\n         LlamaModel,\n         LlamaTokenizer,\n     )\n@@ -54,18 +51,6 @@ class LlamaModelTester(CausalLMModelTester):\n \n @require_torch\n class LlamaModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": LlamaModel,\n-            \"text-classification\": LlamaForSequenceClassification,\n-            \"text-generation\": LlamaForCausalLM,\n-            \"zero-shot\": LlamaForSequenceClassification,\n-            \"question-answering\": LlamaForQuestionAnswering,\n-            \"token-classification\": LlamaForTokenClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = LlamaModelTester\n "
        },
        {
            "sha": "2310605357a4d00dd26e7fa7b9f5bfe6f747153e",
            "filename": "tests/models/longcat_flash/test_modeling_longcat_flash.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -210,15 +210,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class LongcatFlashModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": LongcatFlashModel,\n-            \"text-generation\": LongcatFlashForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     model_split_percents = [0.5, 0.8]\n \n     model_tester_class = LongcatFlashModelTester"
        },
        {
            "sha": "11c47ca8269b712bf4de11fe7ab957d8e70207d5",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -98,9 +98,6 @@ def __init__(\n         self.decoder_layers = decoder_layers\n         self.large_model_config_path = large_model_config_path\n \n-    def get_large_model_config(self):\n-        return LongT5Config.from_pretrained(self.large_model_config_path)\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size)\n         decoder_input_ids = ids_tensor([self.batch_size, self.decoder_seq_length], self.vocab_size)\n@@ -936,9 +933,6 @@ def __init__(\n         self.is_training = is_training\n         self.large_model_config_path = large_model_config_path\n \n-    def get_large_model_config(self):\n-        return LongT5Config.from_pretrained(self.large_model_config_path)\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size)\n "
        },
        {
            "sha": "653496554fc78f1c4960f51aecd3803c9f1ae1ec",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -82,9 +82,6 @@ def __init__(\n         self.pad_token_id = vocab_size - 1\n         self.tie_word_embeddings = tie_word_embeddings\n \n-    def get_large_model_config(self):\n-        return MambaConfig.from_pretrained(\"hf-internal-testing/mamba-2.8b\")\n-\n     def prepare_config_and_inputs(\n         self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n     ):"
        },
        {
            "sha": "0e830152b9a954debbe81158ca44f298473e931a",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -119,9 +119,6 @@ def __init__(\n         self.pad_token_id = vocab_size - 1\n         self.tie_word_embeddings = tie_word_embeddings\n \n-    def get_large_model_config(self):\n-        return Mamba2Config.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\")\n-\n     def prepare_config_and_inputs(\n         self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n     ):"
        },
        {
            "sha": "8f240d9a42bf0df51e2a6467d24032e9e9d8c7df",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -30,9 +30,6 @@\n \n     from transformers import (\n         MiniMaxForCausalLM,\n-        MiniMaxForQuestionAnswering,\n-        MiniMaxForSequenceClassification,\n-        MiniMaxForTokenClassification,\n         MiniMaxModel,\n     )\n     from transformers.models.minimax.modeling_minimax import MiniMaxCache\n@@ -51,17 +48,6 @@ def __init__(self, parent, layer_types=None, block_size=3):\n \n @require_torch\n class MiniMaxModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": MiniMaxModel,\n-            \"text-classification\": MiniMaxForSequenceClassification,\n-            \"token-classification\": MiniMaxForTokenClassification,\n-            \"text-generation\": MiniMaxForCausalLM,\n-            \"question-answering\": MiniMaxForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     model_tester_class = MiniMaxModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146"
        },
        {
            "sha": "012a8acf76373f85440e71900c304b2727d587f2",
            "filename": "tests/models/ministral/test_modeling_ministral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -39,9 +39,6 @@\n     from transformers import (\n         AutoModelForCausalLM,\n         MinistralForCausalLM,\n-        MinistralForQuestionAnswering,\n-        MinistralForSequenceClassification,\n-        MinistralForTokenClassification,\n         MinistralModel,\n     )\n \n@@ -57,17 +54,6 @@ class MinistralModelTester(CausalLMModelTester):\n @require_torch\n class MinistralModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = MinistralModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": MinistralModel,\n-            \"text-classification\": MinistralForSequenceClassification,\n-            \"token-classification\": MinistralForTokenClassification,\n-            \"text-generation\": MinistralForCausalLM,\n-            \"question-answering\": MinistralForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "df3ec6bf7373d9a8e71182ed471ee1a800119e30",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -43,9 +43,6 @@\n \n     from transformers import (\n         MistralForCausalLM,\n-        MistralForQuestionAnswering,\n-        MistralForSequenceClassification,\n-        MistralForTokenClassification,\n         MistralModel,\n     )\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n@@ -58,17 +55,6 @@ class MistralModelTester(CausalLMModelTester):\n \n @require_torch\n class MistralModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": MistralModel,\n-            \"text-classification\": MistralForSequenceClassification,\n-            \"token-classification\": MistralForTokenClassification,\n-            \"text-generation\": MistralForCausalLM,\n-            \"question-answering\": MistralForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     model_tester_class = MistralModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146"
        },
        {
            "sha": "14dddac9541088761b8c2c87cf179f1288795f25",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -34,9 +34,6 @@\n \n     from transformers import (\n         MixtralForCausalLM,\n-        MixtralForQuestionAnswering,\n-        MixtralForSequenceClassification,\n-        MixtralForTokenClassification,\n         MixtralModel,\n     )\n \n@@ -50,18 +47,6 @@ class MixtralModelTester(CausalLMModelTester):\n \n @require_torch\n class MixtralModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": MixtralModel,\n-            \"text-classification\": MixtralForSequenceClassification,\n-            \"token-classification\": MixtralForTokenClassification,\n-            \"text-generation\": MixtralForCausalLM,\n-            \"question-answering\": MixtralForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     model_tester_class = MixtralModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146"
        },
        {
            "sha": "8737f0fa2771e5e3f1a4257011916230910a5991",
            "filename": "tests/models/modernbert_decoder/test_modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmodernbert_decoder%2Ftest_modeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmodernbert_decoder%2Ftest_modeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert_decoder%2Ftest_modeling_modernbert_decoder.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -41,15 +41,6 @@ class ModernBertDecoderModelTester(CausalLMModelTester):\n \n @require_torch\n class ModernBertDecoderModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": ModernBertDecoderModel,\n-            \"text-generation\": ModernBertDecoderForCausalLM,\n-            \"text-classification\": ModernBertDecoderForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     model_tester_class = ModernBertDecoderModelTester\n \n "
        },
        {
            "sha": "31bb3f92144dcf8c7b2776bc5bafc87cfc6e54bc",
            "filename": "tests/models/mpnet/test_modeling_mpnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmpnet%2Ftest_modeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmpnet%2Ftest_modeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpnet%2Ftest_modeling_mpnet.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -85,9 +85,6 @@ def __init__(\n         self.num_choices = num_choices\n         self.scope = scope\n \n-    def get_large_model_config(self):\n-        return MPNetConfig.from_pretrained(\"microsoft/mpnet-base\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n "
        },
        {
            "sha": "3f6ebcf88ccf11dd835f542d6a044866ad813036",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -100,9 +100,6 @@ def __init__(\n         self.eos_token_id = vocab_size - 1\n         self.pad_token_id = vocab_size - 1\n \n-    def get_large_model_config(self):\n-        return MptConfig.from_pretrained(\"mosaicml/mpt-7b\")\n-\n     def prepare_config_and_inputs(self, gradient_checkpointing=False):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n "
        },
        {
            "sha": "fc37c9efa6e6fc25a1a78c9b1d5fed0627f482ac",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -100,9 +100,6 @@ def __init__(\n         self.scope = None\n         self.decoder_layers = decoder_layers\n \n-    def get_large_model_config(self):\n-        return MT5Config.from_pretrained(\"google-t5/t5-base\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size).clamp(2)\n         input_ids[:, -1] = self.eos_token_id  # Eos Token\n@@ -908,9 +905,6 @@ def __init__(\n         self.scope = None\n         self.is_training = is_training\n \n-    def get_large_model_config(self):\n-        return MT5Config.from_pretrained(\"google-t5/t5-base\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size)\n "
        },
        {
            "sha": "42a9914541c52cd190ece779f86692b853d414dd",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -37,9 +37,6 @@\n     from transformers import (\n         AutoTokenizer,\n         NemotronForCausalLM,\n-        NemotronForQuestionAnswering,\n-        NemotronForSequenceClassification,\n-        NemotronForTokenClassification,\n         NemotronModel,\n     )\n \n@@ -55,18 +52,6 @@ class NemotronModelTest(CausalLMModelTest, unittest.TestCase):\n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": NemotronModel,\n-            \"text-classification\": NemotronForSequenceClassification,\n-            \"text-generation\": NemotronForCausalLM,\n-            \"zero-shot\": NemotronForSequenceClassification,\n-            \"question-answering\": NemotronForQuestionAnswering,\n-            \"token-classification\": NemotronForTokenClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False\n \n     # used in `test_torch_compile_for_training`"
        },
        {
            "sha": "c33b392829649e74a19cbf2354e9ea9f6023af2c",
            "filename": "tests/models/olmo3/test_modeling_olmo3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -52,19 +52,10 @@ class Olmo3ModelTester(CausalLMModelTester):\n \n @require_torch\n class Olmo3ModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Olmo3Model,\n-            \"text-generation\": Olmo3ForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False\n     test_torchscript = False\n     test_all_params_have_gradient = False\n     model_tester_class = Olmo3ModelTester\n-    rotary_embedding_layer = Olmo3RotaryEmbedding\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer\n@@ -75,8 +66,6 @@ class Olmo3ModelTest(CausalLMModelTest, unittest.TestCase):\n \n     @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n     def test_model_rope_scaling_from_config(self, scaling_type):\n-        if self.rotary_embedding_layer is None:\n-            self.skipTest(\"Rotary embedding layer not set\")\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n         # Rope only gets applied to full attention layers in Olmo3, so make all layers full attention."
        },
        {
            "sha": "5001626a709cc7d5295d47bdccb32ad1f34a98c2",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -32,8 +32,6 @@\n     from transformers import (\n         AutoTokenizer,\n         PhiForCausalLM,\n-        PhiForSequenceClassification,\n-        PhiForTokenClassification,\n         PhiModel,\n     )\n \n@@ -45,17 +43,6 @@ class PhiModelTester(CausalLMModelTester):\n \n @require_torch\n class PhiModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": PhiModel,\n-            \"text-classification\": PhiForSequenceClassification,\n-            \"token-classification\": PhiForTokenClassification,\n-            \"text-generation\": PhiForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     model_tester_class = PhiModelTester\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79292/workflows/fa2ba644-8953-44a6-8f67-ccd69ca6a476/jobs/1012905"
        },
        {
            "sha": "097c217b4aa730c408a66088f46264393613193d",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -36,8 +36,6 @@\n     from transformers import (\n         AutoTokenizer,\n         Phi3ForCausalLM,\n-        Phi3ForSequenceClassification,\n-        Phi3ForTokenClassification,\n         Phi3Model,\n     )\n \n@@ -92,17 +90,6 @@ class Phi3ModelTester(CausalLMModelTester):\n \n @require_torch\n class Phi3ModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Phi3Model,\n-            \"text-classification\": Phi3ForSequenceClassification,\n-            \"token-classification\": Phi3ForTokenClassification,\n-            \"text-generation\": Phi3ForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     model_tester_class = Phi3ModelTester\n \n "
        },
        {
            "sha": "1a2d52827c2275da17827439d99f49f2cd5f7fbd",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -35,7 +35,6 @@\n     from transformers import (\n         AutoTokenizer,\n         PhimoeForCausalLM,\n-        PhimoeForSequenceClassification,\n         PhimoeModel,\n     )\n \n@@ -93,16 +92,6 @@ class PhimoeModelTester(CausalLMModelTester):\n class PhimoeModelTest(CausalLMModelTest, unittest.TestCase):\n     test_all_params_have_gradient = False\n     model_tester_class = PhimoeModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": PhimoeModel,\n-            \"text-classification\": PhimoeForSequenceClassification,\n-            \"text-generation\": PhimoeForCausalLM,\n-            \"zero-shot\": PhimoeForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79292/workflows/fa2ba644-8953-44a6-8f67-ccd69ca6a476/jobs/1012905\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "dc76f8b00fba3d96bf41d49a6384aec096326767",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -37,9 +37,6 @@\n \n     from transformers import (\n         Qwen2ForCausalLM,\n-        Qwen2ForQuestionAnswering,\n-        Qwen2ForSequenceClassification,\n-        Qwen2ForTokenClassification,\n         Qwen2Model,\n     )\n \n@@ -55,17 +52,6 @@ class Qwen2ModelTester(CausalLMModelTester):\n @require_torch\n class Qwen2ModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Qwen2ModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Qwen2Model,\n-            \"text-classification\": Qwen2ForSequenceClassification,\n-            \"token-classification\": Qwen2ForTokenClassification,\n-            \"text-generation\": Qwen2ForCausalLM,\n-            \"question-answering\": Qwen2ForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "8dcc821333684578cd1f223624dfb7b848cbf6b0",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -35,9 +35,6 @@\n \n     from transformers import (\n         Qwen2MoeForCausalLM,\n-        Qwen2MoeForQuestionAnswering,\n-        Qwen2MoeForSequenceClassification,\n-        Qwen2MoeForTokenClassification,\n         Qwen2MoeModel,\n     )\n \n@@ -52,18 +49,6 @@ class Qwen2MoeModelTester(CausalLMModelTester):\n \n @require_torch\n class Qwen2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Qwen2MoeModel,\n-            \"text-classification\": Qwen2MoeForSequenceClassification,\n-            \"token-classification\": Qwen2MoeForTokenClassification,\n-            \"text-generation\": Qwen2MoeForCausalLM,\n-            \"question-answering\": Qwen2MoeForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     test_all_params_have_gradient = False\n     model_tester_class = Qwen2MoeModelTester\n "
        },
        {
            "sha": "dc383fc69ab33ea6dd9eb7646653d1ad2b437f20",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -36,9 +36,6 @@\n \n     from transformers import (\n         Qwen3ForCausalLM,\n-        Qwen3ForQuestionAnswering,\n-        Qwen3ForSequenceClassification,\n-        Qwen3ForTokenClassification,\n         Qwen3Model,\n     )\n \n@@ -53,17 +50,6 @@ class Qwen3ModelTester(CausalLMModelTester):\n @require_torch\n class Qwen3ModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Qwen3ModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Qwen3Model,\n-            \"text-classification\": Qwen3ForSequenceClassification,\n-            \"token-classification\": Qwen3ForTokenClassification,\n-            \"text-generation\": Qwen3ForCausalLM,\n-            \"question-answering\": Qwen3ForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "162fea8316eb4db07efa23830de461d66ced9cd3",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -34,10 +34,7 @@\n     import torch\n \n     from transformers import (\n-        Qwen3ForQuestionAnswering,\n         Qwen3MoeForCausalLM,\n-        Qwen3MoeForSequenceClassification,\n-        Qwen3MoeForTokenClassification,\n         Qwen3MoeModel,\n     )\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n@@ -50,18 +47,6 @@ class Qwen3MoeModelTester(CausalLMModelTester):\n \n @require_torch\n class Qwen3MoeModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Qwen3MoeModel,\n-            \"text-classification\": Qwen3MoeForSequenceClassification,\n-            \"token-classification\": Qwen3MoeForTokenClassification,\n-            \"text-generation\": Qwen3MoeForCausalLM,\n-            \"question-answering\": Qwen3ForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     test_all_params_have_gradient = False\n     model_tester_class = Qwen3MoeModelTester\n "
        },
        {
            "sha": "ad669a6a8fd6b4afa06b3ee8a4c286ec85237f47",
            "filename": "tests/models/qwen3_next/test_modeling_qwen3_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -27,10 +27,6 @@\n \n     from transformers import (\n         Cache,\n-        Qwen3NextForCausalLM,\n-        Qwen3NextForQuestionAnswering,\n-        Qwen3NextForSequenceClassification,\n-        Qwen3NextForTokenClassification,\n         Qwen3NextModel,\n     )\n     from transformers.models.qwen3_next.modeling_qwen3_next import Qwen3NextDynamicCache\n@@ -58,18 +54,6 @@ def __init__(self, parent):\n \n @require_torch\n class Qwen3NextModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Qwen3NextModel,\n-            \"text-classification\": Qwen3NextForSequenceClassification,\n-            \"token-classification\": Qwen3NextForTokenClassification,\n-            \"text-generation\": Qwen3NextForCausalLM,\n-            \"question-answering\": Qwen3NextForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     model_tester_class = Qwen3NextModelTester\n \n     def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):"
        },
        {
            "sha": "6d048ce3df2b55dacfef1418b1a55e60d3757d68",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -33,7 +33,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import RecurrentGemmaForCausalLM, RecurrentGemmaModel\n+    from transformers import RecurrentGemmaModel\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n@@ -45,14 +45,6 @@ class RecurrentGemmaModelTester(CausalLMModelTester):\n \n @require_torch\n class RecurrentGemmaModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": RecurrentGemmaModel,\n-            \"text-generation\": RecurrentGemmaForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     has_attentions = False\n     model_tester_class = RecurrentGemmaModelTester\n "
        },
        {
            "sha": "ab81758f6ce971d722874f34f9ec72601597dfe6",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -84,9 +84,6 @@ def __init__(\n         self.eos_token_id = vocab_size - 1\n         self.pad_token_id = vocab_size - 1\n \n-    def get_large_model_config(self):\n-        return RwkvConfig.from_pretrained(\"sgugger/rwkv-4-pile-7b\")\n-\n     def prepare_config_and_inputs(\n         self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n     ):"
        },
        {
            "sha": "244372b4c5334d38c469edf4b63191f9dba24208",
            "filename": "tests/models/seed_oss/test_modeling_seed_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseed_oss%2Ftest_modeling_seed_oss.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -35,9 +35,6 @@\n     import torch\n \n     from transformers import (\n-        SeedOssForCausalLM,\n-        SeedOssForSequenceClassification,\n-        SeedOssForTokenClassification,\n         SeedOssModel,\n     )\n \n@@ -50,17 +47,6 @@ class SeedOssModelTester(CausalLMModelTester):\n @require_torch\n class SeedOssModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = SeedOssModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": SeedOssModel,\n-            \"text-classification\": SeedOssForSequenceClassification,\n-            \"token-classification\": SeedOssForTokenClassification,\n-            \"text-generation\": SeedOssForCausalLM,\n-            \"zero-shot\": SeedOssForSequenceClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n "
        },
        {
            "sha": "a7e554606a08a3a5f88cc1c897dec0ed40e06342",
            "filename": "tests/models/smollm3/test_modeling_smollm3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -66,17 +66,6 @@ class SmolLM3ModelTester(CausalLMModelTester):\n @require_torch\n class SmolLM3ModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = SmolLM3ModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": SmolLM3Model,\n-            \"text-classification\": SmolLM3ForSequenceClassification,\n-            \"token-classification\": SmolLM3ForTokenClassification,\n-            \"text-generation\": SmolLM3ForCausalLM,\n-            \"question-answering\": SmolLM3ForQuestionAnswering,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @is_flaky()"
        },
        {
            "sha": "7ca57247bfd69d8c45adbaff9c7ce0684cd7a352",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -33,8 +33,6 @@\n     from transformers import (\n         AutoTokenizer,\n         StableLmForCausalLM,\n-        StableLmForSequenceClassification,\n-        StableLmForTokenClassification,\n         StableLmModel,\n     )\n \n@@ -48,17 +46,6 @@ class StableLmModelTester(CausalLMModelTester):\n \n @require_torch\n class StableLmModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": StableLmModel,\n-            \"text-classification\": StableLmForSequenceClassification,\n-            \"text-generation\": StableLmForCausalLM,\n-            \"zero-shot\": StableLmForSequenceClassification,\n-            \"token-classification\": StableLmForTokenClassification,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = StableLmModelTester\n "
        },
        {
            "sha": "2b799bfd9046ae45500aa99ddddb8e3f75bc6963",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -35,8 +35,6 @@\n     from transformers import (\n         AutoTokenizer,\n         Starcoder2ForCausalLM,\n-        Starcoder2ForSequenceClassification,\n-        Starcoder2ForTokenClassification,\n         Starcoder2Model,\n     )\n \n@@ -51,16 +49,6 @@ class Starcoder2ModelTester(CausalLMModelTester):\n @require_torch\n class Starcoder2ModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Starcoder2ModelTester\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": Starcoder2Model,\n-            \"text-classification\": Starcoder2ForSequenceClassification,\n-            \"token-classification\": Starcoder2ForTokenClassification,\n-            \"text-generation\": Starcoder2ForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n \n \n @slow"
        },
        {
            "sha": "39bcde70169b2392d20d66984b52b43567be2d62",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -109,9 +109,6 @@ def __init__(\n         self.expert_capacity = expert_capacity\n         self.router_jitter_noise = router_jitter_noise\n \n-    def get_large_model_config(self):\n-        return SwitchTransformersConfig.from_pretrained(\"google/switch-base-8\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size)\n         decoder_input_ids = ids_tensor([self.batch_size, self.decoder_seq_length], self.vocab_size)\n@@ -764,9 +761,6 @@ def __init__(\n         self.scope = None\n         self.is_training = is_training\n \n-    def get_large_model_config(self):\n-        return SwitchTransformersConfig.from_pretrained(\"google/switch-base-8\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size)\n "
        },
        {
            "sha": "85d54055bab8359f806c98de5fde76cfd2ab0920",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -109,9 +109,6 @@ def __init__(\n         self.scope = None\n         self.decoder_layers = decoder_layers\n \n-    def get_large_model_config(self):\n-        return T5Config.from_pretrained(\"google-t5/t5-base\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size).clamp(2)\n         input_ids[:, -1] = self.eos_token_id  # Eos Token\n@@ -915,9 +912,6 @@ def __init__(\n         self.scope = None\n         self.is_training = is_training\n \n-    def get_large_model_config(self):\n-        return T5Config.from_pretrained(\"google-t5/t5-base\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size)\n "
        },
        {
            "sha": "3dcad9282a870f40e6a00cc5837b89dd5184af20",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -99,9 +99,6 @@ def __init__(\n         self.scope = None\n         self.decoder_layers = decoder_layers\n \n-    def get_large_model_config(self):\n-        return UMT5Config.from_pretrained(\"google/umt5-base\")\n-\n     def prepare_inputs_dict(\n         self,\n         config,\n@@ -535,9 +532,6 @@ def __init__(\n         self.scope = None\n         self.is_training = is_training\n \n-    def get_large_model_config(self):\n-        return UMT5Config.from_pretrained(\"google-t5/t5-base\")\n-\n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.encoder_seq_length], self.vocab_size)\n "
        },
        {
            "sha": "d4d8c0e11730fc15e287de6f1c6a4d6eea4e6499",
            "filename": "tests/models/vaultgemma/test_modeling_vaultgemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fvaultgemma%2Ftest_modeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fvaultgemma%2Ftest_modeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvaultgemma%2Ftest_modeling_vaultgemma.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -47,7 +47,6 @@\n     import torch\n \n     from transformers import (\n-        VaultGemmaForCausalLM,\n         VaultGemmaModel,\n     )\n \n@@ -59,15 +58,6 @@ class VaultGemmaModelTester(CausalLMModelTester):\n \n @require_torch\n class VaultGemmaModelTest(CausalLMModelTest, unittest.TestCase):\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": VaultGemmaModel,\n-            \"text-generation\": VaultGemmaForCausalLM,\n-        }\n-        if is_torch_available()\n-        else {}\n-    )\n-\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n     model_tester_class = VaultGemmaModelTester"
        },
        {
            "sha": "5fb1b3fb72a09451d977339d706f77d6f996d92c",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -81,9 +81,6 @@ def __init__(\n         self.eos_token_id = 2\n         self.pad_token_id = 1\n \n-    def get_large_model_config(self):\n-        return XGLMConfig.from_pretrained(\"facebook/xglm-564M\")\n-\n     def prepare_config_and_inputs(\n         self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n     ):"
        },
        {
            "sha": "c6186daa80b780da7ea2c82f32cf42fae31bd519",
            "filename": "tests/models/xlstm/test_modeling_xlstm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9556b36b2f5599e190ff6fea644c1fcbdfb2717b/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py?ref=9556b36b2f5599e190ff6fea644c1fcbdfb2717b",
            "patch": "@@ -86,10 +86,6 @@ def __init__(\n         self.step_kernel = step_kernel\n         self.tie_word_embeddings = tie_word_embeddings\n \n-    def get_large_model_config(self):\n-        cfg = xLSTMConfig.from_pretrained(\"NX-AI/xLSTM-7b\")\n-        return cfg\n-\n     def prepare_config_and_inputs(self, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n "
        }
    ],
    "stats": {
        "total": 1190,
        "additions": 184,
        "deletions": 1006
    }
}