{
    "author": "jiqing-feng",
    "message": "Refactor OPT model (#36101)\n\n* remove cross attention\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* remove is_decoder\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix pkv\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "0baf003915d6a9bc97ccc8fd2e1e6680f036013a",
    "files": [
        {
            "sha": "830ae09a51690ce174648fbc31cd275d2910cc94",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 16,
            "deletions": 93,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/0baf003915d6a9bc97ccc8fd2e1e6680f036013a/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0baf003915d6a9bc97ccc8fd2e1e6680f036013a/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=0baf003915d6a9bc97ccc8fd2e1e6680f036013a",
            "patch": "@@ -98,7 +98,6 @@ class OPTAttention(nn.Module):\n     def __init__(\n         self,\n         config: OPTConfig,\n-        is_decoder: bool = False,\n         **kwargs,\n     ):\n         super().__init__()\n@@ -117,7 +116,6 @@ def __init__(\n                 f\" and `num_heads`: {self.num_heads}).\"\n             )\n         self.scaling = self.head_dim**-0.5\n-        self.is_decoder = is_decoder\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n@@ -130,7 +128,6 @@ def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int) -> torch.Tensor:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n@@ -139,44 +136,19 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n         # get key, value proj\n-        if is_cross_attention and past_key_value is not None:\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n+        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+        if past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+\n+        past_key_value = (key_states, value_states)\n \n         proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n@@ -268,52 +240,26 @@ def __init__(self, *args, **kwargs):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         position_ids: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        # if key_value_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = key_value_states is not None\n-\n         bsz, _, _ = hidden_states.size()\n \n         # get query proj\n         query_states = self.q_proj(hidden_states)\n         # get key, value proj\n-        if is_cross_attention and past_key_value is not None:\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n+        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+        if past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+\n+        past_key_value = (key_states, value_states)\n \n         query_length = query_states.shape[1]\n         tgt_len = key_states.shape[-2]\n@@ -380,7 +326,6 @@ class OPTSdpaAttention(OPTAttention):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n@@ -399,44 +344,22 @@ def forward(\n                 layer_head_mask=layer_head_mask,\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n-                key_value_states=key_value_states,\n             )  # TODO after merge add position_ids=position_ids\n-        is_cross_attention = key_value_states is not None\n \n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states) * self.scaling\n         query_states = self._shape(query_states, -1, bsz)\n \n         # get key, value proj\n-        if is_cross_attention and past_key_value is not None:\n-            # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n-        elif is_cross_attention:\n-            # cross_attentions\n-            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n-        elif past_key_value is not None:\n+        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+        if past_key_value is not None:\n             # reuse k, v, self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n             key_states = torch.cat([past_key_value[0], key_states], dim=2)\n             value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-        else:\n-            # self_attention\n-            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-\n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n+\n+        past_key_value = (key_states, value_states)\n \n         # shape now is (bsz, num_heads, seq_len, head_dim), all are continuous\n \n@@ -480,7 +403,7 @@ def __init__(self, config: OPTConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n-        self.self_attn = OPT_ATTENTION_CLASSES[config._attn_implementation](config=config, is_decoder=True)\n+        self.self_attn = OPT_ATTENTION_CLASSES[config._attn_implementation](config=config)\n \n         self.do_layer_norm_before = config.do_layer_norm_before\n         self.dropout = config.dropout"
        }
    ],
    "stats": {
        "total": 109,
        "additions": 16,
        "deletions": 93
    }
}