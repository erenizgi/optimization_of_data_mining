{
    "author": "DeXtAr47-oss",
    "message": "add fuyu fast image processors (#41817)\n\n* added fast processor for fuyu (#36978)\n\n* updated docs for fuyu model (#36978)\n\n* updated test_image_processing  and image_processing_fuyu_fast\n\n* updated fuyu.md and image_processing_fuyu_fast (#36978)\n\n* updated test_image_processing_fuyu (#36978)\n\n* formatted image_processing_fuyu_fast and test_image_processing_fuyu (#36978)\n\n* updated tests and fuyu fast image processing (#36978)\n\n* Merge branch 'fuyu-fast-image-processors' of https://github.com/DeXtAr47-oss/transformers into fuyu-fast-image-processors\n\n* fixed format (#36978)\n\n* formatted files (#36978)\n\n* formatted files\n\n* revert unnecessary changes\n\n* clean up and process by group\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "325810e7fccf8273599c58a525ae0011ea8ba3e6",
    "files": [
        {
            "sha": "57f0de1eb244def3d6549f4797b2e9b913192662",
            "filename": "docs/source/en/model_doc/fuyu.md",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/325810e7fccf8273599c58a525ae0011ea8ba3e6/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/325810e7fccf8273599c58a525ae0011ea8ba3e6/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md?ref=325810e7fccf8273599c58a525ae0011ea8ba3e6",
            "patch": "@@ -75,11 +75,11 @@ A processor requires an image_processor and a tokenizer. Hence, inputs can be lo\n from PIL import Image\n from transformers import AutoTokenizer\n from transformers.models.fuyu.processing_fuyu import FuyuProcessor\n-from transformers.models.fuyu.image_processing_fuyu import FuyuImageProcessor\n+from transformers.models.fuyu.image_processing_fuyu_fast import FuyuImageProcessorFast\n \n \n tokenizer = AutoTokenizer.from_pretrained('adept-hf-collab/fuyu-8b')\n-image_processor = FuyuImageProcessor()\n+image_processor = FuyuImageProcessorFast()\n \n \n processor = FuyuProcessor(image_processor=image_processor, tokenizer=tokenizer)\n@@ -118,6 +118,11 @@ The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.\n [[autodoc]] FuyuImageProcessor\n     - __call__\n \n+## FuyuImageProcessor\n+\n+[[autodoc]] FuyuImageProcessorFast\n+    - __call__\n+\n ## FuyuProcessor\n \n [[autodoc]] FuyuProcessor"
        },
        {
            "sha": "f675da162079270c15934be2a870d301f28492c6",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=325810e7fccf8273599c58a525ae0011ea8ba3e6",
            "patch": "@@ -228,6 +228,7 @@ def pad(\n         padding_mode: Optional[str] = \"constant\",\n         return_mask: bool = False,\n         disable_grouping: Optional[bool] = False,\n+        is_nested: Optional[bool] = False,\n         **kwargs,\n     ) -> Union[tuple[\"torch.Tensor\", \"torch.Tensor\"], \"torch.Tensor\"]:\n         \"\"\"\n@@ -258,7 +259,9 @@ def pad(\n         else:\n             pad_size = get_max_height_width(images)\n \n-        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, disable_grouping=disable_grouping, is_nested=is_nested\n+        )\n         processed_images_grouped = {}\n         processed_masks_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n@@ -281,9 +284,9 @@ def pad(\n                 stacked_masks[..., : image_size[0], : image_size[1]] = 1\n                 processed_masks_grouped[shape] = stacked_masks\n \n-        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=is_nested)\n         if return_mask:\n-            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index)\n+            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index, is_nested=is_nested)\n             return processed_images, processed_masks\n \n         return processed_images"
        },
        {
            "sha": "6c9b69db4555c9b9f075408013775aa85d7e90c2",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=325810e7fccf8273599c58a525ae0011ea8ba3e6",
            "patch": "@@ -98,7 +98,7 @@\n             (\"eomt\", (\"EomtImageProcessor\", \"EomtImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),\n             (\"focalnet\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n-            (\"fuyu\", (\"FuyuImageProcessor\", None)),\n+            (\"fuyu\", (\"FuyuImageProcessor\", \"FuyuImageProcessorFast\")),\n             (\"gemma3\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
        },
        {
            "sha": "eca3cf7c411b9fdad16fea1cc43a40c980bdcd0c",
            "filename": "src/transformers/models/fuyu/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fmodels%2Ffuyu%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fmodels%2Ffuyu%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2F__init__.py?ref=325810e7fccf8273599c58a525ae0011ea8ba3e6",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_fuyu import *\n     from .image_processing_fuyu import *\n+    from .image_processing_fuyu_fast import *\n     from .modeling_fuyu import *\n     from .processing_fuyu import *\n else:"
        },
        {
            "sha": "e86352af1bf5fdd3c21433197c824a5cadca3953",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py?ref=325810e7fccf8273599c58a525ae0011ea8ba3e6",
            "patch": "@@ -29,6 +29,7 @@\n     ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n+    SizeDict,\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n@@ -37,6 +38,7 @@\n     to_numpy_array,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -70,6 +72,21 @@ def make_list_of_list_of_images(\n     raise ValueError(\"images must be a list of list of images or a list of images or an image.\")\n \n \n+class FuyuImagesKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    patch_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 30, \"width\": 30}`):\n+        Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+    padding_value (`float`, *optional*, defaults to 1.0):\n+        The value to pad the image with.\n+    padding_mode (`str`, *optional*, defaults to \"constant\"):\n+        The padding mode to use when padding the image.\n+    \"\"\"\n+\n+    patch_size: Optional[SizeDict]\n+    padding_value: float\n+    padding_mode: str\n+\n+\n class FuyuBatchFeature(BatchFeature):\n     \"\"\"\n     BatchFeature class for Fuyu image processor and processor.\n@@ -232,6 +249,7 @@ class FuyuImageProcessor(BaseImageProcessor):\n         \"image_patch_indices_per_batch\",\n         \"image_patch_indices_per_subsequence\",\n     ]\n+    valid_kwargs = FuyuImagesKwargs\n \n     def __init__(\n         self,"
        },
        {
            "sha": "4c9c2802e8df76a1b7b92872e93e9d3f8ae32709",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu_fast.py",
            "status": "added",
            "additions": 382,
            "deletions": 0,
            "changes": 382,
            "blob_url": "https://github.com/huggingface/transformers/blob/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/325810e7fccf8273599c58a525ae0011ea8ba3e6/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu_fast.py?ref=325810e7fccf8273599c58a525ae0011ea8ba3e6",
            "patch": "@@ -0,0 +1,382 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Fuyu.\"\"\"\n+\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils import get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torchvision_available,\n+    logging,\n+    requires_backends,\n+)\n+from .image_processing_fuyu import FuyuBatchFeature, FuyuImagesKwargs, make_list_of_list_of_images\n+\n+\n+if is_torchvision_available():\n+    from torchvision.transforms.v2 import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@auto_docstring\n+class FuyuImageProcessorFast(BaseImageProcessorFast):\n+    do_resize = True\n+    size = {\"height\": 1080, \"width\": 1920}\n+    resample = PILImageResampling.BILINEAR\n+    do_pad = True\n+    padding_value = 1.0\n+    padding_mode = \"constant\"\n+    do_normalize = True\n+    image_mean = 0.5\n+    image_std = 0.5\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    model_input_names = [\n+        \"images\",\n+        \"image_input_ids\",\n+        \"image_patches\",\n+        \"image_patch_indices_per_batch\",\n+        \"image_patch_indices_per_subsequence\",\n+    ]\n+    valid_kwargs = FuyuImagesKwargs\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        expected_ndims: int = 3,\n+    ) -> ImageInput:\n+        images = self.fetch_images(images)\n+        return make_list_of_list_of_images(images)\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize an image to fit within `(size[\"height\"], size[\"width\"])` while maintaining aspect ratio.\n+        Only resizes if the image is larger than the target size.\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the max size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BILINEAR`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to apply antialiasing when resizing.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        image_height, image_width = image.shape[-2:]\n+        target_height, target_width = size.height, size.width\n+        # Only resize if image is larger than target\n+        if image_width <= target_width and image_height <= target_height:\n+            return image\n+        # Calculate optimal scale factor to fit within target size\n+        height_scale_factor = target_height / image_height\n+        width_scale_factor = target_width / image_width\n+        optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n+\n+        new_height = int(image_height * optimal_scale_factor)\n+        new_width = int(image_width * optimal_scale_factor)\n+\n+        return super().resize(\n+            image, SizeDict(height=new_height, width=new_width), interpolation=interpolation, antialias=antialias\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: Optional[bool],\n+        padding_value: Optional[float],\n+        padding_mode: Optional[str],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> FuyuBatchFeature:\n+        # Group images by size for batched resizing\n+        original_image_sizes = [batch_image[0].shape[-2:] for batch_image in images if batch_image]\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index, is_nested=True)\n+\n+        image_sizes = [batch_image[0].shape[-2:] for batch_image in resized_images if batch_image]\n+        image_unpadded_heights = [[image_size[0]] for image_size in image_sizes]\n+        image_unpadded_widths = [[image_size[1]] for image_size in image_sizes]\n+        image_scale_factors = [\n+            [resized_size[0] / original_size[0]]\n+            for original_size, resized_size in zip(original_image_sizes, image_sizes)\n+        ]\n+        if do_pad:\n+            resized_images = self.pad(\n+                resized_images,\n+                pad_size=size,\n+                fill_value=padding_value,\n+                padding_mode=padding_mode,\n+                disable_grouping=disable_grouping,\n+                is_nested=True,\n+            )\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            resized_images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=True)\n+\n+        return FuyuBatchFeature(\n+            data={\n+                \"images\": processed_images,\n+                \"image_unpadded_heights\": image_unpadded_heights,\n+                \"image_unpadded_widths\": image_unpadded_widths,\n+                \"image_scale_factors\": image_scale_factors,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+    def get_num_patches(self, image_height: int, image_width: int, patch_size: Optional[SizeDict] = None) -> int:\n+        \"\"\"\n+        Calculate number of patches required to encode an image.\n+        Args:\n+            image_height (`int`):\n+                Height of the image.\n+            image_width (`int`):\n+                Width of the image.\n+            patch_size (`SizeDict`, *optional*):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+        \"\"\"\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        if image_height % patch_height != 0:\n+            raise ValueError(f\"{image_height=} must be divisible by {patch_height}\")\n+        if image_width % patch_width != 0:\n+            raise ValueError(f\"{image_width=} must be divisible by {patch_width}\")\n+        num_patches_per_dim_h = image_height // patch_height\n+        num_patches_per_dim_w = image_width // patch_width\n+        num_patches = num_patches_per_dim_h * num_patches_per_dim_w\n+        return num_patches\n+\n+    def patchify_image(self, image: torch.Tensor, patch_size: Optional[SizeDict] = None) -> torch.Tensor:\n+        \"\"\"\n+        Convert an image into a tensor of patches using PyTorch's unfold operation.\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to convert. Shape: [batch, channels, height, width]\n+            patch_size (`SizeDict`, *optional*):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        batch_size, channels, _, _ = image.shape\n+        # Use unfold to extract patches\n+        unfolded_along_height = image.unfold(2, patch_height, patch_height)\n+        patches = unfolded_along_height.unfold(3, patch_width, patch_width)\n+        patches = patches.contiguous()\n+        # Reshape to [batch, num_patches, channels * patch_h * patch_w]\n+        patches = patches.view(batch_size, channels, -1, patch_height, patch_width)\n+        patches = patches.permute(0, 2, 3, 4, 1)\n+        patches = patches.reshape(batch_size, -1, channels * patch_height * patch_width)\n+        return patches\n+\n+    def preprocess_with_tokenizer_info(\n+        self,\n+        image_input: torch.Tensor,\n+        image_present: torch.Tensor,\n+        image_unpadded_h: torch.Tensor,\n+        image_unpadded_w: torch.Tensor,\n+        image_placeholder_id: int,\n+        image_newline_id: int,\n+        variable_sized: bool,\n+        patch_size: Optional[dict[str, int]] = None,\n+    ) -> FuyuBatchFeature:\n+        \"\"\"\n+        Process images for model input. In particular, variable-sized images are handled here.\n+\n+        Args:\n+            image_input (`torch.Tensor` of shape [batch_size, subsequence_size, num_channels, height, width]):\n+                Tensor of images padded to model input size.\n+            image_present (`torch.Tensor` of shape [batch_size, subsequence_size, num_images]):\n+                Tensor of 1s and 0s indicating whether an image is present.\n+            image_unpadded_h (`torch.Tensor` of shape [batch_size, subsequence_size]):\n+                Tensor of unpadded image heights.\n+            image_unpadded_w (`torch.Tensor` of shape [batch_size, subsequence_size]):\n+                Tensor of unpadded image widths.\n+            image_placeholder_id (int):\n+                The id of the image placeholder token. Comes from an associated tokenizer.\n+            image_newline_id (int):\n+                The id of the image newline token. Comes from an associated tokenizer.\n+            variable_sized (bool):\n+                Whether to process images as variable-sized.\n+            patch_size (`dict[str, int]`, *optional*):\n+                Size of the patches.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        else:\n+            patch_size = SizeDict(**patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        # Only images that are present\n+        images: list[list[torch.Tensor]] = []\n+        batch_image_patches: list[list[torch.Tensor]] = []\n+        # Image input ids for every subsequence, including ones with no image present\n+        batch_image_input_ids: list[list[torch.Tensor]] = []\n+        for batch_index in range(image_input.shape[0]):\n+            image_input_ids = []\n+            image_patches = []\n+            for subseq_index in range(image_input.shape[1]):\n+                if image_present[batch_index, subseq_index]:\n+                    image = image_input[batch_index, subseq_index]\n+                    image_height, image_width = image.shape[1], image.shape[2]\n+                    if variable_sized:\n+                        # Calculate new dimensions based on unpadded size\n+                        # The min() is required here due to floating point issues\n+                        new_h = min(\n+                            image_height,\n+                            math.ceil(image_unpadded_h[batch_index, subseq_index] / patch_height) * patch_height,\n+                        )\n+                        new_w = min(\n+                            image_width,\n+                            math.ceil(image_unpadded_w[batch_index, subseq_index] / patch_width) * patch_width,\n+                        )\n+                        image = image[:, :new_h, :new_w]\n+                        image_height, image_width = new_h, new_w\n+                    num_patches = self.get_num_patches(\n+                        image_height=image_height, image_width=image_width, patch_size=patch_size\n+                    )\n+                    # Create tensor of placeholder IDs\n+                    tensor_of_image_ids = torch.full(\n+                        [num_patches], image_placeholder_id, dtype=torch.int32, device=image_input.device\n+                    )\n+                    # Patchify the image\n+                    patches = self.patchify_image(image=image.unsqueeze(0), patch_size=patch_size).squeeze(0)\n+                    assert num_patches == patches.shape[0]\n+                    if variable_sized:\n+                        # Terminate each line with newline ID\n+                        tensor_of_image_ids = tensor_of_image_ids.reshape(-1, image_width // patch_width)\n+                        newline_ids = torch.full(\n+                            [tensor_of_image_ids.shape[0], 1],\n+                            image_newline_id,\n+                            dtype=torch.int32,\n+                            device=image_input.device,\n+                        )\n+                        tensor_of_image_ids = torch.cat([tensor_of_image_ids, newline_ids], dim=1)\n+                        tensor_of_image_ids = tensor_of_image_ids.reshape(-1)\n+                    images.append([image])\n+                    image_input_ids.append(tensor_of_image_ids)\n+                    image_patches.append(patches)\n+                else:\n+                    image_input_ids.append(torch.tensor([], dtype=torch.int32, device=image_input.device))\n+            batch_image_input_ids.append(image_input_ids)\n+            batch_image_patches.append(image_patches)\n+        # Create image patch indices\n+        image_patch_indices_per_batch: list[list[torch.Tensor]] = []\n+        image_patch_indices_per_subsequence: list[list[torch.Tensor]] = []\n+\n+        for sample_image_input_ids in batch_image_input_ids:\n+            index_offset = 0\n+            per_batch_indices = []\n+            per_subsequence_indices = []\n+            for subseq_image_input_ids in sample_image_input_ids:\n+                # Indices of image patches\n+                patches_mask = subseq_image_input_ids == image_placeholder_id\n+                num_patches = torch.count_nonzero(patches_mask)\n+                indices = torch.arange(num_patches, dtype=torch.int64, device=subseq_image_input_ids.device).type_as(\n+                    subseq_image_input_ids\n+                )\n+                # Place those indices in the image input ids token stream, with -1 representing non-index tokens\n+                indices_in_stream_per_batch = torch.full_like(subseq_image_input_ids, -1)\n+                indices_in_stream_per_subsequence = torch.full_like(subseq_image_input_ids, -1)\n+                patches_inds = torch.nonzero(patches_mask, as_tuple=True)[0]\n+\n+                indices_in_stream_per_batch[patches_inds] = indices + index_offset\n+                indices_in_stream_per_subsequence[patches_inds] = indices\n+\n+                per_batch_indices.append(indices_in_stream_per_batch)\n+                per_subsequence_indices.append(indices_in_stream_per_subsequence)\n+                index_offset += num_patches\n+\n+            image_patch_indices_per_batch.append(per_batch_indices)\n+            image_patch_indices_per_subsequence.append(per_subsequence_indices)\n+        return FuyuBatchFeature(\n+            data={\n+                \"images\": images,\n+                \"image_input_ids\": batch_image_input_ids,\n+                \"image_patches\": batch_image_patches,\n+                \"image_patch_indices_per_batch\": image_patch_indices_per_batch,\n+                \"image_patch_indices_per_subsequence\": image_patch_indices_per_subsequence,\n+            }\n+        )\n+\n+    def _further_process_kwargs(\n+        self,\n+        patch_size: Optional[dict[str, int]] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Process Fuyu-specific kwargs before validation.\n+        \"\"\"\n+        kwargs = super()._further_process_kwargs(**kwargs)\n+        if patch_size is not None:\n+            patch_size = SizeDict(**get_size_dict(patch_size, param_name=\"patch_size\"))\n+        kwargs[\"patch_size\"] = patch_size\n+        return kwargs\n+\n+\n+__all__ = [\"FuyuImageProcessorFast\"]"
        },
        {
            "sha": "24b19b01a0293192bbe885e49128ab96358b58f3",
            "filename": "tests/models/fuyu/test_image_processing_fuyu.py",
            "status": "modified",
            "additions": 432,
            "deletions": 29,
            "changes": 461,
            "blob_url": "https://github.com/huggingface/transformers/blob/325810e7fccf8273599c58a525ae0011ea8ba3e6/tests%2Fmodels%2Ffuyu%2Ftest_image_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/325810e7fccf8273599c58a525ae0011ea8ba3e6/tests%2Fmodels%2Ffuyu%2Ftest_image_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_image_processing_fuyu.py?ref=325810e7fccf8273599c58a525ae0011ea8ba3e6",
            "patch": "@@ -1,63 +1,466 @@\n+import io\n import unittest\n \n+import httpx\n import numpy as np\n+import pytest\n+from packaging import version\n \n-from transformers import is_torch_available, is_vision_available\n+from transformers.image_utils import SizeDict\n from transformers.testing_utils import (\n     require_torch,\n+    require_torch_accelerator,\n     require_torchvision,\n     require_vision,\n+    slow,\n+    torch_device,\n )\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin\n \n \n if is_torch_available() and is_vision_available():\n     import torch\n \n-    from transformers import FuyuImageProcessor\n+    from transformers import FuyuImageProcessor, FuyuImageProcessorFast\n \n if is_vision_available():\n     from PIL import Image\n \n \n+class FuyuImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_pad=True,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        patch_size=None,\n+    ):\n+        size = size if size is not None else {\"height\": 180, \"width\": 360}\n+        patch_size = patch_size if patch_size is not None else {\"height\": 30, \"width\": 30}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = 30\n+        self.max_resolution = 360\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_pad = do_pad\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.patch_size = patch_size\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_pad\": self.do_pad,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"patch_size\": self.patch_size,\n+        }\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        \"\"\"Prepares a batch of images for testing\"\"\"\n+        if equal_resolution:\n+            image_inputs = [\n+                np.random.randint(\n+                    0, 256, (self.num_channels, self.max_resolution, self.max_resolution), dtype=np.uint8\n+                )\n+                for _ in range(self.batch_size)\n+            ]\n+        else:\n+            heights = [\n+                h - (h % 30) for h in np.random.randint(self.min_resolution, self.max_resolution, self.batch_size)\n+            ]\n+            widths = [\n+                w - (w % 30) for w in np.random.randint(self.min_resolution, self.max_resolution, self.batch_size)\n+            ]\n+\n+            image_inputs = [\n+                np.random.randint(0, 256, (self.num_channels, height, width), dtype=np.uint8)\n+                for height, width in zip(heights, widths)\n+            ]\n+\n+        if not numpify and not torchify:\n+            image_inputs = [Image.fromarray(np.moveaxis(img, 0, -1)) for img in image_inputs]\n+\n+        if torchify:\n+            image_inputs = [torch.from_numpy(img) for img in image_inputs]\n+\n+        return image_inputs\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+\n @require_torch\n @require_vision\n @require_torchvision\n-class TestFuyuImageProcessor(unittest.TestCase):\n+class FuyuImageProcessorTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = FuyuImageProcessor\n+    fast_image_processing_class = FuyuImageProcessorFast\n+\n+    # Skip tests that expect pixel_values output\n+    test_cast_dtype = None\n+\n     def setUp(self):\n-        self.size = {\"height\": 160, \"width\": 320}\n-        self.processor = FuyuImageProcessor(size=self.size, padding_value=1.0)\n-        self.batch_size = 3\n-        self.channels = 3\n-        self.height = 300\n-        self.width = 300\n+        self.image_processor_tester = FuyuImageProcessingTester(self)\n+        self.image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n \n-        self.image_input = torch.rand(self.batch_size, self.channels, self.height, self.width)\n+        # Initialize image_processor_list (from ImageProcessingTestMixin)\n+        image_processor_list = []\n+        if self.test_slow_image_processor and self.image_processing_class:\n+            image_processor_list.append(self.image_processing_class)\n+        if self.test_fast_image_processor and self.fast_image_processing_class:\n+            image_processor_list.append(self.fast_image_processing_class)\n+        self.image_processor_list = image_processor_list\n \n-        self.image_patch_dim_h = 30\n-        self.image_patch_dim_w = 30\n-        self.sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n-        self.sample_image_pil = Image.fromarray(self.sample_image)\n+    def test_call_pil(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n \n-    def test_patches(self):\n-        expected_num_patches = self.processor.get_num_patches(image_height=self.height, image_width=self.width)\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n+\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_numpy(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n+\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_pytorch(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n \n-        patches_final = self.processor.patchify_image(image=self.image_input)\n-        assert patches_final.shape[1] == expected_num_patches, (\n-            f\"Expected {expected_num_patches} patches, got {patches_final.shape[1]}.\"\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_numpy_4_channels(self):\n+        \"\"\"Skip this test as Fuyu doesn't support arbitrary channels\"\"\"\n+        self.skipTest(\"Fuyu processor is designed for 3-channel RGB images\")\n+\n+    def test_slow_fast_equivalence(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+        dummy_image = Image.open(\n+            io.BytesIO(\n+                httpx.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", follow_redirects=True).content\n+            )\n         )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.images[0][0], encoding_fast.images[0][0])\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        # Compare each image tensor\n+        for slow_img, fast_img in zip(encoding_slow.images, encoding_fast.images):\n+            self._assert_slow_fast_tensors_equivalence(slow_img[0], fast_img[0])\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.images[0][0], output_compiled.images[0][0], atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processor, \"image_std\"))\n+            self.assertTrue(hasattr(image_processor, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processor, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processor, \"patch_size\"))\n+\n+    def test_patches(self):\n+        \"\"\"Test that patchify_image produces the expected number of patches.\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            batch_size = 3\n+            channels = 3\n+            height = 300\n+            width = 300\n+            image_input = torch.rand(batch_size, channels, height, width)\n+\n+            expected_num_patches = image_processor.get_num_patches(image_height=height, image_width=width)\n+            patches_final = image_processor.patchify_image(image=image_input)\n+\n+            self.assertEqual(patches_final.shape[1], expected_num_patches)\n+\n+    def test_patches_match_slow_fast(self):\n+        \"\"\"Test that fast processor produces same patches as slow processor.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast patch equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(\n+                reason=\"Skipping slow/fast patch equivalence test as one of the image processors is not defined\"\n+            )\n+\n+        batch_size = 3\n+        channels = 3\n+        height = 300\n+        width = 300\n+        image_input = torch.rand(batch_size, channels, height, width)\n+\n+        processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        patches_fast = processor_fast.patchify_image(image=image_input)\n+        patches_slow = processor_slow.patchify_image(image=image_input)\n+\n+        self.assertEqual(patches_fast.shape, patches_slow.shape)\n+        torch.testing.assert_close(patches_fast, patches_slow, rtol=1e-4, atol=1e-4)\n \n     def test_scale_to_target_aspect_ratio(self):\n-        # (h:450, w:210) fitting (160, 320) -> (160, 210*160/450)\n-        scaled_image = self.processor.resize(self.sample_image, size=self.size)\n-        self.assertEqual(scaled_image.shape[0], 160)\n-        self.assertEqual(scaled_image.shape[1], 74)\n+        \"\"\"Test that resize maintains aspect ratio correctly.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        if self.test_slow_image_processor and self.image_processing_class:\n+            image_processor = self.image_processing_class(**self.image_processor_dict)\n+            scaled_image = image_processor.resize(sample_image, size=self.image_processor_dict[\"size\"])\n+            self.assertEqual(scaled_image.shape[0], 180)\n+            self.assertEqual(scaled_image.shape[1], 84)\n+\n+        if self.test_fast_image_processor and self.fast_image_processing_class:\n+            image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+            sample_tensor = torch.from_numpy(sample_image).permute(2, 0, 1).float()\n+\n+            size_dict = SizeDict(\n+                height=self.image_processor_dict[\"size\"][\"height\"], width=self.image_processor_dict[\"size\"][\"width\"]\n+            )\n+            scaled_image = image_processor_fast.resize(sample_tensor, size=size_dict)\n+\n+            self.assertEqual(scaled_image.shape[1], 180)\n+            self.assertEqual(scaled_image.shape[2], 84)\n \n     def test_apply_transformation_numpy(self):\n-        transformed_image = self.processor.preprocess(self.sample_image).images[0][0]\n-        self.assertEqual(transformed_image.shape[1], 160)\n-        self.assertEqual(transformed_image.shape[2], 320)\n+        \"\"\"Test preprocessing with numpy input.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            transformed_image = image_processor.preprocess(sample_image).images[0][0]\n+            self.assertEqual(transformed_image.shape[1], 180)\n+            self.assertEqual(transformed_image.shape[2], 360)\n \n     def test_apply_transformation_pil(self):\n-        transformed_image = self.processor.preprocess(self.sample_image_pil).images[0][0]\n-        self.assertEqual(transformed_image.shape[1], 160)\n-        self.assertEqual(transformed_image.shape[2], 320)\n+        \"\"\"Test preprocessing with PIL input.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        sample_image_pil = Image.fromarray(sample_image)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            transformed_image = image_processor.preprocess(sample_image_pil).images[0][0]\n+            self.assertEqual(transformed_image.shape[1], 180)\n+            self.assertEqual(transformed_image.shape[2], 360)\n+\n+    def test_preprocess_output_structure(self):\n+        \"\"\"Test that preprocess returns correct output structure.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            result = image_processor.preprocess(sample_image)\n+\n+            self.assertIn(\"images\", result)\n+            self.assertIn(\"image_unpadded_heights\", result)\n+            self.assertIn(\"image_unpadded_widths\", result)\n+            self.assertIn(\"image_scale_factors\", result)\n+\n+            self.assertEqual(len(result.images), 1)\n+            self.assertEqual(len(result.images[0]), 1)\n+            self.assertEqual(len(result.image_unpadded_heights), 1)\n+            self.assertEqual(len(result.image_unpadded_widths), 1)\n+            self.assertEqual(len(result.image_scale_factors), 1)\n+\n+    def test_batch_processing(self):\n+        \"\"\"Test processing multiple images.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        sample_image_pil = Image.fromarray(sample_image)\n+        images = [sample_image, sample_image_pil]\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            result = image_processor.preprocess(images)\n+\n+            self.assertEqual(len(result.images), 2)\n+            for img in result.images:\n+                self.assertEqual(len(img), 1)\n+                if hasattr(img[0], \"shape\"):\n+                    if len(img[0].shape) == 3:\n+                        self.assertEqual(img[0].shape[1], 180)\n+                        self.assertEqual(img[0].shape[2], 360)\n+\n+    def test_pad_image_fast(self):\n+        \"\"\"Test that padding works correctly for fast processor.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        from transformers.image_utils import SizeDict\n+\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        small_image = torch.rand(3, 100, 100)\n+        size_dict = SizeDict(height=180, width=360)\n+\n+        padded = image_processor_fast.pad([small_image], pad_size=size_dict, fill_value=1.0)[0]\n+        self.assertEqual(padded.shape[1], 180)\n+        self.assertEqual(padded.shape[2], 360)\n+\n+        self.assertTrue(torch.allclose(padded[:, 100:, :], torch.ones_like(padded[:, 100:, :])))\n+        self.assertTrue(torch.allclose(padded[:, :, 100:], torch.ones_like(padded[:, :, 100:])))\n+\n+    def test_preprocess_with_tokenizer_info(self):\n+        \"\"\"Test preprocess_with_tokenizer_info functionality.\"\"\"\n+        batch_size = 2\n+        subseq_size = 1\n+        channels = 3\n+        image_input = torch.rand(batch_size, subseq_size, channels, 180, 360)\n+        image_present = torch.ones(batch_size, subseq_size, dtype=torch.bool)\n+        image_unpadded_h = torch.tensor([[180], [180]])\n+        image_unpadded_w = torch.tensor([[360], [360]])\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            result = image_processor.preprocess_with_tokenizer_info(\n+                image_input=image_input,\n+                image_present=image_present,\n+                image_unpadded_h=image_unpadded_h,\n+                image_unpadded_w=image_unpadded_w,\n+                image_placeholder_id=100,\n+                image_newline_id=101,\n+                variable_sized=True,\n+            )\n+\n+            # Check output structure\n+            self.assertIn(\"images\", result)\n+            self.assertIn(\"image_input_ids\", result)\n+            self.assertIn(\"image_patches\", result)\n+            self.assertIn(\"image_patch_indices_per_batch\", result)\n+            self.assertIn(\"image_patch_indices_per_subsequence\", result)\n+\n+            # Check batch structure\n+            self.assertEqual(len(result.images), batch_size)\n+            self.assertEqual(len(result.image_input_ids), batch_size)\n+            self.assertEqual(len(result.image_patches), batch_size)\n+\n+    def test_device_handling_fast(self):\n+        \"\"\"Test that fast processor can handle device placement.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        if torch.cuda.is_available():\n+            result_cuda = image_processor_fast.preprocess(sample_image, device=\"cuda\")\n+            self.assertEqual(result_cuda.images[0][0].device.type, \"cuda\")\n+\n+        result_cpu = image_processor_fast.preprocess(sample_image, device=\"cpu\")\n+        self.assertEqual(result_cpu.images[0][0].device.type, \"cpu\")\n+\n+    def test_do_not_resize_if_smaller(self):\n+        \"\"\"Test that images smaller than target size are not resized.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        small_image = torch.rand(3, 100, 150)\n+        size_dict = SizeDict(height=180, width=360)\n+\n+        resized = image_processor_fast.resize(small_image, size=size_dict)\n+\n+        self.assertEqual(resized.shape[1], 100)\n+        self.assertEqual(resized.shape[2], 150)"
        }
    ],
    "stats": {
        "total": 882,
        "additions": 847,
        "deletions": 35
    }
}