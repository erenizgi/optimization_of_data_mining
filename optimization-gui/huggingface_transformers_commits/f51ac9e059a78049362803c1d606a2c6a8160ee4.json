{
    "author": "gante",
    "message": "Generate: visit non-llm `prepare_inputs_for_generation` (#34199)\n\n* tmp\r\n\r\n* all visited\r\n\r\n* test all\r\n\r\n* Update src/transformers/models/moshi/modeling_moshi.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* delete another one :D\r\n\r\n---------\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "f51ac9e059a78049362803c1d606a2c6a8160ee4",
    "files": [
        {
            "sha": "9ede527ecb7b80bbbc5eeb1d76a72df08c0c9340",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 10,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -390,13 +390,16 @@ def prepare_inputs_for_generation(\n         # 3. Prepare base model inputs\n         input_ids_key = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and not self.config.is_encoder_decoder and cache_position[0] == 0:\n-            model_inputs[input_ids_key] = None\n-            model_inputs[\"inputs_embeds\"] = inputs_embeds\n+        if not self.config.is_encoder_decoder:\n+            if inputs_embeds is not None and cache_position[0] == 0:\n+                model_inputs[input_ids_key] = None\n+                model_inputs[\"inputs_embeds\"] = inputs_embeds\n+            else:\n+                # `clone` calls in this function ensure a consistent stride. See #32227\n+                model_inputs[input_ids_key] = input_ids.clone(memory_format=torch.contiguous_format)\n+                model_inputs[\"inputs_embeds\"] = None\n         else:\n-            # `clone` calls in this function ensure a consistent stride. See #32227\n             model_inputs[input_ids_key] = input_ids.clone(memory_format=torch.contiguous_format)\n-            model_inputs[\"inputs_embeds\"] = None\n \n         # 4. Create missing `position_ids` on the fly\n         if (\n@@ -428,10 +431,15 @@ def prepare_inputs_for_generation(\n \n             # Create the causal mask with fixed shape in advance, to reduce recompilations. If the function to create\n             # the 4D causal mask exists, it should be present in the base model (XXXModel class).\n-            base_model = getattr(self, self.base_model_prefix)\n-            causal_mask_creation_function = getattr(\n-                base_model, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n-            )\n+            base_model = getattr(self, self.base_model_prefix, None)\n+            if base_model is None:\n+                causal_mask_creation_function = getattr(\n+                    self, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n+                )\n+            else:\n+                causal_mask_creation_function = getattr(\n+                    base_model, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n+                )\n             if causal_mask_creation_function is None:\n                 logger.warning_once(\n                     f\"{self.__class__.__name__} has no `_prepare_4d_causal_attention_mask_with_cache_position` method \"\n@@ -444,10 +452,12 @@ def prepare_inputs_for_generation(\n                     attention_mask,\n                     sequence_length=sequence_length,\n                     target_length=past_key_values.get_max_cache_shape(),\n-                    dtype=self.get_output_embeddings().weight.dtype,\n+                    dtype=self.dtype,\n                     device=device,\n                     cache_position=cache_position,\n                     batch_size=batch_size,\n+                    config=self.config,\n+                    past_key_values=past_key_values,\n                 )\n         if attention_mask is not None:\n             model_inputs[\"attention_mask\"] = attention_mask"
        },
        {
            "sha": "f1c77367e5beb785b12a55d018dba01f1326b16b",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -578,6 +578,7 @@ def set_input_embeddings(self, new_embeddings):\n         self.input_embeds_layer = new_embeddings\n \n     def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwargs):\n+        # Overwritten -- bark has a model-specific hack\n         input_embeds = kwargs.get(\"input_embeds\", None)\n \n         attention_mask = kwargs.get(\"attention_mask\", None)"
        },
        {
            "sha": "520e7dab1f119dc2925a42c77dddd371bf8b0b52",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -3020,23 +3020,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, **kwargs\n-    ):\n-        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n-        if attention_mask is None:\n-            attention_mask = input_ids.new_ones(input_ids.shape)\n-\n-        if past_key_values:\n-            input_ids = input_ids[:, -1:]\n-        # first step, decoder_cached_states are empty\n-        return {\n-            \"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\n-            \"attention_mask\": attention_mask,\n-            \"past_key_values\": past_key_values,\n-            \"use_cache\": use_cache,\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "b0e9a4bbcb91bdb8c819e7ef975f8e1050466f21",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -806,6 +806,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "d0b964a7a6f484fd987a575154c2719520b2224c",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1451,6 +1451,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n@@ -1644,6 +1645,8 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here"
        },
        {
            "sha": "616c93a46e4f4aebb41dd2784457444bdf511a33",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -649,6 +649,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "3c14a6d28dee54fba45396cff11498cda20101dc",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1017,6 +1017,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "659fa154ecf7763d5dbdef56c63b08fcc9ce9794",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1179,6 +1179,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "d1029160dd0cc288d68e9191ee1272a9da2a43e7",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...configuration_utils import PretrainedConfig\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n@@ -166,7 +167,7 @@ def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start\n \n \n @add_start_docstrings(ENCODER_DECODER_START_DOCSTRING)\n-class EncoderDecoderModel(PreTrainedModel):\n+class EncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     r\"\"\"\n     [`EncoderDecoderModel`] is a generic model class that will be instantiated as a transformer architecture with one\n     of the base model classes of the library as encoder and another one as decoder when created with the\n@@ -666,20 +667,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, encoder_outputs=None, **kwargs\n-    ):\n-        decoder_inputs = self.decoder.prepare_inputs_for_generation(input_ids, past_key_values=past_key_values)\n-        input_dict = {\n-            \"attention_mask\": attention_mask,\n-            \"decoder_attention_mask\": decoder_inputs.get(\"attention_mask\"),\n-            \"decoder_input_ids\": decoder_inputs[\"input_ids\"],\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": decoder_inputs.get(\"past_key_values\"),\n-            \"use_cache\": use_cache,\n-        }\n-        return input_dict\n-\n     def resize_token_embeddings(self, *args, **kwargs):\n         raise NotImplementedError(\n             \"Resizing the embedding layers via the EncoderDecoderModel directly is not supported. Please use the\""
        },
        {
            "sha": "504dcf10b206c34301496481f73a7dd951c46b74",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1188,6 +1188,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "c8c758e6888a5973e28d8fb0b2d04afb9183f01a",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -344,6 +344,8 @@ def prepare_inputs_for_generation(\n         image_patches_indices=None,\n         **kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         if past_key_values:\n             input_ids = input_ids[:, -1:]\n "
        },
        {
            "sha": "f164c4add1fb5e8b160f8eaa254dc707a3aa73e1",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -933,6 +933,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "8f7e7364b54c95227ad9e105c8d70e6d814eaf3f",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -905,6 +905,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "0b86a41378fe0f7b42c8afa8fdeb50699f68636f",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1609,6 +1609,8 @@ def forward(\n     def prepare_inputs_for_generation(\n         self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, **kwargs\n     ):\n+        # Overwritten -- `git` has special cache handling and doesn't support generating from `inputs_embeds` atm\n+\n         # cut decoder_input_ids if past_key_values is used\n         if past_key_values is not None:\n             past_length = past_key_values.get_seq_length()"
        },
        {
            "sha": "28bfbabc1fd8e03d0a55a5f3e7970a0de9eefcae",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -863,6 +863,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "359996983eed74fc672606283e8d34ae1b2740c1",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1060,6 +1060,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "6c3f3313f57faf93f62052c2c69e2d75501eeca5",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -764,6 +764,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "1cc9cf369d1887aa09031e25e276b9584187847e",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -958,6 +958,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "bb8c157df30c89c281862e74f325191a761851cf",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -958,6 +958,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "f3e2d67734a703494091a07ea88c2db37911342a",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1194,6 +1194,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "bc983744559fc9aafe707040624e394179c99126",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1441,6 +1441,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n@@ -1674,6 +1675,8 @@ def prepare_inputs_for_generation(\n         use_cache=None,\n         **kwargs,\n     ):\n+        # Overwritten -- custom processing based on `config.use_resampler`\n+\n         model_inputs = {}\n         if image_hidden_states is not None:\n             if self.config.use_resampler:"
        },
        {
            "sha": "daa8bfb055b561d0ed9b1664aae2d9b8af887cec",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1665,6 +1665,9 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n+        # precedence is moved to the model, we can remove this fn)\n+\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         if past_key_values is not None:\n             if inputs_embeds is not None:  # Exception 1"
        },
        {
            "sha": "fb9f0a7c58fa5a26d0a9872c7ccd4bf3a6d5d780",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1256,6 +1256,9 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n+        # precedence is moved to the model, we can remove this fn)\n+\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         if past_key_values is not None:\n             if inputs_embeds is not None:  # Exception 1"
        },
        {
            "sha": "805c82be3881bc9304388cfeff11e98549ff529e",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1160,6 +1160,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "ffd8277f0268a3d90c96a84f78610e095557d5fc",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1696,6 +1696,8 @@ def prepare_inputs_for_generation(\n         use_cache=None,\n         **model_kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         input_shape = input_ids.shape\n         # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n         if attention_mask is None:"
        },
        {
            "sha": "40db21aeaea7d11df4eae4e49c738d8400b91d36",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1053,6 +1053,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "31593bc62d098cbdacd25c45d31b7fdd08d92600",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -590,6 +590,8 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         # Trigger the new behavior if we have more than image embeddings seq length tokens for images\n         legacy_processing = (\n             input_ids is not None"
        },
        {
            "sha": "03ab28dfff9cb11e862a5ed22e49bdcf013b484f",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -968,6 +968,8 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         legacy_processing = (\n             input_ids is not None\n             and (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length"
        },
        {
            "sha": "3fd6bb47fc76613ab5e09e6e8fe4745df55d61de",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1057,6 +1057,8 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- extra custom processing\n+\n         if input_ids is not None:\n             img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n                 1"
        },
        {
            "sha": "ec5a05733ec878c3ea58733e2a85cc37a3713fae",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -572,6 +572,8 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- extra custom processing\n+\n         if input_ids is not None:\n             img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n                 1"
        },
        {
            "sha": "7bacd2a54fc97feddb616861f893f0e933e0cd68",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -728,6 +728,8 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,"
        },
        {
            "sha": "3f26b5fe03d9f1d36a3602b556577854b4d23013",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 72,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1129,78 +1129,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n-        num_logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # `contiguous()` needed for compilation use cases\n-            model_inputs = {\"input_ids\": input_ids.contiguous(), \"inputs_embeds\": None}\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n-        )\n-        return model_inputs\n-\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "9248ad2187c38a4fad610ce7ccaf8a1758cb1858",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1367,76 +1367,6 @@ def forward(\n             router_logits=outputs.router_logits,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        output_router_logits=False,\n-        position_ids=None,\n-        use_cache=True,\n-        num_logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-                \"output_router_logits\": output_router_logits,\n-            }\n-        )\n-        return model_inputs\n-\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "b8d2879612aad219d2ac22c50577d2cfe03d817f",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1785,6 +1785,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n@@ -2171,6 +2172,8 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here"
        },
        {
            "sha": "97200b7d042e61b5efd5025c10b4d84ff725f1bf",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 11,
            "deletions": 143,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1400,90 +1400,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 )\n         return causal_mask\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids: torch.LongTensor,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        use_cache: bool = True,\n-        num_logits_to_keep: Optional[int] = None,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Prepare the model inputs for generation. In includes operations like computing the 4D attention mask or\n-        slicing inputs given the existing cache.\n-        See the documentation in the used model for the arguments (different models might have different requirements\n-        for e.g. `past_key_values`). Should work as is for most LLMs.\n-        \"\"\"\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s\n-                # `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the\n-                # decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case,\n-                # `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            # Create the causal mask with fixed shape in advance, to reduce recompilations. If the function to create\n-            # the 4D causal mask exists, it should be present in the base model (XXXModel class).\n-            attention_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.max_cache_len,\n-                dtype=self.text_embed_tokens.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-                \"last_hidden_state\": kwargs.get(\"last_hidden_state\"),\n-            }\n-        )\n-        return model_inputs\n-\n \n @add_start_docstrings(\n     \"The bare Moshi Model outputting raw hidden-states without any specific head on top.\",\n@@ -2492,66 +2408,18 @@ def prepare_inputs_for_generation(\n         blank_user_audio_codes: Optional[torch.FloatTensor] = None,\n         **kwargs,\n     ):\n+        # Overwritten -- Moshi has custom post-processing\n         # 1. Do usual operations done on LLMs like Gemma - because we pre-processed inputs, the first pass always has inputs_embeds\n-\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            dtype = self.decoder.dtype\n-\n-            attention_mask = self.decoder.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.max_cache_len,\n-                dtype=dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids=input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            num_logits_to_keep=num_logits_to_keep,\n+            **kwargs,\n         )\n \n         # 2. Now that everything is prepared, generate audio_codes using the depth decoder"
        },
        {
            "sha": "626097f5c7cbcca1393c3e463c2b556c76103886",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1345,6 +1345,7 @@ def prepare_inputs_for_generation(\n         guidance_scale=None,\n         **kwargs,\n     ):\n+        # Overwritten -- MusicGen has custom processing\n         if delay_pattern_mask is None:\n             input_ids, delay_pattern_mask = self.build_delay_pattern_mask(\n                 input_ids,\n@@ -2180,6 +2181,7 @@ def prepare_inputs_for_generation(\n         guidance_scale=None,\n         **kwargs,\n     ):\n+        # Overwritten -- MusicGen has custom processing\n         if decoder_delay_pattern_mask is None:\n             decoder_input_ids, decoder_delay_pattern_mask = self.decoder.build_delay_pattern_mask(\n                 decoder_input_ids,"
        },
        {
            "sha": "166623796d65d0016aa269cd52121dd6346570f2",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1254,6 +1254,7 @@ def prepare_inputs_for_generation(\n         guidance_scale=None,\n         **kwargs,\n     ):\n+        # Overwritten -- MusicGen has custom processing\n         if delay_pattern_mask is None:\n             input_ids, delay_pattern_mask = self.build_delay_pattern_mask(\n                 input_ids,\n@@ -2058,6 +2059,7 @@ def prepare_inputs_for_generation(\n         guidance_scale=None,\n         **kwargs,\n     ):\n+        # Overwritten -- MusicGen has custom processing\n         if decoder_delay_pattern_mask is None:\n             decoder_input_ids, decoder_delay_pattern_mask = self.decoder.build_delay_pattern_mask(\n                 decoder_input_ids,"
        },
        {
            "sha": "aa2fd93fbe916afc4130527a5bc9853df67164f7",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -931,6 +931,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "ff45fffb6e7396fe6c3230e9c0b0e412434da833",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -973,6 +973,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "d9d4a9771cd79d6768ed097463a94a139c3f7ff1",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1131,6 +1131,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "0eb2d50e0ad4c4d9f489f609d39fd8c0dc4d99cc",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 24,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -59,6 +59,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     batch_size: int,\n     is_training: bool = False,\n     token_type_ids: torch.Tensor = None,\n+    **kwargs,\n ):\n     \"\"\"\n     Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n@@ -572,6 +573,7 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- custom `position_ids` and `pixel_values` handling\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n@@ -581,33 +583,10 @@ def prepare_inputs_for_generation(\n             cache_position=cache_position,\n             use_cache=use_cache,\n             num_logits_to_keep=num_logits_to_keep,\n+            token_type_ids=token_type_ids,\n             **kwargs,\n         )\n \n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            dtype = self.get_output_embeddings().weight.dtype\n-            min_dtype = torch.finfo(dtype).min\n-\n-            model_inputs[\"attention_mask\"] = _prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n-                dtype=dtype,\n-                device=device,\n-                min_dtype=min_dtype,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-            )\n-\n-        model_inputs[\"token_type_ids\"] = token_type_ids\n-\n         # position_ids in Paligemma are 1-indexed\n         if model_inputs.get(\"position_ids\") is not None:\n             model_inputs[\"position_ids\"] += 1"
        },
        {
            "sha": "61d8b1002f3c1553f26eb568f4c0d09fd3c8fe16",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -800,6 +800,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "807b3fef4f44a0336f01b5423d65d9858817e6ea",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1091,6 +1091,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "a0b4b2ec378e3276de40ed97d5998b5051d615b3",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 57,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1351,63 +1351,16 @@ def prepare_inputs_for_generation(\n             if past_length <= self.config.original_max_position_embeddings:\n                 past_key_values = None\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids=input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            num_logits_to_keep=num_logits_to_keep,\n+            **kwargs,\n         )\n         return model_inputs\n "
        },
        {
            "sha": "1da65d7d39be4b5b203d362fc3247bdf8c5d6db1",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 57,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1541,63 +1541,16 @@ def prepare_inputs_for_generation(\n             if past_length <= self.config.original_max_position_embeddings:\n                 past_key_values = None\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids=input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            num_logits_to_keep=num_logits_to_keep,\n+            **kwargs,\n         )\n         return model_inputs\n "
        },
        {
            "sha": "37090677a6e254386b38815dc08b2382d3a2e9f7",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1739,46 +1739,3 @@ def forward(\n             encoder_hidden_states=encoder_outputs.hidden_states,\n             encoder_attentions=encoder_outputs.attentions,\n         )\n-\n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        flattened_patches: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n-        past_key_values=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        if decoder_attention_mask is None:\n-            decoder_attention_mask = torch.ones_like(input_ids).to(input_ids.device)\n-\n-        # cut decoder_input_ids if past_key_values is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"flattened_patches\": flattened_patches,\n-            \"decoder_input_ids\": input_ids,\n-            \"past_key_values\": past_key_values,\n-            \"encoder_outputs\": encoder_outputs,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,\n-        }"
        },
        {
            "sha": "d6f92e9fe034957e695dd5b7ff6ff4f27b101423",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1299,33 +1299,6 @@ def generate(\n             **kwargs,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            input_ids = input_ids[:, -1:]\n-\n-        return {\n-            \"decoder_input_ids\": input_ids,\n-            \"past_key_values\": past_key_values,\n-            \"encoder_outputs\": encoder_outputs,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return self._shift_right(labels)\n "
        },
        {
            "sha": "2e59ebd5eb98d107706ad20f1c8670c36f72f1ee",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1229,79 +1229,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    # Copied from transformers.models.mistral.modeling_mistral.MistralForCausalLM.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n-        num_logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # `contiguous()` needed for compilation use cases\n-            model_inputs = {\"input_ids\": input_ids.contiguous(), \"inputs_embeds\": None}\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n-        )\n-        return model_inputs\n-\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "e923e535da8e3461f4d27b946b9251e3f6797263",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1253,24 +1253,24 @@ def forward(\n             attention_mask=attention_mask,\n         )\n \n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.prepare_inputs_for_generation with image->audio\n     def prepare_inputs_for_generation(\n         self,\n         input_ids,\n         past_key_values=None,\n         inputs_embeds=None,\n-        input_features=None,  # Ignore copy\n+        input_features=None,\n         attention_mask=None,\n         **kwargs,\n     ):\n+        # Overwritten -- custom processing (note: might not be needed, but there are no generation tests running atm)\n+\n         if past_key_values is not None:\n             if isinstance(past_key_values, Cache):\n                 cache_length = past_key_values.get_seq_length()\n                 past_length = past_key_values.seen_tokens\n             else:\n                 cache_length = past_length = past_key_values[0][0].shape[2]\n \n-            # Ignore copy\n             # Here, we get the attention_mask, which was previously stored in the state after _merge_input_ids_with_audio_features.\n             if input_features is not None and kwargs.get(\"attention_mask\") is not None:\n                 attention_mask = kwargs[\"attention_mask\"]\n@@ -1310,7 +1310,6 @@ def prepare_inputs_for_generation(\n         else:\n             model_inputs = {\"input_ids\": input_ids}\n \n-        # Ignore copy\n         feature_attention_mask = kwargs.get(\"feature_attention_mask\", None)\n         model_inputs.update(\n             {"
        },
        {
            "sha": "1e741b4a9e3e576fee5d183517bd2b99f327cbcd",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1433,79 +1433,6 @@ def forward(\n             router_logits=outputs.router_logits,\n         )\n \n-    # Copied from transformers.models.mistral.modeling_mistral.MistralForCausalLM.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n-        num_logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # `contiguous()` needed for compilation use cases\n-            model_inputs = {\"input_ids\": input_ids.contiguous(), \"inputs_embeds\": None}\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n-        )\n-        return model_inputs\n-\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "5464b40546498adb1573e6b9c5f7bba198a51736",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1803,6 +1803,8 @@ def prepare_inputs_for_generation(\n         video_grid_thw=None,\n         **kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here"
        },
        {
            "sha": "c5c3b20284670554237ec0e2441259eef5d37160",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 132,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -2264,28 +2264,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.t2u_pad_token_id, self.config.t2u_decoder_start_token_id)\n \n@@ -2917,28 +2895,6 @@ def generate(\n             **kwargs,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()\n@@ -3209,28 +3165,6 @@ def generate(\n             **kwargs,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()\n@@ -3560,28 +3494,6 @@ def generate(\n \n         return waveform, waveform_lengths\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()\n@@ -3931,28 +3843,6 @@ def _reorder_cache(past_key_values, beam_idx):\n             )\n         return reordered_past\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n \n @add_start_docstrings(\n     \"The original SeamlessM4T Model transformer which can be used for every tasks available (S2ST, S2TT, T2TT, T2ST).\",\n@@ -4385,28 +4275,6 @@ def generate(\n \n         return waveform, waveform_lengths\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "a8068eb0ad01ea28fcb6a69df3a79642970df021",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 114,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -3175,28 +3175,6 @@ def generate(\n             **kwargs,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()\n@@ -3477,29 +3455,6 @@ def generate(\n             **kwargs,\n         )\n \n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     @staticmethod\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText._reorder_cache\n     def _reorder_cache(past_key_values, beam_idx):\n@@ -3871,29 +3826,6 @@ def generate(\n \n         return waveform, waveform_lengths\n \n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     @staticmethod\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech._reorder_cache\n     def _reorder_cache(past_key_values, beam_idx):\n@@ -4285,29 +4217,6 @@ def _reorder_cache(past_key_values, beam_idx):\n             )\n         return reordered_past\n \n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n \n @add_start_docstrings(\n     \"The original SeamlessM4Tv2 Model transformer which can be used for every tasks available (S2ST, S2TT, T2TT, T2ST).\",\n@@ -4786,29 +4695,6 @@ def generate(\n \n         return waveform, waveform_lengths\n \n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"use_cache\": use_cache,\n-        }\n-\n     @staticmethod\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel._reorder_cache\n     def _reorder_cache(past_key_values, beam_idx):"
        },
        {
            "sha": "a1caa7cf6da2f732e23adce057ce2a59eb172500",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -21,6 +21,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...configuration_utils import PretrainedConfig\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n@@ -169,7 +170,7 @@ def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start\n \n \n @add_start_docstrings(SPEECH_ENCODER_DECODER_START_DOCSTRING)\n-class SpeechEncoderDecoderModel(PreTrainedModel):\n+class SpeechEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     r\"\"\"\n     [`SpeechEncoderDecoderModel`] is a generic model class that will be instantiated as a transformer architecture with\n     one of the base model classes of the library as encoder and another one as decoder when created with the\n@@ -574,20 +575,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, encoder_outputs=None, **kwargs\n-    ):\n-        decoder_inputs = self.decoder.prepare_inputs_for_generation(input_ids, past_key_values=past_key_values)\n-        input_dict = {\n-            \"attention_mask\": attention_mask,\n-            \"decoder_attention_mask\": decoder_inputs.get(\"attention_mask\"),\n-            \"decoder_input_ids\": decoder_inputs[\"input_ids\"],\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": decoder_inputs.get(\"past_key_values\"),\n-            \"use_cache\": use_cache,\n-        }\n-        return input_dict\n-\n     def resize_token_embeddings(self, *args, **kwargs):\n         raise NotImplementedError(\n             \"Resizing the embedding layers via the SpeechEncoderDecoderModel directly is not supported. Please use the\""
        },
        {
            "sha": "aadc1da500ea641219f6876e33674fe79f5fda8f",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1331,33 +1331,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            decoder_input_ids = decoder_input_ids[:, -1:]\n-\n-        return {\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "63b536d185a3793fb207040c2fef5b936eb279f2",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -2425,6 +2425,8 @@ def prepare_inputs_for_generation(\n         encoder_outputs=None,\n         **kwargs,\n     ):\n+        # Note that this model doesn't inherit from the generation mixin, has unique generate function\n+\n         # cut decoder_input_ids if past is used\n         if past_key_values is not None:\n             past_length = past_key_values[0][0].shape[2]"
        },
        {
            "sha": "9b445596f6578ff22720a6cd60bd8c13ab7cf349",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1075,6 +1075,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape"
        },
        {
            "sha": "66d36d6db7ce7b78948a23d0de456496f162a9fe",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1204,79 +1204,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    # Copied from transformers.models.mistral.modeling_mistral.MistralForCausalLM.prepare_inputs_for_generation\n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n-        num_logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # `contiguous()` needed for compilation use cases\n-            model_inputs = {\"input_ids\": input_ids.contiguous(), \"inputs_embeds\": None}\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        if num_logits_to_keep is not None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n-        )\n-        return model_inputs\n-\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "6be8752d5b63b07eda6f115d8dd7a3d9bdbe17f9",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1869,36 +1869,6 @@ def forward(\n             encoder_attentions=encoder_outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            input_ids = input_ids[:, -1:]\n-\n-        return {\n-            \"decoder_input_ids\": input_ids,\n-            \"past_key_values\": past_key_values,\n-            \"encoder_outputs\": encoder_outputs,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,\n-            \"bbox\": kwargs.get(\"bbox\", None),\n-            \"pixel_values\": kwargs.get(\"pixel_values\", None),\n-            \"visual_bbox\": kwargs.get(\"visual_bbox\", None),\n-        }\n-\n     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration._reorder_cache\n     def _reorder_cache(self, past_key_values, beam_idx):\n         # if decoder past is not included in output"
        },
        {
            "sha": "c9703d263e7d20898bc81c6a304f53efe9ab04dd",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -707,6 +707,8 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         if input_ids is not None:\n             img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n                 1"
        },
        {
            "sha": "3af32a9caace0e9fd014f9ce2f9426b31990ccf4",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -581,6 +581,8 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n         # Trigger the new behavior if we have more than image embeddings seq length tokens for images\n         legacy_processing = (\n             input_ids is not None"
        },
        {
            "sha": "b044dda300ab488c348842f153784ac28cff6788",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...configuration_utils import PretrainedConfig\n+from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n@@ -147,7 +148,7 @@ def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start\n \n \n @add_start_docstrings(VISION_ENCODER_DECODER_START_DOCSTRING)\n-class VisionEncoderDecoderModel(PreTrainedModel):\n+class VisionEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     r\"\"\"\n     [`VisionEncoderDecoderModel`] is a generic model class that will be instantiated as a transformer architecture with\n     one of the base vision model classes of the library as encoder and another one as decoder when created with the\n@@ -654,20 +655,6 @@ def forward(\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n-    def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, encoder_outputs=None, **kwargs\n-    ):\n-        decoder_inputs = self.decoder.prepare_inputs_for_generation(input_ids, past_key_values=past_key_values)\n-        input_dict = {\n-            \"attention_mask\": attention_mask,\n-            \"decoder_attention_mask\": decoder_inputs.get(\"attention_mask\"),\n-            \"decoder_input_ids\": decoder_inputs[\"input_ids\"],\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": decoder_inputs.get(\"past_key_values\"),\n-            \"use_cache\": use_cache,\n-        }\n-        return input_dict\n-\n     def resize_token_embeddings(self, *args, **kwargs):\n         raise NotImplementedError(\n             \"Resizing the embedding layers via the VisionEncoderDecoderModel directly is not supported.Please use the\""
        },
        {
            "sha": "ce3df3e16707e56877b85b10cc47a5e0a5dbb2e8",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 40,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -1442,6 +1442,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n@@ -1817,6 +1818,10 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         **kwargs,\n     ):\n+        # Overwritten -- encoder-decoder whisper has custom logic, but it's close to the general function. Next time\n+        # this function needs to be touched, let's try to sort out the commonalities between the two and remove the\n+        # overwrite.\n+\n         decoder_position_ids = None\n         if decoder_attention_mask is not None:\n             decoder_position_ids = (decoder_attention_mask.cumsum(-1) - 1).clamp(min=0)\n@@ -2092,46 +2097,6 @@ def forward(\n             cross_attentions=outputs.cross_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        attention_mask=None,\n-        cache_position=None,\n-        **kwargs,\n-    ):\n-        past_length = 0\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, (Cache, EncoderDecoderCache)):\n-                past_length = cache_position[0] if cache_position is not None else past_key_values.get_seq_length()\n-            else:\n-                past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n-\n-            input_ids = input_ids[:, remove_prefix_length:]\n-\n-        if cache_position is None:\n-            cache_position = torch.arange(past_length, past_length + input_ids.shape[1], device=input_ids.device)\n-        elif use_cache:\n-            cache_position = cache_position[-input_ids.shape[1] :]\n-\n-        return {\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"input_ids\": input_ids,\n-            \"use_cache\": use_cache,\n-            \"attention_mask\": attention_mask,\n-            \"cache_position\": cache_position,\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "ed6a4f68b577f0b82ceff7b0dd869bbce2bec1eb",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f51ac9e059a78049362803c1d606a2c6a8160ee4/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f51ac9e059a78049362803c1d606a2c6a8160ee4/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=f51ac9e059a78049362803c1d606a2c6a8160ee4",
            "patch": "@@ -672,9 +672,13 @@ def is_copy_consistent(filename: str, overwrite: bool = False, buffer: dict = No\n         indent, object_name, replace_pattern = search.groups()\n \n         # Find the file lines, the object's code, and its blocks\n-        target_lines, theoretical_code, theoretical_code_splits = find_code_and_splits(\n-            object_name, base_path, buffer=buffer\n-        )\n+        try:\n+            target_lines, theoretical_code, theoretical_code_splits = find_code_and_splits(\n+                object_name, base_path, buffer=buffer\n+            )\n+        except Exception as exc:\n+            exc.args = (f\"Error while trying to find source code for {filename}.\\n\\n\" + str(exc),)\n+            raise\n \n         # code replaced by the patterns\n         theoretical_code_blocks = OrderedDict()"
        }
    ],
    "stats": {
        "total": 1274,
        "additions": 140,
        "deletions": 1134
    }
}