{
    "author": "lewtun",
    "message": "Guard DeepSpeed imports (#37755)\n\n* Guard DeepSpeed imports\n\n* Fix import\n\n* Import deepspeed consistently\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "acdbe627e323dbc822f21499fead789b439cf45b",
    "files": [
        {
            "sha": "e26801e106f3a67a4b9a2781155c5f5e1f862e17",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 26,
            "deletions": 4,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/acdbe627e323dbc822f21499fead789b439cf45b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acdbe627e323dbc822f21499fead789b439cf45b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=acdbe627e323dbc822f21499fead789b439cf45b",
            "patch": "@@ -57,7 +57,7 @@\n from .generation import CompileConfig, GenerationConfig\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n from .integrations.accelerate import find_tied_parameters, init_empty_weights\n-from .integrations.deepspeed import _load_state_dict_into_zero3_model, is_deepspeed_available\n+from .integrations.deepspeed import _load_state_dict_into_zero3_model\n from .integrations.flash_attention import flash_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n from .integrations.sdpa_attention import sdpa_attention_forward\n@@ -154,9 +154,6 @@\n     from safetensors.torch import save_file as safe_save_file\n \n \n-if is_deepspeed_available():\n-    import deepspeed\n-\n if is_kernels_available():\n     from kernels import get_kernel\n \n@@ -2007,6 +2004,8 @@ def _from_config(cls, config, **kwargs):\n             logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n             # this immediately partitions the model across all gpus, to avoid the overhead in time\n             # and memory copying it on CPU or each GPU first\n+            import deepspeed\n+\n             init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()]\n             with ContextManagers(init_contexts):\n                 model = cls(config, **kwargs)\n@@ -2702,6 +2701,8 @@ def resize_token_embeddings(\n         # Since we are basically reusing the same old embeddings with new weight values, gathering is required\n         is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n+            import deepspeed\n+\n             with deepspeed.zero.GatheredParameters(model_embeds.weight, modifier_rank=None):\n                 vocab_size = model_embeds.weight.shape[0]\n         else:\n@@ -2732,6 +2733,8 @@ def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean\n         # Update new_num_tokens with the actual size of new_embeddings\n         if pad_to_multiple_of is not None:\n             if is_deepspeed_zero3_enabled() and not is_quantized:\n+                import deepspeed\n+\n                 with deepspeed.zero.GatheredParameters(new_embeddings.weight, modifier_rank=None):\n                     new_num_tokens = new_embeddings.weight.shape[0]\n             else:\n@@ -2820,6 +2823,8 @@ def _get_resized_embeddings(\n \n         is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n+            import deepspeed\n+\n             with deepspeed.zero.GatheredParameters(old_embeddings.weight, modifier_rank=None):\n                 old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n         else:\n@@ -2864,6 +2869,8 @@ def _get_resized_embeddings(\n \n             added_num_tokens = new_num_tokens - old_num_tokens\n             if is_deepspeed_zero3_enabled() and not is_quantized:\n+                import deepspeed\n+\n                 with deepspeed.zero.GatheredParameters([old_embeddings.weight], modifier_rank=None):\n                     self._init_added_embeddings_weights_with_mean(\n                         old_embeddings, new_embeddings, old_embedding_dim, old_num_tokens, added_num_tokens\n@@ -2879,6 +2886,8 @@ def _get_resized_embeddings(\n         n = min(old_num_tokens, new_num_tokens)\n \n         if is_deepspeed_zero3_enabled() and not is_quantized:\n+            import deepspeed\n+\n             params = [old_embeddings.weight, new_embeddings.weight]\n             with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                 new_embeddings.weight.data[:n, :] = old_embeddings.weight.data[:n, :]\n@@ -2889,6 +2898,8 @@ def _get_resized_embeddings(\n         # This ensures correct functionality when a Custom Embedding class is passed as input.\n         # The input and output embedding types remain consistent. (c.f. https://github.com/huggingface/transformers/pull/31979)\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n+            import deepspeed\n+\n             params = [old_embeddings.weight, new_embeddings.weight]\n             with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                 old_embeddings.weight = new_embeddings.weight\n@@ -2941,11 +2952,14 @@ def _get_resized_lm_head(\n             `torch.nn.Linear`: Pointer to the resized Linear Module or the old Linear Module if `new_num_tokens` is\n             `None`\n         \"\"\"\n+\n         if new_num_tokens is None:\n             return old_lm_head\n \n         is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n+            import deepspeed\n+\n             with deepspeed.zero.GatheredParameters(old_lm_head.weight, modifier_rank=None):\n                 old_num_tokens, old_lm_head_dim = (\n                     old_lm_head.weight.size() if not transposed else old_lm_head.weight.t().size()\n@@ -2996,6 +3010,8 @@ def _get_resized_lm_head(\n \n             added_num_tokens = new_num_tokens - old_num_tokens\n             if is_deepspeed_zero3_enabled() and not is_quantized:\n+                import deepspeed\n+\n                 params = [old_lm_head.weight]\n                 if has_new_lm_head_bias:\n                     params += [old_lm_head.bias]\n@@ -3016,6 +3032,8 @@ def _get_resized_lm_head(\n         num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n \n         if is_deepspeed_zero3_enabled() and not is_quantized:\n+            import deepspeed\n+\n             params = [old_lm_head.weight, old_lm_head.bias, new_lm_head.weight, new_lm_head.bias]\n             with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                 self._copy_lm_head_original_to_resized(\n@@ -3762,6 +3780,8 @@ def float(self, *args):\n     @classmethod\n     def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n         if is_deepspeed_zero3_enabled():\n+            import deepspeed\n+\n             init_contexts = [no_init_weights()]\n             # We cannot initialize the model on meta device with deepspeed when not quantized\n             if not is_quantized and not _is_ds_init_called:\n@@ -5349,6 +5369,8 @@ def _initialize_missing_keys(\n             not_initialized_submodules = dict(self.named_modules())\n         # This will only initialize submodules that are not marked as initialized by the line above.\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n+            import deepspeed\n+\n             not_initialized_parameters = list(\n                 set(\n                     itertools.chain.from_iterable("
        }
    ],
    "stats": {
        "total": 30,
        "additions": 26,
        "deletions": 4
    }
}