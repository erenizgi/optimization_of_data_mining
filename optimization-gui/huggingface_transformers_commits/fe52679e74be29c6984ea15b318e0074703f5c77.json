{
    "author": "ydshieh",
    "message": "Update tests regarding attention types after  #35235 (#36024)\n\n* update\r\n\r\n* update\r\n\r\n* update\r\n\r\n* dev-ci\r\n\r\n* more changes\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "fe52679e74be29c6984ea15b318e0074703f5c77",
    "files": [
        {
            "sha": "7fedc4e7544186cb7eecfae16dfdb7569e27571f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 24,
            "deletions": 8,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe52679e74be29c6984ea15b318e0074703f5c77/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe52679e74be29c6984ea15b318e0074703f5c77/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=fe52679e74be29c6984ea15b318e0074703f5c77",
            "patch": "@@ -3872,11 +3872,13 @@ def test_attn_implementation_composite_models(self):\n             for name, submodule in model.named_modules():\n                 class_name = submodule.__class__.__name__\n                 if (\n-                    \"SdpaAttention\" in class_name\n-                    or \"SdpaSelfAttention\" in class_name\n-                    or \"FlashAttention\" in class_name\n+                    class_name.endswith(\"Attention\")\n+                    and getattr(submodule, \"config\", None)\n+                    and submodule.config._attn_implementation != \"eager\"\n                 ):\n-                    raise ValueError(f\"The eager model should not have SDPA/FA2 attention layers but got {class_name}\")\n+                    raise ValueError(\n+                        f\"The eager model should not have SDPA/FA2 attention layers but got `{class_name}.config._attn_implementation={submodule.config._attn_implementation}`\"\n+                    )\n \n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_non_composite_models(self):\n@@ -3907,8 +3909,14 @@ def test_sdpa_can_dispatch_non_composite_models(self):\n \n                 for name, submodule in model_eager.named_modules():\n                     class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        raise ValueError(f\"The eager model should not have SDPA attention layers but got {class_name}\")\n+                    if (\n+                        class_name.endswith(\"Attention\")\n+                        and getattr(submodule, \"config\", None)\n+                        and submodule.config._attn_implementation == \"sdpa\"\n+                    ):\n+                        raise ValueError(\n+                            f\"The eager model should not have SDPA attention layers but got `{class_name}.config._attn_implementation={submodule.config._attn_implementation}`\"\n+                        )\n \n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n@@ -3959,7 +3967,11 @@ def test_sdpa_can_dispatch_composite_models(self):\n \n                 for name, submodule in model_eager.named_modules():\n                     class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    if (\n+                        class_name.endswith(\"Attention\")\n+                        and getattr(submodule, \"config\", None)\n+                        and submodule.config._attn_implementation == \"sdpa\"\n+                    ):\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n@@ -4446,7 +4458,11 @@ def test_flash_attn_2_can_dispatch_composite_models(self):\n                     has_fa2 = False\n                     for name, submodule in model_fa2.named_modules():\n                         class_name = submodule.__class__.__name__\n-                        if \"FlashAttention\" in class_name:\n+                        if (\n+                            \"Attention\" in class_name\n+                            and getattr(submodule, \"config\", None)\n+                            and submodule.config._attn_implementation == \"flash_attention_2\"\n+                        ):\n                             has_fa2 = True\n                             break\n                     if not has_fa2:"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 24,
        "deletions": 8
    }
}