{
    "author": "guangy10",
    "message": "Qwen2.5 is ExecuTorch Compatible (#34102)\n\nQwen2 is ExecuTorch Compatible\r\n\r\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "7a08a772cc6e506d4cfe8604c45155b7d02ee677",
    "files": [
        {
            "sha": "1fee3192a6495863873174e9313aabc28acbd3c8",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a08a772cc6e506d4cfe8604c45155b7d02ee677/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a08a772cc6e506d4cfe8604c45155b7d02ee677/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=7a08a772cc6e506d4cfe8604c45155b7d02ee677",
            "patch": "@@ -19,8 +19,10 @@\n import unittest\n \n import pytest\n+from packaging import version\n \n from transformers import AutoTokenizer, Qwen2Config, is_torch_available, set_seed\n+from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     backend_empty_cache,\n     require_bitsandbytes,\n@@ -648,3 +650,56 @@ def test_speculative_generation(self):\n         del model\n         backend_empty_cache(torch_device)\n         gc.collect()\n+\n+    @slow\n+    def test_export_static_cache(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        qwen_model = \"Qwen/Qwen2.5-0.5B\"\n+\n+        tokenizer = AutoTokenizer.from_pretrained(qwen_model, pad_token=\"</s>\", padding_side=\"right\")\n+        EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% sugar. I have a jar of 1000 grams of sugar. I use\"]\n+        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+            \"input_ids\"\n+        ].shape[-1]\n+\n+        # Load model\n+        device = \"cpu\"\n+        dtype = torch.bfloat16\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"\n+        batch_size = 1\n+        model = Qwen2ForCausalLM.from_pretrained(\n+            qwen_model,\n+            device_map=device,\n+            torch_dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=cache_implementation,\n+                max_length=max_generation_length,\n+                cache_config={\n+                    \"batch_size\": batch_size,\n+                    \"max_cache_len\": max_generation_length,\n+                },\n+            ),\n+        )\n+\n+        prompt = [\"My favourite condiment is \"]\n+        prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n+        prompt_token_ids = prompt_tokens[\"input_ids\"]\n+        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+        # Static Cache + export\n+        exported_program = convert_and_export_with_cache(model)\n+        ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+            exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+        )\n+        ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)"
        }
    ],
    "stats": {
        "total": 55,
        "additions": 55,
        "deletions": 0
    }
}