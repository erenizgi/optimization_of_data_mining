{
    "author": "ydshieh",
    "message": "fix `mistral` and `mistral3` tests (#38978)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "2ce02b98bfe135c0a94ab169c5d95c88b91de3c3",
    "files": [
        {
            "sha": "17964dd68c272d68bbac59fdcfb21aa12ee98b64",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 24,
            "deletions": 12,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ce02b98bfe135c0a94ab169c5d95c88b91de3c3/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ce02b98bfe135c0a94ab169c5d95c88b91de3c3/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=2ce02b98bfe135c0a94ab169c5d95c88b91de3c3",
            "patch": "@@ -112,6 +112,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n \n @require_torch_accelerator\n+@require_read_token\n class MistralIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which accelerator are we using for our runners (e.g. A10 or T4)\n     # Depending on the hardware we get different logits / generations\n@@ -121,6 +122,9 @@ class MistralIntegrationTest(unittest.TestCase):\n     def setUpClass(cls):\n         cls.device_properties = get_device_properties()\n \n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n@@ -256,7 +260,7 @@ def test_model_7b_long_prompt_sdpa(self):\n \n     @slow\n     def test_speculative_generation(self):\n-        EXPECTED_TEXT_COMPLETION = \"My favourite condiment is 100% ketchup. I love it on everything. Iâ€™m not a big\"\n+        EXPECTED_TEXT_COMPLETION = \"My favourite condiment is 100% Sriracha. I love it on everything. I have it on my\"\n         prompt = \"My favourite condiment is \"\n         tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", use_fast=False)\n         model = MistralForCausalLM.from_pretrained(\n@@ -273,7 +277,6 @@ def test_speculative_generation(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n     @slow\n-    @require_read_token\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2\n         # work as intended. See https://github.com/pytorch/pytorch/issues/121943\n@@ -339,23 +342,32 @@ def test_compile_static_cache(self):\n @require_torch_accelerator\n class Mask4DTestHard(unittest.TestCase):\n     model_name = \"mistralai/Mistral-7B-v0.1\"\n-    _model = None\n+    model = None\n+    model_dtype = None\n \n-    def tearDown(self):\n+    @classmethod\n+    def setUpClass(cls):\n         cleanup(torch_device, gc_collect=True)\n+        if cls.model_dtype is None:\n+            cls.model_dtype = torch.float16\n+        if cls.model is None:\n+            cls.model = MistralForCausalLM.from_pretrained(cls.model_name, torch_dtype=cls.model_dtype).to(\n+                torch_device\n+            )\n \n-    @property\n-    def model(self):\n-        if self.__class__._model is None:\n-            self.__class__._model = MistralForCausalLM.from_pretrained(\n-                self.model_name, torch_dtype=self.model_dtype\n-            ).to(torch_device)\n-        return self.__class__._model\n+    @classmethod\n+    def tearDownClass(cls):\n+        del cls.model_dtype\n+        del cls.model\n+        cleanup(torch_device, gc_collect=True)\n \n     def setUp(self):\n-        self.model_dtype = torch.float16\n+        cleanup(torch_device, gc_collect=True)\n         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=False)\n \n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def get_test_data(self):\n         template = \"my favorite {}\"\n         items = (\"pet is a\", \"artist plays a\", \"name is L\")  # same number of tokens in each item"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 24,
        "deletions": 12
    }
}