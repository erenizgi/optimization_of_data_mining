{
    "author": "molbap",
    "message": "Make gradient-checkpoint enabling tolerant of models without get_input_embeddings (#42558)\n\n* add embedding getter\n\n* modify your own logic\n\n* a common test\n\n* some adapters are not PreTrainedModel s\n\n* few fixes\n\n* implement correct-ish fix?\n\n* fixup\n\n* this is needed likely\n\n* woops\n\n* solving some cross-imports issues here and there\n\n* more ximports issues\n\n* finally\n\n* revert changes\n\n* fixups\n\n* improve message\n\n* add common tests for input_ids first\n\n* increase test coverage\n\n* bigger update for GC\n\n* copies\n\n* mlcd is getting on my nerves a bit\n\n* ah yes\n\n* for BC\n\n* break a couple modelings\n\n* simplify with base_model\n\n* fix copies for torch checkpointing\n\n* simplify this model\n\n* improve messages",
    "sha": "b712a97d09efb3e6058d364c4e4783356a0250c8",
    "files": [
        {
            "sha": "698cc71d68a2c079e025693dc0e0a6176fb43031",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -36,6 +36,7 @@\n     is_librosa_available,\n     is_mistral_common_available,\n     is_mlx_available,\n+    is_numba_available,\n     is_pretty_midi_available,\n )\n "
        },
        {
            "sha": "fb615b80528dfe8d4420f5feb0a1ba4044d03dad",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 47,
            "deletions": 35,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -1063,54 +1063,52 @@ def get_input_embeddings(self) -> nn.Module:\n             `nn.Module`: A torch module mapping vocabulary to hidden states.\n         \"\"\"\n \n-        # 1) Check if the model has an attribute named 'embed_tokens' (the standard input embedding layer\n-        #  for most NLP models), and if so, return it.\n-\n         name = getattr(self, \"_input_embed_layer\", \"embed_tokens\")\n \n+        # 1) Direct attribute (most NLP models).\n         if (default_embedding := getattr(self, name, None)) is not None:\n             return default_embedding\n-        # 2) encoder/decoder and VLMs like `Gemma3nForConditionalGeneration`\n+        # 2) Nested embeddings (e.g., self.embeddings.patch_embedding for vision/audio models).\n+        if hasattr(self, \"embeddings\") and hasattr(self.embeddings, name):\n+            return getattr(self.embeddings, name)\n+        # 3) Encoder/decoder wrappers (e.g., `self.model.embed_tokens` or similar overrides).\n+        if hasattr(self, \"model\") and hasattr(self.model, name):\n+            return getattr(self.model, name)\n \n-        if hasattr(self, \"model\") and hasattr(self.model, \"embed_tokens\"):\n-            return self.model.embed_tokens\n+        if hasattr(self, \"base_model\"):\n+            base_model = self.base_model\n+            if base_model is not None and base_model is not self:\n+                return base_model.get_input_embeddings()\n \n-        # 3) vanilla decoder‑only architectures\n-        elif hasattr(self, \"embed_tokens\"):\n-            return self.embed_tokens\n-        else:\n-            base_model = getattr(self, \"base_model_prefix\", None)\n-            if base_model is not None:\n-                base_model = getattr(self, base_model, None)\n-                if base_model is not None and base_model is not self:\n-                    return base_model.get_input_embeddings()\n-            raise NotImplementedError(\n-                f\"`get_input_embeddings` not auto‑handled for {self.__class__.__name__}; \"\n-                \"please override in the subclass.\"\n-            )\n+        raise NotImplementedError(\n+            f\"`get_input_embeddings` not auto‑handled for {self.__class__.__name__}; please override in the subclass.\"\n+        )\n \n     def set_input_embeddings(self, value: nn.Module):\n         \"\"\"Fallback setter that handles **~70%** of models in the code-base.\n \n         Order of attempts:\n-        1. `self.model.embed_tokens`\n-        2. `self.embed_tokens`\n-        3. delegate to the *base model* if one exists\n-        4. otherwise raise `NotImplementedError` so subclasses still can (and\n+        1. `self.<_input_embed_layer>` (direct attribute)\n+        2. `self.embeddings.<_input_embed_layer>` (nested embeddings for vision/audio models)\n+        3. `self.model.<_input_embed_layer>` (encoder/decoder models)\n+        4. delegate to the *base model* if one exists\n+        5. otherwise raise `NotImplementedError` so subclasses still can (and\n             should) override for exotic layouts.\n         \"\"\"\n \n-        # 1) encoder/decoder and VLMs like `Gemma3nForConditionalGeneration`\n         name = getattr(self, \"_input_embed_layer\", \"embed_tokens\")\n-        if hasattr(self, \"model\") and hasattr(self.model, name):\n-            setattr(self.model, name, value)\n-        # 2) as well as vanilla decoder‑only architectures\n-        elif hasattr(self, name):\n+        # 1) Direct attribute (most NLP models)\n+        if hasattr(self, name):\n             setattr(self, name, value)\n-        # 3) recurse once into the registered *base* model (e.g. for encoder/decoder)\n-        elif getattr(self, self.base_model_prefix, self) is not self:\n-            base_model = getattr(self, self.base_model_prefix, self)\n-            base_model.set_input_embeddings(value)\n+        # 2) Nested embeddings (e.g., self.embeddings.patch_embedding for vision models)\n+        elif hasattr(self, \"embeddings\") and hasattr(self.embeddings, name):\n+            setattr(self.embeddings, name, value)\n+        # 3) encoder/decoder and VLMs like `Gemma3nForConditionalGeneration`\n+        elif hasattr(self, \"model\") and hasattr(self.model, name):\n+            setattr(self.model, name, value)\n+        # 4) recurse once into the registered *base* model (e.g. for encoder/decoder)\n+        elif hasattr(self, \"base_model\") and self.base_model is not self:\n+            self.base_model.set_input_embeddings(value)\n         else:\n             raise NotImplementedError(\n                 f\"`set_input_embeddings` not auto‑handled for {self.__class__.__name__}; please override in the subclass.\"\n@@ -2043,14 +2041,18 @@ def make_inputs_require_grads(module, input, output):\n \n         hooks = []\n         seen_modules = set()\n+        found_embeddings = False\n \n         for module in self.modules():\n             if not (isinstance(module, PreTrainedModel) and hasattr(module, \"get_input_embeddings\")):\n                 continue\n \n-            input_embeddings = module.get_input_embeddings()\n+            try:\n+                input_embeddings = module.get_input_embeddings()\n+            except NotImplementedError:\n+                continue\n \n-            if input_embeddings is None:\n+            if input_embeddings is None or not hasattr(input_embeddings, \"register_forward_hook\"):\n                 continue\n \n             embedding_id = id(input_embeddings)\n@@ -2059,11 +2061,18 @@ def make_inputs_require_grads(module, input, output):\n \n             seen_modules.add(embedding_id)\n             hooks.append(input_embeddings.register_forward_hook(make_inputs_require_grads))\n+            found_embeddings = True\n \n         self._require_grads_hooks = hooks\n         if hooks:\n             # for BC\n             self._require_grads_hook = hooks[0]\n+        if not found_embeddings:\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} does not expose input embeddings. Gradients cannot flow back to the token \"\n+                \"embeddings when using adapters or gradient checkpointing. Override `get_input_embeddings` to fully \"\n+                \"support those features, or set `_input_embed_layer` to the attribute name that holds the embeddings.\"\n+            )\n \n     def disable_input_require_grads(self):\n         \"\"\"\n@@ -3000,7 +3009,10 @@ def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n                 \"Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\"\n             )\n \n-        if getattr(self, \"_hf_peft_config_loaded\", False):\n+        needs_embedding_grads = self.main_input_name == \"input_ids\"\n+        # we use that also to detect whether or not we have to raise if embeddings are missing (the submodel might not have embeddings at all)\n+        enable_input_grads = needs_embedding_grads or getattr(self, \"_hf_peft_config_loaded\", False)\n+        if enable_input_grads:\n             # When using PEFT + gradient checkpointing + Trainer we need to make sure the input has requires_grad=True\n             # we do it also on PEFT: https://github.com/huggingface/peft/blob/85013987aa82aa1af3da1236b6902556ce3e483e/src/peft/peft_model.py#L334\n             # When training with PEFT, only LoRA layers will have requires grad set to True, but the output of frozen layers need to propagate"
        },
        {
            "sha": "759a198845dab47a8329d6372cdd4af20225be20",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -781,9 +781,9 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                hidden_states,\n+                attention_mask,\n+                output_attentions,\n                 **kwargs,\n             )\n \n@@ -976,6 +976,7 @@ class AlignVisionModel(AlignPreTrainedModel):\n     main_input_name = \"pixel_values\"\n     input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n+    _input_embed_layer = \"convolution\"\n     _no_split_modules = [\"AlignVisionBlock\"]\n \n     def __init__(self, config: AlignVisionConfig):\n@@ -995,9 +996,6 @@ def __init__(self, config: AlignVisionConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self) -> nn.Module:\n-        return self.vision_model.embeddings.convolution\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "0acaf5ead0ae1ff27cfd9f4bec3757512e6e5dd7",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -393,9 +393,9 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                hidden_states,\n+                attention_mask,\n+                output_attentions,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "0fb007a22812153cebfaaff234715cf7f36555c3",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -638,9 +638,9 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                hidden_states,\n+                attention_mask,\n+                output_attentions,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "a51644cbf1e2945d226c69fb46514964379a0033",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -1266,9 +1266,9 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                hidden_states,\n+                attention_mask,\n+                output_attentions,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "24eba74097ba5432c3a21904cdddc6386e03fd2b",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -209,10 +209,9 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n             )\n \n         embeddings = self.projection(pixel_values.to(self.projection.weight.dtype))\n-        patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n         embeddings = embeddings.flatten(2).transpose(1, 2)\n \n-        return embeddings, (patch_height, patch_width)\n+        return embeddings\n \n \n # Based on timm implementation, which can be found here:\n@@ -291,7 +290,7 @@ def forward(\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n     ) -> torch.Tensor:\n         _, _, height, width = pixel_values.shape\n-        embeddings, (patch_height, patch_width) = self.patch_embeddings(pixel_values)\n+        embeddings = self.patch_embeddings(pixel_values)\n         batch_size, seq_len, _ = embeddings.size()\n \n         if bool_masked_pos is not None:\n@@ -308,7 +307,7 @@ def forward(\n \n         embeddings = self.dropout(embeddings)\n \n-        return embeddings, (patch_height, patch_width)\n+        return embeddings\n \n \n class InternVLVisionMLP(nn.Module):\n@@ -455,7 +454,7 @@ def forward(\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n         \"\"\"\n-        embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n+        embedding_output = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n \n         encoder_outputs = self.encoder(embedding_output)\n         sequence_output = encoder_outputs[0]"
        },
        {
            "sha": "e6e7d77fe8c1b4c0b63e9c93d4370d2a694751ec",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -29,7 +29,7 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, torch_int\n from ...utils.generic import check_model_inputs\n from ..clip.modeling_clip import CLIPMLP\n from ..janus.modeling_janus import JanusVisionAttention\n@@ -44,9 +44,6 @@\n from .configuration_internvl import InternVLConfig, InternVLVisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -177,10 +174,9 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n             )\n \n         embeddings = self.projection(pixel_values.to(self.projection.weight.dtype))\n-        patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n         embeddings = embeddings.flatten(2).transpose(1, 2)\n \n-        return embeddings, (patch_height, patch_width)\n+        return embeddings\n \n \n # Based on timm implementation, which can be found here:\n@@ -259,7 +255,7 @@ def forward(\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n     ) -> torch.Tensor:\n         _, _, height, width = pixel_values.shape\n-        embeddings, (patch_height, patch_width) = self.patch_embeddings(pixel_values)\n+        embeddings = self.patch_embeddings(pixel_values)\n         batch_size, seq_len, _ = embeddings.size()\n \n         if bool_masked_pos is not None:\n@@ -276,7 +272,7 @@ def forward(\n \n         embeddings = self.dropout(embeddings)\n \n-        return embeddings, (patch_height, patch_width)\n+        return embeddings\n \n \n class InternVLVisionMLP(CLIPMLP):\n@@ -412,7 +408,7 @@ def forward(\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n         \"\"\"\n-        embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n+        embedding_output = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n \n         encoder_outputs = self.encoder(embedding_output)\n         sequence_output = encoder_outputs[0]"
        },
        {
            "sha": "5a661ea40872158df1c9125d82539acd49ade604",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -337,9 +337,9 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                hidden_states,\n+                attention_mask,\n+                output_attentions,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "d95daea926d6124e850b73eba034bd56b39f5740",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -884,6 +884,12 @@ def __init__(self, config):\n \n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.layoutlmv3.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.layoutlmv3.set_input_embeddings(value)\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -984,6 +990,12 @@ def __init__(self, config):\n \n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.layoutlmv3.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.layoutlmv3.set_input_embeddings(value)\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1104,6 +1116,12 @@ def __init__(self, config):\n \n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.layoutlmv3.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.layoutlmv3.set_input_embeddings(value)\n+\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "04f678bfd1e05cdb4b56aa2916018e99cfedde6a",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 13,
            "deletions": 15,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -279,11 +279,9 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (\n-            ((context_layer, layout_context_layer), attention_probs)\n-            if output_attentions\n-            else ((context_layer, layout_context_layer),)\n-        )\n+        outputs = (context_layer, layout_context_layer)\n+        if output_attentions:\n+            outputs = outputs + (attention_probs,)\n \n         return outputs\n \n@@ -327,9 +325,9 @@ def forward(\n             attention_mask,\n             output_attentions,\n         )\n-        attention_output = self.output(self_outputs[0][0], hidden_states)\n-        layout_attention_output = self.layout_output(self_outputs[0][1], layout_inputs)\n-        outputs = ((attention_output, layout_attention_output),) + self_outputs[1:]  # add attentions if we output them\n+        attention_output = self.output(self_outputs[0], hidden_states)\n+        layout_attention_output = self.layout_output(self_outputs[1], layout_inputs)\n+        outputs = (attention_output, layout_attention_output) + self_outputs[2:]  # add attentions if we output them\n         return outputs\n \n \n@@ -395,18 +393,18 @@ def forward(\n             attention_mask,\n             output_attentions=output_attentions,\n         )\n-        attention_output = self_attention_outputs[0][0]\n-        layout_attention_output = self_attention_outputs[0][1]\n+        attention_output = self_attention_outputs[0]\n+        layout_attention_output = self_attention_outputs[1]\n \n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        outputs = self_attention_outputs[2:]  # add self attentions if we output attention weights\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n         layout_layer_output = apply_chunking_to_forward(\n             self.layout_feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, layout_attention_output\n         )\n-        outputs = ((layer_output, layout_layer_output),) + outputs\n+        outputs = (layer_output, layout_layer_output) + outputs\n \n         return outputs\n \n@@ -451,11 +449,11 @@ def forward(\n                 output_attentions,\n             )\n \n-            hidden_states = layer_outputs[0][0]\n-            layout_inputs = layer_outputs[0][1]\n+            hidden_states = layer_outputs[0]\n+            layout_inputs = layer_outputs[1]\n \n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+                all_self_attentions = all_self_attentions + (layer_outputs[2],)\n \n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)"
        },
        {
            "sha": "0fe9188d69d6501ed9afed480a84c1a61269eff0",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -486,9 +486,9 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                hidden_states,\n+                attention_mask,\n+                output_attentions,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "6257e59ebfa2a0036e2dc15783f3531e0bd9b68f",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -268,7 +268,11 @@ def __init__(self, config):\n         self.post_init()\n \n     def get_input_embeddings(self):\n-        return self.embeddings.patch_embeddings\n+        # Input embeddings correspond to the very first patch-embedding stage.\n+        return self.encoder.patch_embeddings[0]\n+\n+    def set_input_embeddings(self, value):\n+        self.encoder.patch_embeddings[0] = value\n \n     @auto_docstring\n     def forward(\n@@ -333,6 +337,12 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.poolformer.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.poolformer.set_input_embeddings(value)\n+\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "1dc5140f6cc477e54893226970dfec18627f8090",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -502,9 +502,11 @@ def forward(\n         return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n-class SiglipTextTransformer(nn.Module):\n+class SiglipTextTransformer(SiglipPreTrainedModel):\n+    _input_embed_layer = \"token_embedding\"\n+\n     def __init__(self, config: SiglipTextConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.config = config\n         embed_dim = config.hidden_size\n         self.embeddings = SiglipTextEmbeddings(config)\n@@ -614,6 +616,7 @@ def forward(\n \n \n class SiglipVisionTransformer(SiglipPreTrainedModel):\n+    _input_embed_layer = \"patch_embedding\"\n     _can_record_outputs = {\n         \"hidden_states\": SiglipEncoderLayer,\n         \"attentions\": SiglipAttention,\n@@ -776,6 +779,12 @@ def __init__(self, config: SiglipConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.text_model.embeddings.token_embedding\n+\n+    def set_input_embeddings(self, value: nn.Module):\n+        self.text_model.embeddings.token_embedding = value\n+\n     @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n@@ -971,6 +980,12 @@ def __init__(self, config: SiglipConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_model.embeddings.patch_embedding\n+\n+    def set_input_embeddings(self, value: nn.Module):\n+        self.vision_model.embeddings.patch_embedding = value\n+\n     @check_model_inputs\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "160369ed0674760d6cbff65cd762f2199eadb460",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 2,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -484,6 +484,7 @@ def forward(\n \n \n class Siglip2VisionTransformer(Siglip2PreTrainedModel):\n+    _input_embed_layer = \"patch_embedding\"\n     _can_record_outputs = {\n         \"hidden_states\": Siglip2EncoderLayer,\n         \"attentions\": Siglip2Attention,\n@@ -591,9 +592,11 @@ def forward(\n         return embeddings\n \n \n-class Siglip2TextTransformer(nn.Module):\n+class Siglip2TextTransformer(Siglip2PreTrainedModel):\n+    _input_embed_layer = \"token_embedding\"\n+\n     def __init__(self, config: Siglip2TextConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.config = config\n         embed_dim = config.hidden_size\n         self.embeddings = Siglip2TextEmbeddings(config)\n@@ -835,6 +838,12 @@ def __init__(self, config: Siglip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.text_model.embeddings.token_embedding\n+\n+    def set_input_embeddings(self, value: nn.Module):\n+        self.text_model.embeddings.token_embedding = value\n+\n     @filter_out_non_signature_kwargs()\n     @auto_docstring\n     def get_text_features(\n@@ -1053,6 +1062,12 @@ def __init__(self, config: Siglip2Config) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_model.embeddings.patch_embedding\n+\n+    def set_input_embeddings(self, value: nn.Module):\n+        self.vision_model.embeddings.patch_embedding = value\n+\n     @check_model_inputs\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d680936b4c6d83a40fa443fdd76c3e0e006953a6",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -305,9 +305,9 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                hidden_states,\n+                attention_mask,\n+                output_attentions,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "3cf8c8eb1761ee565b25446a7523941bcffa0955",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -913,6 +913,7 @@ class SwitchTransformersModel(SwitchTransformersPreTrainedModel):\n         \"encoder.embed_tokens.weight\": \"shared.weight\",\n         \"decoder.embed_tokens.weight\": \"shared.weight\",\n     }\n+    _input_embed_layer = \"shared\"\n \n     def __init__(self, config: SwitchTransformersConfig):\n         super().__init__(config)\n@@ -932,9 +933,6 @@ def __init__(self, config: SwitchTransformersConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.shared\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)"
        },
        {
            "sha": "7b83245ee2718cdf2514a5e07a32524adce5a720",
            "filename": "src/transformers/models/switch_transformers/modular_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -669,6 +669,7 @@ class SwitchTransformersModel(SwitchTransformersPreTrainedModel):\n         \"encoder.embed_tokens.weight\": \"shared.weight\",\n         \"decoder.embed_tokens.weight\": \"shared.weight\",\n     }\n+    _input_embed_layer = \"shared\"\n \n     def __init__(self, config: SwitchTransformersConfig):\n         super().__init__(config)\n@@ -688,9 +689,6 @@ def __init__(self, config: SwitchTransformersConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.shared\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)"
        },
        {
            "sha": "d6820311362b7298954006ef9e42fc5b3e53fc23",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -132,6 +132,13 @@ def _timm_model_supports_gradient_checkpointing(self):\n     def _set_gradient_checkpointing(self, enable: bool = True, *args, **kwargs):\n         self.timm_model.set_grad_checkpointing(enable)\n \n+    def get_input_embeddings(self):\n+        # TIMM backbones operate directly on images and do not expose token embeddings.\n+        return None\n+\n+    def set_input_embeddings(self, value):\n+        raise NotImplementedError(\"TimmWrapper models do not own token embeddings and cannot set them.\")\n+\n \n class TimmWrapperModel(TimmWrapperPreTrainedModel):\n     \"\"\"\n@@ -147,13 +154,6 @@ def __init__(self, config: TimmWrapperConfig):\n         self.timm_model = _create_timm_model_with_error_handling(config, num_classes=0, **extra_init_kwargs)\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        # Vision backbones from timm do not expose token embeddings, so there is nothing to return.\n-        return None\n-\n-    def set_input_embeddings(self, value):\n-        raise NotImplementedError(\"TimmWrapperModel does not own token embeddings and cannot set them.\")\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "288ae42cbacec6a22092e3bc5cd37eefe9b0f669",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -670,7 +670,7 @@ def forward(\n             else:\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n-                    None,\n+                    attention_mask=None,\n                     output_attentions=output_attentions,\n                 )\n \n@@ -866,8 +866,9 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n+                causal_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask=None,\n                 past_key_values=past_key_values if use_cache else None,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,"
        },
        {
            "sha": "56099f23688ba7c816b1d92ac728c45d5eac8e15",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -118,6 +118,7 @@\n     is_mistral_common_available,\n     is_natten_available,\n     is_nltk_available,\n+    is_numba_available,\n     is_onnx_available,\n     is_openai_available,\n     is_optimum_available,\n@@ -1384,6 +1385,13 @@ def require_pyctcdecode(test_case):\n     return unittest.skipUnless(is_pyctcdecode_available(), \"test requires pyctcdecode\")(test_case)\n \n \n+def require_numba(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires numba\n+    \"\"\"\n+    return unittest.skipUnless(is_numba_available(), \"test requires numba\")(test_case)\n+\n+\n def require_librosa(test_case):\n     \"\"\"\n     Decorator marking a test that requires librosa"
        },
        {
            "sha": "18bfc189cac85b44c3eafc8efbe2b06d712db6f2",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -168,6 +168,7 @@\n     is_ninja_available,\n     is_nltk_available,\n     is_num2words_available,\n+    is_numba_available,\n     is_onnx_available,\n     is_openai_available,\n     is_optimum_available,"
        },
        {
            "sha": "90f71a53d70a641a24cef8957da0d8e2385e5822",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -1106,6 +1106,16 @@ def is_nltk_available() -> bool:\n     return _is_package_available(\"nltk\")\n \n \n+@lru_cache\n+def is_numba_available() -> bool:\n+    is_available = _is_package_available(\"numba\")\n+    if not is_available:\n+        return False\n+\n+    numpy_available, numpy_version = _is_package_available(\"numpy\", return_version=True)\n+    return not numpy_available or version.parse(numpy_version) < version.parse(\"2.2.0\")\n+\n+\n @lru_cache\n def is_torchaudio_available() -> bool:\n     return _is_package_available(\"torchaudio\")"
        },
        {
            "sha": "c4b6417db2c57e3ea0c1ca83877dd9953f7c54c6",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -22,6 +22,7 @@\n from transformers import ClvpConfig, ClvpDecoderConfig, ClvpEncoderConfig\n from transformers.testing_utils import (\n     cleanup,\n+    require_numba,\n     require_torch,\n     slow,\n     torch_device,\n@@ -390,6 +391,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n+@require_numba\n class ClvpModelForConditionalGenerationTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (ClvpModelForConditionalGeneration,) if is_torch_available() else ()\n     # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n@@ -509,6 +511,7 @@ def test_model_from_pretrained(self):\n \n @slow\n @require_torch\n+@require_numba\n class ClvpIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.text = \"This is an example text.\""
        },
        {
            "sha": "b8afbfb4780338cb1d2551cf57635fb5085a9e04",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -271,6 +271,9 @@ class FalconMambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTest\n         else {}\n     )\n \n+    def test_enable_input_require_grads(self):\n+        self.skipTest(\"FalconMamba currently requires CUDA/Metal/XPU to run enable_input_require_grads.\")\n+\n     def setUp(self):\n         self.model_tester = FalconMambaModelTester(self)\n         self.config_tester = ConfigTester("
        },
        {
            "sha": "e4858dbfa15e129fcb1d8e1eec06a18f925a1d3e",
            "filename": "tests/models/fast_vlm/test_modeling_fast_vlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Ffast_vlm%2Ftest_modeling_fast_vlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Ffast_vlm%2Ftest_modeling_fast_vlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffast_vlm%2Ftest_modeling_fast_vlm.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -186,6 +186,9 @@ def setUp(self):\n             self, config_class=FastVlmConfig, has_text_modality=False, common_properties=common_properties\n         )\n \n+    def test_enable_input_require_grads(self):\n+        self.skipTest(\"FastVLM relies on timm architectures unavailable in this test environment.\")\n+\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "e4bddd87672eeb14663df59ca33f3516c9e5faa4",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -247,6 +247,9 @@ def setUp(self):\n             self, config_class=MambaConfig, n_embd=37, common_properties=[\"hidden_size\", \"num_hidden_layers\"]\n         )\n \n+    def test_enable_input_require_grads(self):\n+        self.skipTest(\"Mamba currently requires CUDA/Metal/XPU to run enable_input_require_grads.\")\n+\n     def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n         self.assertIsInstance(past_key_values, MambaCache)\n "
        },
        {
            "sha": "d6dc1b66bcdbacfe3e7faa2913528381d0cc588e",
            "filename": "tests/models/mlcd/test_modeling_mlcd.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -140,6 +140,12 @@ def test_model_get_set_embeddings(self):\n             x = model.get_output_embeddings()\n             self.assertTrue(x is None or isinstance(x, torch.nn.Linear))\n \n+    @unittest.skip(\n+        reason=\"MLCD passes position embeddings as tuples in its vision encoder, which breaks reentrant GC.\"\n+    )\n+    def test_enable_input_require_grads_with_gradient_checkpointing(self):\n+        pass\n+\n \n @require_torch\n class MLCDVisionModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "4cc51ebaaba4d1125ed445a6c1656d43c30da6db",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -49,6 +49,7 @@\n     from torch import nn\n \n     from transformers import SiglipForImageClassification, SiglipModel, SiglipTextModel, SiglipVisionModel\n+    from transformers.models.siglip.modeling_siglip import SiglipVisionTransformer\n \n if is_vision_available():\n     from PIL import Image\n@@ -204,6 +205,22 @@ def test_model_get_set_embeddings(self):\n             x = model.get_output_embeddings()\n             self.assertTrue(x is None or isinstance(x, nn.Linear))\n \n+    def test_vision_transformer_get_set_input_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        transformer = SiglipVisionTransformer(config)\n+\n+        self.assertIsInstance(transformer.get_input_embeddings(), nn.Conv2d)\n+\n+        new_embeddings = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=config.hidden_size,\n+            kernel_size=config.patch_size,\n+            stride=config.patch_size,\n+            padding=\"valid\",\n+        )\n+        transformer.set_input_embeddings(new_embeddings)\n+        self.assertIs(transformer.get_input_embeddings(), new_embeddings)\n+\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "cc4fd12a498326dea20d46a035eb71525380136b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 107,
            "deletions": 0,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -907,6 +907,113 @@ def test_peft_gradient_checkpointing_enable_disable(self):\n                         m.gradient_checkpointing, f\"Module {n} does not have gradient_checkpointing set to False\"\n                     )\n \n+    def test_enable_input_require_grads(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(copy.deepcopy(config))\n+            if not hasattr(model, \"get_input_embeddings\"):\n+                continue\n+            try:\n+                model.enable_input_require_grads()\n+            except NotImplementedError as error:\n+                self.fail(f\"enable_input_require_grads raised NotImplementedError for {model_class.__name__}: {error}\")\n+            finally:\n+                model.disable_input_require_grads()\n+\n+    def test_enable_input_require_grads_with_gradient_checkpointing(self):\n+        if not getattr(self.model_tester, \"is_training\", False):\n+            self.skipTest(reason=\"ModelTester is not configured to run training tests\")\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        if hasattr(config, \"use_cache\"):\n+            config.use_cache = False\n+\n+        has_verified_model = False\n+\n+        for model_class in self.all_model_classes:\n+            if not getattr(model_class, \"supports_gradient_checkpointing\", False):\n+                continue\n+\n+            model = model_class(copy.deepcopy(config))\n+            try:\n+                embeddings_module = model.get_input_embeddings()\n+            except NotImplementedError:\n+                continue\n+            if embeddings_module is None:\n+                continue\n+\n+            embedding_param = getattr(embeddings_module, \"weight\", None)\n+            if embedding_param is None and isinstance(embeddings_module, (tuple, list)):\n+                for candidate in embeddings_module:\n+                    if hasattr(candidate, \"weight\"):\n+                        embedding_param = candidate.weight\n+                        break\n+            if embedding_param is None or not isinstance(embedding_param, torch.Tensor):\n+                continue\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+\n+            model.to(torch_device)\n+            model.train()\n+\n+            torch.manual_seed(0)\n+            outputs = model(**inputs)\n+            loss_tensor = outputs.loss if getattr(outputs, \"loss\", None) is not None else outputs[0]\n+            if isinstance(loss_tensor, (tuple, list)):\n+                loss_tensor = loss_tensor[0]\n+            if loss_tensor is None or not isinstance(loss_tensor, torch.Tensor) or not loss_tensor.requires_grad:\n+                model.zero_grad(set_to_none=True)\n+                continue\n+            loss = loss_tensor.sum()\n+            loss.backward()\n+\n+            baseline_grad = embedding_param.grad\n+            if (\n+                baseline_grad is None\n+                or baseline_grad.abs().sum().item() == 0\n+                or not torch.isfinite(baseline_grad).all()\n+            ):\n+                model.zero_grad(set_to_none=True)\n+                continue\n+\n+            model.zero_grad(set_to_none=True)\n+            model.gradient_checkpointing_enable()\n+            model.enable_input_require_grads()\n+\n+            torch.manual_seed(0)\n+            outputs = model(**inputs)\n+            loss_tensor = outputs.loss if getattr(outputs, \"loss\", None) is not None else outputs[0]\n+            if isinstance(loss_tensor, (tuple, list)):\n+                loss_tensor = loss_tensor[0]\n+            if loss_tensor is None or not isinstance(loss_tensor, torch.Tensor) or not loss_tensor.requires_grad:\n+                model.zero_grad(set_to_none=True)\n+                continue\n+            loss = loss_tensor.sum()\n+            loss.backward()\n+\n+            grad_after_gc = embedding_param.grad\n+            self.assertIsNotNone(\n+                grad_after_gc,\n+                f\"{model_class.__name__} should produce embedding gradients when gradient checkpointing is enabled. \"\n+                \"This typically means the model is not exposing its embeddings via `get_input_embeddings()` or \"\n+                \"a properly configured `_input_embed_layer` attribute.\",\n+            )\n+            self.assertTrue(\n+                torch.isfinite(grad_after_gc).all(),\n+                f\"{model_class.__name__} produced non-finite gradients with gradient checkpointing enabled.\",\n+            )\n+            self.assertGreater(\n+                grad_after_gc.abs().sum().item(),\n+                0,\n+                f\"{model_class.__name__} should keep non-zero embedding gradients with gradient checkpointing enabled.\",\n+            )\n+            has_verified_model = True\n+\n+        if not has_verified_model:\n+            self.skipTest(\n+                reason=\"No model with a differentiable loss was available to verify enable_input_require_grads with gradient checkpointing.\"\n+            )\n+\n     def test_can_init_all_missing_weights(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "fa43039e60fa3bc1e8e9189f915965ea5cc64de6",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b712a97d09efb3e6058d364c4e4783356a0250c8/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b712a97d09efb3e6058d364c4e4783356a0250c8/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=b712a97d09efb3e6058d364c4e4783356a0250c8",
            "patch": "@@ -74,6 +74,8 @@\n     \"Qwen3VLVisionModel\",\n     \"Qwen3VLMoeVisionModel\",\n     \"SwitchTransformersStack\",\n+    \"SiglipTextTransformer\",\n+    \"Siglip2TextTransformer\",\n     \"MaskFormerSwinModel\",\n     \"MaskFormerSwinPreTrainedModel\",\n     \"BridgeTowerTextModel\","
        }
    ],
    "stats": {
        "total": 440,
        "additions": 331,
        "deletions": 109
    }
}