{
    "author": "Cyrilvallez",
    "message": "Harmonize CacheLayer names (#40892)\n\n* unify naming\n\n* style\n\n* doc as well\n\n* post rebase fix\n\n* style\n\n* style\n\n* revert",
    "sha": "3688a977d08dd3d7fd4293e28e011f77efc4fb7a",
    "files": [
        {
            "sha": "67064fbd5d3de39dadc1461355c795dd5d75f68d",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=3688a977d08dd3d7fd4293e28e011f77efc4fb7a",
            "patch": "@@ -149,6 +149,5 @@ def check_output(self, want, got, optionflags):\n     patch_torch_compile_force_graph()\n \n \n-\n if os.environ.get(\"PATCH_TESTING_METHODS_TO_COLLECT_OUTPUTS\", \"\").lower() in (\"yes\", \"true\", \"on\", \"y\", \"1\"):\n     patch_testing_methods_to_collect_info()"
        },
        {
            "sha": "0e192fd47f4299c17a71aada7f66aac053e29939",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=3688a977d08dd3d7fd4293e28e011f77efc4fb7a",
            "patch": "@@ -85,7 +85,7 @@ When you use Transformers' [`Cache`] class, the self-attention module performs s\n \n Caches are structured as a list of layers, where each layer contains a key and value cache. The key and value caches are tensors with the shape `[batch_size, num_heads, seq_len, head_dim]`.\n \n-Layers can be of different types (e.g. `DynamicLayer`, `StaticLayer`, `SlidingWindowLayer`), which mostly changes how sequence length is handled and how the cache is updated.\n+Layers can be of different types (e.g. `DynamicLayer`, `StaticLayer`, `StaticSlidingWindowLayer`), which mostly changes how sequence length is handled and how the cache is updated.\n \n The simplest is a `DynamicLayer` that grows as more tokens are processed. The sequence length dimension (`seq_len`) increases with each new token:\n \n@@ -94,7 +94,7 @@ cache.layers[idx].keys = torch.cat([cache.layers[idx].keys, key_states], dim=-2)\n cache.layers[idx].values = torch.cat([cache.layers[idx].values, value_states], dim=-2)\n ```\n \n-Other layer types like `StaticLayer` and `SlidingWindowLayer` have a fixed sequence length that is set when the cache is created. This makes them compatible with `torch.compile`. In the case of `SlidingWindowLayer`, existing tokens are shifted out of the cache when a new token is added.\n+Other layer types like `StaticLayer` and `StaticSlidingWindowLayer` have a fixed sequence length that is set when the cache is created. This makes them compatible with `torch.compile`. In the case of `StaticSlidingWindowLayer`, existing tokens are shifted out of the cache when a new token is added.\n \n The example below demonstrates how to create a generation loop with [`DynamicCache`]. As discussed, the attention mask is a concatenation of past and current token values and `1` is added to the cache position for the next token.\n "
        },
        {
            "sha": "d47eba82d8ccb62f7d1c56ae46dd1e03ce51a2e2",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=3688a977d08dd3d7fd4293e28e011f77efc4fb7a",
            "patch": "@@ -250,7 +250,7 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n     - update\n     - lazy_initialization\n \n-[[autodoc]] SlidingWindowLayer\n+[[autodoc]] StaticSlidingWindowLayer\n     - update\n     - lazy_initialization\n "
        },
        {
            "sha": "6f2a3242616cabed0f06dbc757c782701db95369",
            "filename": "docs/source/ko/cache_explanation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcache_explanation.md?ref=3688a977d08dd3d7fd4293e28e011f77efc4fb7a",
            "patch": "@@ -84,7 +84,7 @@ Transformers의 [`Cache`] 클래스를 사용할 때, 셀프 어텐션 모듈은\n \n 캐시는 각 레이어가 key와 value 캐시를 포함하는 레이어 목록 형태로 구성되어 있습니다. key 및 value 캐시는 `[batch_size, num_heads, seq_len, head_dim]` 형태의 텐서입니다.\n \n-레이어는 서로 다른 타입일 수 있으며(예: `DynamicLayer`, `StaticLayer`, `SlidingWindowLayer`), 이는 주로 시퀀스 길이를 어떻게 처리하고 캐시를 어떻게 갱신하는지에 따라 달라집니다.\n+레이어는 서로 다른 타입일 수 있으며(예: `DynamicLayer`, `StaticLayer`, `StaticSlidingWindowLayer`), 이는 주로 시퀀스 길이를 어떻게 처리하고 캐시를 어떻게 갱신하는지에 따라 달라집니다.\n \n 가장 단순한 형태는 `DynamicLayer`로, 더 많은 토큰이 처리됨에 따라 점진적으로 확장됩니다. 시퀀스 길이 차원(`seq_len`)은 새로운 토큰이 추가될 때마다 증가합니다:\n \n@@ -93,7 +93,7 @@ cache.layers[idx].keys = torch.cat([cache.layers[idx].keys, key_states], dim=-2)\n cache.layers[idx].values = torch.cat([cache.layers[idx].values, value_states], dim=-2)\n ```\n \n-`StaticLayer`나 `SlidingWindowLayer`와 같은 다른 레이어 타입은 캐시가 생성될 때 고정된 시퀀스 길이를 가지며, 이는 `torch.compile`과 호환되도록 만듭니다. `SlidingWindowLayer`의 경우, 새로운 토큰이 추가되면 기존 토큰은 캐시에서 제거됩니다.\n+`StaticLayer`나 `StaticSlidingWindowLayer`와 같은 다른 레이어 타입은 캐시가 생성될 때 고정된 시퀀스 길이를 가지며, 이는 `torch.compile`과 호환되도록 만듭니다. `StaticSlidingWindowLayer`의 경우, 새로운 토큰이 추가되면 기존 토큰은 캐시에서 제거됩니다.\n \n 아래 예제는 [`DynamicCache`]로 생성 루프를 만드는 방법을 보여줍니다. 논의된 바와 같이, 어텐션 마스크는 과거와 현재 토큰값의 연결이며 다음 토큰을 위해 캐시 위치에 `1`이 추가됩니다.\n "
        },
        {
            "sha": "d97dfb2ae6f3939a74aec473809015a6758a63b6",
            "filename": "docs/source/ko/internal/generation_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md?ref=3688a977d08dd3d7fd4293e28e011f77efc4fb7a",
            "patch": "@@ -330,7 +330,7 @@ generation_output[:2]\n [[autodoc]] StaticLayer\n     - update\n \n-[[autodoc]] SlidingWindowLayer\n+[[autodoc]] StaticSlidingWindowLayer\n     - update\n \n [[autodoc]] QuantoQuantizedLayer"
        },
        {
            "sha": "2cf1d5970b547bcf86548c39001c4499523ea94e",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=3688a977d08dd3d7fd4293e28e011f77efc4fb7a",
            "patch": "@@ -378,6 +378,7 @@\n         \"CacheLayerMixin\",\n         \"DynamicLayer\",\n         \"StaticLayer\",\n+        \"StaticSlidingWindowLayer\",\n         \"SlidingWindowLayer\",\n         \"ChunkedSlidingLayer\",\n         \"QuantoQuantizedLayer\",\n@@ -600,6 +601,7 @@\n     from .cache_utils import SlidingWindowLayer as SlidingWindowLayer\n     from .cache_utils import StaticCache as StaticCache\n     from .cache_utils import StaticLayer as StaticLayer\n+    from .cache_utils import StaticSlidingWindowLayer as StaticSlidingWindowLayer\n     from .configuration_utils import PretrainedConfig as PretrainedConfig\n     from .convert_slow_tokenizer import SLOW_TO_FAST_CONVERTERS as SLOW_TO_FAST_CONVERTERS\n     from .convert_slow_tokenizer import convert_slow_tokenizer as convert_slow_tokenizer"
        },
        {
            "sha": "7c79f7dd4548108f09fc4525f81dcf194b6f97c4",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 5,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3688a977d08dd3d7fd4293e28e011f77efc4fb7a/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=3688a977d08dd3d7fd4293e28e011f77efc4fb7a",
            "patch": "@@ -352,7 +352,7 @@ def get_max_cache_shape(self) -> int:\n         return self.max_cache_len\n \n \n-class SlidingWindowLayer(StaticLayer):\n+class StaticSlidingWindowLayer(StaticLayer):\n     \"\"\"\n     A static cache layer that stores the key and value states as static tensors of shape\n     `[batch_size, num_heads, min(max_cache_len, sliding_window), head_dim]`. It lazily allocates its full backing\n@@ -1080,11 +1080,13 @@ def __init__(\n         layers = []\n         for layer_type in layer_types:\n             if layer_type == \"sliding_attention\":\n-                layer = SlidingWindowLayer(max_cache_len=max_cache_len, sliding_window=config.sliding_window)\n+                layer = StaticSlidingWindowLayer(max_cache_len=max_cache_len, sliding_window=config.sliding_window)\n             elif layer_type == \"chunked_attention\":\n                 # From a cache point of view, both sliding and chunked are the same in how they should behave and how many\n                 # states they should return - only the mask changes to make them different at the end!\n-                layer = SlidingWindowLayer(max_cache_len=max_cache_len, sliding_window=config.attention_chunk_size)\n+                layer = StaticSlidingWindowLayer(\n+                    max_cache_len=max_cache_len, sliding_window=config.attention_chunk_size\n+                )\n             else:\n                 layer = StaticLayer(max_cache_len=max_cache_len)\n             layers.append(layer)\n@@ -1357,11 +1359,20 @@ def is_compileable(self) -> bool:\n ### Deprecated classes\n \n \n-class ChunkedSlidingLayer(SlidingWindowLayer):\n+class SlidingWindowLayer(StaticSlidingWindowLayer):\n+    def __init__(self, max_cache_len: int, sliding_window: int):\n+        logger.warning_once(\n+            \"`SlidingWindowLayer` is deprecated and will be removed in version v4.59 \"\n+            \"Use `StaticSlidingWindowLayer` instead, which is a better name for it.\"\n+        )\n+        super().__init__(max_cache_len, sliding_window)\n+\n+\n+class ChunkedSlidingLayer(StaticSlidingWindowLayer):\n     def __init__(self, max_cache_len: int, sliding_window: int):\n         logger.warning_once(\n             \"`ChunkedSlidingLayer` is deprecated and will be removed in version v4.59 \"\n-            \"Use `SlidingWindowLayer` instead, which has the exact same functionalities.\"\n+            \"Use `StaticSlidingWindowLayer` instead, which has the exact same functionalities.\"\n         )\n         super().__init__(max_cache_len, sliding_window)\n "
        }
    ],
    "stats": {
        "total": 36,
        "additions": 24,
        "deletions": 12
    }
}