{
    "author": "remi-or",
    "message": "[CB] Allow block sharing in hybrid models (#42877)\n\n* Allow block sharing in hybrid architectures\n\n* nit and style\n\n* Better docstring for mark_shareable_blocks_as_complete",
    "sha": "04e78e675ae91cd7b7396b78ba94ae637d37023a",
    "files": [
        {
            "sha": "dbb2f4501d60548f8b48dd701018583231d7db08",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/04e78e675ae91cd7b7396b78ba94ae637d37023a/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04e78e675ae91cd7b7396b78ba94ae637d37023a/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=04e78e675ae91cd7b7396b78ba94ae637d37023a",
            "patch": "@@ -256,7 +256,7 @@ def time_generate(\n         if config.continuous_batching:\n             inputs = self.inputs[\"input_ids\"].tolist()\n             wall_time_0 = time.perf_counter()\n-            outputs = self.model.generate_batch(inputs, allow_prefix_sharing=False, record_timestamps=True)\n+            outputs = self.model.generate_batch(inputs, allow_block_sharing=False, record_timestamps=True)\n         else:\n             streamer = BenchmarkStreamer()\n             wall_time_0 = time.perf_counter()"
        },
        {
            "sha": "d235593b91bd0769ce0cdc60887e0f6fe8ffd6dd",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 23,
            "deletions": 17,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/04e78e675ae91cd7b7396b78ba94ae637d37023a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04e78e675ae91cd7b7396b78ba94ae637d37023a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=04e78e675ae91cd7b7396b78ba94ae637d37023a",
            "patch": "@@ -121,7 +121,7 @@ def __init__(\n         device: torch.device,\n         dtype: torch.dtype = torch.float16,\n         tp_size: int | None = None,\n-        allow_prefix_sharing: bool = True,\n+        allow_block_sharing: bool = True,\n     ) -> None:\n         \"\"\"Initialize a paged attention cache for efficient memory usage. Also turns in prefix sharing if the model has\n         only full attention layers.\n@@ -132,7 +132,8 @@ def __init__(\n             device: Device for the cache tensors\n             dtype: Data type of the cache\n             tp_size: Tensor parallelism size\n-            allow_prefix_sharing: A flag to allow prefix sharing if the model has only full attention layers.\n+            allow_block_sharing: A flag to allow block sharing. If the model has some full attention layers, then prefix\n+                sharing is enabled as well.\n         \"\"\"\n         self.config = config\n         self.dtype = dtype\n@@ -220,19 +221,20 @@ def __init__(\n         logger.info(f\"{self.cache_shape = } {self.key_cache[0].shape = } {self.key_cache[0].numel() = }\")\n \n         # Block management data structures\n+        self.allow_block_sharing = allow_block_sharing\n         self.group_cache_managers: list[CacheAllocator] = []\n         for i, group_type in enumerate(group_types):\n             if group_type == \"full_attention\":\n-                cm = FullAttentionCacheAllocator(i, self.block_size)\n+                cm = FullAttentionCacheAllocator(i, self.block_size, allow_block_sharing=allow_block_sharing)\n             elif group_type == \"sliding_attention\":\n                 cm = SlidingAttentionCacheAllocator(i, self.block_size, config.sliding_window)\n             else:\n                 raise ValueError(f\"Invalid group type: {group_type}\")\n             self.group_cache_managers.append(cm)\n \n-        # We only use prefix sharing if the whole model has only full attention layers\n-        self.use_prefix_sharing = allow_prefix_sharing and group_types == [\"full_attention\"]\n-        self._block_manager = BlockManager(num_blocks, self.block_size, self.use_prefix_sharing)\n+        # We only use prefix sharing if the whole model has only full attention layers and block sharing is allowed\n+        self.use_prefix_sharing = allow_block_sharing and group_types == [\"full_attention\"]\n+        self._block_manager = BlockManager(num_blocks, self.block_size)\n         self.blocks_to_complete: dict[str, int] = {}\n         self._total_prefix_length: int = 0  # a counter to measure the impact of prefix sharing, also used in tests\n \n@@ -352,7 +354,8 @@ def search_prefix_match(self, request_id: str, prompt_ids: list[int]) -> int:\n         allocated_blocks = []\n         for b in range(len(prompt_ids) // self.block_size):\n             tokens = prompt_ids[b * self.block_size : (b + 1) * self.block_size]\n-            current_hash = self._block_manager.compute_hash(current_hash, tokens)\n+            # Prefix sharing is only supported when there is only one full attention layer group, so group_id=0.\n+            current_hash = self._block_manager.compute_hash(current_hash, tokens, group_id=0)\n             block_id = self._block_manager._hash_to_id.get(current_hash)\n             if block_id is not None:\n                 allocated_blocks.append(block_id)\n@@ -369,18 +372,21 @@ def search_prefix_match(self, request_id: str, prompt_ids: list[int]) -> int:\n         self._total_prefix_length += prefix_length\n         return prefix_length\n \n-    def mark_blocks_as_complete(self, state: RequestState) -> None:\n-        \"\"\"Marks the blocks that have been computed in the forward pass as complete. If prefix sharing is off, this is\n-        a no-op.\"\"\"\n-        num_complete_blocks = 0 if not self.use_prefix_sharing else self.blocks_to_complete.pop(state.request_id)\n+    def mark_shareable_blocks_as_complete(self, state: RequestState) -> None:\n+        \"\"\"Marks the blocks allocated to a request (state) as complete if they are shareable and they have been computed\n+        in the forward pass. A complete block is a block where the KV cache has been fully computed: if the block has\n+        enough space to hold the cache for N tokens, the block is marked as complete when the cache data is present for\n+        the N tokens. If block sharing is off, this is a no-op.\"\"\"\n+        num_complete_blocks = 0 if not self.allow_block_sharing else self.blocks_to_complete.pop(state.request_id)\n         if num_complete_blocks == 0:\n             return None\n-        cm = self.group_cache_managers[0]  # if prefix sharing is on, there is only one group\n-        self._block_manager.mark_blocks_as_complete(\n-            num_complete_blocks=num_complete_blocks,\n-            allocated_blocks=cm.block_table[state.request_id],\n-            prompt_ids=(state.initial_tokens + state.generated_tokens),\n-        )\n+        for cm in self.group_cache_managers:\n+            if cm.uses_block_sharing:\n+                self._block_manager.mark_shareable_blocks_as_complete(\n+                    num_complete_blocks=num_complete_blocks,\n+                    allocated_blocks=cm.block_table[state.request_id],\n+                    prompt_ids=(state.initial_tokens + state.generated_tokens),\n+                )\n \n \n # TODO: rework computation with the groups and their sizes"
        },
        {
            "sha": "7f585c595810ba08fc64fba9c7cd08f242b74cdf",
            "filename": "src/transformers/generation/continuous_batching/cache_manager.py",
            "status": "modified",
            "additions": 45,
            "deletions": 34,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/04e78e675ae91cd7b7396b78ba94ae637d37023a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04e78e675ae91cd7b7396b78ba94ae637d37023a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py?ref=04e78e675ae91cd7b7396b78ba94ae637d37023a",
            "patch": "@@ -31,29 +31,31 @@ def reverse_enumerate(xs: list[T]) -> Iterator[tuple[int, T]]:\n         index -= 1\n \n \n-class Block:\n+class Block:  # TODO: rename to ShareableBlock and update the docs\n     \"\"\"A class to represent a block managed by the block manager. We say that a block is complete when the physical KV\n     cache it points to is fully computed. A block can have a parent, which is the block that came before in the\n-    sequence. Once a block is complete, it is given a hash, which takes into account the tokens ids of the block and\n-    its parent's hash (if there is a parent).\"\"\"\n+    sequence. Once a block is complete, it is given a hash, which takes into account the tokens ids of the block, the\n+    layer (group_id) it belong to and its parent's hash (if there is a parent).\"\"\"\n \n-    def __init__(self, id_: int, parent_id: int | None) -> None:\n+    def __init__(self, id_: int, parent_id: int | None, group_id: int) -> None:\n         self.id: int = id_\n         self.parent_id: int | None = parent_id\n+        self.group_id: int = group_id\n         self.hash: int | None = None\n         self.ref_count: int = 1\n \n     def __repr__(self) -> str:\n-        return f\"Block(id={self.id}, parent_id={self.parent_id}, hash={self.hash}, ref_count={self.ref_count})\"\n+        return f\"Block(id={self.id}, parent_id={self.parent_id}, group_id={self.group_id}, hash={self.hash}, ref_count={self.ref_count})\"\n \n     @property\n     def is_complete(self) -> bool:\n         return self.hash is not None\n \n \n class BlockManager:\n-    \"\"\"A class to manage the number of free blocks and block re-use. If prefix sharing is off, the block manager is a\n-    simple FIFO structure where blocks are either free or in use. If prefix sharing is on, blocks can have 3 states:\n+    \"\"\"A class to manage the number of free blocks and block re-use. When a block becomes in use, a flag is passed to\n+    determine if the block is shareable or not. If it is, then a Block object is created and kept track of internally.\n+    It can have the following states:\n       - in use: one or more requests references this block, thus it cannot be written over. The number of requests\n         referencing this block is stored as ref_count in the Block object.\n       - un-initialized: the block points to a space in the KV cache tensor that contains no data yet. Those blocks can\n@@ -63,19 +65,19 @@ class BlockManager:\n         the ref_count of the block and remove it from the list of initialized blocks, because it is now in use.\n         Still, the block can be freed if no un-initialized blocks are left. In that case, we remove its hash from the\n         hash table.\n+    If the block is not shareable, we just use the block manager as a FIFO structure where blocks are either free or in\n+    use. Sharability is determined by the type of cache allocator: blocks created for full attention layers are\n+    shareable, while blocks created for sliding window attention layers are not.\n     There is no structure to keep track of the blocks in use: if a block is neither un-initialized nor initialized,\n     it is in use.\n     \"\"\"\n \n-    def __init__(self, num_blocks: int, block_size: int, use_prefix_sharing: bool) -> None:\n-        \"\"\"Initializes the block manager with a given number of blocks (num_blocks) of size (block_size). Prefix sharing\n-        can be turned on with the (use_prefix_sharing) flag, which only happens if the model has only full attention\n-        layers.\"\"\"\n+    def __init__(self, num_blocks: int, block_size: int) -> None:\n+        \"\"\"Initializes the block manager with a given number of blocks (num_blocks) of size (block_size).\"\"\"\n         self.num_blocks = num_blocks\n         self.block_size = block_size\n         self._uninit_block_ids = deque(range(num_blocks))\n         self._init_block_ids: dict[int, None] = {}  # effectively act as an ordered set\n-        self._use_prefix_sharing = use_prefix_sharing\n         self._hash_to_id: dict[int, int] = {}\n         self._id_to_block: dict[int, Block] = {}\n \n@@ -102,17 +104,20 @@ def has_enough_free_blocks(self, n_blocks: int) -> bool:\n             self._uninit_block_ids.append(id_to_uninitialize)\n         return True\n \n-    def get_free_blocks(self, n_blocks: int, last_block_id: int | None) -> list[int] | None:\n-        \"\"\"Returns a list of (n_blocks) free block and mark them as no longuer free in the internal data structures. One\n-        can also pass a (last_block_id) to indicate the last block id in the sequence, which is used to keep track of\n-        the parent block. If the manager cannot find enough free blocks, it returns None.\"\"\"\n+    def get_free_blocks(\n+        self, n_blocks: int, last_block_id: int | None, shareable: bool, group_id: int\n+    ) -> list[int] | None:\n+        \"\"\"Returns a list of (n_blocks) free block and mark them as no longuer free in the internal data structures.\n+        If the (shareable) flag is set to True, a Block object is created to keep track of the block, with the\n+        (last_block_id) to indicate the last block id in the sequence, also named the parent block. If the manager\n+        cannot find enough free blocks, it returns None.\"\"\"\n         if not self.has_enough_free_blocks(n_blocks):\n             return None\n         allocated_block_ids = [self._uninit_block_ids.popleft() for _ in range(n_blocks)]\n-        # If we use prefix caching, we keep track of the allocated blocks as partial blocks\n-        if self._use_prefix_sharing:\n+        # If the block is shareable, we keep track of the allocated blocks as partial blocks\n+        if shareable:\n             for block_id in allocated_block_ids:\n-                block = Block(block_id, last_block_id)\n+                block = Block(block_id, last_block_id, group_id)\n                 self._id_to_block[block_id] = block\n                 last_block_id = block_id\n         # In both cases, we return the allocated block ids\n@@ -137,23 +142,23 @@ def decrease_ref_count(self, block_id: int) -> None:\n                 self._id_to_block.pop(block_id)\n                 self._uninit_block_ids.append(block_id)\n \n-    def free_blocks(self, blocks: list[int]) -> None:\n-        \"\"\"Marks a list of (blocks) as free. If there is no prefix sharing, we simply add them to the uninitialized\n+    def free_blocks(self, blocks: list[int], shareable: bool) -> None:\n+        \"\"\"Marks a list of (blocks) as free. If the blocks were not (shareable), we simply add them to the uninitialized\n         blocks queue. Otherwise, their new state depends on whether they are complete.\"\"\"\n-        if self._use_prefix_sharing:\n+        if shareable:\n             for block_id in blocks:\n                 self.decrease_ref_count(block_id)\n         else:\n             self._uninit_block_ids.extend(blocks)\n \n-    def mark_blocks_as_complete(\n+    def mark_shareable_blocks_as_complete(\n         self, num_complete_blocks: int, allocated_blocks: list[int], prompt_ids: list[int]\n     ) -> None:\n         \"\"\"Among the list of (allocated_blocks), mark (num_complete_blocks) incomplete blocks as now complete. The list\n         of (prompt_ids) is used to compute the hash of the new block.\"\"\"\n         # Look for the first complete block, starting from the last block in the sequence\n         parent_hash = None\n-        incomplete_blocks: list[Block] = []\n+        incomplete_blocks: list[tuple[int, Block]] = []\n         for i, block_id in reverse_enumerate(allocated_blocks):\n             block = self._id_to_block[block_id]\n             if block.is_complete:\n@@ -178,7 +183,7 @@ def mark_blocks_as_complete(\n             # Otherwise, we compute the hash\n             num_complete_blocks -= 1\n             tokens = prompt_ids[i * self.block_size : (i + 1) * self.block_size]\n-            block.hash = self.compute_hash(parent_hash, tokens)\n+            block.hash = self.compute_hash(parent_hash, tokens, block.group_id)\n \n             existing_block_id = self._hash_to_id.get(block.hash)\n             # If the block hash is already in the hash to id mapping, we reference the existing block instead\n@@ -187,19 +192,20 @@ def mark_blocks_as_complete(\n                 allocated_blocks[i] = existing_block_id\n                 self._id_to_block[existing_block_id].ref_count += 1\n                 new_parent_id = existing_block_id\n-                self.free_blocks([block.id])\n+                self.free_blocks([block.id], shareable=True)\n \n             # Otherwise, we add the completed block to the hash table\n             else:\n+                logger.debug(f\"Adding new block {block.id} (group {block.group_id}) with hash {block.hash}\")\n                 self._hash_to_id[block.hash] = block.id\n \n             # Update loop variables\n             parent_hash = block.hash\n \n-    def compute_hash(self, parent_hash: int | None, tokens: list[int]) -> int:\n-        \"\"\"Computes the hash of a block containing the given (tokens) with a given (parent_hash). If the block has no\n-        parent, the parent hash is None.\"\"\"\n-        return hash((parent_hash, tuple(tokens)))\n+    def compute_hash(self, parent_hash: int | None, tokens: list[int], group_id: int) -> int:\n+        \"\"\"Computes the hash of a block identified by the (tokens) it contains, its (parent_hash) and the layer\n+        (group_id) it belong to. If the block has no parent, the parent hash is None.\"\"\"\n+        return hash((parent_hash, tuple(tokens), group_id))\n \n \n class CacheAllocator(ABC):\n@@ -208,6 +214,7 @@ class CacheAllocator(ABC):\n \n     _index: int\n     block_table: dict[str, list[int]]  # request_id -> list of block_ids allocated to the request\n+    uses_block_sharing: bool  # flag to determine if the blocks are shareable\n \n     @abstractmethod\n     def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> int | None:\n@@ -218,7 +225,7 @@ def free_blocks(self, request_id: str, block_manager: BlockManager) -> None:\n         \"\"\"Frees all blocks associated with a (request_id) using the (block_manager).\"\"\"\n         if request_id in self.block_table:\n             blocks_to_free = self.block_table.pop(request_id)\n-            block_manager.free_blocks(blocks_to_free)\n+            block_manager.free_blocks(blocks_to_free, shareable=self.uses_block_sharing)\n         else:\n             logger.warning(\n                 f\"CacheAllocator {self._index} attempted to free blocks for non-existent request_id: {request_id}\"\n@@ -240,13 +247,14 @@ def get_seqlens_k(self, request_id: str, past_length: int, query_length: int) ->\n class FullAttentionCacheAllocator(CacheAllocator):\n     \"\"\"Cache manager for a group of full attention layers.\"\"\"\n \n-    def __init__(self, index: int, block_size: int) -> None:\n+    def __init__(self, index: int, block_size: int, allow_block_sharing: bool) -> None:\n         \"\"\"Initializes the cache manager for a group of full attention layers.\n         Args:\n             - index: the index of the associated layer group\n             - block_size: the size of the blocks in the cache\n         \"\"\"\n         self._index = index\n+        self.uses_block_sharing = allow_block_sharing\n         self.block_size = block_size\n         self.block_table = {}\n \n@@ -261,7 +269,7 @@ def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockMa\n         else:\n             last_block_id = self.block_table[request_id][-1]\n         # Actual allocation, return early if failed\n-        allocated_blocks = block_manager.get_free_blocks(n_blocks, last_block_id)\n+        allocated_blocks = block_manager.get_free_blocks(n_blocks, last_block_id, self.uses_block_sharing, self._index)\n         if allocated_blocks is None:\n             return None\n         self.block_table[request_id].extend(allocated_blocks)\n@@ -315,6 +323,7 @@ def __init__(self, index: int, block_size: int, sliding_window: int) -> None:\n             - sliding_window: the size of the sliding window\n         \"\"\"\n         self._index = index\n+        self.uses_block_sharing = False\n         self.block_size = block_size\n         self.sliding_window = sliding_window\n         self._max_blocks_per_request = ceil(self.sliding_window / self.block_size)\n@@ -334,7 +343,9 @@ def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockMa\n         after_allocation = min(already_allocated + n_blocks, self._max_blocks_per_request)\n         actual_n_blocks = after_allocation - already_allocated\n         # Classic allocation\n-        allocated_blocks = block_manager.get_free_blocks(actual_n_blocks, None)  # no prefix caching w/ sliding window\n+        allocated_blocks = block_manager.get_free_blocks(\n+            actual_n_blocks, None, self.uses_block_sharing, self._index\n+        )  # no block sharing w/ sliding window\n         if allocated_blocks is None:\n             return None\n         self.block_table[request_id].extend(allocated_blocks)"
        },
        {
            "sha": "28a154465d6831c16b9d93c6aee2847190221aa6",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 20,
            "deletions": 16,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/04e78e675ae91cd7b7396b78ba94ae637d37023a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04e78e675ae91cd7b7396b78ba94ae637d37023a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=04e78e675ae91cd7b7396b78ba94ae637d37023a",
            "patch": "@@ -582,14 +582,14 @@ def update_batch(self) -> None:\n                 # Update the request and stop if it is complete\n                 is_finished = state.update_and_check_completion(token)\n                 # We mark the completed blocks as such\n-                self.cache.mark_blocks_as_complete(state)\n+                self.cache.mark_shareable_blocks_as_complete(state)\n                 if is_finished:\n                     self.metrics.record_request_completion(state.created_time, state.request_id)\n                     self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n                 self._maybe_send_output(state)\n             #  Otherwise, the request is still prefilling, but the prefill has been split\n             elif state.status == RequestStatus.PREFILLING_SPLIT:\n-                self.cache.mark_blocks_as_complete(state)\n+                self.cache.mark_shareable_blocks_as_complete(state)\n                 state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n             else:\n                 raise ValueError(f\"Request {state.request_id} is in an unexpected state: {state.status}\")\n@@ -748,7 +748,7 @@ def __init__(\n         max_queue_size: int = 0,\n         num_q_padding_intervals: int = 0,\n         num_kv_padding_intervals: int = 0,\n-        allow_prefix_sharing: bool = True,\n+        allow_block_sharing: bool = True,\n     ) -> None:\n         \"\"\"Initialize the continuous batching manager.\n \n@@ -758,7 +758,7 @@ def __init__(\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n             num_q_padding_intervals: (optional) Number of intervals used to pad the query dimension\n             num_kv_padding_intervals: (optional) Number of intervals used to pad the keys/values dimension\n-            allow_prefix_sharing: (optional) Whether to allow prefix sharing if the model has only full attention layers\n+            allow_block_sharing: (optional) Whether to allow block sharing if the model has some full attention layers\n         \"\"\"\n         # Reloade paged version if necessary\n         if \"paged|\" not in model.config._attn_implementation:\n@@ -780,7 +780,8 @@ def __init__(\n         self.profile = getattr(generation_config, \"profile\", False)  # TODO: not supported yet\n         self.manual_eviction = manual_eviction\n         self.batch_processor: ContinuousBatchProcessor | None = None\n-        self._allow_prefix_sharing = allow_prefix_sharing\n+        self._allow_block_sharing = allow_block_sharing\n+        self._use_prefix_sharing = allow_block_sharing  # approximation until the cache is created\n \n         self.use_cuda_graph = self._decide_use_cuda_graphs(\n             use_cuda_graph=getattr(generation_config, \"use_cuda_graph\", None),\n@@ -947,7 +948,7 @@ def add_requests(\n         record_timestamps: bool = False,\n     ) -> None:\n         # If there is prefix sharing, we sort the inputs to maximize cache hits\n-        if self._allow_prefix_sharing:\n+        if self._use_prefix_sharing:\n             inputs = sorted(inputs, reverse=True)\n         # Add requests in order\n         for input_ids in inputs:\n@@ -1020,8 +1021,9 @@ def _run_generation_loop(self) -> None:\n                 self.model.device,\n                 self.model.dtype,\n                 tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n-                allow_prefix_sharing=self._allow_prefix_sharing,\n+                allow_block_sharing=self._allow_block_sharing,\n             )\n+            self._use_prefix_sharing = paged_attention_cache.use_prefix_sharing  # update the approximation\n             logger.debug(f\"PagedAttentionCache created in {perf_counter() - t0} seconds\")\n \n             scheduler = None\n@@ -1121,7 +1123,7 @@ def continuous_batching_context_manager(\n         max_queue_size: int = 0,\n         num_q_cuda_graphs: int = 0,\n         num_kv_cuda_graphs: int = 0,\n-        allow_prefix_sharing: bool = True,\n+        allow_block_sharing: bool = True,\n         block: bool = True,\n         timeout: float | None = None,\n     ) -> Generator[ContinuousBatchingManager]:\n@@ -1131,7 +1133,7 @@ def continuous_batching_context_manager(\n             max_queue_size,\n             num_q_cuda_graphs,\n             num_kv_cuda_graphs,\n-            allow_prefix_sharing,\n+            allow_block_sharing,\n         )\n         manager.start()\n         try:\n@@ -1150,7 +1152,7 @@ def init_continuous_batching(\n         max_queue_size: int = 0,\n         num_q_padding_intervals: int = 0,\n         num_kv_padding_intervals: int = 0,\n-        allow_prefix_sharing: bool = True,\n+        allow_block_sharing: bool = True,\n     ) -> ContinuousBatchingManager:\n         \"\"\"Initialize a manager for continuous batching inference.\n \n@@ -1160,7 +1162,7 @@ def init_continuous_batching(\n             max_queue_size: Maximum size of the input request queue\n             num_q_padding_intervals: Number of intervals used to pad the query dimension\n             num_kv_padding_intervals: Number of intervals used to pad the keys/values dimension\n-            allow_prefix_sharing: A flag to allow prefix sharing if the model has only full attention layers\n+            allow_block_sharing: A flag to allow block sharing if the model has some full attention layers\n \n         Returns:\n             `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n@@ -1184,7 +1186,7 @@ def init_continuous_batching(\n             max_queue_size=max_queue_size,\n             num_q_padding_intervals=num_q_padding_intervals,\n             num_kv_padding_intervals=num_kv_padding_intervals,\n-            allow_prefix_sharing=allow_prefix_sharing,\n+            allow_block_sharing=allow_block_sharing,\n         )\n \n     # TODO: support streaming\n@@ -1196,7 +1198,7 @@ def generate_batch(\n         generation_config: GenerationConfig | None = None,\n         num_q_padding_intervals: int = 0,\n         num_kv_padding_intervals: int = 0,\n-        allow_prefix_sharing: bool = True,\n+        allow_block_sharing: bool = True,\n         record_timestamps: bool = False,\n         progress_bar: bool = True,\n         **kwargs,\n@@ -1208,7 +1210,7 @@ def generate_batch(\n             generation_config: Optional generation configuration\n             num_q_padding_intervals: Number of intervals used to pad the query dimension\n             num_kv_padding_intervals: Number of intervals used to pad the keys/values dimension\n-            allow_prefix_sharing: A flag to allow prefix sharing if the model has only full attention layers\n+            allow_block_sharing: A flag to allow block sharing if the model has some full attention layers\n             record_timestamps: If set to true, the requests will have a timestamp for each token generated\n             progress_bar: If set to true, a progress bar will be displayed\n             **kwargs: Additional generation parameters\n@@ -1230,7 +1232,7 @@ def generate_batch(\n                 generation_config=generation_config,\n                 num_q_cuda_graphs=num_q_padding_intervals,\n                 num_kv_cuda_graphs=num_kv_padding_intervals,\n-                allow_prefix_sharing=allow_prefix_sharing,\n+                allow_block_sharing=allow_block_sharing,\n                 block=True,\n                 timeout=5,\n             ) as manager,\n@@ -1243,7 +1245,9 @@ def generate_batch(\n             ) as pbar,\n         ):\n             try:\n-                manager.add_requests(inputs=inputs, max_new_tokens=kwargs.get(\"max_new_tokens\"))\n+                manager.add_requests(\n+                    inputs=inputs, max_new_tokens=kwargs.get(\"max_new_tokens\"), record_timestamps=record_timestamps\n+                )\n                 finished_count = 0\n                 while finished_count < num_requests:\n                     result = manager.get_result(timeout=1)"
        },
        {
            "sha": "109cb635ea5cbcb3c225b8519f1eeb9c2d3ca5fc",
            "filename": "src/transformers/generation/continuous_batching/scheduler.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/04e78e675ae91cd7b7396b78ba94ae637d37023a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04e78e675ae91cd7b7396b78ba94ae637d37023a/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py?ref=04e78e675ae91cd7b7396b78ba94ae637d37023a",
            "patch": "@@ -229,7 +229,7 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n             # Update the token budget\n             token_budget -= request_len\n             # If using prefix sharing, we make note of the blocks that will be computed in the forward pass\n-            if self.cache.use_prefix_sharing:\n+            if self.cache.allow_block_sharing:\n                 tokens_in_current_block = state.current_len() % self.cache.block_size\n                 tokens_after_forward = tokens_in_current_block + request_len\n                 complete_blocks = tokens_after_forward // self.cache.block_size\n@@ -295,7 +295,7 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n             # Update the token budget\n             token_budget -= request_len\n             # If using prefix sharing, we make note of the blocks that will be computed in the forward pass\n-            if self.cache.use_prefix_sharing:\n+            if self.cache.allow_block_sharing:\n                 tokens_in_current_block = state.current_len() % self.cache.block_size\n                 tokens_after_forward = tokens_in_current_block + request_len\n                 complete_blocks = tokens_after_forward // self.cache.block_size"
        },
        {
            "sha": "4bc7fca3204a835f36ad623df411ea9858a8049b",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 77,
            "deletions": 24,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/04e78e675ae91cd7b7396b78ba94ae637d37023a/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04e78e675ae91cd7b7396b78ba94ae637d37023a/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=04e78e675ae91cd7b7396b78ba94ae637d37023a",
            "patch": "@@ -27,7 +27,11 @@\n     GenerationConfig,\n     LogitsProcessorList,\n )\n-from transformers.generation.continuous_batching.cache import group_layers_by_attn_type\n+from transformers.generation.continuous_batching.cache import (\n+    FullAttentionCacheAllocator,\n+    SlidingAttentionCacheAllocator,\n+    group_layers_by_attn_type,\n+)\n from transformers.generation.continuous_batching.continuous_api import build_attention_mask\n from transformers.testing_utils import (\n     Expectations,\n@@ -182,7 +186,7 @@ class ContinuousBatchingGenerationTest(unittest.TestCase):\n     def _test_continuous_batching_parity(\n         self,\n         model_id: str,\n-        allow_prefix_sharing: bool,\n+        allow_block_sharing: bool,\n         attn_implementation: str,\n         use_cuda_graph: bool,\n         use_compile: bool,\n@@ -225,7 +229,7 @@ def _test_continuous_batching_parity(\n \n         # Generation with continuous batching\n         continuous_batching_outputs = model.generate_batch(\n-            inputs=input_ids, generation_config=model.generation_config, allow_prefix_sharing=allow_prefix_sharing\n+            inputs=input_ids, generation_config=model.generation_config, allow_block_sharing=allow_block_sharing\n         )\n \n         # Prepare non-continuous batching inputs\n@@ -269,7 +273,7 @@ def _test_continuous_batching_parity(\n             if continuous_batching_output != generate_output:\n                 decoded_continuous_batching_output = tokenizer.decode(continuous_batching_output)\n                 decoded_generate_output = tokenizer.decode(generate_output)\n-                msg = f\"Test failed for {model_id = } {allow_prefix_sharing = }, {attn_implementation = }, {use_cuda_graph = }, {use_compile = }\\n\"\n+                msg = f\"Test failed for {model_id = } {allow_block_sharing = }, {attn_implementation = }, {use_cuda_graph = }, {use_compile = }\\n\"\n                 msg += f\"User message              : {repr(user_message)}\\n\"\n                 msg += f\"Continuous batching output: {repr(decoded_continuous_batching_output)}\\n\"\n                 msg += f\"Generate output           : {repr(decoded_generate_output)}\"\n@@ -292,14 +296,14 @@ def _test_continuous_batching_parity(\n     @slow\n     def test_continuous_batching_config_combinations(\n         self,\n-        allow_prefix_sharing: bool,\n+        allow_block_sharing: bool,\n         attn_implementation: str,\n         use_cuda_graph: bool,\n         use_compile: bool,\n     ) -> None:\n         model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n         self._test_continuous_batching_parity(\n-            model_id, allow_prefix_sharing, attn_implementation, use_cuda_graph, use_compile\n+            model_id, allow_block_sharing, attn_implementation, use_cuda_graph, use_compile\n         )\n \n     # FIXME: Qwen2.5-0.5B-Instruct is not here because it's  broken (it uses a repetition penalty logits processor)\n@@ -389,11 +393,9 @@ def test_streaming_and_non_streaming_requests_can_alternate(self) -> None:\n     # -----------------------------------------Misc. tests----------------------------------------- #\n     #                     Various tests that don't fit into the other categories                    #\n     # --------------------------------------------------------------------------------------------- #\n-    @require_torch_accelerator\n-    def test_prefix_sharing(self) -> None:\n-        model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n-        max_new_tokens = 32\n-\n+    def _test_block_sharing(\n+        self, model_id: str, expected_layer_types: dict[str, int], input_msg: str, expected_output_tokens: list[int]\n+    ) -> None:\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         model = AutoModelForCausalLM.from_pretrained(model_id)\n \n@@ -402,52 +404,103 @@ def test_prefix_sharing(self) -> None:\n             manager.logit_processor = LogitsProcessorList()\n \n             # Create a request with at least 32 tokens but less than 64 so prefill only generates one complete block\n-            messages = [{\"content\": \"What is the Transformers library known for?\", \"role\": \"user\"}]\n-\n+            messages = [{\"content\": input_msg, \"role\": \"user\"}]\n             inputs = tokenizer.apply_chat_template(\n                 messages, return_tensors=\"pt\", add_generation_prompt=True, return_dict=False\n             )\n             inputs = inputs.to(model.device)[0].tolist()\n             self.assertGreaterEqual(len(inputs), 32, f\"Input length is {len(inputs)} instead of at least 32\")\n             self.assertLess(len(inputs), 64, f\"Input length is {len(inputs)} instead of less than 64\")\n \n-            # First request, which populates the cache with a complete block\n-            request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens)\n+            # First request, which populates the cache w/ 2 complete blocks for each full attention layer group\n+            request_id = manager.add_request(inputs, max_new_tokens=32)\n             chunk_no_reuse = next(manager.request_id_iter(request_id))\n \n+            num_fa = expected_layer_types[\"full_attention\"]\n+            num_sw = expected_layer_types[\"sliding_window\"]\n+\n             hash_table = manager.batch_processor.cache._block_manager._hash_to_id\n             self.assertEqual(\n                 len(hash_table),\n-                2,\n-                f\"There should be 2 blocks, one for the prefill and one for the decode, but {len(hash_table) = }\",\n+                2 * num_fa,  # 2 = 1 for prefill + 1 for decode\n+                f\"There should be {2 * num_fa} blocks, 2 for each full attention layer group, but {len(hash_table) = }\",\n             )\n             total_prefix_length = manager.batch_processor.cache._total_prefix_length\n             self.assertEqual(\n                 total_prefix_length, 0, f\"Expected total prefix length to be 0, got {total_prefix_length}\"\n             )\n \n-            # Second request, which should reuse the same block\n-            request_id = manager.add_request(inputs, max_new_tokens=max_new_tokens)\n+            # Assert the number of layer groups and their types are the expected ones\n+            layer_groups = manager.batch_processor.cache.group_cache_managers\n+            self.assertEqual(\n+                len(layer_groups),\n+                num_fa + num_sw,\n+                f\"There should be {num_fa + num_sw} layer groups, but {len(layer_groups) = }\",\n+            )\n+\n+            layer_group_types = {\"full_attention\": 0, \"sliding_window\": 0}\n+            for cm in layer_groups:\n+                if isinstance(cm, FullAttentionCacheAllocator):\n+                    layer_group_types[\"full_attention\"] += 1\n+                elif isinstance(cm, SlidingAttentionCacheAllocator):\n+                    layer_group_types[\"sliding_window\"] += 1\n+                else:\n+                    raise ValueError(f\"Invalid layer group type: {type(cm)}\")\n+\n+            self.assertEqual(\n+                layer_group_types,\n+                expected_layer_types,\n+                f\"The expected layer group types are\\n{expected_layer_types}\\nbut got\\n{layer_group_types}\",\n+            )\n+\n+            # Second request, which should reuse the same blocks for the full attention layer groups\n+            request_id = manager.add_request(inputs, max_new_tokens=32)\n             chunk_with_reuse = next(manager.request_id_iter(request_id))\n \n             # There should only still be two blocks in the hash table because of block reuse\n             self.assertEqual(\n                 len(hash_table),\n-                2,\n+                2 * num_fa,\n                 f\"Because of block reuse, there should still be two blocks in the hash table, but {len(hash_table) = }\",\n             )\n \n-            # Check that the whole prefill was matched\n+            # Check that the whole prefill was matched if there are only full attention layers\n+            if expected_layer_types[\"sliding_window\"] == 0:\n+                expected_total_prefix_length = 32\n+            else:\n+                expected_total_prefix_length = 0\n             total_prefix_length = manager.batch_processor.cache._total_prefix_length\n             self.assertEqual(\n-                total_prefix_length, 32, f\"Expected total prefix length to be 32, got {total_prefix_length}\"\n+                total_prefix_length,\n+                expected_total_prefix_length,\n+                f\"Expected total prefix length to be {expected_total_prefix_length}, but got {total_prefix_length = }\",\n             )\n \n         # Check the outputs were the same\n         self.assertEqual(chunk_no_reuse.generated_tokens, chunk_with_reuse.generated_tokens)\n \n         # As an additional sanity check, we also compare to the generated tokens when prefix sharing is disabled\n+        print(f\"{chunk_no_reuse.generated_tokens = } {expected_output_tokens = }\")\n+        self.assertEqual(chunk_no_reuse.generated_tokens, expected_output_tokens)\n+\n+    @require_torch_accelerator\n+    def test_prefix_sharing(self) -> None:\n+        model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n+        num_layer_groups = {\"full_attention\": 1, \"sliding_window\": 0}\n+        input_msg = \"What is the Transformers library known for?\"\n         expected_generated_tokens = Expectations({\n-            (\"rocm\", (9, 4)): [785, 80532, 6733, 374, 3881, 369, 1181, 5726, 311, 1855, 323, 36635, 3460, 12934, 4128, 4119, 11, 2670, 1846, 429, 646, 6923, 1467, 11, 14683, 1467, 11, 323, 2736, 1008, 4128, 13904],\n+            (None, None): [785, 80532, 6733, 374, 3881, 369, 1181, 5726, 311, 1855, 323, 36635, 3460, 12934, 4128, 4119, 11, 2670, 1846, 429, 646, 6923, 1467, 11, 14683, 1467, 11, 323, 2736, 1008, 4128, 13904]\n         }).get_expectation()  # fmt: skip\n-        self.assertEqual(chunk_no_reuse.generated_tokens, expected_generated_tokens)\n+\n+        return self._test_block_sharing(model_id, num_layer_groups, input_msg, expected_generated_tokens)\n+\n+    @require_torch_accelerator\n+    def test_block_sharing_with_hybrid_model(self) -> None:\n+        model_id = \"google/gemma-3-1b-it\"\n+        num_layer_groups = {\"full_attention\": 2, \"sliding_window\": 11}\n+        input_msg = \"I am a software engineer looking to use open source software to build a new AI agent. What is the Transformers library known for?\"\n+        expected_generated_tokens = Expectations({\n+            (None, None): [19058, 236764, 1531, 236789, 236751, 2541, 1679, 1144, 506, 128282, 9427, 563, 3224, 573, 236764, 10916, 528, 506, 4403, 529, 3788, 12498, 11362, 236761, 1030, 236789, 236751, 496, 808, 120749, 236829, 532]\n+        }).get_expectation()  # fmt: skip\n+\n+        return self._test_block_sharing(model_id, num_layer_groups, input_msg, expected_generated_tokens)"
        }
    ],
    "stats": {
        "total": 262,
        "additions": 168,
        "deletions": 94
    }
}