{
    "author": "Cyrilvallez",
    "message": "Standardize `PretrainedConfig` to `PreTrainedConfig` (#41300)\n\n* replace\n\n* add metaclass for full BC\n\n* doc\n\n* consistency\n\n* update deprecation message\n\n* revert",
    "sha": "163601c6197e2addccd30d9bf450f664766f2e71",
    "files": [
        {
            "sha": "9cc4def18ab7d9cdf8ca4e4570d11996b2905472",
            "filename": "docs/source/ar/autoclass_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Far%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Far%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fautoclass_tutorial.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -52,7 +52,7 @@\n     <figcaption class=\"mt-2 text-center text-sm text-gray-500\">Ø§Ù„ØµÙˆØ±Ø© ØªÙˆØ¶Ø­ Ù…Ø®Ø·Ø· Ù…Ø±Ø§Ø­Ù„ Ù†Ù…ÙˆØ°Ø¬ Swin.</figcaption>\n </div>\n \n-ÙŠØ³Ù…Ø­ Ù„Ùƒ [`AutoBackbone`] Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙØ¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙƒØ¹Ù…ÙˆØ¯ ÙÙ‚Ø±ÙŠ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø®Ø±Ø§Ø¦Ø· Ù…ÙŠØ²Ø§Øª Ù…Ù† Ù…Ø±Ø§Ø­Ù„ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙÙ‚Ø±ÙŠ. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØ­Ø¯ÙŠØ¯ Ø£Ø­Ø¯ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ [`~PretrainedConfig.from_pretrained`]:\n+ÙŠØ³Ù…Ø­ Ù„Ùƒ [`AutoBackbone`] Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙØ¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙƒØ¹Ù…ÙˆØ¯ ÙÙ‚Ø±ÙŠ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø®Ø±Ø§Ø¦Ø· Ù…ÙŠØ²Ø§Øª Ù…Ù† Ù…Ø±Ø§Ø­Ù„ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙÙ‚Ø±ÙŠ. ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªØ­Ø¯ÙŠØ¯ Ø£Ø­Ø¯ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ [`~PreTrainedConfig.from_pretrained`]:\n \n * `out_indices` Ù‡Ùˆ ÙÙ‡Ø±Ø³ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù†Ù‡Ø§\n * `out_features` Ù‡Ùˆ Ø§Ø³Ù… Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù†Ù‡Ø§"
        },
        {
            "sha": "9908951c592409d0eec640690581f4257b51c15e",
            "filename": "docs/source/ar/create_a_model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Far%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Far%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fcreate_a_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -54,19 +54,19 @@ DistilBertConfig {\n  \n ```\n \n-ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙÙŠ Ø¯Ø§Ù„Ø© [`~PretrainedConfig.from_pretrained`] :\n+ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙÙŠ Ø¯Ø§Ù„Ø© [`~PreTrainedConfig.from_pretrained`] :\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n ```\n \n-Ø¨Ù…Ø¬Ø±Ø¯ Ø£Ù† ØªØµØ¨Ø­ Ø±Ø§Ø¶ÙŠÙ‹Ø§ Ø¹Ù† ØªÙƒÙˆÙŠÙ† Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~PretrainedConfig.save_pretrained`]. ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¹Ù„Ù‰ Ø£Ù†Ù‡ Ù…Ù„Ù JSON ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø¯Ø¯:\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø£Ù† ØªØµØ¨Ø­ Ø±Ø§Ø¶ÙŠÙ‹Ø§ Ø¹Ù† ØªÙƒÙˆÙŠÙ† Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~PreTrainedConfig.save_pretrained`]. ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¹Ù„Ù‰ Ø£Ù†Ù‡ Ù…Ù„Ù JSON ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø¯Ø¯:\n \n ```py\n >>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n ```\n \n-Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ†ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~PretrainedConfig.from_pretrained`]:\n+Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ†ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~PreTrainedConfig.from_pretrained`]:\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")"
        },
        {
            "sha": "cb4a4a3fae1a1a74a5b350d1f26d784636efe232",
            "filename": "docs/source/ar/custom_models.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Far%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Far%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fcustom_models.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,11 +20,11 @@\n ÙÙŠ Ù…Ø«Ø§Ù„Ù†Ø§ØŒ Ø³Ù†Ø¹Ø¯Ù‘Ù„ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· ÙÙŠ ÙØ¦Ø© ResNet Ø§Ù„ØªÙŠ Ù‚Ø¯ Ù†Ø±ØºØ¨ ÙÙŠ Ø¶Ø¨Ø·Ù‡Ø§. Ø³ØªØ¹Ø·ÙŠÙ†Ø§ Ø§Ù„ØªÙƒÙˆÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø£Ù†ÙˆØ§Ø¹ ResNets Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø§Ù„Ù…Ù…ÙƒÙ†Ø©. Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªØ®Ø²ÙŠÙ† Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­ØªÙ‡.\n \n ```python\n-from transformers import PretrainedConfig\n+from transformers import PreTrainedConfig\n from typing import List\n \n \n-class ResnetConfig(PretrainedConfig):\n+class ResnetConfig(PreTrainedConfig):\n     model_type = \"resnet\"\n \n     def __init__(\n@@ -58,11 +58,11 @@ class ResnetConfig(PretrainedConfig):\n ```\n Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ ØªØ°ÙƒØ±Ù‡Ø§ Ø¹Ù†Ø¯ ÙƒØªØ§Ø¨Ø© ØªÙƒÙˆÙŠÙ†Ùƒ Ø§Ù„Ø®Ø§Øµ Ù‡ÙŠ:\n \n-- ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ø« Ù…Ù† `PretrainedConfig`ØŒ\n-- ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚Ø¨Ù„ Ø¯Ø§Ù„Ø©  `__init__` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ `PretrainedConfig` Ø£ÙŠ Ù…Ø¹Ø§Ù…ï»»Øª Ø¥Ø¶Ø§ÙÙŠØ© kwargsØŒ\n+- ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ø« Ù…Ù† `PreTrainedConfig`ØŒ\n+- ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚Ø¨Ù„ Ø¯Ø§Ù„Ø©  `__init__` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ `PreTrainedConfig` Ø£ÙŠ Ù…Ø¹Ø§Ù…ï»»Øª Ø¥Ø¶Ø§ÙÙŠØ© kwargsØŒ\n - ÙŠØ¬Ø¨ ØªÙ…Ø±ÙŠØ± Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ø§Ù…ï»»Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© `__init__` ÙÙ‰ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø§Ø¹Ù„Ù‰.\n \n-ÙŠØ¶Ù…Ù† Ø§Ù„Ø¥Ø±Ø« Ø­ØµÙˆÙ„Ùƒ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— TransformersØŒ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† Ø§Ù„Ù‚ÙŠØ¯ÙŠÙ† Ø§Ù„ØªØ§Ù†Ù‰ ÙˆØ§Ù„Ø«Ø§Ù„Ø« ÙŠØ£ØªÙŠØ§Ù† Ù…Ù† Ø­Ù‚ÙŠÙ‚Ø© Ø£Ù† `PretrainedConfig` Ù„Ø¯ÙŠÙ‡ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø£ÙƒØ«Ø± Ù…Ù† ØªÙ„Ùƒ Ø§Ù„ØªÙŠ ØªÙ‚ÙˆÙ… Ø¨ØªØ¹ÙŠÙŠÙ†Ù‡Ø§. Ø¹Ù†Ø¯ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ ØªÙƒÙˆÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© `from_pretrained`ØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ‚Ø¨Ù„ ØªÙƒÙˆÙŠÙ†Ùƒ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø«Ù… Ø¥Ø±Ø³Ø§Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰.\n+ÙŠØ¶Ù…Ù† Ø§Ù„Ø¥Ø±Ø« Ø­ØµÙˆÙ„Ùƒ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— TransformersØŒ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† Ø§Ù„Ù‚ÙŠØ¯ÙŠÙ† Ø§Ù„ØªØ§Ù†Ù‰ ÙˆØ§Ù„Ø«Ø§Ù„Ø« ÙŠØ£ØªÙŠØ§Ù† Ù…Ù† Ø­Ù‚ÙŠÙ‚Ø© Ø£Ù† `PreTrainedConfig` Ù„Ø¯ÙŠÙ‡ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø£ÙƒØ«Ø± Ù…Ù† ØªÙ„Ùƒ Ø§Ù„ØªÙŠ ØªÙ‚ÙˆÙ… Ø¨ØªØ¹ÙŠÙŠÙ†Ù‡Ø§. Ø¹Ù†Ø¯ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ ØªÙƒÙˆÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© `from_pretrained`ØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ‚Ø¨Ù„ ØªÙƒÙˆÙŠÙ†Ùƒ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø«Ù… Ø¥Ø±Ø³Ø§Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰.\n \n ØªØ­Ø¯ÙŠØ¯ `model_type` Ù„ØªÙƒÙˆÙŠÙ†Ùƒ (Ù‡Ù†Ø§ `model_type=\"resnet\"`) Ù„ÙŠØ³ Ø¥Ù„Ø²Ø§Ù…ÙŠÙ‹Ø§ØŒ Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ\n ØªØ³Ø¬ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© (Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø£Ø®ÙŠØ±).\n@@ -82,7 +82,7 @@ resnet50d_config.save_pretrained(\"custom-resnet\")\n resnet50d_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n ```\n \n-ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰ Ù…Ù† ÙØ¦Ø© [`PretrainedConfig`]ØŒ Ù…Ø«Ù„ [`~PretrainedConfig.push_to_hub`] Ù„ØªØ­Ù…ÙŠÙ„ ØªÙƒÙˆÙŠÙ†Ùƒ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Hub.\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰ Ù…Ù† ÙØ¦Ø© [`PreTrainedConfig`]ØŒ Ù…Ø«Ù„ [`~PreTrainedConfig.push_to_hub`] Ù„ØªØ­Ù…ÙŠÙ„ ØªÙƒÙˆÙŠÙ†Ùƒ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Hub.\n \n ## ÙƒØªØ§Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬ Ù…Ø®ØµØµ\n "
        },
        {
            "sha": "848dcbc30631288185b703131e8a02b81c588cb5",
            "filename": "docs/source/de/add_new_model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fde%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fde%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fadd_new_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -53,7 +53,7 @@ Lassen Sie uns daher ein wenig tiefer in das allgemeine Design der Bibliothek ei\n ### Ãœberblick Ã¼ber die Modelle\n \n Um ein Modell erfolgreich hinzuzufÃ¼gen, ist es wichtig, die Interaktion zwischen Ihrem Modell und seiner Konfiguration zu verstehen,\n-[`PreTrainedModel`] und [`PretrainedConfig`]. Als Beispiel werden wir\n+[`PreTrainedModel`] und [`PreTrainedConfig`]. Als Beispiel werden wir\n das Modell, das zu ğŸ¤— Transformers hinzugefÃ¼gt werden soll, `BrandNewBert` nennen.\n \n Schauen wir uns das mal an:\n@@ -81,10 +81,10 @@ model.config  # model has access to its config\n ```\n \n Ã„hnlich wie das Modell erbt die Konfiguration grundlegende Serialisierungs- und DeserialisierungsfunktionalitÃ¤ten von\n-[`PretrainedConfig`]. Beachten Sie, dass die Konfiguration und das Modell immer in zwei verschiedene Formate serialisiert werden\n+[`PreTrainedConfig`]. Beachten Sie, dass die Konfiguration und das Modell immer in zwei verschiedene Formate serialisiert werden\n unterschiedliche Formate serialisiert werden - das Modell in eine *pytorch_model.bin* Datei und die Konfiguration in eine *config.json* Datei. Aufruf von\n [`~PreTrainedModel.save_pretrained`] wird automatisch\n-[`~PretrainedConfig.save_pretrained`] auf, so dass sowohl das Modell als auch die Konfiguration gespeichert werden.\n+[`~PreTrainedConfig.save_pretrained`] auf, so dass sowohl das Modell als auch die Konfiguration gespeichert werden.\n \n \n ### Code-Stil"
        },
        {
            "sha": "a9d8168f7505ed4dd2a894a37abf70eff4619591",
            "filename": "docs/source/en/add_new_model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fadd_new_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -51,7 +51,7 @@ This section describes how the model and configuration classes interact and the\n \n ### Model and configuration\n \n-All Transformers' models inherit from a base [`PreTrainedModel`] and [`PretrainedConfig`] class. The configuration is the models blueprint.\n+All Transformers' models inherit from a base [`PreTrainedModel`] and [`PreTrainedConfig`] class. The configuration is the models blueprint.\n \n There is never more than two levels of abstraction for any model to keep the code readable. The example model here, BrandNewLlama, inherits from `BrandNewLlamaPreTrainedModel` and [`PreTrainedModel`]. It is important that a new model only depends on [`PreTrainedModel`] so that it can use the [`~PreTrainedModel.from_pretrained`] and [`~PreTrainedModel.save_pretrained`] methods.\n \n@@ -66,9 +66,9 @@ model = BrandNewLlamaModel.from_pretrained(\"username/brand_new_llama\")\n model.config\n ```\n \n-[`PretrainedConfig`] provides the [`~PretrainedConfig.from_pretrained`] and [`~PretrainedConfig.save_pretrained`] methods.\n+[`PreTrainedConfig`] provides the [`~PreTrainedConfig.from_pretrained`] and [`~PreTrainedConfig.save_pretrained`] methods.\n \n-When you use [`PreTrainedModel.save_pretrained`], it automatically calls [`PretrainedConfig.save_pretrained`] so that both the model and configuration are saved together.\n+When you use [`PreTrainedModel.save_pretrained`], it automatically calls [`PreTrainedConfig.save_pretrained`] so that both the model and configuration are saved together.\n \n A model is saved to a `model.safetensors` file and a configuration is saved to a `config.json` file.\n "
        },
        {
            "sha": "c54dc1d00af088260e062b25d7fefeafbb424496",
            "filename": "docs/source/en/backbones.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fbackbones.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fbackbones.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fbackbones.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -22,7 +22,7 @@ Higher-level computer visions tasks, such as object detection or image segmentat\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Backbone.png\"/>\n </div>\n \n-Load a backbone with [`~PretrainedConfig.from_pretrained`] and use the `out_indices` parameter to determine which layer, given by the index, to extract a feature map from.\n+Load a backbone with [`~PreTrainedConfig.from_pretrained`] and use the `out_indices` parameter to determine which layer, given by the index, to extract a feature map from.\n \n ```py\n from transformers import AutoBackbone\n@@ -46,7 +46,7 @@ There are two ways to load a Transformers backbone, [`AutoBackbone`] and a model\n <hfoptions id=\"backbone-classes\">\n <hfoption id=\"AutoBackbone\">\n \n-The [AutoClass](./model_doc/auto) API automatically loads a pretrained vision model with [`~PretrainedConfig.from_pretrained`] as a backbone if it's supported.\n+The [AutoClass](./model_doc/auto) API automatically loads a pretrained vision model with [`~PreTrainedConfig.from_pretrained`] as a backbone if it's supported.\n \n Set the `out_indices` parameter to the layer you'd like to get the feature map from. If you know the name of the layer, you could also use `out_features`. These parameters can be used interchangeably, but if you use both, make sure they refer to the same layer.\n "
        },
        {
            "sha": "f1d5e1b94c4fbdacc8d50c1b7d32e61fce9c446f",
            "filename": "docs/source/en/custom_models.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcustom_models.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -25,23 +25,23 @@ This guide will show you how to customize a ResNet model, enable [AutoClass](./m\n \n ## Configuration\n \n-A configuration, given by the base [`PretrainedConfig`] class, contains all the necessary information to build a model. This is where you'll configure the attributes of the custom ResNet model. Different attributes gives different ResNet model types.\n+A configuration, given by the base [`PreTrainedConfig`] class, contains all the necessary information to build a model. This is where you'll configure the attributes of the custom ResNet model. Different attributes gives different ResNet model types.\n \n The main rules for customizing a configuration are:\n \n-1. A custom configuration must subclass [`PretrainedConfig`]. This ensures a custom model has all the functionality of a Transformers' model such as [`~PretrainedConfig.from_pretrained`], [`~PretrainedConfig.save_pretrained`], and [`~PretrainedConfig.push_to_hub`].\n-2. The [`PretrainedConfig`] `__init__` must accept any `kwargs` and they must be passed to the superclass `__init__`. [`PretrainedConfig`] has more fields than the ones set in your custom configuration, so when you load a configuration with [`~PretrainedConfig.from_pretrained`], those fields need to be accepted by your configuration and passed to the superclass.\n+1. A custom configuration must subclass [`PreTrainedConfig`]. This ensures a custom model has all the functionality of a Transformers' model such as [`~PreTrainedConfig.from_pretrained`], [`~PreTrainedConfig.save_pretrained`], and [`~PreTrainedConfig.push_to_hub`].\n+2. The [`PreTrainedConfig`] `__init__` must accept any `kwargs` and they must be passed to the superclass `__init__`. [`PreTrainedConfig`] has more fields than the ones set in your custom configuration, so when you load a configuration with [`~PreTrainedConfig.from_pretrained`], those fields need to be accepted by your configuration and passed to the superclass.\n \n > [!TIP]\n > It is useful to check the validity of some of the parameters. In the example below, a check is implemented to ensure `block_type` and `stem_type` belong to one of the predefined values.\n >\n > Add `model_type` to the configuration class to enable [AutoClass](./models#autoclass) support.\n \n ```py\n-from transformers import PretrainedConfig\n+from transformers import PreTrainedConfig\n from typing import List\n \n-class ResnetConfig(PretrainedConfig):\n+class ResnetConfig(PreTrainedConfig):\n     model_type = \"resnet\"\n \n     def __init__(\n@@ -74,7 +74,7 @@ class ResnetConfig(PretrainedConfig):\n         super().__init__(**kwargs)\n ```\n \n-Save the configuration to a JSON file in your custom model folder, `custom-resnet`, with [`~PretrainedConfig.save_pretrained`].\n+Save the configuration to a JSON file in your custom model folder, `custom-resnet`, with [`~PreTrainedConfig.save_pretrained`].\n \n ```py\n resnet50d_config = ResnetConfig(block_type=\"bottleneck\", stem_width=32, stem_type=\"deep\", avg_down=True)\n@@ -83,7 +83,7 @@ resnet50d_config.save_pretrained(\"custom-resnet\")\n \n ## Model\n \n-With the custom ResNet configuration, you can now create and customize the model. The model subclasses the base [`PreTrainedModel`] class. Like [`PretrainedConfig`], inheriting from [`PreTrainedModel`] and initializing the superclass with the configuration extends Transformers' functionalities such as saving and loading to the custom model.\n+With the custom ResNet configuration, you can now create and customize the model. The model subclasses the base [`PreTrainedModel`] class. Like [`PreTrainedConfig`], inheriting from [`PreTrainedModel`] and initializing the superclass with the configuration extends Transformers' functionalities such as saving and loading to the custom model.\n \n Transformers' models follow the convention of accepting a `config` object in the `__init__` method. This passes the entire `config` to the model sublayers, instead of breaking the `config` object into multiple arguments that are individually passed to the sublayers.\n \n@@ -235,7 +235,7 @@ from resnet_model.configuration_resnet import ResnetConfig\n from resnet_model.modeling_resnet import ResnetModel, ResnetModelForImageClassification\n ```\n \n-Copy the code from the model and configuration files. To make sure the AutoClass objects are saved with [`~PreTrainedModel.save_pretrained`], call the [`~PretrainedConfig.register_for_auto_class`] method. This modifies the configuration JSON file to include the AutoClass objects and mapping.\n+Copy the code from the model and configuration files. To make sure the AutoClass objects are saved with [`~PreTrainedModel.save_pretrained`], call the [`~PreTrainedConfig.register_for_auto_class`] method. This modifies the configuration JSON file to include the AutoClass objects and mapping.\n \n For a model, pick the appropriate `AutoModelFor` class based on the task.\n "
        },
        {
            "sha": "a690f30f9d60bead141e15d8bbeb5ddbac008476",
            "filename": "docs/source/en/main_classes/configuration.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fmain_classes%2Fconfiguration.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fmain_classes%2Fconfiguration.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fconfiguration.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,16 +16,16 @@ rendered properly in your Markdown viewer.\n \n # Configuration\n \n-The base class [`PretrainedConfig`] implements the common methods for loading/saving a configuration\n+The base class [`PreTrainedConfig`] implements the common methods for loading/saving a configuration\n either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded\n from HuggingFace's AWS S3 repository).\n \n Each derived config class implements model specific attributes. Common attributes present in all config classes are:\n `hidden_size`, `num_attention_heads`, and `num_hidden_layers`. Text models further implement:\n `vocab_size`.\n \n-## PretrainedConfig\n+## PreTrainedConfig\n \n-[[autodoc]] PretrainedConfig\n+[[autodoc]] PreTrainedConfig\n     - push_to_hub\n     - all"
        },
        {
            "sha": "09884bcb71d4560a93aae6d167388f04ffed9ec1",
            "filename": "docs/source/en/model_doc/auto.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -48,7 +48,7 @@ You will then be able to use the auto classes like you would usually do!\n \n <Tip warning={true}>\n \n-If your `NewModelConfig` is a subclass of [`~transformers.PretrainedConfig`], make sure its\n+If your `NewModelConfig` is a subclass of [`~transformers.PreTrainedConfig`], make sure its\n `model_type` attribute is set to the same key you use when registering the config (here `\"new-model\"`).\n \n Likewise, if your `NewModel` is a subclass of [`PreTrainedModel`], make sure its"
        },
        {
            "sha": "d6d823156b2b00e74691ae27cb60a6870ecf8b34",
            "filename": "docs/source/en/quicktour.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquicktour.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -73,7 +73,7 @@ Each pretrained model inherits from three base classes.\n \n | **Class** | **Description** |\n |---|---|\n-| [`PretrainedConfig`] | A file that specifies a models attributes such as the number of attention heads or vocabulary size. |\n+| [`PreTrainedConfig`] | A file that specifies a models attributes such as the number of attention heads or vocabulary size. |\n | [`PreTrainedModel`] | A model (or architecture) defined by the model attributes from the configuration file. A pretrained model only returns the raw hidden states. For a specific task, use the appropriate model head to convert the raw hidden states into a meaningful result (for example, [`LlamaModel`] versus [`LlamaForCausalLM`]). |\n | Preprocessor | A class for converting raw inputs (text, images, audio, multimodal) into numerical inputs to the model. For example, [`PreTrainedTokenizer`] converts text into tensors and [`ImageProcessingMixin`] converts pixels into tensors. |\n "
        },
        {
            "sha": "fc854f94c2dfc7cedff563c7d20e6a83bab77923",
            "filename": "docs/source/en/torchscript.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Ftorchscript.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Ftorchscript.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftorchscript.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -21,7 +21,7 @@ rendered properly in your Markdown viewer.\n Transformers can export a model to TorchScript by:\n \n 1. creating dummy inputs to create a *trace* of the model to serialize to TorchScript\n-2. enabling the `torchscript` parameter in either [`~PretrainedConfig.torchscript`] for a randomly initialized model or [`~PreTrainedModel.from_pretrained`] for a pretrained model\n+2. enabling the `torchscript` parameter in either [`~PreTrainedConfig.torchscript`] for a randomly initialized model or [`~PreTrainedModel.from_pretrained`] for a pretrained model\n \n ## Dummy inputs\n "
        },
        {
            "sha": "23b1e9a099d61ae6207567d36f82b9fbe2fda624",
            "filename": "docs/source/en/transformers_as_backend.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Ftransformers_as_backend.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fen%2Ftransformers_as_backend.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftransformers_as_backend.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -135,9 +135,9 @@ class MyModel(PreTrainedModel):\n \n ```python\n \n-from transformers import PretrainedConfig\n+from transformers import PreTrainedConfig\n \n-class MyConfig(PretrainedConfig):\n+class MyConfig(PreTrainedConfig):\n     base_model_tp_plan = {\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\","
        },
        {
            "sha": "9d9208d38d5408082967aff09f9ae42b4f4c5ead",
            "filename": "docs/source/es/create_a_model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fes%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fes%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fcreate_a_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -83,19 +83,19 @@ DistilBertConfig {\n }\n ```\n \n-Los atributos de los modelos preentrenados pueden ser modificados con la funciÃ³n [`~PretrainedConfig.from_pretrained`]:\n+Los atributos de los modelos preentrenados pueden ser modificados con la funciÃ³n [`~PreTrainedConfig.from_pretrained`]:\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n ```\n \n-Cuando estÃ©s satisfecho con la configuraciÃ³n de tu modelo, puedes guardarlo con la funciÃ³n [`~PretrainedConfig.save_pretrained`]. Tu configuraciÃ³n se guardarÃ¡ en un archivo JSON dentro del directorio que le especifiques como parÃ¡metro.\n+Cuando estÃ©s satisfecho con la configuraciÃ³n de tu modelo, puedes guardarlo con la funciÃ³n [`~PreTrainedConfig.save_pretrained`]. Tu configuraciÃ³n se guardarÃ¡ en un archivo JSON dentro del directorio que le especifiques como parÃ¡metro.\n \n ```py\n >>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n ```\n \n-Para volver a usar el archivo de configuraciÃ³n, puedes cargarlo usando [`~PretrainedConfig.from_pretrained`]:\n+Para volver a usar el archivo de configuraciÃ³n, puedes cargarlo usando [`~PreTrainedConfig.from_pretrained`]:\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")"
        },
        {
            "sha": "ff093a6fecf92b56dcb3c0413d9e5dc8418f72f6",
            "filename": "docs/source/es/custom_models.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fes%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fes%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fcustom_models.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -38,11 +38,11 @@ configuraciones nos darÃ¡n los diferentes tipos de ResNet que son posibles. Lueg\n despuÃ©s de verificar la validez de algunos de ellos.\n \n ```python\n-from transformers import PretrainedConfig\n+from transformers import PreTrainedConfig\n from typing import List\n \n \n-class ResnetConfig(PretrainedConfig):\n+class ResnetConfig(PreTrainedConfig):\n     model_type = \"resnet\"\n \n     def __init__(\n@@ -76,12 +76,12 @@ class ResnetConfig(PretrainedConfig):\n ```\n \n Las tres cosas importantes que debes recordar al escribir tu propia configuraciÃ³n son las siguientes:\n-- tienes que heredar de `PretrainedConfig`,\n-- el `__init__` de tu `PretrainedConfig` debe aceptar cualquier `kwargs`,\n+- tienes que heredar de `PreTrainedConfig`,\n+- el `__init__` de tu `PreTrainedConfig` debe aceptar cualquier `kwargs`,\n - esos `kwargs` deben pasarse a la superclase `__init__`.\n \n La herencia es para asegurarte de obtener toda la funcionalidad de la biblioteca ğŸ¤— Transformers, mientras que las otras dos \n-restricciones provienen del hecho de que una `PretrainedConfig` tiene mÃ¡s campos que los que estÃ¡s configurando. Al recargar una \n+restricciones provienen del hecho de que una `PreTrainedConfig` tiene mÃ¡s campos que los que estÃ¡s configurando. Al recargar una \n `config` con el mÃ©todo `from_pretrained`, esos campos deben ser aceptados por tu `config` y luego enviados a la superclase.\n \n Definir un `model_type` para tu configuraciÃ³n (en este caso `model_type=\"resnet\"`) no es obligatorio, a menos que quieras\n@@ -102,7 +102,7 @@ con el mÃ©todo `from_pretrained`:\n resnet50d_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n ```\n \n-TambiÃ©n puedes usar cualquier otro mÃ©todo de la clase [`PretrainedConfig`], como [`~PretrainedConfig.push_to_hub`], para cargar \n+TambiÃ©n puedes usar cualquier otro mÃ©todo de la clase [`PreTrainedConfig`], como [`~PreTrainedConfig.push_to_hub`], para cargar \n directamente tu configuraciÃ³n en el Hub.\n \n ## Escribir un modelo personalizado"
        },
        {
            "sha": "cec70285b48d7265e57590df011e41888603090e",
            "filename": "docs/source/fr/autoclass_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Ffr%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Ffr%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Fautoclass_tutorial.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -71,7 +71,7 @@ Pour les tÃ¢ches de vision, un processeur d'image traite l'image pour la formate\n     <figcaption class=\"mt-2 text-center text-sm text-gray-500\">Un backbone Swin avec plusieurs Ã©tapes pour produire une carte de caractÃ©ristiques.</figcaption>\n </div>\n \n-[`AutoBackbone`] vous permet d'utiliser des modÃ¨les prÃ©-entraÃ®nÃ©s comme backbones pour obtenir des cartes de caractÃ©ristiques Ã  partir de diffÃ©rentes Ã©tapes du backbone. Vous devez spÃ©cifier l'un des paramÃ¨tres suivants dans [`~PretrainedConfig.from_pretrained`] :\n+[`AutoBackbone`] vous permet d'utiliser des modÃ¨les prÃ©-entraÃ®nÃ©s comme backbones pour obtenir des cartes de caractÃ©ristiques Ã  partir de diffÃ©rentes Ã©tapes du backbone. Vous devez spÃ©cifier l'un des paramÃ¨tres suivants dans [`~PreTrainedConfig.from_pretrained`] :\n \n * `out_indices` est l'index de la couche dont vous souhaitez obtenir la carte de caractÃ©ristiques\n * `out_features` est le nom de la couche dont vous souhaitez obtenir la carte de caractÃ©ristiques"
        },
        {
            "sha": "16e461e4deb56f22fce109ac4bdb071d383587de",
            "filename": "docs/source/it/add_new_model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fit%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fit%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fadd_new_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -67,7 +67,7 @@ Tenendo questi principi in mente, immergiamoci nel design generale della libreri\n ### Panoramica sui modelli\n \n Per aggiungere con successo un modello, Ã© importante capire l'interazione tra il tuo modello e la sua configurazione,\n-[`PreTrainedModel`], e [`PretrainedConfig`]. Per dare un esempio, chiameremo il modello da aggiungere a ğŸ¤— Transformers\n+[`PreTrainedModel`], e [`PreTrainedConfig`]. Per dare un esempio, chiameremo il modello da aggiungere a ğŸ¤— Transformers\n `BrandNewBert`.\n \n Diamo un'occhiata:\n@@ -94,9 +94,9 @@ model.config  # il modello ha accesso al suo config\n ```\n \n Analogamente al modello, la configurazione eredita le funzionalitÃ  base di serializzazione e deserializzazione da\n-[`PretrainedConfig`]. Ã‰ da notare che la configurazione e il modello sono sempre serializzati in due formati differenti -\n+[`PreTrainedConfig`]. Ã‰ da notare che la configurazione e il modello sono sempre serializzati in due formati differenti -\n il modello Ã© serializzato in un file *pytorch_model.bin* mentre la configurazione con *config.json*. Chiamando\n-[`~PreTrainedModel.save_pretrained`] automaticamente chiamerÃ  [`~PretrainedConfig.save_pretrained`], cosicchÃ© sia il\n+[`~PreTrainedModel.save_pretrained`] automaticamente chiamerÃ  [`~PreTrainedConfig.save_pretrained`], cosicchÃ© sia il\n modello che la configurazione siano salvati.\n \n "
        },
        {
            "sha": "d99a4ea02fb9c83930f124e6df13bfa3e475a1a4",
            "filename": "docs/source/it/create_a_model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fit%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fit%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fcreate_a_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -83,19 +83,19 @@ DistilBertConfig {\n }\n ```\n \n-Nella funzione [`~PretrainedConfig.from_pretrained`] possono essere modificati gli attributi del modello pre-allenato:\n+Nella funzione [`~PreTrainedConfig.from_pretrained`] possono essere modificati gli attributi del modello pre-allenato:\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n ```\n \n-Quando la configurazione del modello ti soddisfa, la puoi salvare con [`~PretrainedConfig.save_pretrained`]. Il file della tua configurazione Ã¨ memorizzato come file JSON nella save directory specificata:\n+Quando la configurazione del modello ti soddisfa, la puoi salvare con [`~PreTrainedConfig.save_pretrained`]. Il file della tua configurazione Ã¨ memorizzato come file JSON nella save directory specificata:\n \n ```py\n >>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n ```\n \n-Per riutilizzare la configurazione del file, caricalo con [`~PretrainedConfig.from_pretrained`]:\n+Per riutilizzare la configurazione del file, caricalo con [`~PreTrainedConfig.from_pretrained`]:\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")"
        },
        {
            "sha": "be2e1eada3efea94aba23fea00611fde4c483e33",
            "filename": "docs/source/it/custom_models.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fit%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fit%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fcustom_models.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -37,11 +37,11 @@ Configurazioni differenti ci daranno quindi i differenti possibili tipi di ResNe\n dopo averne controllato la validitÃ .\n \n ```python\n-from transformers import PretrainedConfig\n+from transformers import PreTrainedConfig\n from typing import List\n \n \n-class ResnetConfig(PretrainedConfig):\n+class ResnetConfig(PreTrainedConfig):\n     model_type = \"resnet\"\n \n     def __init__(\n@@ -75,12 +75,12 @@ class ResnetConfig(PretrainedConfig):\n ```\n \n Le tre cose piÃ¹ importanti da ricordare quando scrivi le tue configurazioni sono le seguenti:\n-- Devi ereditare da `Pretrainedconfig`,\n-- Il metodo `__init__` del tuo `Pretrainedconfig` deve accettare i kwargs,\n+- Devi ereditare da `PreTrainedConfig`,\n+- Il metodo `__init__` del tuo `PreTrainedConfig` deve accettare i kwargs,\n - I `kwargs` devono essere passati alla superclass `__init__`\n \n Lâ€™ereditÃ  Ã¨ importante per assicurarsi di ottenere tutte le funzionalitÃ  della libreria ğŸ¤— transformers, \n-mentre gli altri due vincoli derivano dal fatto che un `Pretrainedconfig` ha piÃ¹ campi di quelli che stai settando. \n+mentre gli altri due vincoli derivano dal fatto che un `PreTrainedConfig` ha piÃ¹ campi di quelli che stai settando. \n Quando ricarichi una config da un metodo `from_pretrained`, questi campi devono essere accettati dalla tua config e\n poi inviati alla superclasse.\n \n@@ -102,7 +102,7 @@ config con il metodo `from_pretrained`.\n resnet50d_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n ```\n \n-Puoi anche usare qualunque altro metodo della classe [`PretrainedConfig`], come [`~PretrainedConfig.push_to_hub`]\n+Puoi anche usare qualunque altro metodo della classe [`PreTrainedConfig`], come [`~PreTrainedConfig.push_to_hub`]\n per caricare direttamente la tua configurazione nell'hub.\n \n ## Scrivere un modello personalizzato"
        },
        {
            "sha": "75219dcb8f88b3596a7a91a42c65ef5d4babb6cb",
            "filename": "docs/source/ja/add_new_model.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fadd_new_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -51,7 +51,7 @@ Hugging Faceãƒãƒ¼ãƒ ã®ãƒ¡ãƒ³ãƒãƒ¼ãŒã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã™ã‚‹ã®ã§ã€ä¸€\n \n ### Overview of models\n \n-ãƒ¢ãƒ‡ãƒ«ã‚’æ­£å¸¸ã«è¿½åŠ ã™ã‚‹ãŸã‚ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ãã®è¨­å®šã€[`PreTrainedModel`]ã€ãŠã‚ˆã³[`PretrainedConfig`]ã®ç›¸äº’ä½œç”¨ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚\n+ãƒ¢ãƒ‡ãƒ«ã‚’æ­£å¸¸ã«è¿½åŠ ã™ã‚‹ãŸã‚ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ãã®è¨­å®šã€[`PreTrainedModel`]ã€ãŠã‚ˆã³[`PreTrainedConfig`]ã®ç›¸äº’ä½œç”¨ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚\n ä¾‹ç¤ºçš„ãªç›®çš„ã§ã€ğŸ¤— Transformersã«è¿½åŠ ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ã€ŒBrandNewBertã€ã¨å‘¼ã³ã¾ã™ã€‚\n \n ä»¥ä¸‹ã‚’ã”è¦§ãã ã•ã„ï¼š\n@@ -77,7 +77,7 @@ model = BrandNewBertModel.from_pretrained(\"brandy/brand_new_bert\")\n model.config  # model has access to its config\n ```\n \n-ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã«ã€è¨­å®šã¯[`PretrainedConfig`]ã‹ã‚‰åŸºæœ¬çš„ãªã‚·ãƒªã‚¢ãƒ«åŒ–ãŠã‚ˆã³é€†ã‚·ãƒªã‚¢ãƒ«åŒ–ã®æ©Ÿèƒ½ã‚’ç¶™æ‰¿ã—ã¦ã„ã¾ã™ã€‚æ³¨æ„ã™ã¹ãã¯ã€è¨­å®šã¨ãƒ¢ãƒ‡ãƒ«ã¯å¸¸ã«2ã¤ã®ç•°ãªã‚‹å½¢å¼ã«ã‚·ãƒªã‚¢ãƒ«åŒ–ã•ã‚Œã‚‹ã“ã¨ã§ã™ - ãƒ¢ãƒ‡ãƒ«ã¯*pytorch_model.bin*ãƒ•ã‚¡ã‚¤ãƒ«ã«ã€è¨­å®šã¯*config.json*ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚·ãƒªã‚¢ãƒ«åŒ–ã•ã‚Œã¾ã™ã€‚[`~PreTrainedModel.save_pretrained`]ã‚’å‘¼ã³å‡ºã™ã¨ã€è‡ªå‹•çš„ã«[`~PretrainedConfig.save_pretrained`]ã‚‚å‘¼ã³å‡ºã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã¨è¨­å®šã®ä¸¡æ–¹ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚\n+ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã«ã€è¨­å®šã¯[`PreTrainedConfig`]ã‹ã‚‰åŸºæœ¬çš„ãªã‚·ãƒªã‚¢ãƒ«åŒ–ãŠã‚ˆã³é€†ã‚·ãƒªã‚¢ãƒ«åŒ–ã®æ©Ÿèƒ½ã‚’ç¶™æ‰¿ã—ã¦ã„ã¾ã™ã€‚æ³¨æ„ã™ã¹ãã¯ã€è¨­å®šã¨ãƒ¢ãƒ‡ãƒ«ã¯å¸¸ã«2ã¤ã®ç•°ãªã‚‹å½¢å¼ã«ã‚·ãƒªã‚¢ãƒ«åŒ–ã•ã‚Œã‚‹ã“ã¨ã§ã™ - ãƒ¢ãƒ‡ãƒ«ã¯*pytorch_model.bin*ãƒ•ã‚¡ã‚¤ãƒ«ã«ã€è¨­å®šã¯*config.json*ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚·ãƒªã‚¢ãƒ«åŒ–ã•ã‚Œã¾ã™ã€‚[`~PreTrainedModel.save_pretrained`]ã‚’å‘¼ã³å‡ºã™ã¨ã€è‡ªå‹•çš„ã«[`~PreTrainedConfig.save_pretrained`]ã‚‚å‘¼ã³å‡ºã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã¨è¨­å®šã®ä¸¡æ–¹ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚\n \n ### Code style\n "
        },
        {
            "sha": "98175dff7baf061af2de3a7e87ac4aab81231b94",
            "filename": "docs/source/ja/create_a_model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fcreate_a_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -86,19 +86,19 @@ DistilBertConfig {\n }\n ```\n \n-äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å±æ€§ã¯ã€[`~PretrainedConfig.from_pretrained`] é–¢æ•°ã§å¤‰æ›´ã§ãã¾ã™ï¼š\n+äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å±æ€§ã¯ã€[`~PreTrainedConfig.from_pretrained`] é–¢æ•°ã§å¤‰æ›´ã§ãã¾ã™ï¼š\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n ```\n \n-Once you are satisfied with your model configuration, you can save it with [`PretrainedConfig.save_pretrained`]. Your configuration file is stored as a JSON file in the specified save directory.\n+Once you are satisfied with your model configuration, you can save it with [`PreTrainedConfig.save_pretrained`]. Your configuration file is stored as a JSON file in the specified save directory.\n \n ```py\n >>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n ```\n \n-è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†åˆ©ç”¨ã™ã‚‹ã«ã¯ã€[`~PretrainedConfig.from_pretrained`]ã‚’ä½¿ç”¨ã—ã¦ãã‚Œã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼š\n+è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†åˆ©ç”¨ã™ã‚‹ã«ã¯ã€[`~PreTrainedConfig.from_pretrained`]ã‚’ä½¿ç”¨ã—ã¦ãã‚Œã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼š\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")"
        },
        {
            "sha": "8b69500e23148af602d0e8992c8bdae485a1b654",
            "filename": "docs/source/ja/custom_models.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fcustom_models.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -29,11 +29,11 @@ rendered properly in your Markdown viewer.\n ã“ã®ä¾‹ã§ã¯ã€ResNetã‚¯ãƒ©ã‚¹ã®ã„ãã¤ã‹ã®å¼•æ•°ã‚’å–å¾—ã—ã€èª¿æ•´ã—ãŸã„ã‹ã‚‚ã—ã‚Œãªã„ã¨ã—ã¾ã™ã€‚ç•°ãªã‚‹è¨­å®šã¯ã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®ResNetã‚’æä¾›ã—ã¾ã™ã€‚ãã®å¾Œã€ã“ã‚Œã‚‰ã®å¼•æ•°ã‚’ç¢ºèªã—ãŸå¾Œã€ãã‚Œã‚‰ã®å¼•æ•°ã‚’å˜ã«æ ¼ç´ã—ã¾ã™ã€‚\n \n ```python\n-from transformers import PretrainedConfig\n+from transformers import PreTrainedConfig\n from typing import List\n \n \n-class ResnetConfig(PretrainedConfig):\n+class ResnetConfig(PreTrainedConfig):\n     model_type = \"resnet\"\n \n     def __init__(\n@@ -67,12 +67,12 @@ class ResnetConfig(PretrainedConfig):\n ```\n \n é‡è¦ãªã“ã¨ã‚’3ã¤è¦šãˆã¦ãŠãã¹ããƒã‚¤ãƒ³ãƒˆã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ï¼š\n-- `PretrainedConfig` ã‚’ç¶™æ‰¿ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n-- ã‚ãªãŸã® `PretrainedConfig` ã® `__init__` ã¯ä»»æ„ã® kwargs ã‚’å—ã‘å…¥ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n+- `PreTrainedConfig` ã‚’ç¶™æ‰¿ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n+- ã‚ãªãŸã® `PreTrainedConfig` ã® `__init__` ã¯ä»»æ„ã® kwargs ã‚’å—ã‘å…¥ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n - ã“ã‚Œã‚‰ã® `kwargs` ã¯è¦ªã‚¯ãƒ©ã‚¹ã® `__init__` ã«æ¸¡ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n \n ç¶™æ‰¿ã¯ã€ğŸ¤— Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã™ã¹ã¦ã®æ©Ÿèƒ½ã‚’å–å¾—ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã§ã™ã€‚ä»–ã®2ã¤ã®åˆ¶ç´„ã¯ã€\n-`PretrainedConfig` ãŒè¨­å®šã—ã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä»¥å¤–ã«ã‚‚å¤šãã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰æ¥ã¦ã„ã¾ã™ã€‚\n+`PreTrainedConfig` ãŒè¨­å®šã—ã¦ã„ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä»¥å¤–ã«ã‚‚å¤šãã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰æ¥ã¦ã„ã¾ã™ã€‚\n `from_pretrained` ãƒ¡ã‚½ãƒƒãƒ‰ã§è¨­å®šã‚’å†ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã€ã“ã‚Œã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ã‚ãªãŸã®è¨­å®šã«å—ã‘å…¥ã‚Œã‚‰ã‚Œã€\n ãã®å¾Œã€è¦ªã‚¯ãƒ©ã‚¹ã«é€ä¿¡ã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n \n@@ -95,7 +95,7 @@ resnet50d_config.save_pretrained(\"custom-resnet\")\n resnet50d_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n ```\n \n-ã¾ãŸã€[`PretrainedConfig`] ã‚¯ãƒ©ã‚¹ã®ä»–ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€[`~PretrainedConfig.push_to_hub`] ã‚’ä½¿ç”¨ã—ã¦ã€è¨­å®šã‚’ç›´æ¥ Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n+ã¾ãŸã€[`PreTrainedConfig`] ã‚¯ãƒ©ã‚¹ã®ä»–ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€[`~PreTrainedConfig.push_to_hub`] ã‚’ä½¿ç”¨ã—ã¦ã€è¨­å®šã‚’ç›´æ¥ Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n \n ## Writing a custom model\n "
        },
        {
            "sha": "a9290ac4e980f4ab0298f8260ac4d0d1984bd6e6",
            "filename": "docs/source/ja/main_classes/configuration.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fmain_classes%2Fconfiguration.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fmain_classes%2Fconfiguration.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fconfiguration.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,16 +16,16 @@ rendered properly in your Markdown viewer.\n \n ï¼ƒ æ§‹æˆ\n \n-åŸºæœ¬ã‚¯ãƒ©ã‚¹ [`PretrainedConfig`] ã¯ã€è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰/ä¿å­˜ã™ã‚‹ãŸã‚ã®ä¸€èˆ¬çš„ãªãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã¾ã™ã€‚\n+åŸºæœ¬ã‚¯ãƒ©ã‚¹ [`PreTrainedConfig`] ã¯ã€è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰/ä¿å­˜ã™ã‚‹ãŸã‚ã®ä¸€èˆ¬çš„ãªãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã—ã¾ã™ã€‚\n ãƒ­ãƒ¼ã‚«ãƒ« ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã€ã¾ãŸã¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸ) ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã‚‹äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ§‹æˆã‹ã‚‰\n HuggingFace ã® AWS S3 ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰)ã€‚\n \n å„æ´¾ç”Ÿæ§‹æˆã‚¯ãƒ©ã‚¹ã¯ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®å±æ€§ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ã™ã¹ã¦ã®æ§‹æˆã‚¯ãƒ©ã‚¹ã«å­˜åœ¨ã™ã‚‹å…±é€šã®å±æ€§ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚\n `hidden_â€‹â€‹size`ã€`num_attention_heads`ã€ãŠã‚ˆã³ `num_hidden_â€‹â€‹layers`ã€‚ãƒ†ã‚­ã‚¹ãƒˆ ãƒ¢ãƒ‡ãƒ«ã¯ã•ã‚‰ã«ä»¥ä¸‹ã‚’å®Ÿè£…ã—ã¾ã™ã€‚\n `vocab_size`ã€‚\n \n-## PretrainedConfig\n+## PreTrainedConfig\n \n-[[autodoc]] PretrainedConfig\n+[[autodoc]] PreTrainedConfig\n     - push_to_hub\n     - all"
        },
        {
            "sha": "47453166d372a1a0452a0a104aeac5fb0bae5368",
            "filename": "docs/source/ja/model_doc/auto.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fmodel_doc%2Fauto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fja%2Fmodel_doc%2Fauto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fauto.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -43,7 +43,7 @@ AutoModel.register(NewModelConfig, NewModel)\n \n <Tip warning={true}>\n \n-ã‚ãªãŸã®`NewModelConfig`ãŒ[`~transformers.PretrainedConfig`]ã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ã‚‹å ´åˆã€ãã®`model_type`å±æ€§ãŒã‚³ãƒ³ãƒ•ã‚£ã‚°ã‚’ç™»éŒ²ã™ã‚‹ã¨ãã«ä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼ï¼ˆã“ã“ã§ã¯`\"new-model\"`ï¼‰ã¨åŒã˜ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n+ã‚ãªãŸã®`NewModelConfig`ãŒ[`~transformers.PreTrainedConfig`]ã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ã‚‹å ´åˆã€ãã®`model_type`å±æ€§ãŒã‚³ãƒ³ãƒ•ã‚£ã‚°ã‚’ç™»éŒ²ã™ã‚‹ã¨ãã«ä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼ï¼ˆã“ã“ã§ã¯`\"new-model\"`ï¼‰ã¨åŒã˜ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n \n åŒæ§˜ã«ã€ã‚ãªãŸã®`NewModel`ãŒ[`PreTrainedModel`]ã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ã‚‹å ´åˆã€ãã®`config_class`å±æ€§ãŒãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ã™ã‚‹éš›ã«ä½¿ç”¨ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆã“ã“ã§ã¯`NewModelConfig`ï¼‰ã¨åŒã˜ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n "
        },
        {
            "sha": "a75032c000d0d646087bac319ad7cae2eb49807f",
            "filename": "docs/source/ko/add_new_model.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fko%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fko%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fadd_new_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -46,7 +46,7 @@ Hugging Face íŒ€ì€ í•­ìƒ ë„ì›€ì„ ì¤„ ì¤€ë¹„ê°€ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í˜¼ìê°€\n \n ### ëª¨ë¸ ê°œìš” [[overview-of-models]]\n \n-ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€í•˜ë ¤ë©´ ëª¨ë¸ê³¼ í•´ë‹¹ êµ¬ì„±ì¸ [`PreTrainedModel`] ë° [`PretrainedConfig`] ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ğŸ¤— Transformersì— ì¶”ê°€í•˜ë ¤ëŠ” ëª¨ë¸ì„ `BrandNewBert`ë¼ê³  ë¶€ë¥´ê² ìŠµë‹ˆë‹¤.\n+ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€í•˜ë ¤ë©´ ëª¨ë¸ê³¼ í•´ë‹¹ êµ¬ì„±ì¸ [`PreTrainedModel`] ë° [`PreTrainedConfig`] ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ğŸ¤— Transformersì— ì¶”ê°€í•˜ë ¤ëŠ” ëª¨ë¸ì„ `BrandNewBert`ë¼ê³  ë¶€ë¥´ê² ìŠµë‹ˆë‹¤.\n \n ë‹¤ìŒì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:\n \n@@ -59,7 +59,7 @@ model = BrandNewBertModel.from_pretrained(\"brandy/brand_new_bert\")\n model.config  # model has access to its config\n ```\n \n-ëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ êµ¬ì„±ì€ [`PretrainedConfig`]ì—ì„œ ê¸°ë³¸ ì§ë ¬í™” ë° ì—­ì§ë ¬í™” ê¸°ëŠ¥ì„ ìƒì†ë°›ìŠµë‹ˆë‹¤. êµ¬ì„±ê³¼ ëª¨ë¸ì€ í•­ìƒ *pytorch_model.bin* íŒŒì¼ê³¼ *config.json* íŒŒì¼ë¡œ ê°ê° ë³„ë„ë¡œ ì§ë ¬í™”ë©ë‹ˆë‹¤. [`~PreTrainedModel.save_pretrained`]ë¥¼ í˜¸ì¶œí•˜ë©´ ìë™ìœ¼ë¡œ [`~PretrainedConfig.save_pretrained`]ë„ í˜¸ì¶œë˜ë¯€ë¡œ ëª¨ë¸ê³¼ êµ¬ì„±ì´ ëª¨ë‘ ì €ì¥ë©ë‹ˆë‹¤.\n+ëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ êµ¬ì„±ì€ [`PreTrainedConfig`]ì—ì„œ ê¸°ë³¸ ì§ë ¬í™” ë° ì—­ì§ë ¬í™” ê¸°ëŠ¥ì„ ìƒì†ë°›ìŠµë‹ˆë‹¤. êµ¬ì„±ê³¼ ëª¨ë¸ì€ í•­ìƒ *pytorch_model.bin* íŒŒì¼ê³¼ *config.json* íŒŒì¼ë¡œ ê°ê° ë³„ë„ë¡œ ì§ë ¬í™”ë©ë‹ˆë‹¤. [`~PreTrainedModel.save_pretrained`]ë¥¼ í˜¸ì¶œí•˜ë©´ ìë™ìœ¼ë¡œ [`~PreTrainedConfig.save_pretrained`]ë„ í˜¸ì¶œë˜ë¯€ë¡œ ëª¨ë¸ê³¼ êµ¬ì„±ì´ ëª¨ë‘ ì €ì¥ë©ë‹ˆë‹¤.\n \n \n ### ì½”ë“œ ìŠ¤íƒ€ì¼ [[code-style]]"
        },
        {
            "sha": "3108788a38ee5f997cc3f1ad6fcf1f896635d4ce",
            "filename": "docs/source/ko/custom_models.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fko%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fko%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcustom_models.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -36,11 +36,11 @@ rendered properly in your Markdown viewer.\n ê·¸ëŸ° ë‹¤ìŒ ëª‡ ê°€ì§€ ìœ íš¨ì„±ì„ í™•ì¸í•œ í›„ í•´ë‹¹ ì¸ìˆ˜ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n \n ```python\n-from transformers import PretrainedConfig\n+from transformers import PreTrainedConfig\n from typing import List\n \n \n-class ResnetConfig(PretrainedConfig):\n+class ResnetConfig(PreTrainedConfig):\n     model_type = \"resnet\"\n \n     def __init__(\n@@ -74,12 +74,12 @@ class ResnetConfig(PretrainedConfig):\n ```\n \n ì‚¬ìš©ì ì •ì˜ `configuration`ì„ ì‘ì„±í•  ë•Œ ê¸°ì–µí•´ì•¼ í•  ì„¸ ê°€ì§€ ì¤‘ìš”í•œ ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n-- `PretrainedConfig`ì„ ìƒì†í•´ì•¼ í•©ë‹ˆë‹¤.\n-- `PretrainedConfig`ì˜ `__init__`ì€ ëª¨ë“  kwargsë¥¼ í—ˆìš©í•´ì•¼ í•˜ê³ ,\n+- `PreTrainedConfig`ì„ ìƒì†í•´ì•¼ í•©ë‹ˆë‹¤.\n+- `PreTrainedConfig`ì˜ `__init__`ì€ ëª¨ë“  kwargsë¥¼ í—ˆìš©í•´ì•¼ í•˜ê³ ,\n - ì´ëŸ¬í•œ `kwargs`ëŠ” ìƒìœ„ í´ë˜ìŠ¤ `__init__`ì— ì „ë‹¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n \n ìƒì†ì€ ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ëª¨ë“  ê¸°ëŠ¥ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\n-ì´ëŸ¬í•œ ì ìœ¼ë¡œë¶€í„° ë¹„ë¡¯ë˜ëŠ” ë‘ ê°€ì§€ ì œì•½ ì¡°ê±´ì€ `PretrainedConfig`ì— ì„¤ì •í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ë§ì€ í•„ë“œê°€ ìˆìŠµë‹ˆë‹¤.\n+ì´ëŸ¬í•œ ì ìœ¼ë¡œë¶€í„° ë¹„ë¡¯ë˜ëŠ” ë‘ ê°€ì§€ ì œì•½ ì¡°ê±´ì€ `PreTrainedConfig`ì— ì„¤ì •í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ë§ì€ í•„ë“œê°€ ìˆìŠµë‹ˆë‹¤.\n `from_pretrained` ë©”ì„œë“œë¡œ êµ¬ì„±ì„ ë‹¤ì‹œ ë¡œë“œí•  ë•Œ í•´ë‹¹ í•„ë“œëŠ” êµ¬ì„±ì—ì„œ ìˆ˜ë½í•œ í›„ ìƒìœ„ í´ë˜ìŠ¤ë¡œ ë³´ë‚´ì•¼ í•©ë‹ˆë‹¤.\n \n ëª¨ë¸ì„ auto í´ë˜ìŠ¤ì— ë“±ë¡í•˜ì§€ ì•ŠëŠ” í•œ, `configuration`ì—ì„œ `model_type`ì„ ì •ì˜(ì—¬ê¸°ì„œ `model_type=\"resnet\"`)í•˜ëŠ” ê²ƒì€ í•„ìˆ˜ ì‚¬í•­ì´ ì•„ë‹™ë‹ˆë‹¤ (ë§ˆì§€ë§‰ ì„¹ì…˜ ì°¸ì¡°).\n@@ -99,7 +99,7 @@ resnet50d_config.save_pretrained(\"custom-resnet\")\n resnet50d_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n ```\n \n-êµ¬ì„±ì„ Hubì— ì§ì ‘ ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ [`PretrainedConfig`] í´ë˜ìŠ¤ì˜ [`~PretrainedConfig.push_to_hub`]ì™€ ê°™ì€ ë‹¤ë¥¸ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+êµ¬ì„±ì„ Hubì— ì§ì ‘ ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ [`PreTrainedConfig`] í´ë˜ìŠ¤ì˜ [`~PreTrainedConfig.push_to_hub`]ì™€ ê°™ì€ ë‹¤ë¥¸ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n \n \n ## ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ ì‘ì„±í•˜ê¸°[[writing-a-custom-model]]"
        },
        {
            "sha": "868dd2219ca908b32f3cf6045b6c3ed9614de5ec",
            "filename": "docs/source/ko/main_classes/configuration.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fko%2Fmain_classes%2Fconfiguration.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fko%2Fmain_classes%2Fconfiguration.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Fconfiguration.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,13 +16,13 @@ rendered properly in your Markdown viewer.\n \n # êµ¬ì„±[[configuration]]\n \n-ê¸°ë³¸ í´ë˜ìŠ¤ [`PretrainedConfig`]ëŠ” ë¡œì»¬ íŒŒì¼ì´ë‚˜ ë””ë ‰í† ë¦¬, ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ êµ¬ì„±(HuggingFaceì˜ AWS S3 ì €ì¥ì†Œì—ì„œ ë‹¤ìš´ë¡œë“œë¨)ìœ¼ë¡œë¶€í„° êµ¬ì„±ì„ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜ ì €ì¥í•˜ëŠ” ê³µí†µ ë©”ì„œë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ê° íŒŒìƒ êµ¬ì„± í´ë˜ìŠ¤ëŠ” ëª¨ë¸ë³„ íŠ¹ì„±ì„ êµ¬í˜„í•©ë‹ˆë‹¤. \n+ê¸°ë³¸ í´ë˜ìŠ¤ [`PreTrainedConfig`]ëŠ” ë¡œì»¬ íŒŒì¼ì´ë‚˜ ë””ë ‰í† ë¦¬, ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ êµ¬ì„±(HuggingFaceì˜ AWS S3 ì €ì¥ì†Œì—ì„œ ë‹¤ìš´ë¡œë“œë¨)ìœ¼ë¡œë¶€í„° êµ¬ì„±ì„ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜ ì €ì¥í•˜ëŠ” ê³µí†µ ë©”ì„œë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ê° íŒŒìƒ êµ¬ì„± í´ë˜ìŠ¤ëŠ” ëª¨ë¸ë³„ íŠ¹ì„±ì„ êµ¬í˜„í•©ë‹ˆë‹¤. \n \n ëª¨ë“  êµ¬ì„± í´ë˜ìŠ¤ì— ì¡´ì¬í•˜ëŠ” ê³µí†µ ì†ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: `hidden_size`, `num_attention_heads`, `num_hidden_layers`. í…ìŠ¤íŠ¸ ëª¨ë¸ì€ ì¶”ê°€ë¡œ `vocab_size`ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n \n \n-## PretrainedConfig[[transformers.PretrainedConfig]]\n+## PreTrainedConfig[[transformers.PreTrainedConfig]]\n \n-[[autodoc]] PretrainedConfig\n+[[autodoc]] PreTrainedConfig\n     - push_to_hub\n     - all"
        },
        {
            "sha": "130d1e5eea44cdf4b3290f318caf248ff7e089f3",
            "filename": "docs/source/ko/model_doc/auto.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fko%2Fmodel_doc%2Fauto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fko%2Fmodel_doc%2Fauto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fauto.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -44,7 +44,7 @@ AutoModel.register(NewModelConfig, NewModel)\n \n <Tip warning={true}>\n \n-ë§Œì•½ `NewModelConfig`ê°€ [`~transformers.PretrainedConfig`]ì˜ ì„œë¸Œí´ë˜ìŠ¤ë¼ë©´, í•´ë‹¹ `model_type` ì†ì„±ì´ ë“±ë¡í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í‚¤(ì—¬ê¸°ì„œëŠ” `\"new-model\"`)ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n+ë§Œì•½ `NewModelConfig`ê°€ [`~transformers.PreTrainedConfig`]ì˜ ì„œë¸Œí´ë˜ìŠ¤ë¼ë©´, í•´ë‹¹ `model_type` ì†ì„±ì´ ë“±ë¡í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í‚¤(ì—¬ê¸°ì„œëŠ” `\"new-model\"`)ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n \n ë§ˆì°¬ê°€ì§€ë¡œ, `NewModel`ì´ [`PreTrainedModel`]ì˜ ì„œë¸Œí´ë˜ìŠ¤ë¼ë©´, í•´ë‹¹ `config_class` ì†ì„±ì´ ë“±ë¡í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤(ì—¬ê¸°ì„œëŠ” `NewModelConfig`)ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n "
        },
        {
            "sha": "1d25c98efb6c243de3506a5df74ceb43c87dd874",
            "filename": "docs/source/pt/create_a_model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fpt%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fpt%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fcreate_a_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -83,19 +83,19 @@ DistilBertConfig {\n }\n ```\n \n-Atributos de um modelo prÃ©-treinado podem ser modificados na funÃ§Ã£o [`~PretrainedConfig.from_pretrained`]:\n+Atributos de um modelo prÃ©-treinado podem ser modificados na funÃ§Ã£o [`~PreTrainedConfig.from_pretrained`]:\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n ```\n \n-Uma vez que vocÃª estÃ¡ satisfeito com as configuraÃ§Ãµes do seu modelo, vocÃª consegue salvar elas com [`~PretrainedConfig.save_pretrained`]. Seu arquivo de configuraÃ§Ãµes estÃ¡ salvo como um arquivo JSON no diretÃ³rio especificado:\n+Uma vez que vocÃª estÃ¡ satisfeito com as configuraÃ§Ãµes do seu modelo, vocÃª consegue salvar elas com [`~PreTrainedConfig.save_pretrained`]. Seu arquivo de configuraÃ§Ãµes estÃ¡ salvo como um arquivo JSON no diretÃ³rio especificado:\n \n ```py\n >>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n ```\n \n-Para reusar o arquivo de configuraÃ§Ãµes, carregue com [`~PretrainedConfig.from_pretrained`]:\n+Para reusar o arquivo de configuraÃ§Ãµes, carregue com [`~PreTrainedConfig.from_pretrained`]:\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")"
        },
        {
            "sha": "1e619736771176e3e012f66bc2d5b2e966476fda",
            "filename": "docs/source/pt/custom_models.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fpt%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fpt%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fcustom_models.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -37,11 +37,11 @@ configuraÃ§Ãµes nos darÃ¡ os diferentes tipos de ResNets que sÃ£o possÃ­veis. Em\n apÃ³s verificar a validade de alguns deles.\n \n ```python\n-from transformers import PretrainedConfig\n+from transformers import PreTrainedConfig\n from typing import List\n \n \n-class ResnetConfig(PretrainedConfig):\n+class ResnetConfig(PreTrainedConfig):\n     model_type = \"resnet\"\n \n     def __init__(\n@@ -75,12 +75,12 @@ class ResnetConfig(PretrainedConfig):\n ```\n \n As trÃªs coisas importantes a serem lembradas ao escrever sua prÃ³pria configuraÃ§Ã£o sÃ£o:\n-- vocÃª tem que herdar de `PretrainedConfig`,\n-- o `__init__` do seu `PretrainedConfig` deve aceitar quaisquer kwargs,\n+- vocÃª tem que herdar de `PreTrainedConfig`,\n+- o `__init__` do seu `PreTrainedConfig` deve aceitar quaisquer kwargs,\n - esses `kwargs` precisam ser passados para a superclasse `__init__`.\n \n A heranÃ§a Ã© para garantir que vocÃª obtenha todas as funcionalidades da biblioteca ğŸ¤— Transformers, enquanto as outras duas\n-restriÃ§Ãµes vÃªm do fato de um `PretrainedConfig` ter mais campos do que os que vocÃª estÃ¡ configurando. Ao recarregar um\n+restriÃ§Ãµes vÃªm do fato de um `PreTrainedConfig` ter mais campos do que os que vocÃª estÃ¡ configurando. Ao recarregar um\n config com o mÃ©todo `from_pretrained`, esses campos precisam ser aceitos pelo seu config e entÃ£o enviados para a\n superclasse.\n \n@@ -102,7 +102,7 @@ mÃ©todo `from_pretrained`:\n resnet50d_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n ```\n \n-VocÃª tambÃ©m pode usar qualquer outro mÃ©todo da classe [`PretrainedConfig`], como [`~PretrainedConfig.push_to_hub`] para\n+VocÃª tambÃ©m pode usar qualquer outro mÃ©todo da classe [`PreTrainedConfig`], como [`~PreTrainedConfig.push_to_hub`] para\n carregar diretamente sua configuraÃ§Ã£o para o Hub.\n \n ## Escrevendo um modelo customizado"
        },
        {
            "sha": "a072f1ffc65e519424c0829d46623a5e7e01baa4",
            "filename": "docs/source/zh/create_a_model.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fzh%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fzh%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fcreate_a_model.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -84,19 +84,19 @@ DistilBertConfig {\n }\n ```\n \n-é¢„è®­ç»ƒæ¨¡å‹çš„å±æ€§å¯ä»¥åœ¨ [`~PretrainedConfig.from_pretrained`] å‡½æ•°ä¸­è¿›è¡Œä¿®æ”¹ï¼š\n+é¢„è®­ç»ƒæ¨¡å‹çš„å±æ€§å¯ä»¥åœ¨ [`~PreTrainedConfig.from_pretrained`] å‡½æ•°ä¸­è¿›è¡Œä¿®æ”¹ï¼š\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n ```\n \n-å½“ä½ å¯¹æ¨¡å‹é…ç½®æ»¡æ„æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ [`~PretrainedConfig.save_pretrained`] æ¥ä¿å­˜é…ç½®ã€‚ä½ çš„é…ç½®æ–‡ä»¶å°†ä»¥ JSON æ–‡ä»¶çš„å½¢å¼å­˜å‚¨åœ¨æŒ‡å®šçš„ä¿å­˜ç›®å½•ä¸­ï¼š\n+å½“ä½ å¯¹æ¨¡å‹é…ç½®æ»¡æ„æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ [`~PreTrainedConfig.save_pretrained`] æ¥ä¿å­˜é…ç½®ã€‚ä½ çš„é…ç½®æ–‡ä»¶å°†ä»¥ JSON æ–‡ä»¶çš„å½¢å¼å­˜å‚¨åœ¨æŒ‡å®šçš„ä¿å­˜ç›®å½•ä¸­ï¼š\n \n ```py\n >>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n ```\n \n-è¦é‡ç”¨é…ç½®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ [`~PretrainedConfig.from_pretrained`] è¿›è¡ŒåŠ è½½ï¼š\n+è¦é‡ç”¨é…ç½®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ [`~PreTrainedConfig.from_pretrained`] è¿›è¡ŒåŠ è½½ï¼š\n \n ```py\n >>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")"
        },
        {
            "sha": "0b1349262097c9f3506acc35d6952ce94c1d73dd",
            "filename": "docs/source/zh/custom_models.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fzh%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fzh%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fcustom_models.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -29,11 +29,11 @@ rendered properly in your Markdown viewer.\n æˆ‘ä»¬å°†é‡‡ç”¨ä¸€äº›æˆ‘ä»¬å¯èƒ½æƒ³è¦è°ƒæ•´çš„ ResNet ç±»çš„å‚æ•°ä¸¾ä¾‹ã€‚ä¸åŒçš„é…ç½®å°†ä¸ºæˆ‘ä»¬æä¾›ä¸åŒç±»å‹å¯èƒ½çš„ ResNet æ¨¡å‹ã€‚åœ¨ç¡®è®¤å…¶ä¸­ä¸€äº›å‚æ•°çš„æœ‰æ•ˆæ€§åï¼Œæˆ‘ä»¬åªéœ€å­˜å‚¨è¿™äº›å‚æ•°ã€‚\n \n ```python\n-from transformers import PretrainedConfig\n+from transformers import PreTrainedConfig\n from typing import List\n \n \n-class ResnetConfig(PretrainedConfig):\n+class ResnetConfig(PreTrainedConfig):\n     model_type = \"resnet\"\n \n     def __init__(\n@@ -67,11 +67,11 @@ class ResnetConfig(PretrainedConfig):\n ```\n \n ç¼–å†™è‡ªå®šä¹‰é…ç½®æ—¶éœ€è¦è®°ä½çš„ä¸‰ä¸ªé‡è¦äº‹é¡¹å¦‚ä¸‹ï¼š\n-- å¿…é¡»ç»§æ‰¿è‡ª `PretrainedConfig`ï¼Œ\n-- `PretrainedConfig` çš„ `__init__` æ–¹æ³•å¿…é¡»æ¥å—ä»»ä½• kwargsï¼Œ\n+- å¿…é¡»ç»§æ‰¿è‡ª `PreTrainedConfig`ï¼Œ\n+- `PreTrainedConfig` çš„ `__init__` æ–¹æ³•å¿…é¡»æ¥å—ä»»ä½• kwargsï¼Œ\n - è¿™äº› `kwargs` éœ€è¦ä¼ é€’ç»™è¶…ç±»çš„ `__init__` æ–¹æ³•ã€‚\n \n-ç»§æ‰¿æ˜¯ä¸ºäº†ç¡®ä¿ä½ è·å¾—æ¥è‡ª ğŸ¤— Transformers åº“çš„æ‰€æœ‰åŠŸèƒ½ï¼Œè€Œå¦å¤–ä¸¤ä¸ªçº¦æŸæºäº `PretrainedConfig` çš„å­—æ®µæ¯”ä½ è®¾ç½®çš„å­—æ®µå¤šã€‚åœ¨ä½¿ç”¨ `from_pretrained` æ–¹æ³•é‡æ–°åŠ è½½é…ç½®æ—¶ï¼Œè¿™äº›å­—æ®µéœ€è¦è¢«ä½ çš„é…ç½®æ¥å—ï¼Œç„¶åä¼ é€’ç»™è¶…ç±»ã€‚\n+ç»§æ‰¿æ˜¯ä¸ºäº†ç¡®ä¿ä½ è·å¾—æ¥è‡ª ğŸ¤— Transformers åº“çš„æ‰€æœ‰åŠŸèƒ½ï¼Œè€Œå¦å¤–ä¸¤ä¸ªçº¦æŸæºäº `PreTrainedConfig` çš„å­—æ®µæ¯”ä½ è®¾ç½®çš„å­—æ®µå¤šã€‚åœ¨ä½¿ç”¨ `from_pretrained` æ–¹æ³•é‡æ–°åŠ è½½é…ç½®æ—¶ï¼Œè¿™äº›å­—æ®µéœ€è¦è¢«ä½ çš„é…ç½®æ¥å—ï¼Œç„¶åä¼ é€’ç»™è¶…ç±»ã€‚\n \n ä¸ºä½ çš„é…ç½®å®šä¹‰ `model_type`ï¼ˆæ­¤å¤„ä¸º `model_type=\"resnet\"`ï¼‰ä¸æ˜¯å¿…é¡»çš„ï¼Œé™¤éä½ æƒ³ä½¿ç”¨è‡ªåŠ¨ç±»æ³¨å†Œä½ çš„æ¨¡å‹ï¼ˆè¯·å‚é˜…æœ€åä¸€èŠ‚ï¼‰ã€‚\n \n@@ -88,7 +88,7 @@ resnet50d_config.save_pretrained(\"custom-resnet\")\n resnet50d_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n ```\n \n-ä½ è¿˜å¯ä»¥ä½¿ç”¨ [`PretrainedConfig`] ç±»çš„ä»»ä½•å…¶ä»–æ–¹æ³•ï¼Œä¾‹å¦‚ [`~PretrainedConfig.push_to_hub`]ï¼Œç›´æ¥å°†é…ç½®ä¸Šä¼ åˆ° Hubã€‚\n+ä½ è¿˜å¯ä»¥ä½¿ç”¨ [`PreTrainedConfig`] ç±»çš„ä»»ä½•å…¶ä»–æ–¹æ³•ï¼Œä¾‹å¦‚ [`~PreTrainedConfig.push_to_hub`]ï¼Œç›´æ¥å°†é…ç½®ä¸Šä¼ åˆ° Hubã€‚\n \n ## ç¼–å†™è‡ªå®šä¹‰æ¨¡å‹\n "
        },
        {
            "sha": "dc65efb72267fb463c340cff774fcd305f0f8dcd",
            "filename": "docs/source/zh/main_classes/configuration.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fzh%2Fmain_classes%2Fconfiguration.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/docs%2Fsource%2Fzh%2Fmain_classes%2Fconfiguration.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fconfiguration.md?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,13 +16,13 @@ rendered properly in your Markdown viewer.\n \n # Configuration\n \n-åŸºç±»[`PretrainedConfig`]å®ç°äº†ä»æœ¬åœ°æ–‡ä»¶æˆ–ç›®å½•åŠ è½½/ä¿å­˜é…ç½®çš„å¸¸è§æ–¹æ³•ï¼Œæˆ–ä¸‹è½½åº“æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®ï¼ˆä»HuggingFaceçš„AWS S3åº“ä¸­ä¸‹è½½ï¼‰ã€‚\n+åŸºç±»[`PreTrainedConfig`]å®ç°äº†ä»æœ¬åœ°æ–‡ä»¶æˆ–ç›®å½•åŠ è½½/ä¿å­˜é…ç½®çš„å¸¸è§æ–¹æ³•ï¼Œæˆ–ä¸‹è½½åº“æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®ï¼ˆä»HuggingFaceçš„AWS S3åº“ä¸­ä¸‹è½½ï¼‰ã€‚\n \n æ¯ä¸ªæ´¾ç”Ÿçš„é…ç½®ç±»éƒ½å®ç°äº†ç‰¹å®šäºæ¨¡å‹çš„å±æ€§ã€‚æ‰€æœ‰é…ç½®ç±»ä¸­å…±åŒå­˜åœ¨çš„å±æ€§æœ‰ï¼š`hidden_size`ã€`num_attention_heads` å’Œ `num_hidden_layers`ã€‚æ–‡æœ¬æ¨¡å‹è¿›ä¸€æ­¥æ·»åŠ äº† `vocab_size`ã€‚\n \n \n-## PretrainedConfig\n+## PreTrainedConfig\n \n-[[autodoc]] PretrainedConfig\n+[[autodoc]] PreTrainedConfig\n     - push_to_hub\n     - all"
        },
        {
            "sha": "64d28135943f2f486a9e9980a62d5d57e0ec661e",
            "filename": "examples/legacy/pytorch-lightning/lightning_base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,7 +17,7 @@\n     AutoModelForTokenClassification,\n     AutoModelWithLMHead,\n     AutoTokenizer,\n-    PretrainedConfig,\n+    PreTrainedConfig,\n     PreTrainedTokenizer,\n     is_torch_available,\n )\n@@ -93,7 +93,7 @@ def __init__(\n                 **config_kwargs,\n             )\n         else:\n-            self.config: PretrainedConfig = config\n+            self.config: PreTrainedConfig = config\n \n         extra_model_params = (\"encoder_layerdrop\", \"decoder_layerdrop\", \"dropout\", \"attention_dropout\")\n         for p in extra_model_params:"
        },
        {
            "sha": "534c9fd6616b49c4cfc3d7ef33fd6c9ef50f4e52",
            "filename": "examples/modular-transformers/configuration_duplicated_method.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -5,19 +5,19 @@\n #                          modular_duplicated_method.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class DuplicatedMethodConfig(PretrainedConfig):\n+class DuplicatedMethodConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DuplicatedMethodModel`]. It is used to instantiate an DuplicatedMethod\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the DuplicatedMethod-7B.\n     e.g. [meta-duplicated_method/DuplicatedMethod-2-7b-hf](https://huggingface.co/meta-duplicated_method/DuplicatedMethod-2-7b-hf)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "924e0cccadd728556e06514d082f47d8cfd589d3",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -5,19 +5,19 @@\n #                          modular_my_new_model.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class MyNewModelConfig(PretrainedConfig):\n+class MyNewModelConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`MyNewModelModel`]. It is used to instantiate an MyNewModel\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the MyNewModel-7B.\n     e.g. [meta-my_new_model/MyNewModel-2-7b-hf](https://huggingface.co/meta-my_new_model/MyNewModel-2-7b-hf)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "f8e219f11eb669d2313ce00b3fb8f10c09dee9e5",
            "filename": "examples/modular-transformers/configuration_my_new_model2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -5,18 +5,18 @@\n #                          modular_my_new_model2.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class MyNewModel2Config(PretrainedConfig):\n+class MyNewModel2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GemmaModel`]. It is used to instantiate an Gemma\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma-7B.\n     e.g. [google/gemma-7b](https://huggingface.co/google/gemma-7b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma model. Defines the number of different tokens that can be represented by the"
        },
        {
            "sha": "12bf09a9e34e44e194abdd127df601c3f1c4f2f0",
            "filename": "examples/modular-transformers/configuration_new_model.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_new_model.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -6,17 +6,17 @@\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # Example where we only want to overwrite the defaults of an init\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n \n \n-class NewModelConfig(PretrainedConfig):\n+class NewModelConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`NewModelModel`]. It is used to instantiate an NewModel\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the NewModel-7B.\n     e.g. [google/new_model-7b](https://huggingface.co/google/new_model-7b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the NewModel model. Defines the number of different tokens that can be represented by the"
        },
        {
            "sha": "d6ae897e34f919c44f352bdc487c5eab324b7863",
            "filename": "examples/modular-transformers/modular_my_new_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fmodular_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fmodular_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_my_new_model.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -9,8 +9,8 @@ class MyNewModelConfig(LlamaConfig):\n     defaults will yield a similar configuration to that of the MyNewModel-7B.\n     e.g. [meta-my_new_model/MyNewModel-2-7b-hf](https://huggingface.co/meta-my_new_model/MyNewModel-2-7b-hf)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "45b9427ed9e459698a0b31fe5e0cc239f6e276f0",
            "filename": "examples/modular-transformers/modular_my_new_model2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fmodular_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fmodular-transformers%2Fmodular_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_my_new_model2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -9,8 +9,8 @@ class MyNewModel2Config(LlamaConfig):\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma-7B.\n     e.g. [google/gemma-7b](https://huggingface.co/google/gemma-7b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma model. Defines the number of different tokens that can be represented by the"
        },
        {
            "sha": "866044bdce2590a8db5e4f24123ef265ee1b601a",
            "filename": "examples/pytorch/text-classification/run_glue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -51,7 +51,7 @@\n     DataCollatorWithPadding,\n     EvalPrediction,\n     HfArgumentParser,\n-    PretrainedConfig,\n+    PreTrainedConfig,\n     Trainer,\n     TrainingArguments,\n     default_data_collator,\n@@ -429,7 +429,7 @@ def main():\n     # Some models have set the order of the labels to use, so let's make sure we do use it.\n     label_to_id = None\n     if (\n-        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n+        model.config.label2id != PreTrainedConfig(num_labels=num_labels).label2id\n         and data_args.task_name is not None\n         and not is_regression\n     ):"
        },
        {
            "sha": "072e473dac2c3641ed025d361301c7c68518df68",
            "filename": "examples/pytorch/text-classification/run_glue_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -53,7 +53,7 @@\n     AutoModelForSequenceClassification,\n     AutoTokenizer,\n     DataCollatorWithPadding,\n-    PretrainedConfig,\n+    PreTrainedConfig,\n     SchedulerType,\n     default_data_collator,\n     get_scheduler,\n@@ -367,7 +367,7 @@ def main():\n     # Some models have set the order of the labels to use, so let's make sure we do use it.\n     label_to_id = None\n     if (\n-        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n+        model.config.label2id != PreTrainedConfig(num_labels=num_labels).label2id\n         and args.task_name is not None\n         and not is_regression\n     ):"
        },
        {
            "sha": "1482394321cd6ac8b025f1e8b205f348b4213315",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -48,7 +48,7 @@\n     AutoTokenizer,\n     DataCollatorForTokenClassification,\n     HfArgumentParser,\n-    PretrainedConfig,\n+    PreTrainedConfig,\n     PreTrainedTokenizerFast,\n     Trainer,\n     TrainingArguments,\n@@ -413,7 +413,7 @@ def get_label_list(labels):\n         )\n \n     # Model has labels -> use them.\n-    if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:\n+    if model.config.label2id != PreTrainedConfig(num_labels=num_labels).label2id:\n         if sorted(model.config.label2id.keys()) == sorted(label_list):\n             # Reorganize `label_list` to match the ordering of the model.\n             if labels_are_int:"
        },
        {
            "sha": "138c99b6d0ca4bd9f34f27571747d59f3ec4a74e",
            "filename": "examples/pytorch/token-classification/run_ner_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -57,7 +57,7 @@\n     AutoModelForTokenClassification,\n     AutoTokenizer,\n     DataCollatorForTokenClassification,\n-    PretrainedConfig,\n+    PreTrainedConfig,\n     SchedulerType,\n     default_data_collator,\n     get_scheduler,\n@@ -454,7 +454,7 @@ def get_label_list(labels):\n         model.resize_token_embeddings(len(tokenizer))\n \n     # Model has labels -> use them.\n-    if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id:\n+    if model.config.label2id != PreTrainedConfig(num_labels=num_labels).label2id:\n         if sorted(model.config.label2id.keys()) == sorted(label_list):\n             # Reorganize `label_list` to match the ordering of the model.\n             if labels_are_int:"
        },
        {
            "sha": "553e7ee0e538c30b28b71570c5456a7ec08c14e7",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -59,7 +59,7 @@\n _import_structure = {\n     \"audio_utils\": [],\n     \"commands\": [],\n-    \"configuration_utils\": [\"PretrainedConfig\"],\n+    \"configuration_utils\": [\"PreTrainedConfig\", \"PretrainedConfig\"],\n     \"convert_slow_tokenizers_checkpoints_to_fast\": [],\n     \"data\": [\n         \"DataProcessor\",\n@@ -491,6 +491,7 @@\n     from .cache_utils import StaticCache as StaticCache\n     from .cache_utils import StaticLayer as StaticLayer\n     from .cache_utils import StaticSlidingWindowLayer as StaticSlidingWindowLayer\n+    from .configuration_utils import PreTrainedConfig as PreTrainedConfig\n     from .configuration_utils import PretrainedConfig as PretrainedConfig\n     from .convert_slow_tokenizer import SLOW_TO_FAST_CONVERTERS as SLOW_TO_FAST_CONVERTERS\n     from .convert_slow_tokenizer import convert_slow_tokenizer as convert_slow_tokenizer"
        },
        {
            "sha": "aea93f0e770c4edec0af0e596c31c7435ee50412",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -4,7 +4,7 @@\n \n import torch\n \n-from .configuration_utils import PretrainedConfig\n+from .configuration_utils import PreTrainedConfig\n from .utils import (\n     is_hqq_available,\n     is_quanto_greater,\n@@ -923,7 +923,7 @@ class DynamicCache(Cache):\n             `map(gather_map, zip(*caches))`, i.e. each item in the iterable contains the key and value states\n             for a layer gathered across replicas by torch.distributed (shape=[global batch size, num_heads, seq_len, head_dim]).\n             Note: it needs to be the 1st arg as well to work correctly\n-        config (`PretrainedConfig`, *optional*):\n+        config (`PreTrainedConfig`, *optional*):\n             The config of the model for which this Cache will be used. If passed, it will be used to check for sliding\n             or hybrid layer structure, greatly reducing the memory requirement of the cached tensors to\n             `[batch_size, num_heads, min(seq_len, sliding_window), head_dim]`.\n@@ -953,7 +953,7 @@ class DynamicCache(Cache):\n     def __init__(\n         self,\n         ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.Tensor]]] = None,\n-        config: Optional[PretrainedConfig] = None,\n+        config: Optional[PreTrainedConfig] = None,\n         offloading: bool = False,\n         offload_only_non_sliding: bool = False,\n     ):\n@@ -1036,7 +1036,7 @@ class StaticCache(Cache):\n     See `Cache` for details on common methods that are implemented by all cache classes.\n \n     Args:\n-        config (`PretrainedConfig`):\n+        config (`PreTrainedConfig`):\n             The config of the model for which this Cache will be used. It will be used to check for sliding\n             or hybrid layer structure, and initialize each layer accordingly.\n         max_cache_len (`int`):\n@@ -1070,7 +1070,7 @@ class StaticCache(Cache):\n     # Pass-in kwargs as well to avoid crashing for BC (it used more arguments before)\n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         max_cache_len: int,\n         offloading: bool = False,\n         offload_only_non_sliding: bool = True,\n@@ -1124,7 +1124,7 @@ class QuantizedCache(Cache):\n     Args:\n         backend (`str`):\n             The quantization backend to use. One of `(\"quanto\", \"hqq\").\n-        config (`PretrainedConfig`):\n+        config (`PreTrainedConfig`):\n             The config of the model for which this Cache will be used.\n         nbits (`int`, *optional*, defaults to 4):\n             The number of bits for quantization.\n@@ -1141,7 +1141,7 @@ class QuantizedCache(Cache):\n     def __init__(\n         self,\n         backend: str,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         nbits: int = 4,\n         axis_key: int = 0,\n         axis_value: int = 0,\n@@ -1400,7 +1400,7 @@ def __init__(self) -> None:\n \n \n class OffloadedStaticCache(StaticCache):\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n         logger.warning_once(\n             \"`OffloadedStaticCache` is deprecated and will be removed in version v4.59 \"\n             \"Use `StaticCache(..., offloading=True)` instead\"\n@@ -1409,7 +1409,7 @@ def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs\n \n \n class SlidingWindowCache(StaticCache):\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n         logger.warning_once(\n             \"`SlidingWindowCache` is deprecated and will be removed in version v4.59 \"\n             \"Use `StaticCache(...)` instead which will correctly infer the type of each layer.\"\n@@ -1418,7 +1418,7 @@ def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs\n \n \n class HybridCache(StaticCache):\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n         logger.warning_once(\n             \"`HybridCache` is deprecated and will be removed in version v4.59 \"\n             \"Use `StaticCache(...)` instead which will correctly infer the type of each layer.\"\n@@ -1427,7 +1427,7 @@ def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs\n \n \n class HybridChunkedCache(StaticCache):\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n         logger.warning_once(\n             \"`HybridChunkedCache` is deprecated and will be removed in version v4.59 \"\n             \"Use `StaticCache(...)` instead which will correctly infer the type of each layer.\"\n@@ -1436,7 +1436,7 @@ def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs\n \n \n class OffloadedHybridCache(StaticCache):\n-    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):\n+    def __init__(self, config: PreTrainedConfig, max_cache_len: int, *args, **kwargs):\n         logger.warning_once(\n             \"`OffloadedHybridCache` is deprecated and will be removed in version v4.59 \"\n             \"Use `StaticCache(..., offload=True)` instead which will correctly infer the type of each layer.\"\n@@ -1447,7 +1447,7 @@ def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs\n class QuantoQuantizedCache(QuantizedCache):\n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         nbits: int = 4,\n         axis_key: int = 0,\n         axis_value: int = 0,\n@@ -1464,7 +1464,7 @@ def __init__(\n class HQQQuantizedCache(QuantizedCache):\n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         nbits: int = 4,\n         axis_key: int = 0,\n         axis_value: int = 0,"
        },
        {
            "sha": "de1493d8b4fa114d8d0c9fd722611cd5c80713e5",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 39,
            "deletions": 35,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -46,11 +46,11 @@\n logger = logging.get_logger(__name__)\n \n \n-# type hinting: specifying the type of config class that inherits from PretrainedConfig\n-SpecificPretrainedConfigType = TypeVar(\"SpecificPretrainedConfigType\", bound=\"PretrainedConfig\")\n+# type hinting: specifying the type of config class that inherits from PreTrainedConfig\n+SpecificPreTrainedConfigType = TypeVar(\"SpecificPreTrainedConfigType\", bound=\"PreTrainedConfig\")\n \n \n-class PretrainedConfig(PushToHubMixin):\n+class PreTrainedConfig(PushToHubMixin):\n     # no-format\n     r\"\"\"\n     Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as\n@@ -70,7 +70,7 @@ class PretrainedConfig(PushToHubMixin):\n     - **has_no_defaults_at_init** (`bool`) -- Whether the config class can be initialized without providing input arguments.\n       Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,\n       (but not necessarily) such as [`~transformers.EncoderDecoderConfig`] or [`~RagConfig`]. They have to be initialized from\n-      two or more configs of type [`~transformers.PretrainedConfig`].\n+      two or more configs of type [`~transformers.PreTrainedConfig`].\n     - **keys_to_ignore_at_inference** (`list[str]`) -- A list of keys to ignore by default when looking at dictionary\n       outputs of the model during inference.\n     - **attribute_map** (`dict[str, str]`) -- A dict that maps model specific attribute names to the standardized\n@@ -186,7 +186,7 @@ class PretrainedConfig(PushToHubMixin):\n \n     model_type: str = \"\"\n     base_config_key: str = \"\"\n-    sub_configs: dict[str, type[\"PretrainedConfig\"]] = {}\n+    sub_configs: dict[str, type[\"PreTrainedConfig\"]] = {}\n     has_no_defaults_at_init: bool = False\n     attribute_map: dict[str, str] = {}\n     base_model_tp_plan: Optional[dict[str, Any]] = None\n@@ -432,7 +432,7 @@ def torch_dtype(self, value):\n     def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n         \"\"\"\n         Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n-        [`~PretrainedConfig.from_pretrained`] class method.\n+        [`~PreTrainedConfig.from_pretrained`] class method.\n \n         Args:\n             save_directory (`str` or `os.PathLike`):\n@@ -522,17 +522,17 @@ def _set_token_in_kwargs(kwargs, token=None):\n \n     @classmethod\n     def from_pretrained(\n-        cls: type[SpecificPretrainedConfigType],\n+        cls: type[SpecificPreTrainedConfigType],\n         pretrained_model_name_or_path: Union[str, os.PathLike],\n         cache_dir: Optional[Union[str, os.PathLike]] = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n         token: Optional[Union[str, bool]] = None,\n         revision: str = \"main\",\n         **kwargs,\n-    ) -> SpecificPretrainedConfigType:\n+    ) -> SpecificPreTrainedConfigType:\n         r\"\"\"\n-        Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n+        Instantiate a [`PreTrainedConfig`] (or a derived class) from a pretrained model configuration.\n \n         Args:\n             pretrained_model_name_or_path (`str` or `os.PathLike`):\n@@ -541,7 +541,7 @@ def from_pretrained(\n                 - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n                   huggingface.co.\n                 - a path to a *directory* containing a configuration file saved using the\n-                  [`~PretrainedConfig.save_pretrained`] method, e.g., `./my_model_directory/`.\n+                  [`~PreTrainedConfig.save_pretrained`] method, e.g., `./my_model_directory/`.\n                 - a path or url to a saved configuration JSON *file*, e.g., `./my_model_directory/configuration.json`.\n             cache_dir (`str` or `os.PathLike`, *optional*):\n                 Path to a directory in which a downloaded pretrained model configuration should be cached if the\n@@ -581,12 +581,12 @@ def from_pretrained(\n                 by the `return_unused_kwargs` keyword parameter.\n \n         Returns:\n-            [`PretrainedConfig`]: The configuration object instantiated from this pretrained model.\n+            [`PreTrainedConfig`]: The configuration object instantiated from this pretrained model.\n \n         Examples:\n \n         ```python\n-        # We can't instantiate directly the base class *PretrainedConfig* so let's show the examples on a\n+        # We can't instantiate directly the base class *PreTrainedConfig* so let's show the examples on a\n         # derived class: BertConfig\n         config = BertConfig.from_pretrained(\n             \"google-bert/bert-base-uncased\"\n@@ -636,7 +636,7 @@ def get_config_dict(\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n-        [`PretrainedConfig`] using `from_dict`.\n+        [`PreTrainedConfig`] using `from_dict`.\n \n         Parameters:\n             pretrained_model_name_or_path (`str` or `os.PathLike`):\n@@ -761,20 +761,20 @@ def _get_config_dict(\n \n     @classmethod\n     def from_dict(\n-        cls: type[SpecificPretrainedConfigType], config_dict: dict[str, Any], **kwargs\n-    ) -> SpecificPretrainedConfigType:\n+        cls: type[SpecificPreTrainedConfigType], config_dict: dict[str, Any], **kwargs\n+    ) -> SpecificPreTrainedConfigType:\n         \"\"\"\n-        Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n+        Instantiates a [`PreTrainedConfig`] from a Python dictionary of parameters.\n \n         Args:\n             config_dict (`dict[str, Any]`):\n                 Dictionary that will be used to instantiate the configuration object. Such a dictionary can be\n-                retrieved from a pretrained checkpoint by leveraging the [`~PretrainedConfig.get_config_dict`] method.\n+                retrieved from a pretrained checkpoint by leveraging the [`~PreTrainedConfig.get_config_dict`] method.\n             kwargs (`dict[str, Any]`):\n                 Additional parameters from which to initialize the configuration object.\n \n         Returns:\n-            [`PretrainedConfig`]: The configuration object instantiated from those parameters.\n+            [`PreTrainedConfig`]: The configuration object instantiated from those parameters.\n         \"\"\"\n         return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n         # Those arguments may be passed along for our internal telemetry.\n@@ -815,7 +815,7 @@ def from_dict(\n                 current_attr = getattr(config, key)\n                 # To authorize passing a custom subconfig as kwarg in models that have nested configs.\n                 # We need to update only custom kwarg values instead and keep other attributes in subconfig.\n-                if isinstance(current_attr, PretrainedConfig) and isinstance(value, dict):\n+                if isinstance(current_attr, PreTrainedConfig) and isinstance(value, dict):\n                     current_attr_updated = current_attr.to_dict()\n                     current_attr_updated.update(value)\n                     value = current_attr.__class__(**current_attr_updated)\n@@ -833,17 +833,17 @@ def from_dict(\n \n     @classmethod\n     def from_json_file(\n-        cls: type[SpecificPretrainedConfigType], json_file: Union[str, os.PathLike]\n-    ) -> SpecificPretrainedConfigType:\n+        cls: type[SpecificPreTrainedConfigType], json_file: Union[str, os.PathLike]\n+    ) -> SpecificPreTrainedConfigType:\n         \"\"\"\n-        Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\n+        Instantiates a [`PreTrainedConfig`] from the path to a JSON file of parameters.\n \n         Args:\n             json_file (`str` or `os.PathLike`):\n                 Path to the JSON file containing the parameters.\n \n         Returns:\n-            [`PretrainedConfig`]: The configuration object instantiated from that JSON file.\n+            [`PreTrainedConfig`]: The configuration object instantiated from that JSON file.\n \n         \"\"\"\n         config_dict = cls._dict_from_json_file(json_file)\n@@ -856,7 +856,7 @@ def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n         return json.loads(text)\n \n     def __eq__(self, other):\n-        return isinstance(other, PretrainedConfig) and (self.__dict__ == other.__dict__)\n+        return isinstance(other, PreTrainedConfig) and (self.__dict__ == other.__dict__)\n \n     def __repr__(self):\n         return f\"{self.__class__.__name__} {self.to_json_string()}\"\n@@ -876,7 +876,7 @@ def to_diff_dict(self) -> dict[str, Any]:\n         config_dict = self.to_dict()\n \n         # Get the default config dict (from a fresh PreTrainedConfig instance)\n-        default_config_dict = PretrainedConfig().to_dict()\n+        default_config_dict = PreTrainedConfig().to_dict()\n \n         # get class specific config dict\n         class_config_dict = self.__class__().to_dict() if not self.has_no_defaults_at_init else {}\n@@ -887,7 +887,7 @@ def to_diff_dict(self) -> dict[str, Any]:\n         # except always keep the 'config' attribute.\n         for key, value in config_dict.items():\n             if (\n-                isinstance(getattr(self, key, None), PretrainedConfig)\n+                isinstance(getattr(self, key, None), PreTrainedConfig)\n                 and key in class_config_dict\n                 and isinstance(class_config_dict[key], dict)\n                 or key in self.sub_configs\n@@ -940,7 +940,7 @@ def to_dict(self) -> dict[str, Any]:\n \n         for key, value in output.items():\n             # Deal with nested configs like CLIP\n-            if isinstance(value, PretrainedConfig):\n+            if isinstance(value, PreTrainedConfig):\n                 value = value.to_dict()\n                 del value[\"transformers_version\"]\n \n@@ -964,7 +964,7 @@ def to_json_string(self, use_diff: bool = True) -> str:\n \n         Args:\n             use_diff (`bool`, *optional*, defaults to `True`):\n-                If set to `True`, only the difference between the config instance and the default `PretrainedConfig()`\n+                If set to `True`, only the difference between the config instance and the default `PreTrainedConfig()`\n                 is serialized to JSON string.\n \n         Returns:\n@@ -984,7 +984,7 @@ def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool =\n             json_file_path (`str` or `os.PathLike`):\n                 Path to the JSON file in which this configuration instance's parameters will be saved.\n             use_diff (`bool`, *optional*, defaults to `True`):\n-                If set to `True`, only the difference between the config instance and the default `PretrainedConfig()`\n+                If set to `True`, only the difference between the config instance and the default `PreTrainedConfig()`\n                 is serialized to JSON file.\n         \"\"\"\n         with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n@@ -1137,7 +1137,7 @@ def _get_global_generation_defaults() -> dict[str, Any]:\n \n     def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n         \"\"\"\n-        Gets the non-default generation parameters on the PretrainedConfig instance\n+        Gets the non-default generation parameters on the PreTrainedConfig instance\n         \"\"\"\n         non_default_generation_parameters = {}\n         decoder_attribute_name = None\n@@ -1179,7 +1179,7 @@ def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n \n         return non_default_generation_parameters\n \n-    def get_text_config(self, decoder=None, encoder=None) -> \"PretrainedConfig\":\n+    def get_text_config(self, decoder=None, encoder=None) -> \"PreTrainedConfig\":\n         \"\"\"\n         Returns the text config related to the text input (encoder) or text output (decoder) of the model. The\n         `decoder` and `encoder` input arguments can be used to specify which end of the model we are interested in,\n@@ -1335,21 +1335,25 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n     default = config_obj.__class__().to_dict() if config_obj is not None else {}\n     for key, value in dict_a.items():\n         obj_value = getattr(config_obj, str(key), None)\n-        if isinstance(obj_value, PretrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n+        if isinstance(obj_value, PreTrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n             diff_value = recursive_diff_dict(value, dict_b[key], config_obj=obj_value)\n             diff[key] = diff_value\n         elif key not in dict_b or (value != default[key]):\n             diff[key] = value\n     return diff\n \n \n-PretrainedConfig.push_to_hub = copy_func(PretrainedConfig.push_to_hub)\n-if PretrainedConfig.push_to_hub.__doc__ is not None:\n-    PretrainedConfig.push_to_hub.__doc__ = PretrainedConfig.push_to_hub.__doc__.format(\n+PreTrainedConfig.push_to_hub = copy_func(PreTrainedConfig.push_to_hub)\n+if PreTrainedConfig.push_to_hub.__doc__ is not None:\n+    PreTrainedConfig.push_to_hub.__doc__ = PreTrainedConfig.push_to_hub.__doc__.format(\n         object=\"config\", object_class=\"AutoConfig\", object_files=\"configuration file\"\n     )\n \n \n+# The alias is only here for BC - we did not have the correct CamelCasing before\n+PretrainedConfig = PreTrainedConfig\n+\n+\n ALLOWED_LAYER_TYPES = (\n     \"full_attention\",\n     \"sliding_attention\","
        },
        {
            "sha": "2db48c1638c86d5e7158c42af435aa86d1493dae",
            "filename": "src/transformers/dynamic_module_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fdynamic_module_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fdynamic_module_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdynamic_module_utils.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -613,7 +613,7 @@ def custom_object_save(obj: Any, folder: Union[str, os.PathLike], config: Option\n     Args:\n         obj (`Any`): The object for which to save the module files.\n         folder (`str` or `os.PathLike`): The folder where to save.\n-        config (`PretrainedConfig` or dictionary, `optional`):\n+        config (`PreTrainedConfig` or dictionary, `optional`):\n             A config in which to register the auto_map corresponding to this custom object.\n \n     Returns:"
        },
        {
            "sha": "4c5530ebe7597b06195e67761eb4d25cc1dbbafd",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -23,7 +23,7 @@\n from typing import TYPE_CHECKING, Any, Callable, Optional, Union\n \n from .. import __version__\n-from ..configuration_utils import PretrainedConfig\n+from ..configuration_utils import PreTrainedConfig\n from ..utils import (\n     GENERATION_CONFIG_NAME,\n     ExplicitEnum,\n@@ -1101,13 +1101,13 @@ def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool =\n             writer.write(self.to_json_string(use_diff=use_diff))\n \n     @classmethod\n-    def from_model_config(cls, model_config: PretrainedConfig) -> \"GenerationConfig\":\n+    def from_model_config(cls, model_config: PreTrainedConfig) -> \"GenerationConfig\":\n         \"\"\"\n-        Instantiates a [`GenerationConfig`] from a [`PretrainedConfig`]. This function is useful to convert legacy\n-        [`PretrainedConfig`] objects, which may contain generation parameters, into a stand-alone [`GenerationConfig`].\n+        Instantiates a [`GenerationConfig`] from a [`PreTrainedConfig`]. This function is useful to convert legacy\n+        [`PreTrainedConfig`] objects, which may contain generation parameters, into a stand-alone [`GenerationConfig`].\n \n         Args:\n-            model_config (`PretrainedConfig`):\n+            model_config (`PreTrainedConfig`):\n                 The model config that will be used to instantiate the generation config.\n \n         Returns:"
        },
        {
            "sha": "f46c6fa811fc3692b94174948b1a912acab1f3ff",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,14 +18,14 @@\n \n import torch\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n from ...utils.metrics import attach_tracer, traced\n from .cache_manager import CacheAllocator, FullAttentionCacheAllocator, SlidingAttentionCacheAllocator\n from .requests import get_device_and_memory_breakdown, logger\n \n \n-def group_layers_by_attn_type(config: PretrainedConfig) -> tuple[list[list[int]], list[str]]:\n+def group_layers_by_attn_type(config: PreTrainedConfig) -> tuple[list[list[int]], list[str]]:\n     \"\"\"\n     Group layers depending on the attention mix, according to VLLM's hybrid allocator rules:\n         - Layers in each group need to have the same type of attention\n@@ -119,7 +119,7 @@ class PagedAttentionCache:\n     # TODO: this init is quite long, maybe a refactor is in order\n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         generation_config: GenerationConfig,\n         device: torch.device,\n         dtype: torch.dtype = torch.float16,"
        },
        {
            "sha": "8fad631f3915b5ff023f8d5621ade31d4535db92",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -25,7 +25,7 @@\n from torch import nn\n from tqdm import tqdm\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n@@ -140,7 +140,7 @@ class ContinuousBatchProcessor:\n     def __init__(\n         self,\n         cache: PagedAttentionCache,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         generation_config: GenerationConfig,\n         input_queue: queue.Queue,\n         output_queue: queue.Queue,"
        },
        {
            "sha": "ed8813b4b33c45f2a24d9829249b5d1b45b0d0f5",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -25,7 +25,7 @@\n \n from ..modeling_utils import PreTrainedModel\n from ..utils import ModelOutput, logging\n-from .configuration_utils import PretrainedConfig, WatermarkingConfig\n+from .configuration_utils import PreTrainedConfig, WatermarkingConfig\n from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n \n \n@@ -75,7 +75,7 @@ class WatermarkDetector:\n     See [the paper](https://huggingface.co/papers/2306.04634) for more information.\n \n     Args:\n-        model_config (`PretrainedConfig`):\n+        model_config (`PreTrainedConfig`):\n             The model config that will be used to get model specific arguments used when generating.\n         device (`str`):\n             The device which was used during watermarked text generation.\n@@ -119,7 +119,7 @@ class WatermarkDetector:\n \n     def __init__(\n         self,\n-        model_config: PretrainedConfig,\n+        model_config: PreTrainedConfig,\n         device: str,\n         watermarking_config: Union[WatermarkingConfig, dict],\n         ignore_repeated_ngrams: bool = False,\n@@ -237,13 +237,13 @@ def __call__(\n         return prediction\n \n \n-class BayesianDetectorConfig(PretrainedConfig):\n+class BayesianDetectorConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`BayesianDetectorModel`]. It is used to\n     instantiate a Bayesian Detector model according to the specified arguments.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         watermarking_depth (`int`, *optional*):"
        },
        {
            "sha": "b47af98b8e3c7fda7acefc48691965d6a0522874",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n import torch.nn.functional as F\n \n from .cache_utils import Cache\n-from .configuration_utils import PretrainedConfig\n+from .configuration_utils import PreTrainedConfig\n from .utils import is_torch_xpu_available, logging\n from .utils.generic import GeneralInterface\n from .utils.import_utils import is_torch_flex_attn_available, is_torch_greater_or_equal, is_torchdynamo_compiling\n@@ -662,7 +662,7 @@ def find_packed_sequence_indices(position_ids: torch.Tensor) -> torch.Tensor:\n \n \n def _preprocess_mask_arguments(\n-    config: PretrainedConfig,\n+    config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n     attention_mask: Optional[Union[torch.Tensor, BlockMask]],\n     cache_position: torch.Tensor,\n@@ -675,7 +675,7 @@ def _preprocess_mask_arguments(\n     key-value length and offsets, and if we should early exit or not.\n \n     Args:\n-        config (`PretrainedConfig`):\n+        config (`PreTrainedConfig`):\n             The model config.\n         input_embeds (`torch.Tensor`):\n             The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n@@ -743,7 +743,7 @@ def _preprocess_mask_arguments(\n \n \n def create_causal_mask(\n-    config: PretrainedConfig,\n+    config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n@@ -758,7 +758,7 @@ def create_causal_mask(\n     to what is needed in the `modeling_xxx.py` files).\n \n     Args:\n-        config (`PretrainedConfig`):\n+        config (`PreTrainedConfig`):\n             The model config.\n         input_embeds (`torch.Tensor`):\n             The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n@@ -837,7 +837,7 @@ def create_causal_mask(\n \n \n def create_sliding_window_causal_mask(\n-    config: PretrainedConfig,\n+    config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n@@ -853,7 +853,7 @@ def create_sliding_window_causal_mask(\n     `modeling_xxx.py` files).\n \n     Args:\n-        config (`PretrainedConfig`):\n+        config (`PreTrainedConfig`):\n             The model config.\n         input_embeds (`torch.Tensor`):\n             The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n@@ -934,7 +934,7 @@ def create_sliding_window_causal_mask(\n \n \n def create_chunked_causal_mask(\n-    config: PretrainedConfig,\n+    config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n@@ -950,7 +950,7 @@ def create_chunked_causal_mask(\n     `modeling_xxx.py` files).\n \n     Args:\n-        config (`PretrainedConfig`):\n+        config (`PreTrainedConfig`):\n             The model config.\n         input_embeds (`torch.Tensor`):\n             The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n@@ -1063,7 +1063,7 @@ def create_chunked_causal_mask(\n \n \n def create_masks_for_generate(\n-    config: PretrainedConfig,\n+    config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n@@ -1078,7 +1078,7 @@ def create_masks_for_generate(\n     in order to easily create the masks in advance, when we compile the forwards with Static caches.\n \n     Args:\n-        config (`PretrainedConfig`):\n+        config (`PreTrainedConfig`):\n             The model config.\n         input_embeds (`torch.Tensor`):\n             The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the"
        },
        {
            "sha": "4e24fffe5fd8c6d17618fd144b6ad52c7628adc5",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,7 +16,7 @@\n from functools import wraps\n from typing import Optional\n \n-from .configuration_utils import PretrainedConfig\n+from .configuration_utils import PreTrainedConfig\n from .utils import is_torch_available, logging\n \n \n@@ -90,14 +90,14 @@ def wrapper(self, x, position_ids):\n \n \n def _compute_default_rope_parameters(\n-    config: Optional[PretrainedConfig] = None,\n+    config: Optional[PreTrainedConfig] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies according to the original RoPE implementation\n     Args:\n-        config ([`~transformers.PretrainedConfig`]):\n+        config ([`~transformers.PreTrainedConfig`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -133,14 +133,14 @@ def _compute_default_rope_parameters(\n \n \n def _compute_linear_scaling_rope_parameters(\n-    config: Optional[PretrainedConfig] = None,\n+    config: Optional[PreTrainedConfig] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n     Args:\n-        config ([`~transformers.PretrainedConfig`]):\n+        config ([`~transformers.PreTrainedConfig`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -176,15 +176,15 @@ def _compute_linear_scaling_rope_parameters(\n \n \n def _compute_dynamic_ntk_parameters(\n-    config: Optional[PretrainedConfig] = None,\n+    config: Optional[PreTrainedConfig] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\n \n     Args:\n-        config ([`~transformers.PretrainedConfig`]):\n+        config ([`~transformers.PreTrainedConfig`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -244,14 +244,14 @@ def _compute_dynamic_ntk_parameters(\n \n \n def _compute_yarn_parameters(\n-    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n+    config: PreTrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Please refer to the\n     [original paper](https://huggingface.co/papers/2309.00071)\n \n     Args:\n-        config ([`~transformers.PretrainedConfig`]):\n+        config ([`~transformers.PreTrainedConfig`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -369,14 +369,14 @@ def linear_ramp_factor(min, max, dim):\n \n \n def _compute_longrope_parameters(\n-    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n+    config: PreTrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with LongRoPE scaling. Please refer to the\n     [original implementation](https://github.com/microsoft/LongRoPE)\n \n     Args:\n-        config ([`~transformers.PretrainedConfig`]):\n+        config ([`~transformers.PreTrainedConfig`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -451,13 +451,13 @@ def _compute_longrope_parameters(\n \n \n def _compute_llama3_parameters(\n-    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n+    config: PreTrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies for llama 3.1.\n \n     Args:\n-        config ([`~transformers.PretrainedConfig`]):\n+        config ([`~transformers.PreTrainedConfig`]):\n             The model configuration. This function assumes that the config will provide at least the following\n             properties:\n \n@@ -557,15 +557,15 @@ def _check_received_keys(\n         logger.warning(f\"Unrecognized keys in `rope_scaling` for 'rope_type'='{rope_type}': {unused_keys}\")\n \n \n-def _validate_default_rope_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n+def _validate_default_rope_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\"}\n     received_keys = set(rope_scaling.keys())\n     _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n \n \n-def _validate_linear_scaling_rope_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n+def _validate_linear_scaling_rope_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\"}\n@@ -577,7 +577,7 @@ def _validate_linear_scaling_rope_parameters(config: PretrainedConfig, ignore_ke\n         logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n \n \n-def _validate_dynamic_scaling_rope_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n+def _validate_dynamic_scaling_rope_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\"}\n@@ -591,7 +591,7 @@ def _validate_dynamic_scaling_rope_parameters(config: PretrainedConfig, ignore_k\n         logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n \n \n-def _validate_yarn_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n+def _validate_yarn_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\"}\n@@ -657,7 +657,7 @@ def _validate_yarn_parameters(config: PretrainedConfig, ignore_keys: Optional[se\n         )\n \n \n-def _validate_longrope_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n+def _validate_longrope_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"short_factor\", \"long_factor\"}\n@@ -707,7 +707,7 @@ def _validate_longrope_parameters(config: PretrainedConfig, ignore_keys: Optiona\n                 )\n \n \n-def _validate_llama3_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n+def _validate_llama3_parameters(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\", \"original_max_position_embeddings\", \"low_freq_factor\", \"high_freq_factor\"}\n@@ -754,11 +754,11 @@ def _validate_llama3_parameters(config: PretrainedConfig, ignore_keys: Optional[\n }\n \n \n-def rope_config_validation(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n+def rope_config_validation(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n     \"\"\"\n-    Validate the RoPE config arguments, given a `PretrainedConfig` object\n+    Validate the RoPE config arguments, given a `PreTrainedConfig` object\n     \"\"\"\n-    rope_scaling = getattr(config, \"rope_scaling\", None)  # not a default parameter in `PretrainedConfig`\n+    rope_scaling = getattr(config, \"rope_scaling\", None)  # not a default parameter in `PreTrainedConfig`\n     if rope_scaling is None:\n         return\n "
        },
        {
            "sha": "4ce8cd01de4da4af6c65602ab3e963fd179239e4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -44,7 +44,7 @@\n from torch.distributions import constraints\n from torch.utils.checkpoint import checkpoint\n \n-from .configuration_utils import PretrainedConfig\n+from .configuration_utils import PreTrainedConfig\n from .distributed import DistributedConfig\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig\n@@ -1149,11 +1149,11 @@ def _get_dtype(\n     cls,\n     dtype: Optional[Union[str, torch.dtype, dict]],\n     checkpoint_files: Optional[list[str]],\n-    config: PretrainedConfig,\n+    config: PreTrainedConfig,\n     sharded_metadata: Optional[dict],\n     state_dict: Optional[dict],\n     weights_only: bool,\n-) -> tuple[PretrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n+) -> tuple[PreTrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n     \"\"\"Find the correct `dtype` to use based on provided arguments. Also update the `config` based on the\n     inferred dtype. We do the following:\n     1. If dtype is not None, we use that dtype\n@@ -1780,7 +1780,7 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n \n     Class attributes (overridden by derived classes):\n \n-        - **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class\n+        - **config_class** ([`PreTrainedConfig`]) -- A subclass of [`PreTrainedConfig`] to use as configuration class\n           for this model architecture.\n         - **base_model_prefix** (`str`) -- A string indicating the attribute associated to the base model in derived\n           classes of the same architecture adding modules on top of the base model.\n@@ -1935,12 +1935,12 @@ def __init_subclass__(cls, **kwargs):\n         elif full_annotation is not None:\n             cls.config_class = full_annotation\n \n-    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n+    def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n         super().__init__()\n-        if not isinstance(config, PretrainedConfig):\n+        if not isinstance(config, PreTrainedConfig):\n             raise TypeError(\n                 f\"Parameter config in `{self.__class__.__name__}(config)` should be an instance of class \"\n-                \"`PretrainedConfig`. To create a model from a pretrained model use \"\n+                \"`PreTrainedConfig`. To create a model from a pretrained model use \"\n                 f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n             )\n         self.config = config\n@@ -4250,7 +4250,7 @@ def from_pretrained(\n         cls: type[SpecificPreTrainedModelType],\n         pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n         *model_args,\n-        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,\n+        config: Optional[Union[PreTrainedConfig, str, os.PathLike]] = None,\n         cache_dir: Optional[Union[str, os.PathLike]] = None,\n         ignore_mismatched_sizes: bool = False,\n         force_download: bool = False,\n@@ -4285,11 +4285,11 @@ def from_pretrained(\n                       arguments `config` and `state_dict`).\n             model_args (sequence of positional arguments, *optional*):\n                 All remaining positional arguments will be passed to the underlying model's `__init__` method.\n-            config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\n+            config (`Union[PreTrainedConfig, str, os.PathLike]`, *optional*):\n                 Can be either:\n \n-                    - an instance of a class derived from [`PretrainedConfig`],\n-                    - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\n+                    - an instance of a class derived from [`PreTrainedConfig`],\n+                    - a string or path valid as input to [`~PreTrainedConfig.from_pretrained`].\n \n                 Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n                 be automatically loaded when:\n@@ -4437,7 +4437,7 @@ def from_pretrained(\n                       underlying model's `__init__` method (we assume all relevant updates to the configuration have\n                       already been done)\n                     - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n-                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n+                      initialization function ([`~PreTrainedConfig.from_pretrained`]). Each key of `kwargs` that\n                       corresponds to a configuration attribute will be used to override said attribute with the\n                       supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n                       will be passed to the underlying model's `__init__` function.\n@@ -4574,7 +4574,7 @@ def from_pretrained(\n             raise ValueError(\"accelerate is required when loading a GGUF file `pip install accelerate`.\")\n \n         if commit_hash is None:\n-            if not isinstance(config, PretrainedConfig):\n+            if not isinstance(config, PreTrainedConfig):\n                 # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\n                 resolved_config_file = cached_file(\n                     pretrained_model_name_or_path,\n@@ -4681,7 +4681,7 @@ def from_pretrained(\n             local_files_only = True\n \n         # Load config if we don't provide a configuration\n-        if not isinstance(config, PretrainedConfig):\n+        if not isinstance(config, PreTrainedConfig):\n             config_path = config if config is not None else pretrained_model_name_or_path\n             config, model_kwargs = cls.config_class.from_pretrained(\n                 config_path,"
        },
        {
            "sha": "ee2e1f2052b90b98fb8544c014043d21ad63d44e",
            "filename": "src/transformers/models/aimv2/configuration_aimv2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -21,22 +21,22 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Aimv2VisionConfig(PretrainedConfig):\n+class Aimv2VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Aimv2VisionModel`]. It is used to instantiate a\n     AIMv2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the vision encoder of the AIMv2\n     [apple/aimv2-large-patch14-224](https://huggingface.co/apple/aimv2-large-patch14-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1024):\n@@ -127,15 +127,15 @@ def __init__(\n         self.is_native = is_native\n \n \n-class Aimv2TextConfig(PretrainedConfig):\n+class Aimv2TextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Aimv2TextModel`]. It is used to instantiate a\n     AIMv2 text encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the text encoder of the AIMv2\n     [apple/aimv2-large-patch14-224-lit](https://huggingface.co/apple/aimv2-large-patch14-224-lit) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 49408):\n@@ -212,15 +212,15 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n \n \n-class Aimv2Config(PretrainedConfig):\n+class Aimv2Config(PreTrainedConfig):\n     r\"\"\"\n     [`Aimv2Config`] is the configuration class to store the configuration of a [`Aimv2Model`]. It is used to\n     instantiate a AIMv2 model according to the specified arguments, defining the text model and vision model configs.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the AIMv2\n     [apple/aimv2-large-patch14-224-lit](https://huggingface.co/apple/aimv2-large-patch14-224-lit) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "e60a3d1db27e0ea638c7c2ea4fe5455b31196d01",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -47,8 +47,8 @@ class Aimv2VisionConfig(SiglipVisionConfig):\n     configuration with the defaults will yield a similar configuration to that of the vision encoder of the AIMv2\n     [apple/aimv2-large-patch14-224](https://huggingface.co/apple/aimv2-large-patch14-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1024):\n@@ -147,8 +147,8 @@ class Aimv2TextConfig(SiglipTextConfig):\n     configuration with the defaults will yield a similar configuration to that of the text encoder of the AIMv2\n     [apple/aimv2-large-patch14-224-lit](https://huggingface.co/apple/aimv2-large-patch14-224-lit) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 49408):\n@@ -238,8 +238,8 @@ class Aimv2Config(SiglipConfig):\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the AIMv2\n     [apple/aimv2-large-patch14-224-lit](https://huggingface.co/apple/aimv2-large-patch14-224-lit) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "2ca9c141849ac1c21fc9ac84945493331836b783",
            "filename": "src/transformers/models/albert/configuration_albert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,19 +18,19 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n \n \n-class AlbertConfig(PretrainedConfig):\n+class AlbertConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`AlbertModel`] or a [`TFAlbertModel`]. It is used\n     to instantiate an ALBERT model according to the specified arguments, defining the model architecture. Instantiating\n     a configuration with the defaults will yield a similar configuration to that of the ALBERT\n     [albert/albert-xxlarge-v2](https://huggingface.co/albert/albert-xxlarge-v2) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30000):"
        },
        {
            "sha": "f1c7ff16ad0bb7ef3d0c79a9ac5ec7a024391c2b",
            "filename": "src/transformers/models/align/configuration_align.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"ALIGN model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class AlignTextConfig(PretrainedConfig):\n+class AlignTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`AlignTextModel`]. It is used to instantiate a\n     ALIGN text encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the text encoder of the ALIGN\n     [kakaobrain/align-base](https://huggingface.co/kakaobrain/align-base) architecture. The default values here are\n     copied from BERT.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n@@ -128,16 +128,16 @@ def __init__(\n         self.pad_token_id = pad_token_id\n \n \n-class AlignVisionConfig(PretrainedConfig):\n+class AlignVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`AlignVisionModel`]. It is used to instantiate a\n     ALIGN vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the vision encoder of the ALIGN\n     [kakaobrain/align-base](https://huggingface.co/kakaobrain/align-base) architecture. The default values are copied\n     from EfficientNet (efficientnet-b7)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):\n@@ -250,15 +250,15 @@ def __init__(\n         self.num_hidden_layers = sum(num_block_repeats) * 4\n \n \n-class AlignConfig(PretrainedConfig):\n+class AlignConfig(PreTrainedConfig):\n     r\"\"\"\n     [`AlignConfig`] is the configuration class to store the configuration of a [`AlignModel`]. It is used to\n     instantiate a ALIGN model according to the specified arguments, defining the text model and vision model configs.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the ALIGN\n     [kakaobrain/align-base](https://huggingface.co/kakaobrain/align-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "af1ae041edd630730531957c1ec64991627c4182",
            "filename": "src/transformers/models/altclip/configuration_altclip.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"AltCLIP model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class AltCLIPTextConfig(PretrainedConfig):\n+class AltCLIPTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`AltCLIPTextModel`]. It is used to instantiate a\n     AltCLIP text model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the AltCLIP\n     [BAAI/AltCLIP](https://huggingface.co/BAAI/AltCLIP) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -139,15 +139,15 @@ def __init__(\n         self.project_dim = project_dim\n \n \n-class AltCLIPVisionConfig(PretrainedConfig):\n+class AltCLIPVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`AltCLIPModel`]. It is used to instantiate an\n     AltCLIP model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the AltCLIP\n     [BAAI/AltCLIP](https://huggingface.co/BAAI/AltCLIP) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -232,15 +232,15 @@ def __init__(\n         self.hidden_act = hidden_act\n \n \n-class AltCLIPConfig(PretrainedConfig):\n+class AltCLIPConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`AltCLIPModel`]. It is used to instantiate an\n     AltCLIP model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the AltCLIP\n     [BAAI/AltCLIP](https://huggingface.co/BAAI/AltCLIP) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "c8380ee29bc345b6e724f96e18b39f4619d5b12f",
            "filename": "src/transformers/models/apertus/configuration_apertus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,19 +20,19 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class ApertusConfig(PretrainedConfig):\n+class ApertusConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ApertusModel`]. It is used to instantiate a Apertus\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Apertus-8B.\n     e.g. [swiss-ai/Apertus-8B](https://huggingface.co/swiss-ai/Apertus-8B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "d13dbb69f0f45ab60a9b8a3e73260d1c81193c2b",
            "filename": "src/transformers/models/apertus/modular_apertus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -48,8 +48,8 @@ class ApertusConfig(LlamaConfig):\n     defaults will yield a similar configuration to that of the Apertus-8B.\n     e.g. [swiss-ai/Apertus-8B](https://huggingface.co/swiss-ai/Apertus-8B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "9f61687ea8faca43c05f77310be166e887e5d60d",
            "filename": "src/transformers/models/arcee/configuration_arcee.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,11 +19,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class ArceeConfig(PretrainedConfig):\n+class ArceeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ArceeModel`]. It is used to instantiate an Arcee\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n@@ -33,8 +33,8 @@ class ArceeConfig(PretrainedConfig):\n     [arcee-ai/AFM-4.5B](https://huggingface.co/arcee-ai/AFM-4.5B)\n     and were used to build the examples below.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 32000):"
        },
        {
            "sha": "3fb38fcb4ce10f90de1204056dd7bca3d7c36d54",
            "filename": "src/transformers/models/arcee/modular_arcee.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -39,8 +39,8 @@ class ArceeConfig(LlamaConfig):\n     [arcee-ai/AFM-4.5B](https://huggingface.co/arcee-ai/AFM-4.5B)\n     and were used to build the examples below.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 32000):"
        },
        {
            "sha": "451acad6520088de4c5396bc997a3830edc976f1",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,12 +20,12 @@\n # limitations under the License.\n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n-class AriaTextConfig(PretrainedConfig):\n+class AriaTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This class handles the configuration for the text component of the Aria model.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the model of the Aria\n@@ -220,15 +220,15 @@ def __init__(\n         self.moe_num_shared_experts = moe_num_shared_experts\n \n \n-class AriaConfig(PretrainedConfig):\n+class AriaConfig(PreTrainedConfig):\n     r\"\"\"\n     This class handles the configuration for both vision and text components of the Aria model,\n     as well as additional parameters for image token handling and projector mapping.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the model of the Aria\n     [rhymes-ai/Aria](https://huggingface.co/rhymes-ai/Aria) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`AriaVisionConfig` or `dict`, *optional*):"
        },
        {
            "sha": "1d820c00cf0aec60f039a4ccd530264f7e61d229",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -21,7 +21,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_transforms import PaddingMode, convert_to_rgb, pad, resize, to_channel_dimension_format\n from ...image_utils import (\n@@ -221,15 +221,15 @@ def __init__(\n         self.moe_num_shared_experts = moe_num_shared_experts\n \n \n-class AriaConfig(PretrainedConfig):\n+class AriaConfig(PreTrainedConfig):\n     r\"\"\"\n     This class handles the configuration for both vision and text components of the Aria model,\n     as well as additional parameters for image token handling and projector mapping.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the model of the Aria\n     [rhymes-ai/Aria](https://huggingface.co/rhymes-ai/Aria) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`AriaVisionConfig` or `dict`, *optional*):"
        },
        {
            "sha": "a7165c3f8afeb381739a0eb60e552e6dfd397004",
            "filename": "src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,23 +16,23 @@\n \n from typing import Any\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ASTConfig(PretrainedConfig):\n+class ASTConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ASTModel`]. It is used to instantiate an AST\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the AST\n     [MIT/ast-finetuned-audioset-10-10-0.4593](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):"
        },
        {
            "sha": "ee67f203b66a49c37cc40cfcb8d8ebf349b9e315",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -23,7 +23,7 @@\n from collections.abc import Iterator\n from typing import Any, TypeVar, Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...utils import (\n     CONFIG_NAME,\n@@ -65,7 +65,7 @@\n             model's configuration. Use [`~BaseAutoModelClass.from_pretrained`] to load the model weights.\n \n         Args:\n-            config ([`PretrainedConfig`]):\n+            config ([`PreTrainedConfig`]):\n                 The model class to instantiate is selected based on the configuration class:\n \n                 List options\n@@ -104,7 +104,7 @@\n                       [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n             model_args (additional positional arguments, *optional*):\n                 Will be passed along to the underlying model `__init__()` method.\n-            config ([`PretrainedConfig`], *optional*):\n+            config ([`PreTrainedConfig`], *optional*):\n                 Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n                 be automatically loaded when:\n \n@@ -155,7 +155,7 @@\n                       underlying model's `__init__` method (we assume all relevant updates to the configuration have\n                       already been done)\n                     - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n-                      initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n+                      initialization function ([`~PreTrainedConfig.from_pretrained`]). Each key of `kwargs` that\n                       corresponds to a configuration attribute will be used to override said attribute with the\n                       supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n                       will be passed to the underlying model's `__init__` function.\n@@ -243,7 +243,7 @@ def from_config(cls, config, **kwargs):\n         )\n \n     @classmethod\n-    def _prepare_config_for_auto_class(cls, config: PretrainedConfig) -> PretrainedConfig:\n+    def _prepare_config_for_auto_class(cls, config: PreTrainedConfig) -> PreTrainedConfig:\n         \"\"\"Additional autoclass-specific config post-loading manipulation. May be overridden in subclasses.\"\"\"\n         return config\n \n@@ -284,7 +284,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             hub_kwargs[\"token\"] = token\n \n         if commit_hash is None:\n-            if not isinstance(config, PretrainedConfig):\n+            if not isinstance(config, PreTrainedConfig):\n                 # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\n                 resolved_config_file = cached_file(\n                     pretrained_model_name_or_path,\n@@ -315,7 +315,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n                     adapter_kwargs[\"_adapter_model_path\"] = pretrained_model_name_or_path\n                     pretrained_model_name_or_path = adapter_config[\"base_model_name_or_path\"]\n \n-        if not isinstance(config, PretrainedConfig):\n+        if not isinstance(config, PreTrainedConfig):\n             kwargs_orig = copy.deepcopy(kwargs)\n             # ensure not to pollute the config object with dtype=\"auto\" - since it's\n             # meaningless in the context of the config object - torch.dtype values are acceptable\n@@ -396,7 +396,7 @@ def register(cls, config_class, model_class, exist_ok=False) -> None:\n         Register a new model for this class.\n \n         Args:\n-            config_class ([`PretrainedConfig`]):\n+            config_class ([`PreTrainedConfig`]):\n                 The configuration corresponding to the model to register.\n             model_class ([`PreTrainedModel`]):\n                 The model to register.\n@@ -553,7 +553,7 @@ def add_generation_mixin_to_remote_model(model_class):\n     return model_class\n \n \n-class _LazyAutoMapping(OrderedDict[type[PretrainedConfig], _LazyAutoMappingValue]):\n+class _LazyAutoMapping(OrderedDict[type[PreTrainedConfig], _LazyAutoMappingValue]):\n     \"\"\"\n     \" A mapping config to object (model or tokenizer for instance) that will load keys and values when it is accessed.\n \n@@ -574,7 +574,7 @@ def __len__(self) -> int:\n         common_keys = set(self._config_mapping.keys()).intersection(self._model_mapping.keys())\n         return len(common_keys) + len(self._extra_content)\n \n-    def __getitem__(self, key: type[PretrainedConfig]) -> _LazyAutoMappingValue:\n+    def __getitem__(self, key: type[PreTrainedConfig]) -> _LazyAutoMappingValue:\n         if key in self._extra_content:\n             return self._extra_content[key]\n         model_type = self._reverse_config_mapping[key.__name__]\n@@ -596,15 +596,15 @@ def _load_attr_from_module(self, model_type, attr):\n             self._modules[module_name] = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n         return getattribute_from_module(self._modules[module_name], attr)\n \n-    def keys(self) -> list[type[PretrainedConfig]]:\n+    def keys(self) -> list[type[PreTrainedConfig]]:\n         mapping_keys = [\n             self._load_attr_from_module(key, name)\n             for key, name in self._config_mapping.items()\n             if key in self._model_mapping\n         ]\n         return mapping_keys + list(self._extra_content.keys())\n \n-    def get(self, key: type[PretrainedConfig], default: _T) -> Union[_LazyAutoMappingValue, _T]:\n+    def get(self, key: type[PreTrainedConfig], default: _T) -> Union[_LazyAutoMappingValue, _T]:\n         try:\n             return self.__getitem__(key)\n         except KeyError:\n@@ -621,7 +621,7 @@ def values(self) -> list[_LazyAutoMappingValue]:\n         ]\n         return mapping_values + list(self._extra_content.values())\n \n-    def items(self) -> list[tuple[type[PretrainedConfig], _LazyAutoMappingValue]]:\n+    def items(self) -> list[tuple[type[PreTrainedConfig], _LazyAutoMappingValue]]:\n         mapping_items = [\n             (\n                 self._load_attr_from_module(key, self._config_mapping[key]),\n@@ -632,7 +632,7 @@ def items(self) -> list[tuple[type[PretrainedConfig], _LazyAutoMappingValue]]:\n         ]\n         return mapping_items + list(self._extra_content.items())\n \n-    def __iter__(self) -> Iterator[type[PretrainedConfig]]:\n+    def __iter__(self) -> Iterator[type[PreTrainedConfig]]:\n         return iter(self.keys())\n \n     def __contains__(self, item: type) -> bool:\n@@ -643,7 +643,7 @@ def __contains__(self, item: type) -> bool:\n         model_type = self._reverse_config_mapping[item.__name__]\n         return model_type in self._model_mapping\n \n-    def register(self, key: type[PretrainedConfig], value: _LazyAutoMappingValue, exist_ok=False) -> None:\n+    def register(self, key: type[PreTrainedConfig], value: _LazyAutoMappingValue, exist_ok=False) -> None:\n         \"\"\"\n         Register a new model in this mapping.\n         \"\"\""
        },
        {
            "sha": "a2e4f05bae525859c511f8d2e60bbc84a7e14aa5",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -22,7 +22,7 @@\n from collections.abc import Callable, Iterator, KeysView, ValuesView\n from typing import Any, TypeVar, Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...utils import CONFIG_NAME, logging\n \n@@ -1031,7 +1031,7 @@ def config_class_to_model_type(config) -> Union[str, None]:\n     return None\n \n \n-class _LazyConfigMapping(OrderedDict[str, type[PretrainedConfig]]):\n+class _LazyConfigMapping(OrderedDict[str, type[PreTrainedConfig]]):\n     \"\"\"\n     A dictionary that lazily load its values when they are requested.\n     \"\"\"\n@@ -1041,7 +1041,7 @@ def __init__(self, mapping) -> None:\n         self._extra_content = {}\n         self._modules = {}\n \n-    def __getitem__(self, key: str) -> type[PretrainedConfig]:\n+    def __getitem__(self, key: str) -> type[PreTrainedConfig]:\n         if key in self._extra_content:\n             return self._extra_content[key]\n         if key not in self._mapping:\n@@ -1061,10 +1061,10 @@ def __getitem__(self, key: str) -> type[PretrainedConfig]:\n     def keys(self) -> list[str]:\n         return list(self._mapping.keys()) + list(self._extra_content.keys())\n \n-    def values(self) -> list[type[PretrainedConfig]]:\n+    def values(self) -> list[type[PreTrainedConfig]]:\n         return [self[k] for k in self._mapping] + list(self._extra_content.values())\n \n-    def items(self) -> list[tuple[str, type[PretrainedConfig]]]:\n+    def items(self) -> list[tuple[str, type[PreTrainedConfig]]]:\n         return [(k, self[k]) for k in self._mapping] + list(self._extra_content.items())\n \n     def __iter__(self) -> Iterator[str]:\n@@ -1073,7 +1073,7 @@ def __iter__(self) -> Iterator[str]:\n     def __contains__(self, item: object) -> bool:\n         return item in self._mapping or item in self._extra_content\n \n-    def register(self, key: str, value: type[PretrainedConfig], exist_ok=False) -> None:\n+    def register(self, key: str, value: type[PreTrainedConfig], exist_ok=False) -> None:\n         \"\"\"\n         Register a new configuration in this mapping.\n         \"\"\"\n@@ -1219,7 +1219,7 @@ def __init__(self) -> None:\n         )\n \n     @classmethod\n-    def for_model(cls, model_type: str, *args, **kwargs) -> PretrainedConfig:\n+    def for_model(cls, model_type: str, *args, **kwargs) -> PreTrainedConfig:\n         if model_type in CONFIG_MAPPING:\n             config_class = CONFIG_MAPPING[model_type]\n             return config_class(*args, **kwargs)\n@@ -1245,7 +1245,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n                     - A string, the *model id* of a pretrained model configuration hosted inside a model repo on\n                       huggingface.co.\n                     - A path to a *directory* containing a configuration file saved using the\n-                      [`~PretrainedConfig.save_pretrained`] method, or the [`~PreTrainedModel.save_pretrained`] method,\n+                      [`~PreTrainedConfig.save_pretrained`] method, or the [`~PreTrainedModel.save_pretrained`] method,\n                       e.g., `./my_model_directory/`.\n                     - A path or url to a saved configuration JSON *file*, e.g.,\n                       `./my_model_directory/configuration.json`.\n@@ -1326,7 +1326,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         code_revision = kwargs.pop(\"code_revision\", None)\n \n-        config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n+        config_dict, unused_kwargs = PreTrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n         has_remote_code = \"auto_map\" in config_dict and \"AutoConfig\" in config_dict[\"auto_map\"]\n         has_local_code = \"model_type\" in config_dict and config_dict[\"model_type\"] in CONFIG_MAPPING\n         if has_remote_code:\n@@ -1387,9 +1387,9 @@ def register(model_type, config, exist_ok=False) -> None:\n \n         Args:\n             model_type (`str`): The model type like \"bert\" or \"gpt\".\n-            config ([`PretrainedConfig`]): The config to register.\n+            config ([`PreTrainedConfig`]): The config to register.\n         \"\"\"\n-        if issubclass(config, PretrainedConfig) and config.model_type != model_type:\n+        if issubclass(config, PreTrainedConfig) and config.model_type != model_type:\n             raise ValueError(\n                 \"The config you are passing has a `model_type` attribute that is not consistent with the model type \"\n                 f\"you passed (config has {config.model_type} and you passed {model_type}. Fix one of those so they \""
        },
        {
            "sha": "38f09a5a3ee8eb7e252fc56d5846c0b430fb10f6",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Optional, Union\n \n # Build the list of all feature extractors\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...feature_extraction_utils import FeatureExtractionMixin\n from ...utils import CONFIG_NAME, FEATURE_EXTRACTOR_NAME, cached_file, logging\n@@ -309,7 +309,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n         # If we don't find the feature extractor class in the feature extractor config, let's try the model config.\n         if feature_extractor_class is None and feature_extractor_auto_map is None:\n-            if not isinstance(config, PretrainedConfig):\n+            if not isinstance(config, PreTrainedConfig):\n                 config = AutoConfig.from_pretrained(\n                     pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n                 )\n@@ -358,7 +358,7 @@ def register(config_class, feature_extractor_class, exist_ok=False):\n         Register a new feature extractor for this class.\n \n         Args:\n-            config_class ([`PretrainedConfig`]):\n+            config_class ([`PreTrainedConfig`]):\n                 The configuration corresponding to the model to register.\n             feature_extractor_class ([`FeatureExtractorMixin`]): The feature extractor to register.\n         \"\"\""
        },
        {
            "sha": "eeea333aa2e8ed500386dbf9a8e65594d224f59b",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -22,7 +22,7 @@\n from typing import TYPE_CHECKING, Optional, Union\n \n # Build the list of all image processors\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...image_processing_utils import ImageProcessingMixin\n from ...image_processing_utils_fast import BaseImageProcessorFast\n@@ -502,7 +502,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n \n         # If we don't find the image processor class in the image processor config, let's try the model config.\n         if image_processor_type is None and image_processor_auto_map is None:\n-            if not isinstance(config, PretrainedConfig):\n+            if not isinstance(config, PreTrainedConfig):\n                 config = AutoConfig.from_pretrained(\n                     pretrained_model_name_or_path,\n                     trust_remote_code=trust_remote_code,\n@@ -629,7 +629,7 @@ def register(\n         Register a new image processor for this class.\n \n         Args:\n-            config_class ([`PretrainedConfig`]):\n+            config_class ([`PreTrainedConfig`]):\n                 The configuration corresponding to the model to register.\n             image_processor_class ([`ImageProcessingMixin`]): The image processor to register.\n         \"\"\""
        },
        {
            "sha": "cb2eb94cecd4609685ded32acc81769f886b45e7",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -21,7 +21,7 @@\n from collections import OrderedDict\n \n # Build the list of all feature extractors\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...feature_extraction_utils import FeatureExtractionMixin\n from ...image_processing_utils import ImageProcessingMixin\n@@ -356,7 +356,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n         if processor_class is None:\n             # Otherwise, load config, if it can be loaded.\n-            if not isinstance(config, PretrainedConfig):\n+            if not isinstance(config, PreTrainedConfig):\n                 config = AutoConfig.from_pretrained(\n                     pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n                 )\n@@ -430,7 +430,7 @@ def register(config_class, processor_class, exist_ok=False):\n         Register a new processor for this class.\n \n         Args:\n-            config_class ([`PretrainedConfig`]):\n+            config_class ([`PreTrainedConfig`]):\n                 The configuration corresponding to the model to register.\n             processor_class ([`ProcessorMixin`]): The processor to register.\n         \"\"\""
        },
        {
            "sha": "0b9f97c2ba2d4f03cd7b4d74888fa2640628aa32",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -23,7 +23,7 @@\n \n from transformers.utils.import_utils import is_mistral_common_available\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...modeling_gguf_pytorch_utils import load_gguf_checkpoint\n from ...tokenization_utils import PreTrainedTokenizer\n@@ -962,7 +962,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                       applicable to all derived classes)\n             inputs (additional positional arguments, *optional*):\n                 Will be passed along to the Tokenizer `__init__()` method.\n-            config ([`PretrainedConfig`], *optional*)\n+            config ([`PreTrainedConfig`], *optional*)\n                 The configuration object used to determine the tokenizer class to instantiate.\n             cache_dir (`str` or `os.PathLike`, *optional*):\n                 Path to a directory in which a downloaded pretrained model configuration should be cached if the\n@@ -1076,7 +1076,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n \n         # If that did not work, let's try to use the config.\n         if config_tokenizer_class is None:\n-            if not isinstance(config, PretrainedConfig):\n+            if not isinstance(config, PreTrainedConfig):\n                 if gguf_file:\n                     gguf_path = cached_file(pretrained_model_name_or_path, gguf_file, **kwargs)\n                     config_dict = load_gguf_checkpoint(gguf_path, return_tensors=False)[\"config\"]\n@@ -1170,7 +1170,7 @@ def register(config_class, slow_tokenizer_class=None, fast_tokenizer_class=None,\n \n \n         Args:\n-            config_class ([`PretrainedConfig`]):\n+            config_class ([`PreTrainedConfig`]):\n                 The configuration corresponding to the model to register.\n             slow_tokenizer_class ([`PretrainedTokenizer`], *optional*):\n                 The slow tokenizer to register."
        },
        {
            "sha": "751112e150ddb38114ade514c4b64e1453a8a3c0",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -22,7 +22,7 @@\n from typing import TYPE_CHECKING, Optional, Union\n \n # Build the list of all video processors\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...utils import CONFIG_NAME, VIDEO_PROCESSOR_NAME, cached_file, is_torchvision_available, logging\n from ...utils.import_utils import requires\n@@ -321,7 +321,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n \n         # If we don't find the video processor class in the video processor config, let's try the model config.\n         if video_processor_class is None and video_processor_auto_map is None:\n-            if not isinstance(config, PretrainedConfig):\n+            if not isinstance(config, PreTrainedConfig):\n                 config = AutoConfig.from_pretrained(\n                     pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n                 )\n@@ -374,7 +374,7 @@ def register(\n         Register a new video processor for this class.\n \n         Args:\n-            config_class ([`PretrainedConfig`]):\n+            config_class ([`PreTrainedConfig`]):\n                 The configuration corresponding to the model to register.\n             video_processor_class ([`BaseVideoProcessor`]):\n                 The video processor to register."
        },
        {
            "sha": "57baeb42c9f5106f1dd191532c4f81e4a2166100",
            "filename": "src/transformers/models/autoformer/configuration_autoformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,23 +16,23 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class AutoformerConfig(PretrainedConfig):\n+class AutoformerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of an [`AutoformerModel`]. It is used to instantiate an\n     Autoformer model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Autoformer\n     [huggingface/autoformer-tourism-monthly](https://huggingface.co/huggingface/autoformer-tourism-monthly)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         prediction_length (`int`):"
        },
        {
            "sha": "c08053b2fe9bdfe07bbd0cf877866db116f5370c",
            "filename": "src/transformers/models/aya_vision/configuration_aya_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"AyaVision model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class AyaVisionConfig(PretrainedConfig):\n+class AyaVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`AyaVisionForConditionalGeneration`]. It is used to instantiate an\n     AyaVision model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of AyaVision.\n     e.g. [CohereForAI/aya-vision-8b](https://huggingface.co/CohereForAI/aya-vision-8b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):"
        },
        {
            "sha": "dd31dba59498e83bfc85ec58e5ff80f9833a4079",
            "filename": "src/transformers/models/bamba/configuration_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,14 +14,14 @@\n # limitations under the License.\n \"\"\"Bamba model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BambaConfig(PretrainedConfig):\n+class BambaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BambaModel`]. It is used to instantiate a\n     BambaModel model according to the specified arguments, defining the model architecture. Instantiating a configuration\n@@ -30,8 +30,8 @@ class BambaConfig(PretrainedConfig):\n     The BambaModel is a hybrid [mamba2](https://github.com/state-spaces/mamba) architecture with SwiGLU.\n     The checkpoints are  jointly trained by IBM, Princeton, and UIUC.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 128000):"
        },
        {
            "sha": "d5ec180c459beefbf08c92050af8c9e072a78e9d",
            "filename": "src/transformers/models/bark/configuration_bark.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,7 +16,7 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import add_start_docstrings, logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n@@ -30,8 +30,8 @@\n     defaults will yield a similar configuration to that of the Bark [suno/bark](https://huggingface.co/suno/bark)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         block_size (`int`, *optional*, defaults to 1024):\n@@ -62,7 +62,7 @@\n \"\"\"\n \n \n-class BarkSubModelConfig(PretrainedConfig):\n+class BarkSubModelConfig(PreTrainedConfig):\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     attribute_map = {\n@@ -180,16 +180,16 @@ def __init__(self, tie_word_embeddings=True, n_codes_total=8, n_codes_given=1, *\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n-class BarkConfig(PretrainedConfig):\n+class BarkConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`BarkModel`]. It is used to instantiate a Bark\n     model according to the specified sub-models configurations, defining the model architecture.\n \n     Instantiating a configuration with the defaults will yield a similar configuration to that of the Bark\n     [suno/bark](https://huggingface.co/suno/bark) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n     semantic_config ([`BarkSemanticConfig`], *optional*):\n@@ -282,7 +282,7 @@ def from_sub_model_configs(\n         semantic_config: BarkSemanticConfig,\n         coarse_acoustics_config: BarkCoarseConfig,\n         fine_acoustics_config: BarkFineConfig,\n-        codec_config: PretrainedConfig,\n+        codec_config: PreTrainedConfig,\n         **kwargs,\n     ):\n         r\"\"\""
        },
        {
            "sha": "d82669a87854a545f6bf0c91db914bfccb055d97",
            "filename": "src/transformers/models/bark/generation_configuration_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -315,7 +315,7 @@ def from_sub_model_configs(\n \n     def to_dict(self):\n         \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n+        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`].\n \n         Returns:\n             `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,"
        },
        {
            "sha": "0e9c3b3b931fca8ac8fdf40b042c54ae02ede72d",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,7 +20,7 @@\n from typing import Any\n \n from ... import PreTrainedTokenizer\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n from ...onnx.utils import compute_effective_axis_dimension\n from ...utils import is_torch_available, logging\n@@ -29,15 +29,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class BartConfig(PretrainedConfig):\n+class BartConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BartModel`]. It is used to instantiate a BART\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the BART\n     [facebook/bart-large](https://huggingface.co/facebook/bart-large) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "c8a4a2e51a55ea1c786b21eb1908ffd125431108",
            "filename": "src/transformers/models/beit/configuration_beit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,12 +20,12 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n-class BeitConfig(BackboneConfigMixin, PretrainedConfig):\n+class BeitConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BeitModel`]. It is used to instantiate an BEiT\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the"
        },
        {
            "sha": "3a313fd3b6bccf44a8638be3500eee8413f15bc9",
            "filename": "src/transformers/models/bert/configuration_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,23 +18,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BertConfig(PretrainedConfig):\n+class BertConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BertModel`] or a [`TFBertModel`]. It is used to\n     instantiate a BERT model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the BERT\n     [google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "b709660be77460d9ceb35464f6b7b8395d7e983e",
            "filename": "src/transformers/models/bert_generation/configuration_bert_generation.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,19 +14,19 @@\n # limitations under the License.\n \"\"\"BertGeneration model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n \n \n-class BertGenerationConfig(PretrainedConfig):\n+class BertGenerationConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BertGenerationPreTrainedModel`]. It is used to\n     instantiate a BertGeneration model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the BertGeneration\n     [google/bert_for_seq_generation_L-24_bbc_encoder](https://huggingface.co/google/bert_for_seq_generation_L-24_bbc_encoder)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 50358):"
        },
        {
            "sha": "a7b44f98def746cb365f30153f20b1aabedfe04f",
            "filename": "src/transformers/models/big_bird/configuration_big_bird.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconfiguration_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconfiguration_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconfiguration_big_bird.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,23 +17,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BigBirdConfig(PretrainedConfig):\n+class BigBirdConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BigBirdModel`]. It is used to instantiate an\n     BigBird model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the BigBird\n     [google/bigbird-roberta-base](https://huggingface.co/google/bigbird-roberta-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "252c714994b54691f8376b84a62a3b8627773116",
            "filename": "src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n from typing import Any\n \n from ... import PreTrainedTokenizer\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n from ...onnx.utils import compute_effective_axis_dimension\n from ...utils import is_torch_available, logging\n@@ -28,15 +28,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class BigBirdPegasusConfig(PretrainedConfig):\n+class BigBirdPegasusConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BigBirdPegasusModel`]. It is used to instantiate\n     an BigBirdPegasus model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the BigBirdPegasus\n     [google/bigbird-pegasus-large-arxiv](https://huggingface.co/google/bigbird-pegasus-large-arxiv) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "2e2b5c140e9d6c9f3406f8eac3984c8bc5cccf41",
            "filename": "src/transformers/models/biogpt/configuration_biogpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconfiguration_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconfiguration_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconfiguration_biogpt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"BioGPT model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BioGptConfig(PretrainedConfig):\n+class BioGptConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BioGptModel`]. It is used to instantiate an\n     BioGPT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the BioGPT\n     [microsoft/biogpt](https://huggingface.co/microsoft/biogpt) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "db9f2d2b89866c451ede063e952cbd0cf233431c",
            "filename": "src/transformers/models/bit/configuration_bit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbit%2Fconfiguration_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbit%2Fconfiguration_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fconfiguration_bit.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"BiT model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BitConfig(BackboneConfigMixin, PretrainedConfig):\n+class BitConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BitModel`]. It is used to instantiate an BiT\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the BiT\n     [google/bit-50](https://huggingface.co/google/bit-50) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "5e467443f28cf9138abaf642039d46af8827c78a",
            "filename": "src/transformers/models/bitnet/configuration_bitnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,22 +13,22 @@\n # See the License for the specific language governing permissions and\n \"\"\"BitNet model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BitNetConfig(PretrainedConfig):\n+class BitNetConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BitNetModel`]. It is used to instantiate an BitNet\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of\n     BitNet b1.58 2B4T [microsoft/bitnet-b1.58-2B-4T](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "ec0862b1c4ad8b09f19d366356fe2b1953e5b384",
            "filename": "src/transformers/models/blenderbot/configuration_blenderbot.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n from typing import Any\n \n from ... import PreTrainedTokenizer\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...file_utils import is_torch_available\n from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n from ...onnx.utils import compute_effective_axis_dimension\n@@ -29,15 +29,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class BlenderbotConfig(PretrainedConfig):\n+class BlenderbotConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BlenderbotModel`]. It is used to instantiate an\n     Blenderbot model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Blenderbot\n     [facebook/blenderbot-3B](https://huggingface.co/facebook/blenderbot-3B) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "d609611bd90d5776417be8f40fee0ae99b1785f1",
            "filename": "src/transformers/models/blenderbot_small/configuration_blenderbot_small.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n from typing import Any\n \n from ... import PreTrainedTokenizer\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...file_utils import is_torch_available\n from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n from ...onnx.utils import compute_effective_axis_dimension\n@@ -29,15 +29,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class BlenderbotSmallConfig(PretrainedConfig):\n+class BlenderbotSmallConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BlenderbotSmallModel`]. It is used to instantiate\n     an BlenderbotSmall model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the BlenderbotSmall\n     [facebook/blenderbot_small-90M](https://huggingface.co/facebook/blenderbot_small-90M) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "66e918499d556ffbbcb5116a5dcab46034bc1fe4",
            "filename": "src/transformers/models/blip/configuration_blip.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"Blip model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BlipTextConfig(PretrainedConfig):\n+class BlipTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BlipTextModel`]. It is used to instantiate a BLIP\n     text model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the `BlipText` used by the [base\n     architectures](https://huggingface.co/Salesforce/blip-vqa-base).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -145,15 +145,15 @@ def __init__(\n         self.label_smoothing = label_smoothing\n \n \n-class BlipVisionConfig(PretrainedConfig):\n+class BlipVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BlipVisionModel`]. It is used to instantiate a\n     BLIP vision model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration defaults will yield a similar configuration to that of the Blip-base\n     [Salesforce/blip-vqa-base](https://huggingface.co/Salesforce/blip-vqa-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -227,15 +227,15 @@ def __init__(\n         self.hidden_act = hidden_act\n \n \n-class BlipConfig(PretrainedConfig):\n+class BlipConfig(PreTrainedConfig):\n     r\"\"\"\n     [`BlipConfig`] is the configuration class to store the configuration of a [`BlipModel`]. It is used to instantiate\n     a BLIP model according to the specified arguments, defining the text model and vision model configs. Instantiating\n     a configuration with the defaults will yield a similar configuration to that of the BLIP-base\n     [Salesforce/blip-vqa-base](https://huggingface.co/Salesforce/blip-vqa-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "1d4e38cd1189c9f37ef1434e61c676a8c1b85ee1",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,7 +16,7 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -25,15 +25,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class Blip2VisionConfig(PretrainedConfig):\n+class Blip2VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Blip2VisionModel`]. It is used to instantiate a\n     BLIP-2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration defaults will yield a similar configuration to that of the BLIP-2\n     [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1408):\n@@ -107,14 +107,14 @@ def __init__(\n         self.qkv_bias = qkv_bias\n \n \n-class Blip2QFormerConfig(PretrainedConfig):\n+class Blip2QFormerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Blip2QFormerModel`]. It is used to instantiate a\n     BLIP-2 Querying Transformer (Q-Former) model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the BLIP-2\n     [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b) architecture. Configuration objects\n-    inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the documentation from\n-    [`PretrainedConfig`] for more information.\n+    inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the documentation from\n+    [`PreTrainedConfig`] for more information.\n \n     Note that [`Blip2QFormerModel`] is very similar to [`BertLMHeadModel`] with interleaved cross-attention.\n \n@@ -215,23 +215,23 @@ def __init__(\n         self.use_qformer_text_input = use_qformer_text_input\n \n \n-class Blip2Config(PretrainedConfig):\n+class Blip2Config(PreTrainedConfig):\n     r\"\"\"\n     [`Blip2Config`] is the configuration class to store the configuration of a [`Blip2ForConditionalGeneration`]. It is\n     used to instantiate a BLIP-2 model according to the specified arguments, defining the vision model, Q-Former model\n     and language model configs. Instantiating a configuration with the defaults will yield a similar configuration to\n     that of the BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`dict`, *optional*):\n             Dictionary of configuration options used to initialize [`Blip2VisionConfig`].\n         qformer_config (`dict`, *optional*):\n             Dictionary of configuration options used to initialize [`Blip2QFormerConfig`].\n         text_config (`dict`, *optional*):\n-            Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n+            Dictionary of configuration options used to initialize any [`PreTrainedConfig`].\n         num_query_tokens (`int`, *optional*, defaults to 32):\n             The number of query tokens passed through the Transformer.\n         image_text_hidden_size (`int`, *optional*, defaults to 256):\n@@ -262,7 +262,7 @@ class Blip2Config(PretrainedConfig):\n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n \n-    >>> # We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PretrainedConfig\n+    >>> # We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PreTrainedConfig\n \n     >>> # Initializing BLIP-2 vision, BLIP-2 Q-Former and language model configurations\n     >>> vision_config = Blip2VisionConfig()\n@@ -321,7 +321,7 @@ def from_vision_qformer_text_configs(\n         cls,\n         vision_config: Blip2VisionConfig,\n         qformer_config: Blip2QFormerConfig,\n-        text_config: Optional[PretrainedConfig] = None,\n+        text_config: Optional[PreTrainedConfig] = None,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -334,7 +334,7 @@ def from_vision_qformer_text_configs(\n             qformer_config (`dict`):\n                 Dictionary of configuration options used to initialize [`Blip2QFormerConfig`].\n             text_config (`dict`, *optional*):\n-                Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n+                Dictionary of configuration options used to initialize any [`PreTrainedConfig`].\n \n         Returns:\n             [`Blip2Config`]: An instance of a configuration object"
        },
        {
            "sha": "b963a5ebe56a719ee983e202e23944f57ab52ba2",
            "filename": "src/transformers/models/bloom/configuration_bloom.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -24,23 +24,23 @@\n if TYPE_CHECKING:\n     from ... import PreTrainedTokenizer\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfigWithPast, PatchingSpec\n from ...utils import is_torch_available, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BloomConfig(PretrainedConfig):\n+class BloomConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`BloomModel`]. It is used to instantiate a Bloom\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to the Bloom architecture\n     [bigscience/bloom](https://huggingface.co/bigscience/bloom).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -147,7 +147,7 @@ class BloomOnnxConfig(OnnxConfigWithPast):\n \n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         task: str = \"default\",\n         patching_specs: Optional[list[PatchingSpec]] = None,\n         use_past: bool = False,"
        },
        {
            "sha": "35c087cc82bac4217581d943ee4a22dc80323d55",
            "filename": "src/transformers/models/blt/configuration_blt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,14 +14,14 @@\n # limitations under the License.\n \"\"\"Blt model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BltLocalEncoderConfig(PretrainedConfig):\n+class BltLocalEncoderConfig(PreTrainedConfig):\n     \"\"\"\n     Configuration class for the Blt Local Encoder component.\n     \"\"\"\n@@ -71,7 +71,7 @@ def __init__(\n         super().__init__(**kwargs, tie_word_embeddings=False)\n \n \n-class BltLocalDecoderConfig(PretrainedConfig):\n+class BltLocalDecoderConfig(PreTrainedConfig):\n     \"\"\"\n     Configuration class for the Blt Local Decoder component.\n     \"\"\"\n@@ -121,7 +121,7 @@ def __init__(\n         super().__init__(**kwargs, tie_word_embeddings=False)\n \n \n-class BltGlobalTransformerConfig(PretrainedConfig):\n+class BltGlobalTransformerConfig(PreTrainedConfig):\n     \"\"\"\n     Configuration class for the Blt Global Transformer component.\n     \"\"\"\n@@ -163,7 +163,7 @@ def __init__(\n         super().__init__(**kwargs, tie_word_embeddings=False)\n \n \n-class BltPatcherConfig(PretrainedConfig):\n+class BltPatcherConfig(PreTrainedConfig):\n     r\"\"\"\n     Configuration class for the Blt Patcher/Entropy model component.\n \n@@ -239,13 +239,13 @@ def __init__(\n         super().__init__(**kwargs, tie_word_embeddings=False)\n \n \n-class BltConfig(PretrainedConfig):\n+class BltConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BltModel`]. It is used to instantiate a\n     Blt model according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n             vocab_size (`int`, *optional*, defaults to 260):"
        },
        {
            "sha": "f2ae05304e476e686043fcce14272a816c495a7f",
            "filename": "src/transformers/models/bridgetower/configuration_bridgetower.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,21 +14,21 @@\n # limitations under the License.\n \"\"\"BridgeTower model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BridgeTowerVisionConfig(PretrainedConfig):\n+class BridgeTowerVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the vision configuration of a [`BridgeTowerModel`]. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the bridgetower-base\n     [BridgeTower/bridgetower-base](https://huggingface.co/BridgeTower/bridgetower-base/) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):\n@@ -94,15 +94,15 @@ def __init__(\n         self.remove_last_layer = remove_last_layer\n \n \n-class BridgeTowerTextConfig(PretrainedConfig):\n+class BridgeTowerTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the text configuration of a [`BridgeTowerModel`]. The default values here\n     are copied from RoBERTa. Instantiating a configuration with the defaults will yield a similar configuration to that\n     of the bridgetower-base [BridegTower/bridgetower-base](https://huggingface.co/BridgeTower/bridgetower-base/)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):\n@@ -202,15 +202,15 @@ def __init__(\n         self.eos_token_id = eos_token_id\n \n \n-class BridgeTowerConfig(PretrainedConfig):\n+class BridgeTowerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BridgeTowerModel`]. It is used to instantiate a\n     BridgeTower model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the bridgetower-base\n     [BridgeTower/bridgetower-base](https://huggingface.co/BridgeTower/bridgetower-base/) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         share_cross_modal_transformer_layers (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "d6dbee9b3247a83a603ab0cf2cf1ee6193366547",
            "filename": "src/transformers/models/bros/configuration_bros.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbros%2Fconfiguration_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fbros%2Fconfiguration_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fconfiguration_bros.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"Bros model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class BrosConfig(PretrainedConfig):\n+class BrosConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`BrosModel`] or a [`TFBrosModel`]. It is used to\n     instantiate a Bros model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Bros\n     [jinho8345/bros-base-uncased](https://huggingface.co/jinho8345/bros-base-uncased) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):"
        },
        {
            "sha": "44008254c0d79427b4890ce8748db24a441bdb3e",
            "filename": "src/transformers/models/camembert/configuration_camembert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,23 +18,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class CamembertConfig(PretrainedConfig):\n+class CamembertConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`CamembertModel`] or a [`TFCamembertModel`]. It is\n     used to instantiate a Camembert model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the Camembert\n     [almanach/camembert-base](https://huggingface.co/almanach/camembert-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "13aded9a1b2ada3aa7e5efbf73e4690c48ce2816",
            "filename": "src/transformers/models/canine/configuration_canine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcanine%2Fconfiguration_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcanine%2Fconfiguration_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fconfiguration_canine.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"CANINE model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class CanineConfig(PretrainedConfig):\n+class CanineConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CanineModel`]. It is used to instantiate an\n     CANINE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the CANINE\n     [google/canine-s](https://huggingface.co/google/canine-s) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "dc75c1730434dad98058bef73b960b47db261947",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,19 +16,19 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ChameleonVQVAEConfig(PretrainedConfig):\n+class ChameleonVQVAEConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ChameleonVQModel`]. It is used to instantiate a\n     `ChameleonVQModel` according to the specified arguments, defining the model architecture.\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information. Instantiating a\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information. Instantiating a\n     configuration with the defaults will yield a similar configuration to the VQModel of the\n     [meta/chameleon-7B](https://huggingface.co/meta/chameleon-7B).\n \n@@ -97,15 +97,15 @@ def __init__(\n         self.initializer_range = initializer_range\n \n \n-class ChameleonConfig(PretrainedConfig):\n+class ChameleonConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ChameleonModel`]. It is used to instantiate a\n     chameleon model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the\n     [meta/chameleon-7B](https://huggingface.co/meta/chameleon-7B).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "68631dd18a3c63d188a8f103f58965deb6ff9a79",
            "filename": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -22,24 +22,24 @@\n if TYPE_CHECKING:\n     from ...processing_utils import ProcessorMixin\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ChineseCLIPTextConfig(PretrainedConfig):\n+class ChineseCLIPTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ChineseCLIPModel`]. It is used to instantiate a\n     Chinese CLIP model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Chinese CLIP\n     [OFA-Sys/chinese-clip-vit-base-patch16](https:\n         //huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -142,15 +142,15 @@ def __init__(\n         self.use_cache = use_cache\n \n \n-class ChineseCLIPVisionConfig(PretrainedConfig):\n+class ChineseCLIPVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ChineseCLIPModel`]. It is used to instantiate an\n     ChineseCLIP model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the ChineseCLIP\n     [OFA-Sys/chinese-clip-vit-base-patch16](https://huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -233,16 +233,16 @@ def __init__(\n         self.hidden_act = hidden_act\n \n \n-class ChineseCLIPConfig(PretrainedConfig):\n+class ChineseCLIPConfig(PreTrainedConfig):\n     r\"\"\"\n     [`ChineseCLIPConfig`] is the configuration class to store the configuration of a [`ChineseCLIPModel`]. It is used\n     to instantiate Chinese-CLIP model according to the specified arguments, defining the text model and vision model\n     configs. Instantiating a configuration with the defaults will yield a similar configuration to that of the\n     Chinese-CLIP [OFA-Sys/chinese-clip-vit-base-patch16](https://huggingface.co/OFA-Sys/chinese-clip-vit-base-patch16)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "029b93a67f89c4408ad9fa55a1705ff1cb71a635",
            "filename": "src/transformers/models/clap/configuration_clap.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"CLAP model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ClapTextConfig(PretrainedConfig):\n+class ClapTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ClapTextModel`]. It is used to instantiate a CLAP\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the CLAP\n     [calp-hsat-fused](https://huggingface.co/laion/clap-hsat-fused) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -136,15 +136,15 @@ def __init__(\n         self.projection_dim = projection_dim\n \n \n-class ClapAudioConfig(PretrainedConfig):\n+class ClapAudioConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ClapAudioModel`]. It is used to instantiate a\n     CLAP audio encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the audio encoder of the CLAP\n     [laion/clap-htsat-fused](https://huggingface.co/laion/clap-htsat-fused) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         window_size (`int`, *optional*, defaults to 8):\n@@ -289,15 +289,15 @@ def __init__(\n         self.projection_hidden_act = projection_hidden_act\n \n \n-class ClapConfig(PretrainedConfig):\n+class ClapConfig(PreTrainedConfig):\n     r\"\"\"\n     [`ClapConfig`] is the configuration class to store the configuration of a [`ClapModel`]. It is used to instantiate\n     a CLAP model according to the specified arguments, defining the text model and audio model configs. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the CLAP\n     [laion/clap-htsat-fused](https://huggingface.co/laion/clap-htsat-fused) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "84dbbb6ac7a0bbb5226405e3c2ba9912189449a9",
            "filename": "src/transformers/models/clip/configuration_clip.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -22,23 +22,23 @@\n if TYPE_CHECKING:\n     from ...processing_utils import ProcessorMixin\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class CLIPTextConfig(PretrainedConfig):\n+class CLIPTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CLIPTextModel`]. It is used to instantiate a CLIP\n     text encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the text encoder of the CLIP\n     [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 49408):\n@@ -131,15 +131,15 @@ def __init__(\n         self.attention_dropout = attention_dropout\n \n \n-class CLIPVisionConfig(PretrainedConfig):\n+class CLIPVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CLIPVisionModel`]. It is used to instantiate a\n     CLIP vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the vision encoder of the CLIP\n     [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):\n@@ -223,15 +223,15 @@ def __init__(\n         self.hidden_act = hidden_act\n \n \n-class CLIPConfig(PretrainedConfig):\n+class CLIPConfig(PreTrainedConfig):\n     r\"\"\"\n     [`CLIPConfig`] is the configuration class to store the configuration of a [`CLIPModel`]. It is used to instantiate\n     a CLIP model according to the specified arguments, defining the text model and vision model configs. Instantiating\n     a configuration with the defaults will yield a similar configuration to that of the CLIP\n     [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "74345502e419c5a7a8634d99820f88473c473f6c",
            "filename": "src/transformers/models/clipseg/configuration_clipseg.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"CLIPSeg model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class CLIPSegTextConfig(PretrainedConfig):\n+class CLIPSegTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CLIPSegModel`]. It is used to instantiate an\n     CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the CLIPSeg\n     [CIDAS/clipseg-rd64](https://huggingface.co/CIDAS/clipseg-rd64) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 49408):\n@@ -116,15 +116,15 @@ def __init__(\n         self.attention_dropout = attention_dropout\n \n \n-class CLIPSegVisionConfig(PretrainedConfig):\n+class CLIPSegVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CLIPSegModel`]. It is used to instantiate an\n     CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the CLIPSeg\n     [CIDAS/clipseg-rd64](https://huggingface.co/CIDAS/clipseg-rd64) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):\n@@ -204,15 +204,15 @@ def __init__(\n         self.hidden_act = hidden_act\n \n \n-class CLIPSegConfig(PretrainedConfig):\n+class CLIPSegConfig(PreTrainedConfig):\n     r\"\"\"\n     [`CLIPSegConfig`] is the configuration class to store the configuration of a [`CLIPSegModel`]. It is used to\n     instantiate a CLIPSeg model according to the specified arguments, defining the text model and vision model configs.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the CLIPSeg\n     [CIDAS/clipseg-rd64](https://huggingface.co/CIDAS/clipseg-rd64) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "6fac97a4012268fb99542e33fab53f0585831490",
            "filename": "src/transformers/models/clvp/configuration_clvp.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,22 +17,22 @@\n import os\n from typing import Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ClvpEncoderConfig(PretrainedConfig):\n+class ClvpEncoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ClvpEncoder`]. It is used to instantiate a CLVP\n     text or CLVP speech encoder according to the specified arguments. Instantiating a configuration with the defaults\n     will yield a similar configuration to that of the encoder of the CLVP\n     [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 256):\n@@ -156,15 +156,15 @@ def from_pretrained(\n         return cls.from_dict(config_dict, **kwargs)\n \n \n-class ClvpDecoderConfig(PretrainedConfig):\n+class ClvpDecoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ClvpDecoder`]. It is used to instantiate a CLVP\n     Decoder Model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Decoder part of the CLVP\n     [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     The architecture is similar to GPT2.\n \n@@ -313,15 +313,15 @@ def __init__(\n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n \n \n-class ClvpConfig(PretrainedConfig):\n+class ClvpConfig(PreTrainedConfig):\n     r\"\"\"\n     [`ClvpConfig`] is the configuration class to store the configuration of a [`ClvpModelForConditionalGeneration`]. It\n     is used to instantiate a CLVP model according to the specified arguments, defining the text model, speech model and\n     decoder model configs. Instantiating a configuration with the defaults will yield a similar configuration to that\n     of the CLVP [susnato/clvp_dev](https://huggingface.co/susnato/clvp_dev) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "7c78672959d8e7c6fb645ed78ecf14f3d928750a",
            "filename": "src/transformers/models/codegen/configuration_codegen.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,22 +19,22 @@\n from typing import Any, Optional\n \n from ... import PreTrainedTokenizer, is_torch_available\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfigWithPast, PatchingSpec\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class CodeGenConfig(PretrainedConfig):\n+class CodeGenConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CodeGenModel`]. It is used to instantiate a\n     CodeGen model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the CodeGen\n     [Salesforce/codegen-2B-mono](https://huggingface.co/Salesforce/codegen-2B-mono) architecture. Configuration objects\n-    inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the documentation from\n-    [`PretrainedConfig`] for more information.\n+    inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the documentation from\n+    [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 50400):\n@@ -150,7 +150,7 @@ def __init__(\n class CodeGenOnnxConfig(OnnxConfigWithPast):\n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         task: str = \"default\",\n         patching_specs: Optional[list[PatchingSpec]] = None,\n         use_past: bool = False,"
        },
        {
            "sha": "00e97e42533263af1c07829cc0fd23ee9cdefb8d",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,21 +19,21 @@\n # limitations under the License.\n \"\"\"Cohere model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class CohereConfig(PretrainedConfig):\n+class CohereConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CohereModel`]. It is used to instantiate an Cohere\n     model according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information. Instantiating a configuration\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the [CohereForAI/c4ai-command-r-v01](https://huggingface.co/CohereForAI/c4ai-command-r-v01) model.\n \n "
        },
        {
            "sha": "bfd00c5e853044d003d4c3e5819e9dde16f40bcb",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,17 +19,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class Cohere2Config(PretrainedConfig):\n+class Cohere2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CohereModel`]. It is used to instantiate an Cohere\n     model according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information. Instantiating a configuration\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the [CohereForAI/c4ai-command-r-v01](https://huggingface.co/CohereForAI/c4ai-command-r-v01) model.\n \n "
        },
        {
            "sha": "16c8b23d8bf72bafadd87ab3251ff6d90086c6f3",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n import torch.nn as nn\n \n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n@@ -44,13 +44,13 @@\n logger = logging.get_logger(__name__)\n \n \n-class Cohere2Config(PretrainedConfig):\n+class Cohere2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CohereModel`]. It is used to instantiate an Cohere\n     model according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information. Instantiating a configuration\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the [CohereForAI/c4ai-command-r-v01](https://huggingface.co/CohereForAI/c4ai-command-r-v01) model.\n \n "
        },
        {
            "sha": "688da89165c97a87c7d41cf8313571728c07a3e9",
            "filename": "src/transformers/models/cohere2_vision/configuration_cohere2_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fconfiguration_cohere2_vision.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -12,19 +12,19 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n-class Cohere2VisionConfig(PretrainedConfig):\n+class Cohere2VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Cohere2VisionForConditionalGeneration`]. It is used to instantiate an\n     Cohere2 Vision model according to the specified arguments, defining the model architecture.\n \n     [CohereLabs/command-a-vision-07-2025](https://huggingface.co/CohereLabs/command-a-vision-07-2025)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):"
        },
        {
            "sha": "bcdbca3610ed4ac2ec5c5cb0b7a3a4687e3d545d",
            "filename": "src/transformers/models/colpali/configuration_colpali.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconfiguration_colpali.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,14 +17,14 @@\n import logging\n from copy import deepcopy\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.getLogger(__name__)\n \n \n-class ColPaliConfig(PretrainedConfig):\n+class ColPaliConfig(PreTrainedConfig):\n     r\"\"\"\n     Configuration class to store the configuration of a [`ColPaliForRetrieval`]. It is used to instantiate an instance\n     of `ColPaliForRetrieval` according to the specified arguments, defining the model architecture following the methodology\n@@ -36,13 +36,13 @@ class ColPaliConfig(PretrainedConfig):\n     Note that contrarily to what the class name suggests (actually the name refers to the ColPali **methodology**), you can\n     use a different VLM backbone model than PaliGemma by passing the corresponding VLM configuration to the class constructor.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        vlm_config (`PretrainedConfig`, *optional*):\n+        vlm_config (`PreTrainedConfig`, *optional*):\n             Configuration of the VLM backbone model.\n-        text_config (`PretrainedConfig`, *optional*):\n+        text_config (`PreTrainedConfig`, *optional*):\n             Configuration of the text backbone model. Overrides the `text_config` attribute of the `vlm_config` if provided.\n         embedding_dim (`int`, *optional*, defaults to 128):\n             Dimension of the multi-vector embeddings produced by the model.\n@@ -58,7 +58,7 @@ class ColPaliConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"colpali\"\n-    sub_configs = {\"vlm_config\": PretrainedConfig, \"text_config\": AutoConfig}\n+    sub_configs = {\"vlm_config\": PreTrainedConfig, \"text_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -83,9 +83,9 @@ def __init__(\n                     f\"The model type `{vlm_config['model_type']}` is not supported. Please provide a valid model type.\"\n                 )\n             vlm_config = CONFIG_MAPPING[vlm_config[\"model_type\"]](**vlm_config)\n-        elif not isinstance(vlm_config, PretrainedConfig):\n+        elif not isinstance(vlm_config, PreTrainedConfig):\n             raise TypeError(\n-                f\"Invalid type for `vlm_config`. Expected `PretrainedConfig`, `dict`, or `None`, but got {type(vlm_config)}.\"\n+                f\"Invalid type for `vlm_config`. Expected `PreTrainedConfig`, `dict`, or `None`, but got {type(vlm_config)}.\"\n             )\n \n         self.vlm_config = vlm_config"
        },
        {
            "sha": "bc5256fa5ff2a08ceaa64344f4f133f37a240b9a",
            "filename": "src/transformers/models/colqwen2/configuration_colqwen2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,15 +16,15 @@\n from copy import deepcopy\n from typing import Any\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ColQwen2Config(PretrainedConfig):\n+class ColQwen2Config(PreTrainedConfig):\n     r\"\"\"\n     Configuration class to store the configuration of a [`ColQ2en2ForRetrieval`]. It is used to instantiate an instance\n     of `ColQwen2ForRetrieval` according to the specified arguments, defining the model architecture following the methodology\n@@ -33,11 +33,11 @@ class ColQwen2Config(PretrainedConfig):\n     Instantiating a configuration with the defaults will yield a similar configuration to the vision encoder used by the pre-trained\n     ColQwen2-v1.0 model, e.g. [vidore/colqwen2-v1.0-hf](https://huggingface.co/vidore/colqwen2-v1.0-hf).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        vlm_config (`PretrainedConfig`, *optional*):\n+        vlm_config (`PreTrainedConfig`, *optional*):\n             Configuration of the VLM backbone model.\n         embedding_dim (`int`, *optional*, defaults to 128):\n             Dimension of the multi-vector embeddings produced by the model.\n@@ -54,7 +54,7 @@ class ColQwen2Config(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"colqwen2\"\n-    sub_configs: dict[str, Any] = {\"vlm_config\": PretrainedConfig}\n+    sub_configs: dict[str, Any] = {\"vlm_config\": PreTrainedConfig}\n \n     def __init__(\n         self,\n@@ -75,17 +75,17 @@ def __init__(\n                     \"The `model_type` key is missing in the `vlm_config` dictionary. Please provide the model type.\"\n                 )\n             vlm_config = CONFIG_MAPPING[vlm_config[\"model_type\"]](**vlm_config)\n-        elif not isinstance(vlm_config, PretrainedConfig):\n+        elif not isinstance(vlm_config, PreTrainedConfig):\n             raise TypeError(\n-                f\"Invalid type for `vlm_config`. Expected `PretrainedConfig`, `dict`, or `None`, but got {type(vlm_config)}.\"\n+                f\"Invalid type for `vlm_config`. Expected `PreTrainedConfig`, `dict`, or `None`, but got {type(vlm_config)}.\"\n             )\n \n         self.vlm_config = vlm_config\n         self.embedding_dim = embedding_dim\n         self.initializer_range = initializer_range\n         super().__init__(**kwargs)\n \n-    def get_text_config(self, *args, **kwargs) -> PretrainedConfig:\n+    def get_text_config(self, *args, **kwargs) -> PreTrainedConfig:\n         return self.vlm_config.get_text_config(*args, **kwargs)\n \n "
        },
        {
            "sha": "3b794d73195f972fb8e98a5d41323edddf32f5b1",
            "filename": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n@@ -29,21 +29,21 @@\n logger = logging.get_logger(__name__)\n \n \n-class ConditionalDetrConfig(PretrainedConfig):\n+class ConditionalDetrConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ConditionalDetrModel`]. It is used to instantiate\n     a Conditional DETR model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Conditional DETR\n     [microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n             API.\n-        backbone_config (`PretrainedConfig` or `dict`, *optional*):\n+        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n             The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n             case it will default to `ResNetConfig()`.\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "397f1aa66292965b65766ccae2f6e6a377517d81",
            "filename": "src/transformers/models/convbert/configuration_convbert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,23 +17,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ConvBertConfig(PretrainedConfig):\n+class ConvBertConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ConvBertModel`]. It is used to instantiate an\n     ConvBERT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the ConvBERT\n     [YituTech/conv-bert-base](https://huggingface.co/YituTech/conv-bert-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "034fe648cdd9e351dc8aff0491f94ef57b27c6c3",
            "filename": "src/transformers/models/convnext/configuration_convnext.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n@@ -28,15 +28,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class ConvNextConfig(BackboneConfigMixin, PretrainedConfig):\n+class ConvNextConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ConvNextModel`]. It is used to instantiate an\n     ConvNeXT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the ConvNeXT\n     [facebook/convnext-tiny-224](https://huggingface.co/facebook/convnext-tiny-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "99c55fedcc45580189fe818812033fca65174176",
            "filename": "src/transformers/models/convnextv2/configuration_convnextv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconfiguration_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconfiguration_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconfiguration_convnextv2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"ConvNeXTV2 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ConvNextV2Config(BackboneConfigMixin, PretrainedConfig):\n+class ConvNextV2Config(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ConvNextV2Model`]. It is used to instantiate an\n     ConvNeXTV2 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the ConvNeXTV2\n     [facebook/convnextv2-tiny-1k-224](https://huggingface.co/facebook/convnextv2-tiny-1k-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "db210ab579726fd67d7abb64791514a341eea9b8",
            "filename": "src/transformers/models/cpmant/configuration_cpmant.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcpmant%2Fconfiguration_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcpmant%2Fconfiguration_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fconfiguration_cpmant.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"CPMAnt model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class CpmAntConfig(PretrainedConfig):\n+class CpmAntConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CpmAntModel`]. It is used to instantiate an\n     CPMAnt model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the CPMAnt\n     [openbmb/cpm-ant-10b](https://huggingface.co/openbmb/cpm-ant-10b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30720):"
        },
        {
            "sha": "5771fa57314c88057cd8bafad62a6a56643fbd8a",
            "filename": "src/transformers/models/csm/configuration_csm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n from ..auto.configuration_auto import AutoConfig\n@@ -22,16 +22,16 @@\n logger = logging.get_logger(__name__)\n \n \n-class CsmDepthDecoderConfig(PretrainedConfig):\n+class CsmDepthDecoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CsmDepthDecoderModel`]. It is used to instantiate an CSM depth decoder\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield\n     a similar configuration to that of the csm-1b.\n \n     e.g. [sesame/csm-1b](https://huggingface.co/sesame/csm-1b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -204,16 +204,16 @@ def __init__(\n         rope_config_validation(self)\n \n \n-class CsmConfig(PretrainedConfig):\n+class CsmConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CsmForConditionalGeneration`]. It is used to instantiate an CSM\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the csm-1b.\n \n     e.g. [sesame/csm-1b](https://huggingface.co/sesame/csm-1b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_codebooks (`int`, *optional*, defaults to 32):\n@@ -313,7 +313,7 @@ class CsmConfig(PretrainedConfig):\n             Whether to tie the codebook tokens embeddings of the backbone model to the codebook tokens embeddings of the depth decoder.\n         depth_decoder_config (`CsmDepthDecoderConfig`, *optional*):\n             Configuration for the depth decoder.\n-        codec_config (`PretrainedConfig`, *optional*):\n+        codec_config (`PreTrainedConfig`, *optional*):\n             Configuration for the codec.\n \n     ```python\n@@ -394,7 +394,7 @@ def __init__(\n             logger.info(\"codec_config is None, using default audio encoder config.\")\n         elif isinstance(codec_config, dict):\n             self.codec_config = AutoConfig.for_model(**codec_config)\n-        elif isinstance(codec_config, PretrainedConfig):\n+        elif isinstance(codec_config, PreTrainedConfig):\n             self.codec_config = codec_config\n \n         self.text_vocab_size = text_vocab_size"
        },
        {
            "sha": "89257003ef3faaa134e8e54bc14e822d8752e4ea",
            "filename": "src/transformers/models/ctrl/configuration_ctrl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fctrl%2Fconfiguration_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fctrl%2Fconfiguration_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fconfiguration_ctrl.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"Salesforce CTRL configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class CTRLConfig(PretrainedConfig):\n+class CTRLConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`CTRLModel`] or a [`TFCTRLModel`]. It is used to\n     instantiate a CTRL model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the\n     [Salesforce/ctrl](https://huggingface.co/Salesforce/ctrl) architecture from SalesForce.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 246534):"
        },
        {
            "sha": "453c93ad0d9ea38ad03725a2f3e4f012a2f8d70a",
            "filename": "src/transformers/models/cvt/configuration_cvt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcvt%2Fconfiguration_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fcvt%2Fconfiguration_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fconfiguration_cvt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"CvT model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class CvtConfig(PretrainedConfig):\n+class CvtConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`CvtModel`]. It is used to instantiate a CvT model\n     according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the CvT\n     [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "beb3cf6c0563719c3be80f99e264595cbacf3fbf",
            "filename": "src/transformers/models/d_fine/configuration_d_fine.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,7 +18,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n from ..auto import CONFIG_MAPPING\n@@ -29,13 +29,13 @@\n \n # TODO: Attribute map assignment logic should be fixed in modular\n # as well as super() call parsing because otherwise we cannot re-write args after initialization\n-class DFineConfig(PretrainedConfig):\n+class DFineConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`DFineModel`]. It is used to instantiate a D-FINE\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of D-FINE-X-COCO \"[ustc-community/dfine-xlarge-coco\"](https://huggingface.co/ustc-community/dfine-xlarge-coco\").\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         initializer_range (`float`, *optional*, defaults to 0.01):\n@@ -413,12 +413,12 @@ def sub_configs(self):\n         )\n \n     @classmethod\n-    def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n+    def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n         configuration.\n \n             Args:\n-                backbone_config ([`PretrainedConfig`]):\n+                backbone_config ([`PreTrainedConfig`]):\n                     The backbone configuration.\n \n             Returns:"
        },
        {
            "sha": "a7d9f241e71e90af1221aa8e72511d1ad0489016",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -21,7 +21,7 @@\n from torch import nn\n \n from ...activations import ACT2CLS\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...image_transforms import corners_to_center_format\n from ...utils import is_torchdynamo_compiling, logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n@@ -48,13 +48,13 @@\n \n # TODO: Attribute map assignment logic should be fixed in modular\n # as well as super() call parsing because otherwise we cannot re-write args after initialization\n-class DFineConfig(PretrainedConfig):\n+class DFineConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`DFineModel`]. It is used to instantiate a D-FINE\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of D-FINE-X-COCO \"[ustc-community/dfine-xlarge-coco\"](https://huggingface.co/ustc-community/dfine-xlarge-coco\").\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         initializer_range (`float`, *optional*, defaults to 0.01):\n@@ -432,12 +432,12 @@ def sub_configs(self):\n         )\n \n     @classmethod\n-    def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n+    def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n         configuration.\n \n             Args:\n-                backbone_config ([`PretrainedConfig`]):\n+                backbone_config ([`PreTrainedConfig`]):\n                     The backbone configuration.\n \n             Returns:"
        },
        {
            "sha": "a5116765f91e7fd5c1eff37fec9e7c51ad34060a",
            "filename": "src/transformers/models/dab_detr/configuration_dab_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"DAB-DETR model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n from ..auto import CONFIG_MAPPING\n@@ -23,21 +23,21 @@\n logger = logging.get_logger(__name__)\n \n \n-class DabDetrConfig(PretrainedConfig):\n+class DabDetrConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DabDetrModel`]. It is used to instantiate\n     a DAB-DETR model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the DAB-DETR\n     [IDEA-Research/dab_detr-base](https://huggingface.co/IDEA-Research/dab_detr-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n             API.\n-        backbone_config (`PretrainedConfig` or `dict`, *optional*):\n+        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n             The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n             case it will default to `ResNetConfig()`.\n         backbone (`str`, *optional*, defaults to `\"resnet50\"`):"
        },
        {
            "sha": "b7a2d50a3b775094ddacd65aa345f243731c056e",
            "filename": "src/transformers/models/dac/configuration_dac.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdac%2Fconfiguration_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdac%2Fconfiguration_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fconfiguration_dac.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,22 +18,22 @@\n \n import numpy as np\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DacConfig(PretrainedConfig):\n+class DacConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of an [`DacModel`]. It is used to instantiate a\n     Dac model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the\n     [descript/dac_16khz](https://huggingface.co/descript/dac_16khz) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         encoder_hidden_size (`int`, *optional*, defaults to 64):"
        },
        {
            "sha": "d9283bae130e6c48fc8026036f028c812cd8af05",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_audio.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,22 +16,22 @@\n \n import math\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Data2VecAudioConfig(PretrainedConfig):\n+class Data2VecAudioConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Data2VecAudioModel`]. It is used to instantiate\n     an Data2VecAudio model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Data2VecAudio\n     [facebook/data2vec-audio-base-960h](https://huggingface.co/facebook/data2vec-audio-base-960h) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "938b70f12fe352d4fbf533ad6f6e7a8242b0e4eb",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,23 +17,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Data2VecTextConfig(PretrainedConfig):\n+class Data2VecTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Data2VecTextModel`] and [`Data2VecTextModel`]. It\n     is used to instantiate a Data2VecText model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the Data2VecText\n     [facebook/data2vec-text-base](https://huggingface.co/facebook/data2vec-text-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "970da97e284c6673153dbb55278504a892106799",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_vision.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,15 +19,15 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Data2VecVisionConfig(PretrainedConfig):\n+class Data2VecVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Data2VecVisionModel`]. It is used to instantiate\n     an Data2VecVision model according to the specified arguments, defining the model architecture. Instantiating a"
        },
        {
            "sha": "bb4888f3ee8c69b8e5c0b84946a3b81b0af1b3cb",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,21 +16,21 @@\n \n from typing import Any, Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DbrxAttentionConfig(PretrainedConfig):\n+class DbrxAttentionConfig(PreTrainedConfig):\n     \"\"\"Configuration class for Dbrx Attention.\n \n     [`DbrxAttention`] class. It is used to instantiate attention layers\n     according to the specified arguments, defining the layers architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         attn_pdrop (`float`, *optional*, defaults to 0.0):\n@@ -56,14 +56,14 @@ def __init__(\n         self.kv_n_heads = kv_n_heads\n \n \n-class DbrxFFNConfig(PretrainedConfig):\n+class DbrxFFNConfig(PreTrainedConfig):\n     \"\"\"Configuration class for Dbrx FFN.\n \n     [`DbrxFFN`] class. It is used to instantiate feedforward layers according to\n     the specified arguments, defining the layers architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         ffn_act_fn (`dict`, *optional*, defaults to `None`): A dict specifying activation function for the FFN.\n@@ -110,15 +110,15 @@ def __init__(\n             raise ValueError(f\"Found unknown {kwargs=}\")\n \n \n-class DbrxConfig(PretrainedConfig):\n+class DbrxConfig(PreTrainedConfig):\n     r\"\"\"\n \n     This is the configuration class to store the configuration of a [`DbrxModel`]. It is used to instantiate a Dbrx model according to the\n     specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a different configuration to that of the [databricks/dbrx-instruct](https://huggingface.co/databricks/dbrx-instruct) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "768085916fc72ffedaac086c45cd21536adc29aa",
            "filename": "src/transformers/models/deberta/configuration_deberta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,7 +18,7 @@\n from collections.abc import Mapping\n from typing import TYPE_CHECKING, Any, Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n@@ -30,15 +30,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class DebertaConfig(PretrainedConfig):\n+class DebertaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DebertaModel`] or a [`TFDebertaModel`]. It is\n     used to instantiate a DeBERTa model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the DeBERTa\n     [microsoft/deberta-base](https://huggingface.co/microsoft/deberta-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Arguments:\n         vocab_size (`int`, *optional*, defaults to 50265):"
        },
        {
            "sha": "055c0e176a0f676db9345dfbb33fb23c5de4b3c2",
            "filename": "src/transformers/models/deberta_v2/configuration_deberta_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,7 +18,7 @@\n from collections.abc import Mapping\n from typing import TYPE_CHECKING, Any, Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n@@ -30,15 +30,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class DebertaV2Config(PretrainedConfig):\n+class DebertaV2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DebertaV2Model`]. It is used to instantiate a\n     DeBERTa-v2 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the DeBERTa\n     [microsoft/deberta-v2-xlarge](https://huggingface.co/microsoft/deberta-v2-xlarge) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Arguments:\n         vocab_size (`int`, *optional*, defaults to 128100):"
        },
        {
            "sha": "12268319991cba1687309d54a53cdcd9cc995f84",
            "filename": "src/transformers/models/decision_transformer/configuration_decision_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fconfiguration_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fconfiguration_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fconfiguration_decision_transformer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"Decision Transformer model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DecisionTransformerConfig(PretrainedConfig):\n+class DecisionTransformerConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`DecisionTransformerModel`]. It is used to\n     instantiate a Decision Transformer model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the standard\n     DecisionTransformer architecture. Many of the config options are used to instantiate the GPT2 model that is used as\n     part of the architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "32401bcb8a6cbdcc29033f586e8e701f3d568889",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,17 +20,17 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class DeepseekV2Config(PretrainedConfig):\n+class DeepseekV2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DeepseekV2Model`]. It is used to instantiate a DeepSeek\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of DeepSeek-V2-Lite\" [deepseek-ai/DeepSeek-V2-Lite\"](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite\").\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 32000):"
        },
        {
            "sha": "af8c9417965351ab9398cc302d066d5af067353b",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -49,8 +49,8 @@ class DeepseekV2Config(LlamaConfig):\n     This is the configuration class to store the configuration of a [`DeepseekV2Model`]. It is used to instantiate a DeepSeek\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of DeepSeek-V2-Lite\" [deepseek-ai/DeepSeek-V2-Lite\"](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite\").\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 32000):"
        },
        {
            "sha": "1469ae71861a4b8e5d314860d261c832068bff44",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,21 +16,21 @@\n # limitations under the License.\n \"\"\"DeepSeekV3 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n DEEPSEEK_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n \n \n-class DeepseekV3Config(PretrainedConfig):\n+class DeepseekV3Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DeepseekV3Model`]. It is used to instantiate an DeepSeek\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the DeepSeek-V3.\n     e.g. [bzantium/tiny-deepseek-v3](https://huggingface.co/bzantium/tiny-deepseek-v3)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "4fe3a5e4d82c405213be561c8b0d86ea61b54417",
            "filename": "src/transformers/models/deepseek_vl/configuration_deepseek_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,23 +20,23 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DeepseekVLConfig(PretrainedConfig):\n+class DeepseekVLConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DeepseekVLModel`]. It is used to instantiate a\n     DeepseekVL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the DeepseekVL\n     [deepseek-community/deepseek-vl-1.3b-chat](https://huggingface.co/deepseek-community/deepseek-vl-1.3b-chat) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):"
        },
        {
            "sha": "ed5f7d655e344d87289b829f69c7aa00525218d8",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,7 +17,7 @@\n import torch\n import torch.nn as nn\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -39,15 +39,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class DeepseekVLConfig(PretrainedConfig):\n+class DeepseekVLConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DeepseekVLModel`]. It is used to instantiate a\n     DeepseekVL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the DeepseekVL\n     [deepseek-community/deepseek-vl-1.3b-chat](https://huggingface.co/deepseek-community/deepseek-vl-1.3b-chat) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):"
        },
        {
            "sha": "6a99cc4dab970440406fd43bb1536c96121f4129",
            "filename": "src/transformers/models/deepseek_vl_hybrid/configuration_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconfiguration_deepseek_vl_hybrid.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,23 +20,23 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DeepseekVLHybridConfig(PretrainedConfig):\n+class DeepseekVLHybridConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DeepseekVLHybridModel`]. It is used to instantiate a\n     DeepseekVLHybrid model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the DeepseekVLHybrid\n     [deepseek-community/deepseek-vl-7b-chat](https://huggingface.co/deepseek-community/deepseek-vl-7b-chat) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):"
        },
        {
            "sha": "7c7dddd1d28f7acf6be0134a77649853c62b1810",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -86,8 +86,8 @@ class DeepseekVLHybridConfig(DeepseekVLConfig):\n     with the defaults will yield a similar configuration to that of the DeepseekVLHybrid\n     [deepseek-community/deepseek-vl-7b-chat](https://huggingface.co/deepseek-community/deepseek-vl-7b-chat) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):"
        },
        {
            "sha": "ccd546b979da462ee654b44033c0ac9e8e0eb2e9",
            "filename": "src/transformers/models/deformable_detr/configuration_deformable_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Deformable DETR model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n from ..auto import CONFIG_MAPPING\n@@ -23,21 +23,21 @@\n logger = logging.get_logger(__name__)\n \n \n-class DeformableDetrConfig(PretrainedConfig):\n+class DeformableDetrConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DeformableDetrModel`]. It is used to instantiate\n     a Deformable DETR model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Deformable DETR\n     [SenseTime/deformable-detr](https://huggingface.co/SenseTime/deformable-detr) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n             API.\n-        backbone_config (`PretrainedConfig` or `dict`, *optional*):\n+        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n             The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n             case it will default to `ResNetConfig()`.\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "984d93eea926fe5907a97409a53fda635123d5f1",
            "filename": "src/transformers/models/deit/configuration_deit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,24 +19,24 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DeiTConfig(PretrainedConfig):\n+class DeiTConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DeiTModel`]. It is used to instantiate an DeiT\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the DeiT\n     [facebook/deit-base-distilled-patch16-224](https://huggingface.co/facebook/deit-base-distilled-patch16-224)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "e4861ff0f0fa2211c52bebbe59a5054b07d49de0",
            "filename": "src/transformers/models/deprecated/deta/configuration_deta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,26 +14,26 @@\n # limitations under the License.\n \"\"\"DETA model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n from ...auto import CONFIG_MAPPING\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DetaConfig(PretrainedConfig):\n+class DetaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DetaModel`]. It is used to instantiate a DETA\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the DETA\n     [SenseTime/deformable-detr](https://huggingface.co/SenseTime/deformable-detr) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`PretrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n+        backbone_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "feb173c3d8892e650e88011c5c95598227458203",
            "filename": "src/transformers/models/deprecated/efficientformer/configuration_efficientformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconfiguration_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconfiguration_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconfiguration_efficientformer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"EfficientFormer model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class EfficientFormerConfig(PretrainedConfig):\n+class EfficientFormerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of an [`EfficientFormerModel`]. It is used to\n     instantiate an EfficientFormer model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the EfficientFormer\n     [snap-research/efficientformer-l1](https://huggingface.co/snap-research/efficientformer-l1) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         depths (`List(int)`, *optional*, defaults to `[3, 2, 6, 4]`)"
        },
        {
            "sha": "aa08a1d05c7559d7dbffdb184d66cb8028d3869d",
            "filename": "src/transformers/models/deprecated/ernie_m/configuration_ernie_m.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fconfiguration_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fconfiguration_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fconfiguration_ernie_m.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,19 +17,19 @@\n \n from __future__ import annotations\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n \n \n-class ErnieMConfig(PretrainedConfig):\n+class ErnieMConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ErnieMModel`]. It is used to instantiate a\n     Ernie-M model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the `Ernie-M`\n     [susnato/ernie-m-base_pytorch](https://huggingface.co/susnato/ernie-m-base_pytorch) architecture.\n \n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 250002):"
        },
        {
            "sha": "424a35b241dfc65404a6137b97010caa900f5490",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/configuration_gptsan_japanese.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconfiguration_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconfiguration_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconfiguration_gptsan_japanese.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"GPTSAN-japanese model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GPTSanJapaneseConfig(PretrainedConfig):\n+class GPTSanJapaneseConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GPTSanJapaneseModel`]. It is used to instantiate\n     a GPTSANJapanese model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the GPTSANJapanese\n     [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Arguments:\n         vocab_size (`int`, *optional*, defaults to 36000):"
        },
        {
            "sha": "1a2bb9587d56ccd90fb452b01025a5fe70273261",
            "filename": "src/transformers/models/deprecated/graphormer/configuration_graphormer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,22 +16,22 @@\n \n from typing import Optional\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GraphormerConfig(PretrainedConfig):\n+class GraphormerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`~GraphormerModel`]. It is used to instantiate an\n     Graphormer model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Graphormer\n     [graphormer-base-pcqm4mv1](https://huggingface.co/graphormer-base-pcqm4mv1) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "1c9b44cebe4eecd585005827799c687c95aa3a95",
            "filename": "src/transformers/models/deprecated/jukebox/configuration_jukebox.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconfiguration_jukebox.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,7 +17,7 @@\n import os\n from typing import Union\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n@@ -136,16 +136,16 @@ def enc_dec_with_lyrics(layer):\n }\n \n \n-class JukeboxPriorConfig(PretrainedConfig):\n+class JukeboxPriorConfig(PreTrainedConfig):\n     \"\"\"\n         This is the configuration class to store the configuration of a [`JukeboxPrior`]. It is used to instantiate a\n         `JukeboxPrior` according to the specified arguments, defining the model architecture. Instantiating a\n         configuration with the defaults will yield a similar configuration to that of the top level prior from the\n         [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox\n     -1b-lyrics) architecture.\n \n-        Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-        documentation from [`PretrainedConfig`] for more information.\n+        Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+        documentation from [`PreTrainedConfig`] for more information.\n \n \n \n@@ -364,15 +364,15 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike],\n         return cls.from_dict(config_dict, **kwargs)\n \n \n-class JukeboxVQVAEConfig(PretrainedConfig):\n+class JukeboxVQVAEConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`JukeboxVQVAE`]. It is used to instantiate a\n     `JukeboxVQVAE` according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the VQVAE from\n     [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         act_fn (`str`, *optional*, defaults to `\"relu\"`):\n@@ -488,12 +488,12 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike],\n         return cls.from_dict(config_dict, **kwargs)\n \n \n-class JukeboxConfig(PretrainedConfig):\n+class JukeboxConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`JukeboxModel`].\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information. Instantiating a configuration with the defaults will\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information. Instantiating a configuration with the defaults will\n     yield a similar configuration to that of\n     [openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics) architecture.\n "
        },
        {
            "sha": "b6767ec9b65cfb90e5838aaf2d0ba77b8d31dcfe",
            "filename": "src/transformers/models/deprecated/mctct/configuration_mctct.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fconfiguration_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fconfiguration_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fconfiguration_mctct.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"M-CTC-T model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class MCTCTConfig(PretrainedConfig):\n+class MCTCTConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`MCTCTModel`]. It is used to instantiate an\n     M-CTC-T model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the M-CTC-T\n     [speechbrain/m-ctc-t-large](https://huggingface.co/speechbrain/m-ctc-t-large) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "6ec419e2493c31c9ce758ee1312caa18005885d1",
            "filename": "src/transformers/models/deprecated/mega/configuration_mega.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,23 +17,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....onnx import OnnxConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class MegaConfig(PretrainedConfig):\n+class MegaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`MegaModel`]. It is used to instantiate a Mega\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Mega\n     [mnaylor/mega-base-wikitext](https://huggingface.co/mnaylor/mega-base-wikitext) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "c4de795e05796369f3ca74c7eeb18f0118f18f95",
            "filename": "src/transformers/models/deprecated/nat/configuration_nat.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fconfiguration_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fconfiguration_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fconfiguration_nat.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"Neighborhood Attention Transformer model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n from ....utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n logger = logging.get_logger(__name__)\n \n \n-class NatConfig(BackboneConfigMixin, PretrainedConfig):\n+class NatConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`NatModel`]. It is used to instantiate a Nat model\n     according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Nat\n     [shi-labs/nat-mini-in1k-224](https://huggingface.co/shi-labs/nat-mini-in1k-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         patch_size (`int`, *optional*, defaults to 4):"
        },
        {
            "sha": "77ed0c2f66d7fdb304fe3e6775c58d635896ab74",
            "filename": "src/transformers/models/deprecated/nezha/configuration_nezha.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fconfiguration_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fconfiguration_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fconfiguration_nezha.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -1,15 +1,15 @@\n-from .... import PretrainedConfig\n+from .... import PreTrainedConfig\n \n \n-class NezhaConfig(PretrainedConfig):\n+class NezhaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of an [`NezhaModel`]. It is used to instantiate an Nezha\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Nezha\n     [sijunhe/nezha-cn-base](https://huggingface.co/sijunhe/nezha-cn-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "32827d0077518b5a3b1838bf49b53fe5de12f393",
            "filename": "src/transformers/models/deprecated/open_llama/configuration_open_llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,22 +19,22 @@\n # limitations under the License.\n \"\"\"Open-Llama model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class OpenLlamaConfig(PretrainedConfig):\n+class OpenLlamaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`OpenLlamaModel`]. It is used to instantiate an\n     Open-Llama model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the\n     [s-JoL/Open-Llama-V1](https://huggingface.co/s-JoL/Open-Llama-V1).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "20e65ba2340aae80120790ab146bd2e662ed673f",
            "filename": "src/transformers/models/deprecated/qdqbert/configuration_qdqbert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fconfiguration_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fconfiguration_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fconfiguration_qdqbert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"QDQBERT model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class QDQBertConfig(PretrainedConfig):\n+class QDQBertConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`QDQBertModel`]. It is used to instantiate an\n     QDQBERT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the BERT\n     [google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "6c6f9d4cea79203b322a92bd8bfe462b183091cf",
            "filename": "src/transformers/models/deprecated/realm/configuration_realm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fconfiguration_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fconfiguration_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fconfiguration_realm.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,14 +14,14 @@\n # limitations under the License.\n \"\"\"REALM model configuration.\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class RealmConfig(PretrainedConfig):\n+class RealmConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of\n \n@@ -37,8 +37,8 @@ class RealmConfig(PretrainedConfig):\n     [google/realm-cc-news-pretrained-embedder](https://huggingface.co/google/realm-cc-news-pretrained-embedder)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "cfa30aa2ad66c14965b22e7e5494c2cf7bd79d1b",
            "filename": "src/transformers/models/deprecated/retribert/configuration_retribert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fconfiguration_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fconfiguration_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fconfiguration_retribert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"RetriBERT model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class RetriBertConfig(PretrainedConfig):\n+class RetriBertConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`RetriBertModel`]. It is used to instantiate a\n     RetriBertModel model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the RetriBERT\n     [yjernite/retribert-base-uncased](https://huggingface.co/yjernite/retribert-base-uncased) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "9128840ce9d0cc84963635dc74544bfc49bbb2d9",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/configuration_speech_to_text_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fconfiguration_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fconfiguration_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fconfiguration_speech_to_text_2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"Speech2Text model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Speech2Text2Config(PretrainedConfig):\n+class Speech2Text2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Speech2Text2ForCausalLM`]. It is used to\n     instantiate an Speech2Text2 model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the Speech2Text2\n     [facebook/s2t-wav2vec2-large-en-de](https://huggingface.co/facebook/s2t-wav2vec2-large-en-de) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "1e3a5f5ddb15dc1d066bef6a584bda6f04b913d7",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/configuration_trajectory_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconfiguration_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconfiguration_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fconfiguration_trajectory_transformer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,14 +14,14 @@\n # limitations under the License.\n \"\"\"TrajectoryTransformer model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class TrajectoryTransformerConfig(PretrainedConfig):\n+class TrajectoryTransformerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`TrajectoryTransformerModel`]. It is used to\n     instantiate an TrajectoryTransformer model according to the specified arguments, defining the model architecture.\n@@ -30,8 +30,8 @@ class TrajectoryTransformerConfig(PretrainedConfig):\n     [CarlCochet/trajectory-transformer-halfcheetah-medium-v2](https://huggingface.co/CarlCochet/trajectory-transformer-halfcheetah-medium-v2)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "4b349a76eb394e8080cc2e4783b4b08efda86ee6",
            "filename": "src/transformers/models/deprecated/transfo_xl/configuration_transfo_xl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -15,22 +15,22 @@\n # limitations under the License.\n \"\"\"Transformer XL configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class TransfoXLConfig(PretrainedConfig):\n+class TransfoXLConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`TransfoXLModel`] or a [`TFTransfoXLModel`]. It is\n     used to instantiate a Transformer-XL model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the TransfoXL\n     [transfo-xl/transfo-xl-wt103](https://huggingface.co/transfo-xl/transfo-xl-wt103) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 267735):"
        },
        {
            "sha": "e3cd88760cbf03cbc9bfd39a0e03f6751ae91488",
            "filename": "src/transformers/models/deprecated/tvlt/configuration_tvlt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fconfiguration_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fconfiguration_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fconfiguration_tvlt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"TVLT model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class TvltConfig(PretrainedConfig):\n+class TvltConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`TvltModel`]. It is used to instantiate a TVLT\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the TVLT\n     [ZinengTang/tvlt-base](https://huggingface.co/ZinengTang/tvlt-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         image_size (`int`, *optional*, defaults to 224):"
        },
        {
            "sha": "33ffea192ddf9abc0bceceb7c26f5c7a207310fe",
            "filename": "src/transformers/models/deprecated/van/configuration_van.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconfiguration_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconfiguration_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconfiguration_van.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"VAN model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class VanConfig(PretrainedConfig):\n+class VanConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`VanModel`]. It is used to instantiate a VAN model\n     according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the VAN\n     [Visual-Attention-Network/van-base](https://huggingface.co/Visual-Attention-Network/van-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         image_size (`int`, *optional*, defaults to 224):"
        },
        {
            "sha": "534c5f463b82d2ae44dfc6f560749e2bc57fa88d",
            "filename": "src/transformers/models/deprecated/vit_hybrid/configuration_vit_hybrid.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"ViT Hybrid model configuration\"\"\"\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n from ...auto.configuration_auto import CONFIG_MAPPING\n from ...bit import BitConfig\n@@ -23,18 +23,18 @@\n logger = logging.get_logger(__name__)\n \n \n-class ViTHybridConfig(PretrainedConfig):\n+class ViTHybridConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ViTHybridModel`]. It is used to instantiate a ViT\n     Hybrid model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the ViT Hybrid\n     [google/vit-hybrid-base-bit-384](https://huggingface.co/google/vit-hybrid-base-bit-384) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict[str, Any], PretrainedConfig]`, *optional*):\n+        backbone_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*):\n             The configuration of the backbone in a dictionary or the config object of the backbone.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "ba0197164371f39ba2fca6e5fef74e52ff74920f",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/configuration_xlm_prophetnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fconfiguration_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fconfiguration_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fconfiguration_xlm_prophetnet.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,23 +16,23 @@\n \n from typing import Callable, Optional, Union\n \n-from ....configuration_utils import PretrainedConfig\n+from ....configuration_utils import PreTrainedConfig\n from ....utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class XLMProphetNetConfig(PretrainedConfig):\n+class XLMProphetNetConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`XLMProphetNetModel`]. It is used to instantiate a\n     XLMProphetNet model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the XLMProphetNet\n     [microsoft/xprophetnet-large-wiki100-cased](https://huggingface.co/microsoft/xprophetnet-large-wiki100-cased)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         activation_dropout (`float`, *optional*, defaults to 0.1):"
        },
        {
            "sha": "bf8b70f03fda43f7d73a0609995d968fd34f6d83",
            "filename": "src/transformers/models/depth_anything/configuration_depth_anything.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n from ..auto.configuration_auto import CONFIG_MAPPING\n@@ -25,18 +25,18 @@\n logger = logging.get_logger(__name__)\n \n \n-class DepthAnythingConfig(PretrainedConfig):\n+class DepthAnythingConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DepthAnythingModel`]. It is used to instantiate a DepthAnything\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the DepthAnything\n     [LiheYoung/depth-anything-small-hf](https://huggingface.co/LiheYoung/depth-anything-small-hf) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict[str, Any], PretrainedConfig]`, *optional*):\n+        backbone_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*):\n             The configuration of the backbone model. Only used in case `is_hybrid` is `True` or in case you want to\n             leverage the [`AutoBackbone`] API.\n         backbone (`str`, *optional*):\n@@ -161,7 +161,7 @@ def sub_configs(self):\n \n     def to_dict(self):\n         \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`]. Returns:\n+        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`]. Returns:\n             `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)"
        },
        {
            "sha": "8817420dffedce87a1fd305762df96f865526267",
            "filename": "src/transformers/models/depth_pro/configuration_depth_pro.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,23 +16,23 @@\n \n from copy import deepcopy\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DepthProConfig(PretrainedConfig):\n+class DepthProConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DepthProModel`]. It is used to instantiate a\n     DepthPro model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the DepthPro\n     [apple/DepthPro](https://huggingface.co/apple/DepthPro) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         fusion_hidden_size (`int`, *optional*, defaults to 256):\n@@ -61,13 +61,13 @@ class DepthProConfig(PretrainedConfig):\n             Whether to use `DepthProFovModel` to generate the field of view.\n         num_fov_head_layers (`int`, *optional*, defaults to 2):\n             Number of convolution layers in the head of `DepthProFovModel`.\n-        image_model_config (`Union[dict[str, Any], PretrainedConfig]`, *optional*):\n+        image_model_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*):\n             The configuration of the image encoder model, which is loaded using the [`AutoModel`] API.\n             By default, Dinov2 model is used as backbone.\n-        patch_model_config (`Union[dict[str, Any], PretrainedConfig]`, *optional*):\n+        patch_model_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*):\n             The configuration of the patch encoder model, which is loaded using the [`AutoModel`] API.\n             By default, Dinov2 model is used as backbone.\n-        fov_model_config (`Union[dict[str, Any], PretrainedConfig]`, *optional*):\n+        fov_model_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*):\n             The configuration of the fov encoder model, which is loaded using the [`AutoModel`] API.\n             By default, Dinov2 model is used as backbone.\n \n@@ -187,15 +187,15 @@ def __init__(\n                     )\n                     sub_config.update({\"image_size\": patch_size})\n                 sub_config = CONFIG_MAPPING[sub_config[\"model_type\"]](**sub_config)\n-            elif isinstance(sub_config, PretrainedConfig):\n+            elif isinstance(sub_config, PreTrainedConfig):\n                 image_size = getattr(sub_config, \"image_size\", None)\n                 if image_size != patch_size:\n                     raise ValueError(\n                         f\"`config.{sub_config_key}.image_size={image_size}` should match `config.patch_size={patch_size}`.\"\n                     )\n             else:\n                 raise TypeError(\n-                    f\"Invalid type for `sub_config`. Expected `PretrainedConfig`, `dict`, or `None`, but got {type(sub_config)}.\"\n+                    f\"Invalid type for `sub_config`. Expected `PreTrainedConfig`, `dict`, or `None`, but got {type(sub_config)}.\"\n                 )\n \n             setattr(self, sub_config_key, sub_config)"
        },
        {
            "sha": "f7c80b704cec643293059244037f415759e110df",
            "filename": "src/transformers/models/detr/configuration_detr.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n@@ -29,21 +29,21 @@\n logger = logging.get_logger(__name__)\n \n \n-class DetrConfig(PretrainedConfig):\n+class DetrConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DetrModel`]. It is used to instantiate a DETR\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the DETR\n     [facebook/detr-resnet-50](https://huggingface.co/facebook/detr-resnet-50) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n             API.\n-        backbone_config (`PretrainedConfig` or `dict`, *optional*):\n+        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n             The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n             case it will default to `ResNetConfig()`.\n         num_channels (`int`, *optional*, defaults to 3):\n@@ -261,11 +261,11 @@ def sub_configs(self):\n         )\n \n     @classmethod\n-    def from_backbone_config(cls, backbone_config: PretrainedConfig, **kwargs):\n+    def from_backbone_config(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`DetrConfig`] (or a derived class) from a pre-trained backbone model configuration.\n \n         Args:\n-            backbone_config ([`PretrainedConfig`]):\n+            backbone_config ([`PreTrainedConfig`]):\n                 The backbone configuration.\n         Returns:\n             [`DetrConfig`]: An instance of a configuration object"
        },
        {
            "sha": "c487781fc94cb232de8ef192fa77fd33699954ef",
            "filename": "src/transformers/models/dia/configuration_dia.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,21 +16,21 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DiaEncoderConfig(PretrainedConfig):\n+class DiaEncoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DiaEncoder`]. It is used to instantiate a Dia\n     encoder according to the specified arguments, defining the encoder architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         max_position_embeddings (`int`, *optional*, defaults to 1024):\n@@ -138,13 +138,13 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class DiaDecoderConfig(PretrainedConfig):\n+class DiaDecoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DiaDecoder`]. It is used to instantiate a Dia\n     decoder according to the specified arguments, defining the decoder architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         max_position_embeddings (`int`, *optional*, defaults to 3072):\n@@ -279,15 +279,15 @@ def __init__(\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n \n-class DiaConfig(PretrainedConfig):\n+class DiaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DiaModel`]. It is used to instantiate a\n     Dia model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the\n     [nari-labs/Dia-1.6B](https://huggingface.co/nari-labs/Dia-1.6B) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         encoder_config (`DiaEncoderConfig`, *optional*):"
        },
        {
            "sha": "26902fcc2784dfefc1c82e86453c4998d03a074c",
            "filename": "src/transformers/models/diffllama/configuration_diffllama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,18 +17,18 @@\n # limitations under the License.\n \"\"\"DiffLlama model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class DiffLlamaConfig(PretrainedConfig):\n+class DiffLlamaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DiffLlamaModel`]. It is used to instantiate an DiffLlama\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults\n     will yield a similar configuration to that of the [kajuma/DiffLlama-0.3B-handcut](https://huggingface.co/kajuma/DiffLlama-0.3B-handcut).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "35307e6a42fe0ee10aee0dedddda6dac7a8b2ea2",
            "filename": "src/transformers/models/dinat/configuration_dinat.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinat%2Fconfiguration_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinat%2Fconfiguration_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fconfiguration_dinat.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"Dilated Neighborhood Attention Transformer model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DinatConfig(BackboneConfigMixin, PretrainedConfig):\n+class DinatConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DinatModel`]. It is used to instantiate a Dinat\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Dinat\n     [shi-labs/dinat-mini-in1k-224](https://huggingface.co/shi-labs/dinat-mini-in1k-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         patch_size (`int`, *optional*, defaults to 4):"
        },
        {
            "sha": "f00d0fc6c2462dd029f4c42b13c51c71b7f2dd92",
            "filename": "src/transformers/models/dinov2/configuration_dinov2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n@@ -28,15 +28,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class Dinov2Config(BackboneConfigMixin, PretrainedConfig):\n+class Dinov2Config(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Dinov2Model`]. It is used to instantiate an\n     Dinov2 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Dinov2\n     [google/dinov2-base-patch16-224](https://huggingface.co/google/dinov2-base-patch16-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):"
        },
        {
            "sha": "8bcd5936d788c98572cf67045f939d39da0c44e8",
            "filename": "src/transformers/models/dinov2_with_registers/configuration_dinov2_with_registers.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconfiguration_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconfiguration_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconfiguration_dinov2_with_registers.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,19 +20,19 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n-class Dinov2WithRegistersConfig(BackboneConfigMixin, PretrainedConfig):\n+class Dinov2WithRegistersConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Dinov2WithRegistersModel`]. It is used to instantiate an\n     Dinov2WithRegisters model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the DINOv2 with Registers\n     [facebook/dinov2-with-registers-base](https://huggingface.co/facebook/dinov2-with-registers-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):"
        },
        {
            "sha": "05a843361db49c27bf55fc59135232b5f6d71d7c",
            "filename": "src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -27,7 +27,7 @@\n     Dinov2PatchEmbeddings,\n     Dinov2PreTrainedModel,\n )\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging, torch_int\n@@ -37,15 +37,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class Dinov2WithRegistersConfig(BackboneConfigMixin, PretrainedConfig):\n+class Dinov2WithRegistersConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Dinov2WithRegistersModel`]. It is used to instantiate an\n     Dinov2WithRegisters model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the DINOv2 with Registers\n     [facebook/dinov2-with-registers-base](https://huggingface.co/facebook/dinov2-with-registers-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):"
        },
        {
            "sha": "e2422545c46b2c4896277432edcbd79f78f24135",
            "filename": "src/transformers/models/dinov3_convnext/configuration_dinov3_convnext.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconfiguration_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconfiguration_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconfiguration_dinov3_convnext.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,22 +16,22 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DINOv3ConvNextConfig(PretrainedConfig):\n+class DINOv3ConvNextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DINOv3ConvNextModel`]. It is used to instantiate an\n     DINOv3ConvNext model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the DINOv3ConvNext\n     [facebook/dinov3-convnext-tiny-pretrain-lvd1689m](https://huggingface.co/facebook/dinov3-convnext-tiny-pretrain-lvd1689m) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "ce1fd939e18fdac7516d7ac5b2c7862c4d78862b",
            "filename": "src/transformers/models/dinov3_vit/configuration_dinov3_vit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconfiguration_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconfiguration_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fconfiguration_dinov3_vit.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,22 +16,22 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DINOv3ViTConfig(PretrainedConfig):\n+class DINOv3ViTConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DINOv3Model`]. It is used to instantiate an\n     DINOv3 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the DINOv3\n     [facebook/dinov3-vits16-pretrain-lvd1689m](https://huggingface.co/facebook/dinov3-vits16-pretrain-lvd1689m) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         patch_size (`int`, *optional*, defaults to 16):"
        },
        {
            "sha": "93b38a32d53acd8ddfb0faf8d7b0794863f98e76",
            "filename": "src/transformers/models/distilbert/configuration_distilbert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,23 +17,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DistilBertConfig(PretrainedConfig):\n+class DistilBertConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DistilBertModel`] or a [`TFDistilBertModel`]. It\n     is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT\n     [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):"
        },
        {
            "sha": "f6e220e9f1166b051a6055ef9f1d5452ca023e42",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -26,7 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import get_activation\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -85,7 +85,7 @@ def _create_sinusoidal_embeddings(n_pos: int, dim: int, out: torch.Tensor):\n \n \n class Embeddings(nn.Module):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__()\n         self.word_embeddings = nn.Embedding(config.vocab_size, config.dim, padding_idx=config.pad_token_id)\n         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.dim)\n@@ -153,7 +153,7 @@ def eager_attention_forward(\n \n \n class DistilBertSelfAttention(nn.Module):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__()\n         self.config = config\n \n@@ -227,7 +227,7 @@ def forward(\n \n \n class FFN(nn.Module):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__()\n         self.dropout = nn.Dropout(p=config.dropout)\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -248,7 +248,7 @@ def ff_chunk(self, input: torch.Tensor) -> torch.Tensor:\n \n \n class TransformerBlock(GradientCheckpointingLayer):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__()\n \n         # Have an even number of Configure multi-heads\n@@ -283,7 +283,7 @@ def forward(\n \n \n class Transformer(nn.Module):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__()\n         self.n_layers = config.n_layers\n         self.layer = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n@@ -341,7 +341,7 @@ def _init_weights(self, module: nn.Module):\n \n @auto_docstring\n class DistilBertModel(DistilBertPreTrainedModel):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__(config)\n \n         self.embeddings = Embeddings(config)  # Embeddings\n@@ -481,7 +481,7 @@ def _update_full_mask(\n class DistilBertForMaskedLM(DistilBertPreTrainedModel):\n     _tied_weights_keys = [\"vocab_projector.weight\"]\n \n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__(config)\n \n         self.activation = get_activation(config.activation)\n@@ -583,7 +583,7 @@ def forward(\n     \"\"\"\n )\n class DistilBertForSequenceClassification(DistilBertPreTrainedModel):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n         self.config = config\n@@ -681,7 +681,7 @@ def forward(\n \n @auto_docstring\n class DistilBertForQuestionAnswering(DistilBertPreTrainedModel):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__(config)\n \n         self.distilbert = DistilBertModel(config)\n@@ -783,7 +783,7 @@ def forward(\n \n @auto_docstring\n class DistilBertForTokenClassification(DistilBertPreTrainedModel):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n \n@@ -858,7 +858,7 @@ def forward(\n \n @auto_docstring\n class DistilBertForMultipleChoice(DistilBertPreTrainedModel):\n-    def __init__(self, config: PretrainedConfig):\n+    def __init__(self, config: PreTrainedConfig):\n         super().__init__(config)\n \n         self.distilbert = DistilBertModel(config)"
        },
        {
            "sha": "4cbc2b866c1a9f27b9db0917a7a93ee934d9d74d",
            "filename": "src/transformers/models/doge/configuration_doge.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,17 +20,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class DogeConfig(PretrainedConfig):\n+class DogeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DogeModel`]. It is used to instantiate an Doge\n     model according to the specified arguments, defining the model architecture like [SmallDoge/Doge-320M](https://huggingface.co/SmallDoge/Doge-320M).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 32768):"
        },
        {
            "sha": "efe9cbb0cad1b5da7cd872c098f078b747c4abfc",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -25,7 +25,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...integrations.flex_attention import compile_friendly_flex_attention\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n@@ -52,13 +52,13 @@\n     from torch.nn.attention.flex_attention import BlockMask\n \n \n-class DogeConfig(PretrainedConfig):\n+class DogeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DogeModel`]. It is used to instantiate an Doge\n     model according to the specified arguments, defining the model architecture like [SmallDoge/Doge-320M](https://huggingface.co/SmallDoge/Doge-320M).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 32768):"
        },
        {
            "sha": "fc99888ae735f8da3e336d11bf27368f4af5b754",
            "filename": "src/transformers/models/donut/configuration_donut_swin.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdonut%2Fconfiguration_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdonut%2Fconfiguration_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fconfiguration_donut_swin.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"Donut Swin Transformer model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DonutSwinConfig(PretrainedConfig):\n+class DonutSwinConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DonutSwinModel`]. It is used to instantiate a\n     Donut model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Donut\n     [naver-clova-ix/donut-base](https://huggingface.co/naver-clova-ix/donut-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         image_size (`int`, *optional*, defaults to 224):"
        },
        {
            "sha": "092662e004fd75ed61b8ba5613f10cd1d1b1af17",
            "filename": "src/transformers/models/dots1/configuration_dots1.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -12,22 +12,22 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Dots1Config(PretrainedConfig):\n+class Dots1Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Dots1Model`]. It is used to instantiate a\n     `dots.llm1` model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of\n     [rednote-hilab/dots.llm1.base](https://huggingface.co/rednote-hilab/dots.llm1.base).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 152064):"
        },
        {
            "sha": "dc2fd4833b6b158064c9703d975966c0e0fd87f9",
            "filename": "src/transformers/models/dpr/configuration_dpr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdpr%2Fconfiguration_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdpr%2Fconfiguration_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fconfiguration_dpr.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,14 +14,14 @@\n # limitations under the License.\n \"\"\"DPR model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DPRConfig(PretrainedConfig):\n+class DPRConfig(PreTrainedConfig):\n     r\"\"\"\n     [`DPRConfig`] is the configuration class to store the configuration of a *DPRModel*.\n "
        },
        {
            "sha": "a217616a8b3ba7a97be9619cc6c748d2cef1cdb0",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n from ..auto.configuration_auto import CONFIG_MAPPING\n@@ -26,15 +26,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class DPTConfig(PretrainedConfig):\n+class DPTConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DPTModel`]. It is used to instantiate an DPT\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the DPT\n     [Intel/dpt-large](https://huggingface.co/Intel/dpt-large) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -104,7 +104,7 @@ class DPTConfig(PretrainedConfig):\n             Used only for the `hybrid` embedding type. The shape of the feature maps of the backbone.\n         neck_ignore_stages (`list[int]`, *optional*, defaults to `[0, 1]`):\n             Used only for the `hybrid` embedding type. The stages of the readout layers to ignore.\n-        backbone_config (`Union[dict[str, Any], PretrainedConfig]`, *optional*):\n+        backbone_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*):\n             The configuration of the backbone model. Only used in case `is_hybrid` is `True` or in case you want to\n             leverage the [`AutoBackbone`] API.\n         backbone (`str`, *optional*):\n@@ -200,9 +200,9 @@ def __init__(\n             if isinstance(backbone_config, dict):\n                 logger.info(\"Initializing the config with a `BiT` backbone.\")\n                 backbone_config = BitConfig(**backbone_config)\n-            elif not isinstance(backbone_config, PretrainedConfig):\n+            elif not isinstance(backbone_config, PreTrainedConfig):\n                 raise ValueError(\n-                    f\"backbone_config must be a dictionary or a `PretrainedConfig`, got {backbone_config.__class__}.\"\n+                    f\"backbone_config must be a dictionary or a `PreTrainedConfig`, got {backbone_config.__class__}.\"\n                 )\n             self.backbone_config = backbone_config\n             self.backbone_featmap_shape = backbone_featmap_shape\n@@ -277,7 +277,7 @@ def __init__(\n \n     def to_dict(self):\n         \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`]. Returns:\n+        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`]. Returns:\n             `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)"
        },
        {
            "sha": "a260b279bacdfd610f981ce6761e4a2328506a67",
            "filename": "src/transformers/models/edgetam/configuration_edgetam.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,22 +19,22 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n-class EdgeTamVisionConfig(PretrainedConfig):\n+class EdgeTamVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EdgeTamVisionModel`]. It is used to instantiate a SAM\n     vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n     defaults will yield a similar configuration to that of SAM 2.1 Hiera-tiny\n     [facebook/EdgeTAM](https://huggingface.co/facebook/EdgeTAM) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict, \"PretrainedConfig\"]`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*):\n             Configuration for the vision backbone. This is used to instantiate the backbone using\n             `AutoModel.from_config`.\n         backbone_channel_list (`List[int]`, *optional*, defaults to `[384, 192, 96, 48]`):\n@@ -120,13 +120,13 @@ def __init__(\n         self.initializer_range = initializer_range\n \n \n-class EdgeTamPromptEncoderConfig(PretrainedConfig):\n+class EdgeTamPromptEncoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EdgeTamPromptEncoder`]. The [`EdgeTamPromptEncoder`]\n     module is used to encode the input 2D points and bounding boxes.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 256):\n@@ -172,13 +172,13 @@ def __init__(\n         self.scale = scale\n \n \n-class EdgeTamMaskDecoderConfig(PretrainedConfig):\n+class EdgeTamMaskDecoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EdgeTamMaskDecoder`]. It is used to instantiate a EDGETAM\n     memory encoder according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 256):\n@@ -245,15 +245,15 @@ def __init__(\n         self.attention_downsample_rate = attention_downsample_rate\n \n \n-class EdgeTamConfig(PretrainedConfig):\n+class EdgeTamConfig(PreTrainedConfig):\n     r\"\"\"\n     [`EdgeTamConfig`] is the configuration class to store the configuration of a [`EdgeTamModel`]. It is used to instantiate a\n     EDGETAM model according to the specified arguments, defining the memory attention, memory encoder, and image encoder\n     configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 2.1 Hiera-tiny\n     [facebook/edgetam.1-hiera-tiny](https://huggingface.co/facebook/edgetam.1-hiera-tiny) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (Union[`dict`, `EdgeTamVisionConfig`], *optional*):"
        },
        {
            "sha": "2489a57c11c4da1e36f593cedc8c601f676a5e09",
            "filename": "src/transformers/models/edgetam/modular_edgetam.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -33,7 +33,7 @@\n )\n from transformers.utils.generic import TransformersKwargs, check_model_inputs\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n@@ -46,18 +46,18 @@\n     from transformers.models.timm_wrapper.modeling_timm_wrapper import TimmWrapperModel\n \n \n-class EdgeTamVisionConfig(PretrainedConfig):\n+class EdgeTamVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EdgeTamVisionModel`]. It is used to instantiate a SAM\n     vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n     defaults will yield a similar configuration to that of SAM 2.1 Hiera-tiny\n     [facebook/EdgeTAM](https://huggingface.co/facebook/EdgeTAM) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict, \"PretrainedConfig\"]`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*):\n             Configuration for the vision backbone. This is used to instantiate the backbone using\n             `AutoModel.from_config`.\n         backbone_channel_list (`List[int]`, *optional*, defaults to `[384, 192, 96, 48]`):"
        },
        {
            "sha": "01d62318a25cb81cceafe0b6e6767a32fc673526",
            "filename": "src/transformers/models/edgetam_video/configuration_edgetam_video.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fconfiguration_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fconfiguration_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fconfiguration_edgetam_video.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,17 +19,17 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n-class EdgeTamVideoPromptEncoderConfig(PretrainedConfig):\n+class EdgeTamVideoPromptEncoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EdgeTamVideoPromptEncoder`]. The [`EdgeTamVideoPromptEncoder`]\n     module is used to encode the input 2D points and bounding boxes.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 256):\n@@ -75,13 +75,13 @@ def __init__(\n         self.scale = scale\n \n \n-class EdgeTamVideoMaskDecoderConfig(PretrainedConfig):\n+class EdgeTamVideoMaskDecoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EdgeTamVideoMaskDecoder`]. It is used to instantiate a EDGETAM_VIDEO\n     memory encoder according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 256):\n@@ -148,15 +148,15 @@ def __init__(\n         self.attention_downsample_rate = attention_downsample_rate\n \n \n-class EdgeTamVideoConfig(PretrainedConfig):\n+class EdgeTamVideoConfig(PreTrainedConfig):\n     r\"\"\"\n     [`EdgeTamVideoConfig`] is the configuration class to store the configuration of a [`EdgeTamVideoModel`]. It is used to instantiate a\n     EDGETAM model according to the specified arguments, defining the memory attention, memory encoder, and image encoder\n     configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 2.1 Hiera-tiny\n     [facebook/EdgeTAM](https://huggingface.co/facebook/EdgeTAM) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (Union[`dict`, `EdgeTamVideoVisionConfig`], *optional*):"
        },
        {
            "sha": "0ea225871110c83c9f8ac64724c232ceb7df68f4",
            "filename": "src/transformers/models/edgetam_video/modular_edgetam_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -28,7 +28,7 @@\n from transformers.utils.generic import OutputRecorder\n \n from ...activations import ACT2FN\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n@@ -75,8 +75,8 @@ class EdgeTamVideoConfig(Sam2VideoConfig):\n     configs. Instantiating a configuration defaults will yield a similar configuration to that of the SAM 2.1 Hiera-tiny\n     [facebook/EdgeTAM](https://huggingface.co/facebook/EdgeTAM) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (Union[`dict`, `EdgeTamVideoVisionConfig`], *optional*):\n@@ -274,7 +274,7 @@ def __init__(\n         memory_fuser_hidden_act=\"gelu\",\n         **kwargs,\n     ):\n-        PretrainedConfig.__init__(**kwargs)\n+        PreTrainedConfig.__init__(**kwargs)\n         vision_config = vision_config if vision_config is not None else {}\n         prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n         mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}"
        },
        {
            "sha": "09d7a7b9a07af11d1053f0a6ce963ce295f8c56a",
            "filename": "src/transformers/models/efficientloftr/configuration_efficientloftr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,19 +13,19 @@\n # limitations under the License.\n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class EfficientLoFTRConfig(PretrainedConfig):\n+class EfficientLoFTRConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EfficientLoFTRFromKeypointMatching`].\n     It is used to instantiate a EfficientLoFTR model according to the specified arguments, defining the model\n     architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the\n     EfficientLoFTR [zju-community/efficientloftr](https://huggingface.co/zju-community/efficientloftr) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         stage_num_blocks (`List`, *optional*, defaults to [1, 2, 4, 14]):"
        },
        {
            "sha": "1520c99b6d1c3c3123e82c0e28c5cb7df70be598",
            "filename": "src/transformers/models/efficientnet/configuration_efficientnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fconfiguration_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fconfiguration_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fconfiguration_efficientnet.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,23 +19,23 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class EfficientNetConfig(PretrainedConfig):\n+class EfficientNetConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EfficientNetModel`]. It is used to instantiate an\n     EfficientNet model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the EfficientNet\n     [google/efficientnet-b7](https://huggingface.co/google/efficientnet-b7) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "fc577e42e75a294eb36e41453d558f112cf094a4",
            "filename": "src/transformers/models/electra/configuration_electra.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,23 +18,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ElectraConfig(PretrainedConfig):\n+class ElectraConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ElectraModel`] or a [`TFElectraModel`]. It is\n     used to instantiate a ELECTRA model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the ELECTRA\n     [google/electra-small-discriminator](https://huggingface.co/google/electra-small-discriminator) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "3d737806ee186a5a1b9255f3def6778d903121eb",
            "filename": "src/transformers/models/emu3/configuration_emu3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,18 +16,18 @@\n \n from typing import Any, Optional, Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class Emu3VQVAEConfig(PretrainedConfig):\n+class Emu3VQVAEConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Emu3VQVAE`]. It is used to instantiate an VQ-VAE\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a configuration to the VQ model presented in Emu3 paper.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n     Args:\n         codebook_size (`int`, *optional*, defaults to 32768):\n             Codebook size of the VQ model.\n@@ -110,15 +110,15 @@ def __init__(\n         self.attention_dropout = attention_dropout\n \n \n-class Emu3TextConfig(PretrainedConfig):\n+class Emu3TextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Emu3TextModel`]. It is used to instantiate a\n     emu3 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the\n     [Emu3-community/Emu3-Chat-hf](https://huggingface.co/Emu3-community/Emu3-Chat-hf).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -276,15 +276,15 @@ def __init__(\n         )\n \n \n-class Emu3Config(PretrainedConfig):\n+class Emu3Config(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`Emu3Model`]. It is used to instantiate a\n     emu3 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the\n     [Emu3-community/Emu3-Chat-hf](https://huggingface.co/Emu3-community/Emu3-Chat-hf).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "fe3e03b0022df86d86ef1a3922828db7221e27b9",
            "filename": "src/transformers/models/encodec/configuration_encodec.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fencodec%2Fconfiguration_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fencodec%2Fconfiguration_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fconfiguration_encodec.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,22 +19,22 @@\n \n import numpy as np\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class EncodecConfig(PretrainedConfig):\n+class EncodecConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of an [`EncodecModel`]. It is used to instantiate a\n     Encodec model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the\n     [facebook/encodec_24khz](https://huggingface.co/facebook/encodec_24khz) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         target_bandwidths (`list[float]`, *optional*, defaults to `[1.5, 3.0, 6.0, 12.0, 24.0]`):"
        },
        {
            "sha": "041c653bad18a257738410c023d928e2f7cb078c",
            "filename": "src/transformers/models/encoder_decoder/configuration_encoder_decoder.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -15,30 +15,30 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class EncoderDecoderConfig(PretrainedConfig):\n+class EncoderDecoderConfig(PreTrainedConfig):\n     r\"\"\"\n     [`EncoderDecoderConfig`] is the configuration class to store the configuration of a [`EncoderDecoderModel`]. It is\n     used to instantiate an Encoder Decoder model according to the specified arguments, defining the encoder and decoder\n     configs.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         kwargs (*optional*):\n             Dictionary of keyword arguments. Notably:\n \n-                - **encoder** ([`PretrainedConfig`], *optional*) -- An instance of a configuration object that defines\n+                - **encoder** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that defines\n                   the encoder config.\n-                - **decoder** ([`PretrainedConfig`], *optional*) -- An instance of a configuration object that defines\n+                - **decoder** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that defines\n                   the decoder config.\n \n     Examples:\n@@ -92,8 +92,8 @@ def __init__(self, **kwargs):\n \n     @classmethod\n     def from_encoder_decoder_configs(\n-        cls, encoder_config: PretrainedConfig, decoder_config: PretrainedConfig, **kwargs\n-    ) -> PretrainedConfig:\n+        cls, encoder_config: PreTrainedConfig, decoder_config: PreTrainedConfig, **kwargs\n+    ) -> PreTrainedConfig:\n         r\"\"\"\n         Instantiate a [`EncoderDecoderConfig`] (or a derived class) from a pre-trained encoder model configuration and\n         decoder model configuration."
        },
        {
            "sha": "5e35cc4f3b2d4c0f42fe3b61bd1bff472c5cde81",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...cache_utils import Cache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n from ...modeling_utils import PreTrainedModel\n@@ -82,7 +82,7 @@ class EncoderDecoderModel(PreTrainedModel, GenerationMixin):\n \n     def __init__(\n         self,\n-        config: Optional[PretrainedConfig] = None,\n+        config: Optional[PreTrainedConfig] = None,\n         encoder: Optional[PreTrainedModel] = None,\n         decoder: Optional[PreTrainedModel] = None,\n     ):"
        },
        {
            "sha": "58cb35d27621222fc7debc4775d80c4c15cbef63",
            "filename": "src/transformers/models/eomt/configuration_eomt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Feomt%2Fconfiguration_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Feomt%2Fconfiguration_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fconfiguration_eomt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,19 +19,19 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n \n \n-class EomtConfig(PretrainedConfig):\n+class EomtConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EomtForUniversalSegmentation`]. It is used to instantiate an EoMT model\n     according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the EoMT\n     [tue-mps/coco_panoptic_eomt_large_640](https://huggingface.co/tue-mps/coco_panoptic_eomt_large_640)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1024):"
        },
        {
            "sha": "823cf59d98d4f918d656e5a139e6ccdc8d92f933",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -56,8 +56,8 @@ class EomtConfig(ViTConfig):\n     [tue-mps/coco_panoptic_eomt_large_640](https://huggingface.co/tue-mps/coco_panoptic_eomt_large_640)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1024):"
        },
        {
            "sha": "f39af6b347928851efd18fb25906aff9165e27ce",
            "filename": "src/transformers/models/ernie/configuration_ernie.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,23 +18,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class ErnieConfig(PretrainedConfig):\n+class ErnieConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ErnieModel`] or a [`TFErnieModel`]. It is used to\n     instantiate a ERNIE model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the ERNIE\n     [nghuyong/ernie-3.0-base-zh](https://huggingface.co/nghuyong/ernie-3.0-base-zh) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "c8539c1adf97e935bad2e1b85c149eeb3a023d47",
            "filename": "src/transformers/models/ernie4_5/configuration_ernie4_5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,19 +13,19 @@\n # limitations under the License.\n \"\"\"Ernie 4.5 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class Ernie4_5Config(PretrainedConfig):\n+class Ernie4_5Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Ernie4_5Model`]. It is used to instantiate an Ernie 4.5\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Ernie 4.5 0.3B.\n     e.g. [baidu/ERNIE-4.5-0.3B-PT](https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "e7ebe24e0e8721c43508d066fb4ca60bac901550",
            "filename": "src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,22 +13,22 @@\n # limitations under the License.\n \"\"\"Ernie 4.5 MoE model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Ernie4_5_MoeConfig(PretrainedConfig):\n+class Ernie4_5_MoeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Ernie4_5_MoeModel`]. It is used to instantiate a\n     Ernie 4.5 MoE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of [baidu/ERNIE-4.5-21B-A3B-PT](https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-PT).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "a156e6780455dc70c7a6fc62c29e87eccba1d360",
            "filename": "src/transformers/models/esm/configuration_esm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,7 +17,7 @@\n from dataclasses import asdict, dataclass\n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n@@ -26,15 +26,15 @@\n # TODO Update this\n \n \n-class EsmConfig(PretrainedConfig):\n+class EsmConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`ESMModel`]. It is used to instantiate a ESM model\n     according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the ESM\n     [facebook/esm-1b](https://huggingface.co/facebook/esm-1b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -159,7 +159,7 @@ def __init__(\n \n     def to_dict(self):\n         \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n+        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`].\n \n         Returns:\n             `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n@@ -193,7 +193,7 @@ def __post_init__(self):\n \n     def to_dict(self):\n         \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n+        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`].\n \n         Returns:\n             `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n@@ -258,7 +258,7 @@ def __post_init__(self):\n \n     def to_dict(self):\n         \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n+        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`].\n \n         Returns:\n             `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,"
        },
        {
            "sha": "01d95d1c5862792bf945d088c2e2bfe053291821",
            "filename": "src/transformers/models/evolla/configuration_evolla.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fconfiguration_evolla.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,20 +14,20 @@\n # limitations under the License.\n \"\"\"Evolla model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class SaProtConfig(PretrainedConfig):\n+class SaProtConfig(PreTrainedConfig):\n     r\"\"\"This is the configuration class to store the configuration of a [`EvollaSaProtProteinEncoder`]. It is used to instantiate a\n     SaProt model according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 446):\n@@ -97,16 +97,16 @@ def __init__(\n         self.token_dropout = token_dropout\n \n \n-class EvollaConfig(PretrainedConfig):\n+class EvollaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`EvollaModel`]. It is used to instantiate an\n     Evolla model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Evolla-10B.\n \n     e.g. [westlake-repl/Evolla-10B-hf](https://huggingface.co/westlake-repl/Evolla-10B-hf)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         protein_encoder_config (`dict`, *optional*):"
        },
        {
            "sha": "7eac2bd588c36ec9278c66e2f06738149b494cfd",
            "filename": "src/transformers/models/exaone4/configuration_exaone4.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,17 +19,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n \n \n-class Exaone4Config(PretrainedConfig):\n+class Exaone4Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Exaone4Model`]. It is used to\n     instantiate a EXAONE 4.0 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the EXAONE-4.0-32B [LGAI-EXAONE/EXAONE-4.0-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model\n-    outputs. Read the documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model\n+    outputs. Read the documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 102400):"
        },
        {
            "sha": "20f59e4bc3402dd8f2117eee5ddacd40ebbbfed8",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -23,7 +23,7 @@\n from transformers.utils.generic import check_model_inputs\n \n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -57,14 +57,14 @@\n _CONFIG_FOR_DOC = \"Exaone4Config\"\n \n \n-class Exaone4Config(PretrainedConfig):\n+class Exaone4Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Exaone4Model`]. It is used to\n     instantiate a EXAONE 4.0 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the EXAONE-4.0-32B [LGAI-EXAONE/EXAONE-4.0-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model\n-    outputs. Read the documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model\n+    outputs. Read the documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 102400):"
        },
        {
            "sha": "27e12b43dcaca4e2fe66e78e0c5fb3fa1f46fcbe",
            "filename": "src/transformers/models/falcon/configuration_falcon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"Falcon configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class FalconConfig(PretrainedConfig):\n+class FalconConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FalconModel`]. It is used to instantiate a Falcon\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the\n     [tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "fc46100ab92e109bff079235b677003007d6ad07",
            "filename": "src/transformers/models/falcon_h1/configuration_falcon_h1.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"FalconH1 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class FalconH1Config(PretrainedConfig):\n+class FalconH1Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FalconH1Model`]. It is used to instantiate a\n     FalconH1Model model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with defaults taken from [ibm-fms/FalconH1-9.8b-2.2T-hf](https://huggingface.co/ibm-fms/FalconH1-9.8b-2.2T-hf).\n     The FalconH1Model is a hybrid [mamba2](https://github.com/state-spaces/mamba) architecture with SwiGLU.\n     The checkpoints are  jointly trained by IBM, Princeton, and UIUC.\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n     Args:\n         vocab_size (`int`, *optional*, defaults to 128000):\n             Vocabulary size of the FalconH1 model. Defines the number of different tokens that can be represented by the"
        },
        {
            "sha": "93730940c9afa067ab1373afabf143540dc99251",
            "filename": "src/transformers/models/falcon_mamba/configuration_falcon_mamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -21,18 +21,18 @@\n \n import math\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n \n \n-class FalconMambaConfig(PretrainedConfig):\n+class FalconMambaConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`FalconMambaModel`]. It is used to instantiate a FALCON_MAMBA\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the FALCON_MAMBA\n     [tiiuae/falcon-mamba-7b](https://huggingface.co/tiiuae/falcon-mamba-7b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "71011df87985c9caf6144940ed8b60685feaedf7",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -28,7 +28,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n@@ -64,7 +64,7 @@ class FalconMambaCache:\n     Cache for falcon_mamba model which does not have attention mechanism and key value states.\n \n     Arguments:\n-        config (`PretrainedConfig):\n+        config (`PreTrainedConfig):\n             The configuration file defining the shape-related attributes required to initialize the static cache.\n         max_batch_size (`int`):\n             The maximum batch size with which the model will be used. Note that a new instance must be instantiated if\n@@ -98,7 +98,7 @@ class FalconMambaCache:\n     # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         max_batch_size: int,\n         dtype: torch.dtype = torch.float16,\n         device: Union[torch.device, str, None] = None,"
        },
        {
            "sha": "5370e5fe19daef471faf390bab4282dfc04be02e",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -64,8 +64,8 @@ class FalconMambaConfig(MambaConfig):\n     defaults will yield a similar configuration to that of the FALCON_MAMBA\n     [tiiuae/falcon-mamba-7b](https://huggingface.co/tiiuae/falcon-mamba-7b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -205,7 +205,7 @@ class FalconMambaCache(MambaCache):\n     Cache for falcon_mamba model which does not have attention mechanism and key value states.\n \n     Arguments:\n-        config (`PretrainedConfig):\n+        config (`PreTrainedConfig):\n             The configuration file defining the shape-related attributes required to initialize the static cache.\n         max_batch_size (`int`):\n             The maximum batch size with which the model will be used. Note that a new instance must be instantiated if"
        },
        {
            "sha": "64c6a4eac8d7579e4f8c7c57d7bd2fa255601b07",
            "filename": "src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,23 +16,23 @@\n \n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class FastSpeech2ConformerConfig(PretrainedConfig):\n+class FastSpeech2ConformerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FastSpeech2ConformerModel`]. It is used to\n     instantiate a FastSpeech2Conformer model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the\n     FastSpeech2Conformer [espnet/fastspeech2_conformer](https://huggingface.co/espnet/fastspeech2_conformer)\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 384):\n@@ -325,16 +325,16 @@ def __init__(\n         )\n \n \n-class FastSpeech2ConformerHifiGanConfig(PretrainedConfig):\n+class FastSpeech2ConformerHifiGanConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FastSpeech2ConformerHifiGanModel`]. It is used to\n     instantiate a FastSpeech2Conformer HiFi-GAN vocoder model according to the specified arguments, defining the model\n     architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the\n     FastSpeech2Conformer\n     [espnet/fastspeech2_conformer_hifigan](https://huggingface.co/espnet/fastspeech2_conformer_hifigan) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         model_in_dim (`int`, *optional*, defaults to 80):\n@@ -405,7 +405,7 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class FastSpeech2ConformerWithHifiGanConfig(PretrainedConfig):\n+class FastSpeech2ConformerWithHifiGanConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`FastSpeech2ConformerWithHifiGan`]. It is used to\n     instantiate a `FastSpeech2ConformerWithHifiGanModel` model according to the specified sub-models configurations,\n@@ -416,8 +416,8 @@ class FastSpeech2ConformerWithHifiGanConfig(PretrainedConfig):\n     FastSpeech2ConformerHifiGan\n     [espnet/fastspeech2_conformer_hifigan](https://huggingface.co/espnet/fastspeech2_conformer_hifigan) architectures.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         model_config (`typing.Dict`, *optional*):"
        },
        {
            "sha": "d0773d7b2e4bf4fa40b1256e1da81ad93c8d40ae",
            "filename": "src/transformers/models/flaubert/configuration_flaubert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,23 +17,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class FlaubertConfig(PretrainedConfig):\n+class FlaubertConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`FlaubertModel`] or a [`TFFlaubertModel`]. It is\n     used to instantiate a FlauBERT model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the FlauBERT\n     [flaubert/flaubert_base_uncased](https://huggingface.co/flaubert/flaubert_base_uncased) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         pre_norm (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "b5fcee2374f7a7fff047425303654a840509d70f",
            "filename": "src/transformers/models/flava/configuration_flava.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,23 +16,23 @@\n \n from typing import Any, Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class FlavaImageConfig(PretrainedConfig):\n+class FlavaImageConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FlavaImageModel`]. It is used to instantiate an\n     FLAVA model according to the specified arguments, defining the model architecture.\n \n     Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA\n     [facebook/flava-full](https://huggingface.co/facebook/flava-full) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -125,16 +125,16 @@ def __init__(\n         self.vocab_size = vocab_size\n \n \n-class FlavaTextConfig(PretrainedConfig):\n+class FlavaTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FlavaTextModel`]. It is used to instantiate an\n     FLAVA model according to the specified arguments, defining the model architecture.\n \n     Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA\n     [facebook/flava-full](https://huggingface.co/facebook/flava-full) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -238,16 +238,16 @@ def __init__(\n         self.pad_token_id = pad_token_id\n \n \n-class FlavaMultimodalConfig(PretrainedConfig):\n+class FlavaMultimodalConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FlavaMultimodalModel`]. It is used to instantiate\n     an FLAVA model according to the specified arguments, defining the model architecture.\n \n     Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA\n     [facebook/flava-full](https://huggingface.co/facebook/flava-full) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -324,7 +324,7 @@ def __init__(\n         self.use_cls_token = use_cls_token\n \n \n-class FlavaImageCodebookConfig(PretrainedConfig):\n+class FlavaImageCodebookConfig(PreTrainedConfig):\n     model_type = \"flava_image_codebook\"\n     base_config_key = \"image_codebook_config\"\n \n@@ -334,8 +334,8 @@ class FlavaImageCodebookConfig(PretrainedConfig):\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA\n     [facebook/flava-image-codebook](https://huggingface.co/facebook/flava-image-codebook) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_groups (`int`, *optional*, defaults to 4):\n@@ -392,15 +392,15 @@ def __init__(\n         self.initializer_range = initializer_range\n \n \n-class FlavaConfig(PretrainedConfig):\n+class FlavaConfig(PreTrainedConfig):\n     r\"\"\"\n     [`FlavaConfig`] is the configuration class to store the configuration of a [`FlavaModel`]. It is used to\n     instantiate FLAVA model according to the specified arguments, defining the text model, image model, image codebook\n     and multimodal model configs. Instantiating a configuration with the defaults will yield a similar configuration to\n     that of the FLAVA [facebook/flava-full](https://huggingface.co/facebook/flava-full) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "f762e56a86be0ca68785d08f39e70aac890d9258",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,18 +20,18 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class FlexOlmoConfig(PretrainedConfig):\n+class FlexOlmoConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FlexOlmoModel`]. It is used to instantiate an FlexOlmo\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the [allenai/FlexOlmo-7x7B-1T](https://huggingface.co/allenai/FlexOlmo-7x7B-1T).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "9ccec9454f593a84388908a93a14689b44d51931",
            "filename": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -41,8 +41,8 @@ class FlexOlmoConfig(OlmoeConfig):\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the [allenai/FlexOlmo-7x7B-1T](https://huggingface.co/allenai/FlexOlmo-7x7B-1T).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "6a32f47223d486fec57f35989077941e23366755",
            "filename": "src/transformers/models/florence2/configuration_florence2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconfiguration_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconfiguration_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fconfiguration_florence2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,22 +18,22 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Florence2VisionConfig(PretrainedConfig):\n+class Florence2VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Florence2VisionModel`]. It is used to instantiate a Florence2VisionModel\n     according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Florence2VisionModel architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         in_channels (`int`, *optional*, defaults to 3):\n@@ -134,16 +134,16 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class Florence2Config(PretrainedConfig):\n+class Florence2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Florence2ForConditionalGeneration`]. It is used to instantiate an\n     Florence-2 model according to the specified arguments, defining the model architecture.\n \n     Instantiating a configuration with the defaults will yield a similar configuration to that of the Florence-2\n     [microsoft/Florence-2-base](https://huggingface.co/microsoft/Florence-2-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "64b2c307533cf1dc27a585b88cdec079b7aab4d6",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -23,7 +23,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput\n@@ -45,14 +45,14 @@\n logger = logging.get_logger(__name__)\n \n \n-class Florence2VisionConfig(PretrainedConfig):\n+class Florence2VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Florence2VisionModel`]. It is used to instantiate a Florence2VisionModel\n     according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Florence2VisionModel architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         in_channels (`int`, *optional*, defaults to 3):\n@@ -153,16 +153,16 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class Florence2Config(PretrainedConfig):\n+class Florence2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Florence2ForConditionalGeneration`]. It is used to instantiate an\n     Florence-2 model according to the specified arguments, defining the model architecture.\n \n     Instantiating a configuration with the defaults will yield a similar configuration to that of the Florence-2\n     [microsoft/Florence-2-base](https://huggingface.co/microsoft/Florence-2-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "9ffc125e8ef5555fe7fc4ecf444d3bf57f45340a",
            "filename": "src/transformers/models/fnet/configuration_fnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffnet%2Fconfiguration_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffnet%2Fconfiguration_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fconfiguration_fnet.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"FNet model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class FNetConfig(PretrainedConfig):\n+class FNetConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FNetModel`]. It is used to instantiate an FNet\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the FNet\n     [google/fnet-base](https://huggingface.co/google/fnet-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "eabc9a8ba0afa18790a17694f387105cb9411d1c",
            "filename": "src/transformers/models/focalnet/configuration_focalnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconfiguration_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconfiguration_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconfiguration_focalnet.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"FocalNet model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n logger = logging.get_logger(__name__)\n \n \n-class FocalNetConfig(BackboneConfigMixin, PretrainedConfig):\n+class FocalNetConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FocalNetModel`]. It is used to instantiate a\n     FocalNet model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the FocalNet\n     [microsoft/focalnet-tiny](https://huggingface.co/microsoft/focalnet-tiny) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         image_size (`int`, *optional*, defaults to 224):"
        },
        {
            "sha": "a1075016c3f46ace3309035803f75dcbeccee5a6",
            "filename": "src/transformers/models/fsmt/configuration_fsmt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,14 +14,14 @@\n # limitations under the License.\n \"\"\"FSMT configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DecoderConfig(PretrainedConfig):\n+class DecoderConfig(PreTrainedConfig):\n     r\"\"\"\n     Configuration class for FSMT's decoder specific things. note: this is a private helper class\n     \"\"\"\n@@ -35,15 +35,15 @@ def __init__(self, vocab_size=0, bos_token_id=0, is_encoder_decoder=True, **kwar\n         self.is_encoder_decoder = is_encoder_decoder\n \n \n-class FSMTConfig(PretrainedConfig):\n+class FSMTConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FSMTModel`]. It is used to instantiate a FSMT\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the FSMT\n     [facebook/wmt19-en-ru](https://huggingface.co/facebook/wmt19-en-ru) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         langs (`list[str]`):"
        },
        {
            "sha": "8e09c4886c5d2e782174f8de78a2769a837aa014",
            "filename": "src/transformers/models/funnel/configuration_funnel.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"Funnel Transformer model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class FunnelConfig(PretrainedConfig):\n+class FunnelConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FunnelModel`] or a [`TFBertModel`]. It is used to\n     instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel\n     Transformer [funnel-transformer/small](https://huggingface.co/funnel-transformer/small) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):"
        },
        {
            "sha": "c13208e8825d684caff9ff0b236fd77ff2476208",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"Fuyu model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class FuyuConfig(PretrainedConfig):\n+class FuyuConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`FuyuForCausalLM`]. It is used to instantiate an\n     Fuyu model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the\n     [adept/fuyu-8b](https://huggingface.co/adept/fuyu-8b).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "6b248b76f3c1c8d87af01c64fd27d261241e3ff3",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,17 +19,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n \n \n-class GemmaConfig(PretrainedConfig):\n+class GemmaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GemmaModel`]. It is used to instantiate an Gemma\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma-7B.\n     e.g. [google/gemma-7b](https://huggingface.co/google/gemma-7b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):"
        },
        {
            "sha": "7ba39f9490def89da7b67e723a6016b779d77fd7",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,7 +20,7 @@\n from torch import nn\n \n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -52,14 +52,14 @@\n logger = logging.get_logger(__name__)\n \n \n-class GemmaConfig(PretrainedConfig):\n+class GemmaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GemmaModel`]. It is used to instantiate an Gemma\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma-7B.\n     e.g. [google/gemma-7b](https://huggingface.co/google/gemma-7b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):"
        },
        {
            "sha": "ef55c16e5d45202bd7bfd6ca903492a175ff2123",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,17 +19,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n \n \n-class Gemma2Config(PretrainedConfig):\n+class Gemma2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma2Model`]. It is used to instantiate an Gemma2\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma2-7B.\n     e.g. [google/gemma2-7b](https://huggingface.co/google/gemma2-7b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):"
        },
        {
            "sha": "61db13152c1b7998ddb11da00845af1ab7ad6faf",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,7 +20,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -47,14 +47,14 @@\n logger = logging.get_logger(__name__)\n \n \n-class Gemma2Config(PretrainedConfig):\n+class Gemma2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma2Model`]. It is used to instantiate an Gemma2\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma2-7B.\n     e.g. [google/gemma2-7b](https://huggingface.co/google/gemma2-7b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 256000):"
        },
        {
            "sha": "04483c5c38f1a3cff64908d60186e8c61d42b160",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -21,7 +21,7 @@\n # limitations under the License.\n from typing import Any, Optional, Union\n \n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n from ..siglip import SiglipVisionConfig\n@@ -30,14 +30,14 @@\n logger = logging.get_logger(__name__)\n \n \n-class Gemma3TextConfig(PretrainedConfig):\n+class Gemma3TextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3TextModel`]. It is used to instantiate an Gemma3Text\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma3Text-7B.\n     e.g. [google/gemma3_text-7b](https://huggingface.co/google/gemma3_text-7b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 262208):\n@@ -245,16 +245,16 @@ def __init__(\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n-class Gemma3Config(PretrainedConfig):\n+class Gemma3Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3ForConditionalGeneration`]. It is used to instantiate an\n     Gemma3ForConditionalGeneration according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the PaliGemma-2B.\n \n     e.g. [google/gemma-3-4b](https://huggingface.co/google/gemma-3-4b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[Gemma3TextConfig, dict]`, *optional*):"
        },
        {
            "sha": "d136b44e2f6fdc7994580df2c2a5d63acb772231",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -29,7 +29,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -759,7 +759,7 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n \n \n def create_causal_mask_mapping(\n-    config: PretrainedConfig,\n+    config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,\n@@ -1214,7 +1214,7 @@ def prepare_inputs_for_generation(\n \n     @staticmethod\n     def create_masks_for_generate(\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         input_embeds: torch.Tensor,\n         attention_mask: Optional[torch.Tensor],\n         cache_position: torch.Tensor,"
        },
        {
            "sha": "826b04a3898db0bbcf31ca83f7e44d6d195b2e0d",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -21,7 +21,7 @@\n import torch.nn as nn\n \n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n@@ -56,14 +56,14 @@\n logger = logging.get_logger(__name__)\n \n \n-class Gemma3TextConfig(Gemma2Config, PretrainedConfig):\n+class Gemma3TextConfig(Gemma2Config, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3TextModel`]. It is used to instantiate an Gemma3Text\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma3Text-7B.\n     e.g. [google/gemma3_text-7b](https://huggingface.co/google/gemma3_text-7b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 262208):\n@@ -210,7 +210,7 @@ def __init__(\n         use_bidirectional_attention=False,\n         **kwargs,\n     ):\n-        PretrainedConfig.__init__(\n+        PreTrainedConfig.__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n@@ -256,16 +256,16 @@ def __init__(\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n-class Gemma3Config(PretrainedConfig):\n+class Gemma3Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3ForConditionalGeneration`]. It is used to instantiate an\n     Gemma3ForConditionalGeneration according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the PaliGemma-2B.\n \n     e.g. [google/gemma-3-4b](https://huggingface.co/google/gemma-3-4b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[Gemma3TextConfig, dict]`, *optional*):\n@@ -725,7 +725,7 @@ def forward(self, vision_outputs: torch.Tensor):\n \n \n def create_causal_mask_mapping(\n-    config: PretrainedConfig,\n+    config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n     cache_position: torch.Tensor,"
        },
        {
            "sha": "838baa0b496a3d6a299a07e29c66985403dca6df",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -22,7 +22,7 @@\n from collections.abc import Sequence\n from typing import Any, Optional, Union\n \n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import is_timm_available, logging, requires_backends\n \n@@ -34,7 +34,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Gemma3nTextConfig(PretrainedConfig):\n+class Gemma3nTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3nTextModel`]. It is used to instantiate an\n     Gemma3nTextModel model according to the specified arguments, defining the model architecture. Instantiating a\n@@ -301,7 +301,7 @@ def __init__(\n         self.activation_sparsity_pattern = activation_sparsity_pattern\n \n \n-class Gemma3nAudioConfig(PretrainedConfig):\n+class Gemma3nAudioConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3nAudioEncoder`]. It is used to instantiate\n     an `Gemma3nAudioEncoder` model according to the specified arguments, defining the model architecture. Instantiating\n@@ -440,7 +440,7 @@ def __init__(\n         self.sscp_conv_stride_size = sscp_conv_stride_size\n \n \n-class Gemma3nVisionConfig(PretrainedConfig):\n+class Gemma3nVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration for a timm backbone [`TimmWrapper`]. It is used to\n     instantiate an timm model model according to the specified arguments, defining the model architecture.\n@@ -558,7 +558,7 @@ def to_dict(self) -> dict[str, Any]:\n         return output\n \n \n-class Gemma3nConfig(PretrainedConfig):\n+class Gemma3nConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3nForConditionalGeneration`]. It is used to\n     instantiate a Gemma3nForConditionalGeneration according to the specified arguments, defining the model\n@@ -567,8 +567,8 @@ class Gemma3nConfig(PretrainedConfig):\n \n     e.g. [google/gemma-3n-E4B](https://huggingface.co/google/gemma-3n-E4B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[Gemma3nTextConfig, dict]`, *optional*):"
        },
        {
            "sha": "95e891d9ad2bc6ebd72c60d014d106150385bbbe",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -24,7 +24,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n@@ -62,7 +62,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Gemma3nTextConfig(Gemma2Config, PretrainedConfig):\n+class Gemma3nTextConfig(Gemma2Config, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3nTextModel`]. It is used to instantiate an\n     Gemma3nTextModel model according to the specified arguments, defining the model architecture. Instantiating a\n@@ -244,7 +244,7 @@ def __init__(\n         activation_sparsity_pattern: Optional[Union[float, Sequence[float]]] = None,\n         **kwargs,\n     ):\n-        PretrainedConfig.__init__(\n+        PreTrainedConfig.__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,\n             eos_token_id=eos_token_id,\n@@ -314,7 +314,7 @@ def __init__(\n         self.activation_sparsity_pattern = activation_sparsity_pattern\n \n \n-class Gemma3nAudioConfig(PretrainedConfig):\n+class Gemma3nAudioConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3nAudioEncoder`]. It is used to instantiate\n     an `Gemma3nAudioEncoder` model according to the specified arguments, defining the model architecture. Instantiating\n@@ -522,7 +522,7 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n \n \n-class Gemma3nConfig(PretrainedConfig):\n+class Gemma3nConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3nForConditionalGeneration`]. It is used to\n     instantiate a Gemma3nForConditionalGeneration according to the specified arguments, defining the model\n@@ -531,8 +531,8 @@ class Gemma3nConfig(PretrainedConfig):\n \n     e.g. [google/gemma-3n-E4B](https://huggingface.co/google/gemma-3n-E4B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[Gemma3nTextConfig, dict]`, *optional*):"
        },
        {
            "sha": "cd8b1c92255870d065e4bf8ebcb05caee725ee5d",
            "filename": "src/transformers/models/git/configuration_git.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GitVisionConfig(PretrainedConfig):\n+class GitVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GitVisionModel`]. It is used to instantiate a GIT\n     vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the vision encoder of the GIT\n     [microsoft/git-base](https://huggingface.co/microsoft/git-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):\n@@ -102,15 +102,15 @@ def __init__(\n         self.hidden_act = hidden_act\n \n \n-class GitConfig(PretrainedConfig):\n+class GitConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GitModel`]. It is used to instantiate a GIT model\n     according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the GIT\n     [microsoft/git-base](https://huggingface.co/microsoft/git-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`dict`, *optional*):"
        },
        {
            "sha": "66a38610beb2b025401295ce1283920c18e020d3",
            "filename": "src/transformers/models/glm/configuration_glm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,17 +14,17 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n \n \n-class GlmConfig(PretrainedConfig):\n+class GlmConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GlmModel`]. It is used to instantiate an Glm\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Glm-4-9b-chat.\n     e.g. [THUDM/glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n     Args:\n         vocab_size (`int`, *optional*, defaults to 151552):\n             Vocabulary size of the Glm model. Defines the number of different tokens that can be represented by the"
        },
        {
            "sha": "10017e6d2ad92b43cb532a9930ee599fb7912524",
            "filename": "src/transformers/models/glm4/configuration_glm4.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,17 +14,17 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n \n \n-class Glm4Config(PretrainedConfig):\n+class Glm4Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4Model`]. It is used to instantiate an Glm4\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Glm4-4-9b-chat.\n     e.g. [THUDM/GLM-4-9B-0414](https://huggingface.co/THUDM/GLM-4-9B-0414)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n     Args:\n         vocab_size (`int`, *optional*, defaults to 151552):\n             Vocabulary size of the Glm4 model. Defines the number of different tokens that can be represented by the"
        },
        {
            "sha": "c86066daa98db72d60fe0ee036724e779147e716",
            "filename": "src/transformers/models/glm4_moe/configuration_glm4_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,18 +19,18 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class Glm4MoeConfig(PretrainedConfig):\n+class Glm4MoeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4MoeModel`]. It is used to instantiate a\n     Glm4Moe model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of [THUDM/GLM-4-100B-A10B](https://huggingface.co/THUDM/GLM-4-100B-A10B).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "56cf3f08dd739864b904d294ef5744981eb65e7f",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n import torch\n from torch import nn\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n from ..cohere.modeling_cohere import CohereAttention\n@@ -38,14 +38,14 @@\n logger = logging.get_logger(__name__)\n \n \n-class Glm4MoeConfig(PretrainedConfig):\n+class Glm4MoeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4MoeModel`]. It is used to instantiate a\n     Glm4Moe model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of [THUDM/GLM-4-100B-A10B](https://huggingface.co/THUDM/GLM-4-100B-A10B).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "8128065148c5aa918d8abf3a5fddb554d54fefd4",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,11 +18,11 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class Glm4vVisionConfig(PretrainedConfig):\n+class Glm4vVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vVisionModel`]. It is used to instantiate an Glm4vVisionModel\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield\n@@ -119,15 +119,15 @@ def __init__(\n         self.attention_dropout = attention_dropout\n \n \n-class Glm4vTextConfig(PretrainedConfig):\n+class Glm4vTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vModel`]. It is used to instantiate a\n     GLM-4.1V model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of\n     GLM-4.1V-9B-Thinking [THUDM/GLM-4.1V-9B-Thinking](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 151552):\n@@ -272,15 +272,15 @@ def __init__(\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n-class Glm4vConfig(PretrainedConfig):\n+class Glm4vConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vModel`]. It is used to instantiate a\n     GLM-4.1V model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of\n     GLM-4.1V-9B-Thinking [THUDM/GLM-4.1V-9B-Thinking](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "05beda2de1bb1f3d9d0f03d315b0c8edcf9e2a92",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -23,7 +23,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...masking_utils import create_causal_mask\n@@ -61,7 +61,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Glm4vVisionConfig(PretrainedConfig):\n+class Glm4vVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vVisionModel`]. It is used to instantiate an Glm4vVisionModel\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield\n@@ -158,15 +158,15 @@ def __init__(\n         self.attention_dropout = attention_dropout\n \n \n-class Glm4vTextConfig(PretrainedConfig):\n+class Glm4vTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vModel`]. It is used to instantiate a\n     GLM-4.1V model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of\n     GLM-4.1V-9B-Thinking [THUDM/GLM-4.1V-9B-Thinking](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 151552):\n@@ -311,15 +311,15 @@ def __init__(\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n-class Glm4vConfig(PretrainedConfig):\n+class Glm4vConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vModel`]. It is used to instantiate a\n     GLM-4.1V model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of\n     GLM-4.1V-9B-Thinking [THUDM/GLM-4.1V-9B-Thinking](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "51ba96d96beb34729a4d122dd8375bee12867feb",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,11 +18,11 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class Glm4vMoeVisionConfig(PretrainedConfig):\n+class Glm4vMoeVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vMoeVisionModel`]. It is used to instantiate an Glm4vMoeVisionModel\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield\n@@ -119,15 +119,15 @@ def __init__(\n         self.attention_dropout = attention_dropout\n \n \n-class Glm4vMoeTextConfig(PretrainedConfig):\n+class Glm4vMoeTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vMoeModel`]. It is used to instantiate a\n     GLM-4.5V model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of\n     GLM-4.5V [zai-org/GLM-4.5V](https://huggingface.co/zai-org/GLM-4.5V).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 151424):\n@@ -307,15 +307,15 @@ def __init__(\n         self.norm_topk_prob = norm_topk_prob\n \n \n-class Glm4vMoeConfig(PretrainedConfig):\n+class Glm4vMoeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vMoeModel`]. It is used to instantiate a\n     GLM-4.5V model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of\n     GLM-4.5V [zai-org/GLM-4.5V](https://huggingface.co/zai-org/GLM-4.5V).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "2130b460d889a18e97c445d8f7f973a5d3d840d7",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,7 +18,7 @@\n import torch.nn as nn\n \n from ...cache_utils import Cache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n@@ -57,8 +57,8 @@ class Glm4vMoeTextConfig(Glm4MoeConfig):\n     configuration with the defaults will yield a similar configuration to that of\n     GLM-4.5V [zai-org/GLM-4.5V](https://huggingface.co/zai-org/GLM-4.5V).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 151424):\n@@ -199,7 +199,7 @@ def __init__(\n         norm_topk_prob=True,\n         **kwargs,\n     ):\n-        PretrainedConfig.__init__(self, tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        PreTrainedConfig.__init__(self, tie_word_embeddings=tie_word_embeddings, **kwargs)\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -242,8 +242,8 @@ class Glm4vMoeConfig(Glm4vConfig):\n     configuration with the defaults will yield a similar configuration to that of\n     GLM-4.5V [zai-org/GLM-4.5V](https://huggingface.co/zai-org/GLM-4.5V).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "9df5e8d77c372900f1bcbc56c12155bdc4e2a673",
            "filename": "src/transformers/models/glpn/configuration_glpn.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglpn%2Fconfiguration_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fglpn%2Fconfiguration_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fconfiguration_glpn.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"GLPN model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GLPNConfig(PretrainedConfig):\n+class GLPNConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GLPNModel`]. It is used to instantiate an GLPN\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the GLPN\n     [vinvino02/glpn-kitti](https://huggingface.co/vinvino02/glpn-kitti) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "524226b099b29e23a8c04ba09d5f262c8a70da00",
            "filename": "src/transformers/models/got_ocr2/configuration_got_ocr2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,19 +20,19 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n-class GotOcr2VisionConfig(PretrainedConfig):\n+class GotOcr2VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GotOcr2VisionModel`]. It is used to instantiate a GOT_OCR2\n     vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n     defaults will yield a similar configuration to that of the SAM ViT-h\n     [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):\n@@ -115,16 +115,16 @@ def __init__(\n         self.mlp_dim = mlp_dim\n \n \n-class GotOcr2Config(PretrainedConfig):\n+class GotOcr2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GotOcr2ForConditionalGeneration`]. It is used to instantiate a\n     GotOcr2 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of GOT-OCR-2.0.\n \n     e.g [stepfun-ai/GOT-OCR-2.0-hf](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "9c3bce47fff0f9d5607784ee19db33e069800806",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,7 +20,7 @@\n import torch.nn as nn\n \n from ...cache_utils import Cache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, logging\n@@ -45,15 +45,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class GotOcr2VisionConfig(PretrainedConfig):\n+class GotOcr2VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GotOcr2VisionModel`]. It is used to instantiate a GOT_OCR2\n     vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n     defaults will yield a similar configuration to that of the SAM ViT-h\n     [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):\n@@ -136,16 +136,16 @@ def __init__(\n         self.mlp_dim = mlp_dim\n \n \n-class GotOcr2Config(PretrainedConfig):\n+class GotOcr2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GotOcr2ForConditionalGeneration`]. It is used to instantiate a\n     GotOcr2 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of GOT-OCR-2.0.\n \n     e.g [stepfun-ai/GOT-OCR-2.0-hf](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "a6667d8a57c83c5d11bc23527bedcb8b6def07ce",
            "filename": "src/transformers/models/gpt2/configuration_gpt2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,23 +20,23 @@\n from typing import Any, Optional\n \n from ... import PreTrainedTokenizer, is_torch_available\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfigWithPast, PatchingSpec\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GPT2Config(PretrainedConfig):\n+class GPT2Config(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`GPT2Model`] or a [`TFGPT2Model`]. It is used to\n     instantiate a GPT-2 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the GPT-2\n     [openai-community/gpt2](https://huggingface.co/openai-community/gpt2) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:\n@@ -193,7 +193,7 @@ def __init__(\n class GPT2OnnxConfig(OnnxConfigWithPast):\n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         task: str = \"default\",\n         patching_specs: Optional[list[PatchingSpec]] = None,\n         use_past: bool = False,"
        },
        {
            "sha": "b243964d40c07086ee543c655b13314b247e448a",
            "filename": "src/transformers/models/gpt_bigcode/configuration_gpt_bigcode.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fconfiguration_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fconfiguration_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fconfiguration_gpt_bigcode.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"GPTBigCode configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GPTBigCodeConfig(PretrainedConfig):\n+class GPTBigCodeConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`GPTBigCodeModel`]. It is used to instantiate a\n     GPTBigCode model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the GPTBigCode\n     [gpt_bigcode](https://huggingface.co/gpt_bigcode) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "ff0e8c568053bcb9d853de727a47f81922e66355",
            "filename": "src/transformers/models/gpt_neo/configuration_gpt_neo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fconfiguration_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fconfiguration_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fconfiguration_gpt_neo.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,23 +19,23 @@\n from typing import Any\n \n from ... import PreTrainedTokenizer, is_torch_available\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfigWithPast\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GPTNeoConfig(PretrainedConfig):\n+class GPTNeoConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GPTNeoModel`]. It is used to instantiate a GPT\n     Neo model according to the specified arguments, defining the model architecture. Instantiating a configuration with\n     the defaults will yield a similar configuration to that of the GPTNeo\n     [EleutherAI/gpt-neo-1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "1e1210be7117f29bf8c25db05343aeef4d5c7add",
            "filename": "src/transformers/models/gpt_neox/configuration_gpt_neox.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"GPTNeoX model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GPTNeoXConfig(PretrainedConfig):\n+class GPTNeoXConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GPTNeoXModel`]. It is used to instantiate an\n     GPTNeoX model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the GPTNeoX\n     [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "b8343b73510d1f3cdf249b7c733ac1150ccd323a",
            "filename": "src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"GPTNeoX Japanese model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GPTNeoXJapaneseConfig(PretrainedConfig):\n+class GPTNeoXJapaneseConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GPTNeoXModelJapanese`]. It is used to instantiate\n     a GPTNeoX model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the GPTNeoXJapanese\n     [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information. Default configs is set as 2.7B model\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information. Default configs is set as 2.7B model\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 32000):"
        },
        {
            "sha": "65e6606a6be9250efbc6104c1d66e67e298a73da",
            "filename": "src/transformers/models/gpt_oss/configuration_gpt_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,11 +14,11 @@\n # limitations under the License.\n \"\"\"openai model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class GptOssConfig(PretrainedConfig):\n+class GptOssConfig(PreTrainedConfig):\n     r\"\"\"\n     This will yield a configuration to that of the BERT\n     [google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) architecture."
        },
        {
            "sha": "44ed7aea8f13ef9810f0617ba3e327742efb250d",
            "filename": "src/transformers/models/gptj/configuration_gptj.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,21 +19,21 @@\n from typing import Any, Optional\n \n from ... import PreTrainedTokenizer, is_torch_available\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfigWithPast, PatchingSpec\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GPTJConfig(PretrainedConfig):\n+class GPTJConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GPTJModel`]. It is used to instantiate a GPT-J\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the GPT-J\n     [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B) architecture. Configuration objects inherit from\n-    [`PretrainedConfig`] and can be used to control the model outputs. Read the documentation from [`PretrainedConfig`]\n+    [`PreTrainedConfig`] and can be used to control the model outputs. Read the documentation from [`PreTrainedConfig`]\n     for more information.\n \n     Args:\n@@ -139,7 +139,7 @@ def __init__(\n class GPTJOnnxConfig(OnnxConfigWithPast):\n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         task: str = \"default\",\n         patching_specs: Optional[list[PatchingSpec]] = None,\n         use_past: bool = False,"
        },
        {
            "sha": "04c797ecbfd91ae2774e97ac2ed994a87e7a4f42",
            "filename": "src/transformers/models/granite/configuration_granite.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,22 +19,22 @@\n # limitations under the License.\n \"\"\"Granite model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GraniteConfig(PretrainedConfig):\n+class GraniteConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GraniteModel`]. It is used to instantiate an Granite\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Granite-3B.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "8487ab9de6e468aa7b7c1d199d1d108bcd650ac2",
            "filename": "src/transformers/models/granite_speech/configuration_granite_speech.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,19 +14,19 @@\n # limitations under the License.\n \"\"\"Config class for Granite Speech.\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n-class GraniteSpeechEncoderConfig(PretrainedConfig):\n+class GraniteSpeechEncoderConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GraniteSpeechCTCEncoder`]. It is used to instantiate\n     a Granite Speech audio encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the dfefaults will yield a similar configuration to that of the audio encoder of the Granite Speech\n     architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         input_dim (`int`, *optional*, defaults to 160):\n@@ -104,13 +104,13 @@ def __init__(\n         self.max_pos_emb = max_pos_emb\n \n \n-class GraniteSpeechConfig(PretrainedConfig):\n+class GraniteSpeechConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GraniteSpeechForConditionalGeneration`]. It is used to instantiate an\n     Granite Speech model according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `GraniteConfig`):"
        },
        {
            "sha": "beb4e7d7b0aea969754bd7254dd2a89f373f15f3",
            "filename": "src/transformers/models/granitemoe/configuration_granitemoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,22 +19,22 @@\n # limitations under the License.\n \"\"\"GraniteMoe model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GraniteMoeConfig(PretrainedConfig):\n+class GraniteMoeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GraniteMoeModel`]. It is used to instantiate an GraniteMoe\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the GraniteMoe-3B.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "eed40cd6f0129a1cc8f269205f5183817498d965",
            "filename": "src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -15,20 +15,20 @@\n # limitations under the License.\n \"\"\"GraniteMoeHybrid model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GraniteMoeHybridConfig(PretrainedConfig):\n+class GraniteMoeHybridConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GraniteMoeHybridConfig`]. It is used to\n     instantiate an GraniteMoeHybrid model according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "e75688a6f45ec136afd848c312de84168df0c017",
            "filename": "src/transformers/models/granitemoeshared/configuration_granitemoeshared.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,22 +19,22 @@\n # limitations under the License.\n \"\"\"GraniteMoeShared model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class GraniteMoeSharedConfig(PretrainedConfig):\n+class GraniteMoeSharedConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GraniteMoeSharedModel`]. It is used to instantiate an GraniteMoeShared\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the [ibm-research/moe-7b-1b-active-shared-experts](https://huggingface.co/ibm-research/moe-7b-1b-active-shared-experts).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "0944bfc015bc1d040fd954b604302b3e7fa8dc35",
            "filename": "src/transformers/models/grounding_dino/configuration_grounding_dino.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Grounding DINO model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n from ..auto import CONFIG_MAPPING\n@@ -23,18 +23,18 @@\n logger = logging.get_logger(__name__)\n \n \n-class GroundingDinoConfig(PretrainedConfig):\n+class GroundingDinoConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GroundingDinoModel`]. It is used to instantiate a\n     Grounding DINO model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the Grounding DINO\n     [IDEA-Research/grounding-dino-tiny](https://huggingface.co/IDEA-Research/grounding-dino-tiny) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`PretrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n+        backbone_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n@@ -299,9 +299,9 @@ def sub_configs(self):\n         sub_configs = {}\n         backbone_config = getattr(self, \"backbone_config\", None)\n         text_config = getattr(self, \"text_config\", None)\n-        if isinstance(backbone_config, PretrainedConfig):\n+        if isinstance(backbone_config, PreTrainedConfig):\n             sub_configs[\"backbone_config\"] = type(backbone_config)\n-        if isinstance(text_config, PretrainedConfig):\n+        if isinstance(text_config, PreTrainedConfig):\n             sub_configs[\"text_config\"] = type(self.text_config)\n         return sub_configs\n "
        },
        {
            "sha": "0f432e621b5c6e1289afa54217491af67e93700d",
            "filename": "src/transformers/models/groupvit/configuration_groupvit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,7 +18,7 @@\n from collections.abc import Mapping\n from typing import TYPE_CHECKING, Any\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n@@ -30,15 +30,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class GroupViTTextConfig(PretrainedConfig):\n+class GroupViTTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GroupViTTextModel`]. It is used to instantiate an\n     GroupViT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the GroupViT\n     [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 49408):\n@@ -122,15 +122,15 @@ def __init__(\n         self.attention_dropout = attention_dropout\n \n \n-class GroupViTVisionConfig(PretrainedConfig):\n+class GroupViTVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`GroupViTVisionModel`]. It is used to instantiate\n     an GroupViT model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the GroupViT\n     [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 384):\n@@ -230,15 +230,15 @@ def __init__(\n         self.assign_mlp_ratio = assign_mlp_ratio\n \n \n-class GroupViTConfig(PretrainedConfig):\n+class GroupViTConfig(PreTrainedConfig):\n     r\"\"\"\n     [`GroupViTConfig`] is the configuration class to store the configuration of a [`GroupViTModel`]. It is used to\n     instantiate a GroupViT model according to the specified arguments, defining the text model and vision model\n     configs. Instantiating a configuration with the defaults will yield a similar configuration to that of the GroupViT\n     [nvidia/groupvit-gcc-yfcc](https://huggingface.co/nvidia/groupvit-gcc-yfcc) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`dict`, *optional*):"
        },
        {
            "sha": "a76a553e1425d281668f5b51b2b8ed26eb3c0d5c",
            "filename": "src/transformers/models/helium/configuration_helium.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,17 +14,17 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n \n \n-class HeliumConfig(PretrainedConfig):\n+class HeliumConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`HeliumModel`]. It is used to instantiate an Helium\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Helium 2b model.\n     e.g. [kyutai/helium-2b](https://huggingface.co/kyutai/helium-2b)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 48000):"
        },
        {
            "sha": "74cee382360fe54ad6605567df6d967fc907f41f",
            "filename": "src/transformers/models/hgnet_v2/configuration_hgnet_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fconfiguration_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fconfiguration_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fconfiguration_hgnet_v2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,19 +20,19 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n # TODO: Modular conversion for resnet must be fixed as\n # it provides incorrect import for configuration like resnet_resnet\n-class HGNetV2Config(BackboneConfigMixin, PretrainedConfig):\n+class HGNetV2Config(BackboneConfigMixin, PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`HGNetV2Backbone`]. It is used to instantiate a HGNet-V2\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of D-FINE-X-COCO B4 \"[ustc-community/dfine_x_coco\"](https://huggingface.co/ustc-community/dfine_x_coco\").\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "627c6aed625559086e9a9e1c0fa6147cfa9e2bab",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,7 +20,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_outputs import (\n     BackboneOutput,\n     BaseModelOutputWithNoAttention,\n@@ -36,13 +36,13 @@\n \n # TODO: Modular conversion for resnet must be fixed as\n # it provides incorrect import for configuration like resnet_resnet\n-class HGNetV2Config(BackboneConfigMixin, PretrainedConfig):\n+class HGNetV2Config(BackboneConfigMixin, PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`HGNetV2Backbone`]. It is used to instantiate a HGNet-V2\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of D-FINE-X-COCO B4 \"[ustc-community/dfine_x_coco\"](https://huggingface.co/ustc-community/dfine_x_coco\").\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "11e220ab8e1e8caf08b3772ec9c116dc516877ec",
            "filename": "src/transformers/models/hiera/configuration_hiera.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhiera%2Fconfiguration_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhiera%2Fconfiguration_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fconfiguration_hiera.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"Hiera model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n logger = logging.get_logger(__name__)\n \n \n-class HieraConfig(BackboneConfigMixin, PretrainedConfig):\n+class HieraConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`HieraModel`]. It is used to instantiate a Hiera\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Hiera\n     [facebook/hiera-base-224](https://huggingface.co/facebook/hiera-base-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         embed_dim (`int`, *optional*, defaults to 96):"
        },
        {
            "sha": "b0486a9e76392a2a21c9ee7896255004345827a4",
            "filename": "src/transformers/models/hubert/configuration_hubert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -17,22 +17,22 @@\n import functools\n import operator\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class HubertConfig(PretrainedConfig):\n+class HubertConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`HubertModel`]. It is used to instantiate an\n     Hubert model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Hubert\n     [facebook/hubert-base-ls960](https://huggingface.co/facebook/hubert-base-ls960) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "083fb8e3c40d70630647c887135e07c116ee4077",
            "filename": "src/transformers/models/hunyuan_v1_dense/configuration_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"HunYuanDenseV1 model configuration\"\"\"\n \n-from transformers.configuration_utils import PretrainedConfig\n+from transformers.configuration_utils import PreTrainedConfig\n from transformers.utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class HunYuanDenseV1Config(PretrainedConfig):\n+class HunYuanDenseV1Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`HunYuanDenseV1Config`]. It is used to instantiate an\n     HunYuan model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the HunYuan-7B.\n     Hunyuan-7B-Instruct [tencent/Hunyuan-7B-Instruct](https://huggingface.co/tencent/Hunyuan-7B-Instruct).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "431e19861ab729e20442f2e2d769157fb8c4eb62",
            "filename": "src/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,22 +16,22 @@\n \n from typing import Union\n \n-from transformers.configuration_utils import PretrainedConfig\n+from transformers.configuration_utils import PreTrainedConfig\n from transformers.utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class HunYuanMoEV1Config(PretrainedConfig):\n+class HunYuanMoEV1Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`HunYuanMoEV1Model`]. It is used to instantiate an\n     HunYuan model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the HunYuan-7B.\n     Hunyuan-A13B-Instruct [tencent/Hunyuan-A13B-Instruct](https://huggingface.co/tencent/Hunyuan-A13B-Instruct).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "5653daafc9bc4a8d218546f31b73693c2b53e50f",
            "filename": "src/transformers/models/ibert/configuration_ibert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,23 +19,23 @@\n from collections import OrderedDict\n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class IBertConfig(PretrainedConfig):\n+class IBertConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`IBertModel`]. It is used to instantiate a I-BERT\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the IBERT\n     [kssteven/ibert-roberta-base](https://huggingface.co/kssteven/ibert-roberta-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):"
        },
        {
            "sha": "833ef91dcd4854ccc70d868d03c8ec14deaf40d0",
            "filename": "src/transformers/models/idefics/configuration_idefics.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fidefics%2Fconfiguration_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fidefics%2Fconfiguration_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fconfiguration_idefics.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,23 +19,23 @@\n # limitations under the License.\n \"\"\"Idefics model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class IdeficsVisionConfig(PretrainedConfig):\n+class IdeficsVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`IdeficsModel`]. It is used to instantiate an\n     Idefics model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Idefics-9B.\n \n     e.g. [HuggingFaceM4/idefics-9b](https://huggingface.co/HuggingFaceM4/idefics-9b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         embed_dim (`int`, *optional*, defaults to 768):\n@@ -103,16 +103,16 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class IdeficsPerceiverConfig(PretrainedConfig):\n+class IdeficsPerceiverConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`IdeficsModel`]. It is used to instantiate an\n     Idefics model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Idefics-9B.\n \n     e.g. [HuggingFaceM4/idefics-9b](https://huggingface.co/HuggingFaceM4/idefics-9b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         use_resampler (`bool`, *optional*, defaults to `False`):\n@@ -151,16 +151,16 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class IdeficsConfig(PretrainedConfig):\n+class IdeficsConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`IdeficsModel`]. It is used to instantiate an\n     Idefics model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Idefics-9B.\n \n     e.g. [HuggingFaceM4/idefics-9b](https://huggingface.co/HuggingFaceM4/idefics-9b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         additional_vocab_size (`int`, *optional*, defaults to 0):\n@@ -316,7 +316,7 @@ def __init__(\n         )\n \n         # IMPORTANT: Do not do any __init__ args-based checks in the constructor, since\n-        # PretrainedConfig.from_dict first instantiates the class with the config dict and only then\n+        # PreTrainedConfig.from_dict first instantiates the class with the config dict and only then\n         # updates the config object with `kwargs` from from_pretrained, so during the instantiation\n         # of this object many attributes have default values and haven't yet been overridden.\n         # Do any required checks inside `from_pretrained` once the superclass' `from_pretrained` was run."
        },
        {
            "sha": "5d730a03cf1214ee5b2a7b12f254636a3a4b0138",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -32,7 +32,7 @@\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import ModelOutput\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PretrainedConfig, PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedConfig, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n@@ -485,7 +485,7 @@ def __init__(\n         num_heads: int,\n         dropout: float = 0.0,\n         is_cross_attention: bool = False,\n-        config: Optional[PretrainedConfig] = None,\n+        config: Optional[PreTrainedConfig] = None,\n         qk_layer_norms: bool = False,\n         layer_idx: Optional[int] = None,\n     ):"
        },
        {
            "sha": "2edb798a053bf9caea6d60d6677a920841fe48ba",
            "filename": "src/transformers/models/idefics2/configuration_idefics2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,24 +13,24 @@\n # limitations under the License.\n \"\"\"Idefics2 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Idefics2VisionConfig(PretrainedConfig):\n+class Idefics2VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Idefics2VisionModel`]. It is used to instantiate a\n     Idefics2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the SigLIP checkpoint\n     [google/siglip-base-patch16-224](https://huggingface.co/google/siglip-base-patch16-224) used in the Idefics2 model\n     [HuggingFaceM4/idefics2-8b](https://huggingface.co/HuggingFaceM4/idefics2-8b).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):\n@@ -106,10 +106,10 @@ def __init__(\n         self.initializer_range = initializer_range\n \n \n-class Idefics2PerceiverConfig(PretrainedConfig):\n+class Idefics2PerceiverConfig(PreTrainedConfig):\n     r\"\"\"\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n@@ -168,15 +168,15 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class Idefics2Config(PretrainedConfig):\n+class Idefics2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Idefics2Model`]. It is used to instantiate a\n     Idefics2 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the model of the Idefics2\n     [HuggingFaceM4/idefics2-8b](https://huggingface.co/HuggingFaceM4/idefics2-8b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         use_cache (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "159fa48a4f5fbd3f3c7d731026f8fcd004398a14",
            "filename": "src/transformers/models/idefics3/configuration_idefics3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,24 +13,24 @@\n # limitations under the License.\n \"\"\"Idefics3 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Idefics3VisionConfig(PretrainedConfig):\n+class Idefics3VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Idefics3VisionModel`]. It is used to instantiate a\n     Idefics3 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the SigLIP checkpoint\n     [google/siglip-base-patch16-224](https://huggingface.co/google/siglip-base-patch16-224) used in the Idefics3 model\n     [HuggingFaceM4/Idefics3-8B-Llama3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3).\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1152):\n@@ -106,15 +106,15 @@ def __init__(\n         self.initializer_range = initializer_range\n \n \n-class Idefics3Config(PretrainedConfig):\n+class Idefics3Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Idefics3Model`]. It is used to instantiate a\n     Idefics3 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the model of the Idefics3\n     [HuggingFaceM4/Idefics3-8B-Llama3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         use_cache (`bool`, *optional*, defaults to `True`):\n@@ -126,7 +126,7 @@ class Idefics3Config(PretrainedConfig):\n             Whether or not to tie the word embeddings with the token embeddings.\n         vision_config (`IdeficsVisionConfig` or `dict`, *optional*, defaults to `IdeficsVisionConfig`):\n             Custom vision config or dict for the vision tower\n-        text_config (`PretrainedConfig` or `dict`, *optional*, defaults to `LlamaConfig`):\n+        text_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `LlamaConfig`):\n             Custom text config or dict for the text model\n         scale_factor (`int`, *optional*, defaults to 2):\n             The scale factor for the image encoder."
        },
        {
            "sha": "c5568374e3d0305a67d8299a90014cd9c677815c",
            "filename": "src/transformers/models/ijepa/configuration_ijepa.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fconfiguration_ijepa.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,18 +14,18 @@\n # limitations under the License.\n \"\"\"I-JEPA model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n \n \n-class IJepaConfig(PretrainedConfig):\n+class IJepaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`IJepaModel`]. It is used to instantiate an IJEPA\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the I-JEPA\n     [facebook/ijepa_vith14_1k](https://huggingface.co/facebook/ijepa_vith14_1k) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "abd07c5feb46da7368f716d8dac5131cf3a4ef3b",
            "filename": "src/transformers/models/imagegpt/configuration_imagegpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconfiguration_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconfiguration_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconfiguration_imagegpt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,7 +18,7 @@\n from collections.abc import Mapping\n from typing import TYPE_CHECKING, Any\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n@@ -29,15 +29,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class ImageGPTConfig(PretrainedConfig):\n+class ImageGPTConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`ImageGPTModel`] or a [`TFImageGPTModel`]. It is\n     used to instantiate a GPT-2 model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the ImageGPT\n     [openai/imagegpt-small](https://huggingface.co/openai/imagegpt-small) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "7a3c5949ada43094962a02a8072da5421fa443c2",
            "filename": "src/transformers/models/informer/configuration_informer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finformer%2Fconfiguration_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finformer%2Fconfiguration_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fconfiguration_informer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,22 +16,22 @@\n \n from typing import Optional, Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class InformerConfig(PretrainedConfig):\n+class InformerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of an [`InformerModel`]. It is used to instantiate an\n     Informer model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Informer\n     [huggingface/informer-tourism-monthly](https://huggingface.co/huggingface/informer-tourism-monthly) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         prediction_length (`int`):"
        },
        {
            "sha": "ac8dd0637406ac1d1647feca30d55198007332fc",
            "filename": "src/transformers/models/instructblip/configuration_instructblip.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"InstructBLIP model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -23,15 +23,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class InstructBlipVisionConfig(PretrainedConfig):\n+class InstructBlipVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`InstructBlipVisionModel`]. It is used to\n     instantiate a InstructBLIP vision encoder according to the specified arguments, defining the model architecture.\n     Instantiating a configuration defaults will yield a similar configuration to that of the InstructBLIP\n     [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1408):\n@@ -107,14 +107,14 @@ def __init__(\n         self.qkv_bias = qkv_bias\n \n \n-class InstructBlipQFormerConfig(PretrainedConfig):\n+class InstructBlipQFormerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`InstructBlipQFormerModel`]. It is used to\n     instantiate a InstructBLIP Querying Transformer (Q-Former) model according to the specified arguments, defining the\n     model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of\n     the InstructBLIP [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)\n-    architecture. Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs.\n-    Read the documentation from [`PretrainedConfig`] for more information.\n+    architecture. Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs.\n+    Read the documentation from [`PreTrainedConfig`] for more information.\n \n     Note that [`InstructBlipQFormerModel`] is very similar to [`BertLMHeadModel`] with interleaved cross-attention.\n \n@@ -211,24 +211,24 @@ def __init__(\n         self.encoder_hidden_size = encoder_hidden_size\n \n \n-class InstructBlipConfig(PretrainedConfig):\n+class InstructBlipConfig(PreTrainedConfig):\n     r\"\"\"\n     [`InstructBlipConfig`] is the configuration class to store the configuration of a\n     [`InstructBlipForConditionalGeneration`]. It is used to instantiate a InstructBLIP model according to the specified\n     arguments, defining the vision model, Q-Former model and language model configs. Instantiating a configuration with\n     the defaults will yield a similar configuration to that of the InstructBLIP\n     [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`dict`, *optional*):\n             Dictionary of configuration options used to initialize [`InstructBlipVisionConfig`].\n         qformer_config (`dict`, *optional*):\n             Dictionary of configuration options used to initialize [`InstructBlipQFormerConfig`].\n         text_config (`dict`, *optional*):\n-            Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n+            Dictionary of configuration options used to initialize any [`PreTrainedConfig`].\n         num_query_tokens (`int`, *optional*, defaults to 32):\n             The number of query tokens passed through the Transformer.\n \n@@ -257,7 +257,7 @@ class InstructBlipConfig(PretrainedConfig):\n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n \n-    >>> # We can also initialize a InstructBlipConfig from a InstructBlipVisionConfig, InstructBlipQFormerConfig and any PretrainedConfig\n+    >>> # We can also initialize a InstructBlipConfig from a InstructBlipVisionConfig, InstructBlipQFormerConfig and any PreTrainedConfig\n \n     >>> # Initializing InstructBLIP vision, InstructBLIP Q-Former and language model configurations\n     >>> vision_config = InstructBlipVisionConfig()\n@@ -317,7 +317,7 @@ def from_vision_qformer_text_configs(\n         cls,\n         vision_config: InstructBlipVisionConfig,\n         qformer_config: InstructBlipQFormerConfig,\n-        text_config: PretrainedConfig,\n+        text_config: PreTrainedConfig,\n         **kwargs,\n     ):\n         r\"\"\""
        },
        {
            "sha": "89c45472b8883d9c7041c8b3f261a76913c33b77",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,7 +20,7 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -29,15 +29,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class InstructBlipVideoVisionConfig(PretrainedConfig):\n+class InstructBlipVideoVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`InstructBlipVideoVisionModel`]. It is used to\n     instantiate a InstructBlipVideo vision encoder according to the specified arguments, defining the model architecture.\n     Instantiating a configuration defaults will yield a similar configuration to that of the InstructBlipVideo\n     [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1408):\n@@ -113,14 +113,14 @@ def __init__(\n         self.qkv_bias = qkv_bias\n \n \n-class InstructBlipVideoQFormerConfig(PretrainedConfig):\n+class InstructBlipVideoQFormerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`InstructBlipVideoQFormerModel`]. It is used to\n     instantiate a InstructBlipVideo Querying Transformer (Q-Former) model according to the specified arguments, defining the\n     model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of\n     the InstructBlipVideo [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5)\n-    architecture. Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs.\n-    Read the documentation from [`PretrainedConfig`] for more information.\n+    architecture. Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs.\n+    Read the documentation from [`PreTrainedConfig`] for more information.\n \n     Note that [`InstructBlipVideoQFormerModel`] is very similar to [`BertLMHeadModel`] with interleaved cross-attention.\n \n@@ -217,24 +217,24 @@ def __init__(\n         self.encoder_hidden_size = encoder_hidden_size\n \n \n-class InstructBlipVideoConfig(PretrainedConfig):\n+class InstructBlipVideoConfig(PreTrainedConfig):\n     r\"\"\"\n     [`InstructBlipVideoConfig`] is the configuration class to store the configuration of a\n     [`InstructBlipVideoForConditionalGeneration`]. It is used to instantiate a Instructblipvideo model according to the specified\n     arguments, defining the vision model, Q-Former model and language model configs. Instantiating a configuration with\n     the defaults will yield a similar configuration to that of the Instructblipvideo\n     [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`dict`, *optional*):\n             Dictionary of configuration options used to initialize [`InstructBlipVideoVisionConfig`].\n         qformer_config (`dict`, *optional*):\n             Dictionary of configuration options used to initialize [`InstructBlipVideoQFormerConfig`].\n         text_config (`dict`, *optional*):\n-            Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n+            Dictionary of configuration options used to initialize any [`PreTrainedConfig`].\n         num_query_tokens (`int`, *optional*, defaults to 32):\n             The number of query tokens passed through the Transformer.\n \n@@ -263,7 +263,7 @@ class InstructBlipVideoConfig(PretrainedConfig):\n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n \n-    >>> # We can also initialize a InstructBlipVideoConfig from a InstructBlipVideoVisionConfig, InstructBlipVideoQFormerConfig and any PretrainedConfig\n+    >>> # We can also initialize a InstructBlipVideoConfig from a InstructBlipVideoVisionConfig, InstructBlipVideoQFormerConfig and any PreTrainedConfig\n \n     >>> # Initializing Instructblipvideo vision, Instructblipvideo Q-Former and language model configurations\n     >>> vision_config = InstructBlipVideoVisionConfig()\n@@ -323,7 +323,7 @@ def from_vision_qformer_text_configs(\n         cls,\n         vision_config: InstructBlipVideoVisionConfig,\n         qformer_config: InstructBlipVideoQFormerConfig,\n-        text_config: PretrainedConfig,\n+        text_config: PreTrainedConfig,\n         **kwargs,\n     ):\n         r\"\"\""
        },
        {
            "sha": "3f96eb3f88aff96485567fadb01400fa5c797cba",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -31,7 +31,7 @@\n     TransformersKwargs,\n )\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n from ...processing_utils import Unpack\n@@ -50,24 +50,24 @@ class InstructBlipVideoQFormerConfig(InstructBlipQFormerConfig):\n     pass\n \n \n-class InstructBlipVideoConfig(PretrainedConfig):\n+class InstructBlipVideoConfig(PreTrainedConfig):\n     r\"\"\"\n     [`InstructBlipVideoConfig`] is the configuration class to store the configuration of a\n     [`InstructBlipVideoForConditionalGeneration`]. It is used to instantiate a Instructblipvideo model according to the specified\n     arguments, defining the vision model, Q-Former model and language model configs. Instantiating a configuration with\n     the defaults will yield a similar configuration to that of the Instructblipvideo\n     [Salesforce/instruct-blip-flan-t5](https://huggingface.co/Salesforce/instruct-blip-flan-t5) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`dict`, *optional*):\n             Dictionary of configuration options used to initialize [`InstructBlipVideoVisionConfig`].\n         qformer_config (`dict`, *optional*):\n             Dictionary of configuration options used to initialize [`InstructBlipVideoQFormerConfig`].\n         text_config (`dict`, *optional*):\n-            Dictionary of configuration options used to initialize any [`PretrainedConfig`].\n+            Dictionary of configuration options used to initialize any [`PreTrainedConfig`].\n         num_query_tokens (`int`, *optional*, defaults to 32):\n             The number of query tokens passed through the Transformer.\n \n@@ -96,7 +96,7 @@ class InstructBlipVideoConfig(PretrainedConfig):\n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n \n-    >>> # We can also initialize a InstructBlipVideoConfig from a InstructBlipVideoVisionConfig, InstructBlipVideoQFormerConfig and any PretrainedConfig\n+    >>> # We can also initialize a InstructBlipVideoConfig from a InstructBlipVideoVisionConfig, InstructBlipVideoQFormerConfig and any PreTrainedConfig\n \n     >>> # Initializing Instructblipvideo vision, Instructblipvideo Q-Former and language model configurations\n     >>> vision_config = InstructBlipVideoVisionConfig()\n@@ -156,7 +156,7 @@ def from_vision_qformer_text_configs(\n         cls,\n         vision_config: InstructBlipVideoVisionConfig,\n         qformer_config: InstructBlipVideoQFormerConfig,\n-        text_config: PretrainedConfig,\n+        text_config: PreTrainedConfig,\n         **kwargs,\n     ):\n         r\"\"\""
        },
        {
            "sha": "a5849c1d69c9b725020fe4a0fe01245cf2a30c76",
            "filename": "src/transformers/models/internvl/configuration_internvl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finternvl%2Fconfiguration_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Finternvl%2Fconfiguration_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconfiguration_internvl.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,11 +14,11 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n-class InternVLVisionConfig(PretrainedConfig):\n+class InternVLVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`InternVLVisionModel`]. It is used to instantiate an InternVLVisionModel\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield\n@@ -139,15 +139,15 @@ def __init__(\n         self.use_mean_pooling = use_mean_pooling\n \n \n-class InternVLConfig(PretrainedConfig):\n+class InternVLConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`InternVLForConditionalGeneration`]. It is used to instantiate a\n     InternVL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of InternVL3-1B.\n     e.g. [OpenGVLab/InternVL3-1B-hf](https://huggingface.co/OpenGVLab/InternVL3-1B-hf)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "d9b22056f59202930a4daeeadd8a0748a4294f22",
            "filename": "src/transformers/models/jamba/configuration_jamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,23 +16,23 @@\n \n import math\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class JambaConfig(PretrainedConfig):\n+class JambaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`JambaModel`]. It is used to instantiate a\n     Jamba model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Jamba-v0.1 model.\n \n     [ai21labs/Jamba-v0.1](https://huggingface.co/ai21labs/Jamba-v0.1)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "4f4f29e21741beda62f7167fcaf07cf33d14bbf9",
            "filename": "src/transformers/models/janus/configuration_janus.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,21 +19,21 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class JanusVisionConfig(PretrainedConfig):\n+class JanusVisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`JanusVisionModel`]. It is used to instantiate a\n     `JanusVisionModel` according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n     Args:\n         hidden_size (`int`, *optional*, defaults to 1024):\n             Dimensionality of the encoder layers and the pooler layer.\n@@ -122,12 +122,12 @@ def __init__(\n         self.num_image_tokens = num_image_tokens\n \n \n-class JanusVQVAEConfig(PretrainedConfig):\n+class JanusVQVAEConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`JanusVQVAEModel`]. It is used to instantiate a\n     `JanusVQVAEModel` according to the specified arguments, defining the model architecture.\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information. Instantiating a\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information. Instantiating a\n     configuration with the defaults will yield a similar configuration to the VQModel of the\n     [deepseek-community/Janus-Pro-1B](https://huggingface.co/deepseek-community/Janus-Pro-1B).\n \n@@ -209,7 +209,7 @@ def __init__(\n         self.image_token_embed_dim = image_token_embed_dim\n \n \n-class JanusConfig(PretrainedConfig):\n+class JanusConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`JanusModel`]. It is used to instantiate an\n     Janus model according to the specified arguments, defining the model architecture. Instantiating a configuration\n@@ -218,8 +218,8 @@ class JanusConfig(PretrainedConfig):\n     e.g. [deepseek-community/Janus-Pro-1B](https://huggingface.co/deepseek-community/Janus-Pro-1B) or\n     [deepseek-community/Janus-Pro-7B](https://huggingface.co/deepseek-community/Janus-Pro-7B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n@@ -277,7 +277,7 @@ def __init__(\n         elif text_config is None:\n             logger.info(\"`text_config` is None. Initializing with default values\")\n             self.text_config = CONFIG_MAPPING[\"llama\"]()\n-        elif isinstance(text_config, PretrainedConfig):\n+        elif isinstance(text_config, PreTrainedConfig):\n             self.text_config = text_config\n         else:\n             raise ValueError("
        },
        {
            "sha": "a2f2541d84fa54903e46e6e3fbef05a59519b343",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -28,7 +28,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...generation import ClassifierFreeGuidanceLogitsProcessor, GenerationMixin, GenerationMode, LogitsProcessorList\n from ...generation.utils import GenerateDecoderOnlyOutput\n from ...image_processing_utils import BatchFeature, get_size_dict\n@@ -86,8 +86,8 @@ class JanusVisionConfig(SiglipVisionConfig):\n     This is the configuration class to store the configuration of a [`JanusVisionModel`]. It is used to instantiate a\n     `JanusVisionModel` according to the specified arguments, defining the model architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n     Args:\n         hidden_size (`int`, *optional*, defaults to 1024):\n             Dimensionality of the encoder layers and the pooler layer.\n@@ -182,8 +182,8 @@ class JanusVQVAEConfig(ChameleonVQVAEConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`JanusVQVAEModel`]. It is used to instantiate a\n     `JanusVQVAEModel` according to the specified arguments, defining the model architecture.\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information. Instantiating a\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information. Instantiating a\n     configuration with the defaults will yield a similar configuration to the VQModel of the\n     [deepseek-community/Janus-Pro-1B](https://huggingface.co/deepseek-community/Janus-Pro-1B).\n \n@@ -268,7 +268,7 @@ def __init__(\n         del self.attn_type\n \n \n-class JanusConfig(PretrainedConfig):\n+class JanusConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`JanusModel`]. It is used to instantiate an\n     Janus model according to the specified arguments, defining the model architecture. Instantiating a configuration\n@@ -277,8 +277,8 @@ class JanusConfig(PretrainedConfig):\n     e.g. [deepseek-community/Janus-Pro-1B](https://huggingface.co/deepseek-community/Janus-Pro-1B) or\n     [deepseek-community/Janus-Pro-7B](https://huggingface.co/deepseek-community/Janus-Pro-7B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n@@ -336,7 +336,7 @@ def __init__(\n         elif text_config is None:\n             logger.info(\"`text_config` is None. Initializing with default values\")\n             self.text_config = CONFIG_MAPPING[\"llama\"]()\n-        elif isinstance(text_config, PretrainedConfig):\n+        elif isinstance(text_config, PreTrainedConfig):\n             self.text_config = text_config\n         else:\n             raise ValueError("
        },
        {
            "sha": "bc466a8f44ae26889a32189be248fe2686ff3135",
            "filename": "src/transformers/models/jetmoe/configuration_jetmoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,23 +14,23 @@\n # limitations under the License.\n \"\"\"JetMoe model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class JetMoeConfig(PretrainedConfig):\n+class JetMoeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`JetMoeModel`]. It is used to instantiate a\n     JetMoe model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a configuration of the JetMoe-4B.\n \n     [jetmoe/jetmoe-8b](https://huggingface.co/jetmoe/jetmoe-8b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "1ad8b133f0216a9a0457e53a8e2ab5874eefeee9",
            "filename": "src/transformers/models/kosmos2/configuration_kosmos2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fconfiguration_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fconfiguration_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fconfiguration_kosmos2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"KOSMOS-2 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Kosmos2TextConfig(PretrainedConfig):\n+class Kosmos2TextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Kosmos2TextModel`]. It is used to instantiate a\n     KOSMOS-2 text decoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the text decoder of the KOSMOS-2\n     [microsoft/kosmos-2-patch14-224](https://huggingface.co/microsoft/kosmos-2-patch14-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 65037):\n@@ -129,15 +129,15 @@ def __init__(\n         self.use_cache = use_cache\n \n \n-class Kosmos2VisionConfig(PretrainedConfig):\n+class Kosmos2VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Kosmos2VisionModel`]. It is used to instantiate a\n     KOSMOS-2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the vision encoder of the KOSMOS-2\n     [microsoft/kosmos-2-patch14-224](https://huggingface.co/microsoft/kosmos-2-patch14-224) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1024):\n@@ -203,7 +203,7 @@ def __init__(\n         self.hidden_act = hidden_act\n \n \n-class Kosmos2Config(PretrainedConfig):\n+class Kosmos2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Kosmos2Model`]. It is used to instantiate a\n     KOSMOS-2 model according to the specified arguments, defining the model architecture. Instantiating a configuration"
        },
        {
            "sha": "d3044eb4cb26c70b65cee1d9a1d1910c496a2626",
            "filename": "src/transformers/models/kosmos2_5/configuration_kosmos2_5.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fconfiguration_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fconfiguration_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fconfiguration_kosmos2_5.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"KOSMOS-2.5 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Kosmos2_5TextConfig(PretrainedConfig):\n+class Kosmos2_5TextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Kosmos2_5TextModel`]. It is used to instantiate a\n     KOSMOS-2.5 text decoder according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the text decoder of the KOSMOS-2.5\n     [microsoft/kosmos-2.5](https://huggingface.co/microsoft/kosmos-2.5) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 108481):\n@@ -123,15 +123,15 @@ def __init__(\n         self.use_cache = use_cache\n \n \n-class Kosmos2_5VisionConfig(PretrainedConfig):\n+class Kosmos2_5VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Kosmos2_5VisionModel`]. It is used to\n     instantiate a KOSMOS-2.5 vision encoder according to the specified arguments, defining the model architecture.\n     Instantiating a configuration defaults will yield a similar configuration to that of the vision encoder of the KOSMOS-2.5\n     [microsoft/kosmos-2.5](https://huggingface.co/microsoft/kosmos-2.5) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 1536):\n@@ -209,7 +209,7 @@ def __init__(\n         self.initializer_range = initializer_range\n \n \n-class Kosmos2_5Config(PretrainedConfig):\n+class Kosmos2_5Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Kosmos2_5Model`]. It is used to instantiate a\n     KOSMOS-2.5 model according to the specified arguments, defining the model architecture. Instantiating a configuration"
        },
        {
            "sha": "940986c407defb21f6c7fe5234d04931aba78312",
            "filename": "src/transformers/models/kyutai_speech_to_text/configuration_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,15 +13,15 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.s\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto.configuration_auto import AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class KyutaiSpeechToTextConfig(PretrainedConfig):\n+class KyutaiSpeechToTextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`KyutaiSpeechToTextForConditionalGeneration`].\n     It is used to instantiate a Kyutai Speech-to-Text model according to the specified arguments, defining the model\n@@ -30,8 +30,8 @@ class KyutaiSpeechToTextConfig(PretrainedConfig):\n \n     e.g. [kyutai/stt-2.6b-en-trfs](https://huggingface.co/kyutai/stt-2.6b-en-trfs)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         codebook_vocab_size (`int`, *optional*, defaults to 2049):\n@@ -87,13 +87,13 @@ class KyutaiSpeechToTextConfig(PretrainedConfig):\n             Padding token id.\n         bos_token_id (`int`, *optional*, defaults to 48000):\n             Beginning of stream token id for text tokens.\n-        codec_config (`PretrainedConfig`, *optional*):\n+        codec_config (`PreTrainedConfig`, *optional*):\n             Configuration for the codec.\n         kwargs (*optional*):\n             Dictionary of keyword arguments. Notably:\n-                - **audio_encoder_config** ([`PretrainedConfig`], *optional*) -- An instance of a configuration object that\n+                - **audio_encoder_config** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that\n                   defines the audio encoder config.\n-                - **depth__config** ([`PretrainedConfig`], *optional*) -- An instance of a configuration object that\n+                - **depth__config** ([`PreTrainedConfig`], *optional*) -- An instance of a configuration object that\n                   defines the depth decoder config.\n \n \n@@ -151,7 +151,7 @@ def __init__(\n             logger.info(\"codec_config is None, using default audio encoder config.\")\n         elif isinstance(codec_config, dict):\n             self.codec_config = AutoConfig.for_model(**codec_config)\n-        elif isinstance(codec_config, PretrainedConfig):\n+        elif isinstance(codec_config, PreTrainedConfig):\n             self.codec_config = codec_config\n \n         self.num_codebooks = num_codebooks"
        },
        {
            "sha": "2a3c84901a5e0e772859b21136b2cd4157d9f106",
            "filename": "src/transformers/models/layoutlm/configuration_layoutlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,15 +18,15 @@\n from collections.abc import Mapping\n from typing import Any, Optional\n \n-from ... import PretrainedConfig, PreTrainedTokenizer\n+from ... import PreTrainedConfig, PreTrainedTokenizer\n from ...onnx import OnnxConfig, PatchingSpec\n from ...utils import is_torch_available, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LayoutLMConfig(PretrainedConfig):\n+class LayoutLMConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LayoutLMModel`]. It is used to instantiate a\n     LayoutLM model according to the specified arguments, defining the model architecture. Instantiating a configuration\n@@ -130,7 +130,7 @@ def __init__(\n class LayoutLMOnnxConfig(OnnxConfig):\n     def __init__(\n         self,\n-        config: PretrainedConfig,\n+        config: PreTrainedConfig,\n         task: str = \"default\",\n         patching_specs: Optional[list[PatchingSpec]] = None,\n     ):"
        },
        {
            "sha": "b204f769ec093230d754522ae961eed3fffce27c",
            "filename": "src/transformers/models/layoutlmv2/configuration_layoutlmv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fconfiguration_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fconfiguration_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fconfiguration_layoutlmv2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"LayoutLMv2 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import is_detectron2_available, logging\n \n \n@@ -26,15 +26,15 @@\n     import detectron2\n \n \n-class LayoutLMv2Config(PretrainedConfig):\n+class LayoutLMv2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LayoutLMv2Model`]. It is used to instantiate an\n     LayoutLMv2 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the LayoutLMv2\n     [microsoft/layoutlmv2-base-uncased](https://huggingface.co/microsoft/layoutlmv2-base-uncased) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):"
        },
        {
            "sha": "cbef304b8ffe563f4db260e32256b69ee5523cfa",
            "filename": "src/transformers/models/layoutlmv3/configuration_layoutlmv3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fconfiguration_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fconfiguration_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fconfiguration_layoutlmv3.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,7 +20,7 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...onnx.utils import compute_effective_axis_dimension\n from ...utils import logging\n@@ -33,15 +33,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class LayoutLMv3Config(PretrainedConfig):\n+class LayoutLMv3Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LayoutLMv3Model`]. It is used to instantiate an\n     LayoutLMv3 model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the LayoutLMv3\n     [microsoft/layoutlmv3-base](https://huggingface.co/microsoft/layoutlmv3-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 50265):"
        },
        {
            "sha": "7873eaf8855c44ab469bccb69a8424797c3bbeac",
            "filename": "src/transformers/models/led/configuration_led.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fled%2Fconfiguration_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fled%2Fconfiguration_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fconfiguration_led.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,22 +16,22 @@\n \n from typing import Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LEDConfig(PretrainedConfig):\n+class LEDConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LEDModel`]. It is used to instantiate an LED\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the LED\n     [allenai/led-base-16384](https://huggingface.co/allenai/led-base-16384) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "e52f74ddd6292ce9a9b5472c23f635ae185d0005",
            "filename": "src/transformers/models/levit/configuration_levit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flevit%2Fconfiguration_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flevit%2Fconfiguration_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fconfiguration_levit.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,23 +19,23 @@\n \n from packaging import version\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LevitConfig(PretrainedConfig):\n+class LevitConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LevitModel`]. It is used to instantiate a LeViT\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the LeViT\n     [facebook/levit-128S](https://huggingface.co/facebook/levit-128S) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         image_size (`int`, *optional*, defaults to 224):"
        },
        {
            "sha": "3b75a640fc4c9c8492b633eceb451664c2c52159",
            "filename": "src/transformers/models/lfm2/configuration_lfm2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,18 +13,18 @@\n # limitations under the License.\n from typing import Optional\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n \n \n-class Lfm2Config(PretrainedConfig):\n+class Lfm2Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Lfm2Model`]. It is used to instantiate a LFM2\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the LFM2-1.2B model.\n     e.g. [LiquidAI/LFM2-1.2B](https://huggingface.co/LiquidAI/LFM2-1.2B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "22679854a7af7ccad5bceba8bc448b061e6c489f",
            "filename": "src/transformers/models/lfm2_vl/configuration_lfm2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fconfiguration_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fconfiguration_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fconfiguration_lfm2_vl.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,24 +14,24 @@\n # limitations under the License.\n \"\"\"PyTorch LFM2-VL model.\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Lfm2VlConfig(PretrainedConfig):\n+class Lfm2VlConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Lfm2VlForConditionalGeneration`]. It is used to instantiate an\n     Lfm2Vl model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Lfm2-VL-1.6B.\n \n     e.g. [LiquidAI/LFM2-VL-1.6B](https://huggingface.co/LiquidAI/LFM2-VL-1.6B)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`AutoConfig | dict`,  *optional*, defaults to `Siglip2ImageConfig`):"
        },
        {
            "sha": "f91b88f6bd73134b4ac8d850ce520c7877a66560",
            "filename": "src/transformers/models/lightglue/configuration_lightglue.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fconfiguration_lightglue.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,20 +19,20 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n from ..superpoint import SuperPointConfig\n \n \n-class LightGlueConfig(PretrainedConfig):\n+class LightGlueConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LightGlueForKeypointMatching`]. It is used to\n     instantiate a LightGlue model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the LightGlue\n     [ETH-CVG/lightglue_superpoint](https://huggingface.co/ETH-CVG/lightglue_superpoint) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         keypoint_detector_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SuperPointConfig`):"
        },
        {
            "sha": "4018622a689994f43767c570f8eb0d8e9ccf2321",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -20,7 +20,7 @@\n from torch import nn\n from torch.nn.utils.rnn import pad_sequence\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...image_utils import ImageInput, is_vision_available, to_numpy_array\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -43,15 +43,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class LightGlueConfig(PretrainedConfig):\n+class LightGlueConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LightGlueForKeypointMatching`]. It is used to\n     instantiate a LightGlue model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the LightGlue\n     [ETH-CVG/lightglue_superpoint](https://huggingface.co/ETH-CVG/lightglue_superpoint) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         keypoint_detector_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SuperPointConfig`):"
        },
        {
            "sha": "c8e4827191216c6c8c20933dd8f342198587c61f",
            "filename": "src/transformers/models/lilt/configuration_lilt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flilt%2Fconfiguration_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flilt%2Fconfiguration_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fconfiguration_lilt.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,21 +14,21 @@\n # limitations under the License.\n \"\"\"LiLT configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LiltConfig(PretrainedConfig):\n+class LiltConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LiltModel`]. It is used to instantiate a LiLT\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the LiLT\n     [SCUT-DLVCLab/lilt-roberta-en-base](https://huggingface.co/SCUT-DLVCLab/lilt-roberta-en-base) architecture.\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):"
        },
        {
            "sha": "5501785d46dae8fa6794d88edbbbf2ab4a085eae",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,19 +19,19 @@\n # limitations under the License.\n \"\"\"LLaMA model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class LlamaConfig(PretrainedConfig):\n+class LlamaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the LLaMA-7B.\n     e.g. [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "214a90a1bc543490c9c69185547cdce3cc231697",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,23 +16,23 @@\n \n import warnings\n \n-from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Llama4VisionConfig(PretrainedConfig):\n+class Llama4VisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Llama4VisionModel`]. It is used to instantiate a\n     Llama4 vision model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Llama4 109B.\n \n     e.g. [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         hidden_size (`int`, *optional*, defaults to 768):\n@@ -141,16 +141,16 @@ def vision_feature_layer(self, value):\n         super().__init__(**kwargs)\n \n \n-class Llama4TextConfig(PretrainedConfig):\n+class Llama4TextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Llama4TextModel`]. It is used to instantiate a\n     Llama4 text model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Llama4 109B.\n \n     e.g. [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 202048):\n@@ -394,16 +394,16 @@ def __init__(\n         layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n-class Llama4Config(PretrainedConfig):\n+class Llama4Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Llama4Model`]. It is used to instantiate an\n     Llama4 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Llama4 109B.\n \n     e.g. [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "40157ab63b91a5b28543561837ce4ea055727de1",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,24 +13,24 @@\n # limitations under the License.\n \"\"\"Llava model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LlavaConfig(PretrainedConfig):\n+class LlavaConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LlavaForConditionalGeneration`]. It is used to instantiate an\n     Llava model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the Llava-9B.\n \n     e.g. [llava-hf/llava-9b](https://huggingface.co/llava-hf/llava-9b)\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `CLIPVisionConfig`):"
        },
        {
            "sha": "a319ae7b1307967ad2c12cb76def2f4fcadd8d41",
            "filename": "src/transformers/models/llava_next/configuration_llava_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -13,23 +13,23 @@\n # limitations under the License.\n \"\"\"Llava-NeXT model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LlavaNextConfig(PretrainedConfig):\n+class LlavaNextConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LlavaNextForConditionalGeneration`]. It is used to instantiate an\n     Llava-NeXT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the [llava-hf/llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf)\n     model.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `CLIPVisionConfig`):"
        },
        {
            "sha": "7b82b5ac5b89464998126a2b5ae9e7ff4ff5d431",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,18 +19,18 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n-class LlavaNextVideoConfig(PretrainedConfig):\n+class LlavaNextVideoConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LlavaNextVideoForConditionalGeneration`]. It is used to instantiate an\n     Llava-NeXT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the [llava-hf/LLaVA-NeXT-Video-7B-hf](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf)\n     model.\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `CLIPVisionConfig`):"
        },
        {
            "sha": "2ba202f668e97e085deb63200b8a53cff3155fe2",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -30,7 +30,7 @@\n )\n \n from ...cache_utils import Cache\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n from ...utils import logging\n@@ -40,14 +40,14 @@\n logger = logging.get_logger(__name__)\n \n \n-class LlavaNextVideoConfig(PretrainedConfig):\n+class LlavaNextVideoConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LlavaNextVideoForConditionalGeneration`]. It is used to instantiate an\n     Llava-NeXT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the [llava-hf/LLaVA-NeXT-Video-7B-hf](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf)\n     model.\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `CLIPVisionConfig`):"
        },
        {
            "sha": "9fd1e850f0e5b6f9758f1a73da47cfe64a2874df",
            "filename": "src/transformers/models/llava_onevision/configuration_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import (\n     logging,\n )\n@@ -24,15 +24,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class LlavaOnevisionConfig(PretrainedConfig):\n+class LlavaOnevisionConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LlavaOnevisionForConditionalGeneration`]. It is used to instantiate an\n     Llava-NeXT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the [llava-hf/llava-onevision-qwen2-7b-ov-hf](https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-ov-hf)\n     model.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SiglipVisionConfig`):"
        },
        {
            "sha": "e9703a6b55c2bbd268c6c72fcee7ceba9998ded2",
            "filename": "src/transformers/models/longcat_flash/configuration_longcat_flash.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -15,18 +15,18 @@\n \n \"\"\"LongCat Flash model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n-class LongcatFlashConfig(PretrainedConfig):\n+class LongcatFlashConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LongcatFlashModel`]. It is used to instantiate\n     a LongCat Flash model according to the specified arguments, defining the model architecture. Instantiating a\n     configuration with the defaults will yield a similar configuration to that of the LongCat Flash architecture.\n     e.g. [meituan-longcat/LongCat-Flash-Chat](https://huggingface.co/meituan-longcat/LongCat-Flash-Chat)\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "204e78a6b2aa912c6a8a171a59fc1201c0e90b2d",
            "filename": "src/transformers/models/longformer/configuration_longformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -18,7 +18,7 @@\n from collections.abc import Mapping\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig\n from ...utils import logging\n \n@@ -31,7 +31,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class LongformerConfig(PretrainedConfig):\n+class LongformerConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LongformerModel`] or a [`TFLongformerModel`]. It\n     is used to instantiate a Longformer model according to the specified arguments, defining the model architecture.\n@@ -42,8 +42,8 @@ class LongformerConfig(PretrainedConfig):\n     [allenai/longformer-base-4096](https://huggingface.co/allenai/longformer-base-4096) architecture with a sequence\n     length 4,096.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 30522):\n@@ -141,7 +141,7 @@ def __init__(\n \n class LongformerOnnxConfig(OnnxConfig):\n     def __init__(\n-        self, config: \"PretrainedConfig\", task: str = \"default\", patching_specs: \"Optional[list[PatchingSpec]]\" = None\n+        self, config: \"PreTrainedConfig\", task: str = \"default\", patching_specs: \"Optional[list[PatchingSpec]]\" = None\n     ):\n         super().__init__(config, task, patching_specs)\n         config.onnx_export = True"
        },
        {
            "sha": "4889eff64672559c2893d95170d6581a6fcf40df",
            "filename": "src/transformers/models/longt5/configuration_longt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,23 +16,23 @@\n \n from collections.abc import Mapping\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxSeq2SeqConfigWithPast\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LongT5Config(PretrainedConfig):\n+class LongT5Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LongT5Model`]. It is\n     used to instantiate a LongT5 model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the LongT5\n     [google/long-t5-local-base](https://huggingface.co/google/long-t5-local-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n     Arguments:\n         vocab_size (`int`, *optional*, defaults to 32128):"
        },
        {
            "sha": "0a0d3061235bdf0a22d4b115f3aff482f168d75e",
            "filename": "src/transformers/models/luke/configuration_luke.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fluke%2Fconfiguration_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fluke%2Fconfiguration_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fconfiguration_luke.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"LUKE configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LukeConfig(PretrainedConfig):\n+class LukeConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LukeModel`]. It is used to instantiate a LUKE\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the LUKE\n     [studio-ousia/luke-base](https://huggingface.co/studio-ousia/luke-base) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "101a0b4965eedcf581f2e17e75c0352e4b09df66",
            "filename": "src/transformers/models/lxmert/configuration_lxmert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flxmert%2Fconfiguration_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Flxmert%2Fconfiguration_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fconfiguration_lxmert.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -14,22 +14,22 @@\n # limitations under the License.\n \"\"\"LXMERT model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class LxmertConfig(PretrainedConfig):\n+class LxmertConfig(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LxmertModel`] or a [`TFLxmertModel`]. It is used\n     to instantiate a LXMERT model according to the specified arguments, defining the model architecture. Instantiating\n     a configuration with the defaults will yield a similar configuration to that of the Lxmert\n     [unc-nlp/lxmert-base-uncased](https://huggingface.co/unc-nlp/lxmert-base-uncased) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "426413a0f1ed480779f0dc48b743fd195e4dfb15",
            "filename": "src/transformers/models/m2m_100/configuration_m2m_100.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fconfiguration_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fconfiguration_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fconfiguration_m2m_100.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -19,7 +19,7 @@\n from typing import Any\n \n from ... import PreTrainedTokenizer\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...onnx import OnnxConfig, OnnxSeq2SeqConfigWithPast\n from ...onnx.utils import compute_effective_axis_dimension\n from ...utils import is_torch_available, logging\n@@ -28,15 +28,15 @@\n logger = logging.get_logger(__name__)\n \n \n-class M2M100Config(PretrainedConfig):\n+class M2M100Config(PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`M2M100Model`]. It is used to instantiate an\n     M2M100 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of the M2M100\n     [facebook/m2m100_418M](https://huggingface.co/facebook/m2m100_418M) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        },
        {
            "sha": "918edc412d102ec1a2ba38a898b8ca78d04c6e00",
            "filename": "src/transformers/models/mamba/configuration_mamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fmamba%2Fconfiguration_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163601c6197e2addccd30d9bf450f664766f2e71/src%2Ftransformers%2Fmodels%2Fmamba%2Fconfiguration_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fconfiguration_mamba.py?ref=163601c6197e2addccd30d9bf450f664766f2e71",
            "patch": "@@ -16,22 +16,22 @@\n \n import math\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-class MambaConfig(PretrainedConfig):\n+class MambaConfig(PreTrainedConfig):\n     \"\"\"\n     This is the configuration class to store the configuration of a [`MambaModel`]. It is used to instantiate a MAMBA\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the MAMBA\n     [state-spaces/mamba-2.8b](https://huggingface.co/state-spaces/mamba-2.8b) architecture.\n \n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n \n \n     Args:"
        }
    ],
    "stats": {
        "total": 5632,
        "additions": 2819,
        "deletions": 2813
    }
}