{
    "author": "Rocketknight1",
    "message": "Allow parse_response to accept token IDs (#41849)\n\n* Allow tokenizer.parse_response() to accept IDs/arrays directly\n\n* Allow tokenizer.parse_response() to accept IDs/arrays directly",
    "sha": "38df1e946dea636e3dc5875b858fc77bb5fccaec",
    "files": [
        {
            "sha": "afdd8270987adeb5462687bcc5d022d4dd554d3d",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 25,
            "deletions": 6,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/38df1e946dea636e3dc5875b858fc77bb5fccaec/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38df1e946dea636e3dc5875b858fc77bb5fccaec/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=38df1e946dea636e3dc5875b858fc77bb5fccaec",
            "patch": "@@ -18,6 +18,8 @@\n of output with special method for the Fast tokenizers)\n \"\"\"\n \n+from __future__ import annotations\n+\n import copy\n import json\n import os\n@@ -783,7 +785,7 @@ def as_tensor(value, dtype=None):\n \n         return self\n \n-    def to(self, device: Union[str, \"torch.device\"], *, non_blocking: bool = False) -> \"BatchEncoding\":\n+    def to(self, device: Union[str, torch.device], *, non_blocking: bool = False) -> BatchEncoding:\n         \"\"\"\n         Send all values to device by calling `v.to(device, non_blocking=non_blocking)` (PyTorch only).\n \n@@ -1858,7 +1860,11 @@ def get_chat_template(self, chat_template: Optional[str] = None, tools: Optional\n \n         return chat_template\n \n-    def parse_response(self, response: str, schema: Optional[Union[list, dict]] = None):\n+    def parse_response(\n+        self,\n+        response: str | list[str | int | list[int]] | np.ndarray | torch.Tensor,\n+        schema: list | dict | None = None,\n+    ):\n         \"\"\"\n         Converts an output string created by generating text from a model into a parsed message dictionary.\n         This method is intended for use with chat models, and will read the tokenizer's `response_schema` attribute to\n@@ -1869,16 +1875,29 @@ def parse_response(self, response: str, schema: Optional[Union[list, dict]] = No\n \n         Args:\n             response (`str`):\n-                The output string generated by the model. This should be the decoded string, not raw tokens.\n+                The output string generated by the model. This can be either a decoded string or list of strings,\n+                or token IDs as a list/array.\n             schema (`Union[list, dict]`, *optional*):\n                 A response schema that indicates the expected output format and how parsing should be performed.\n                 If not provided, the tokenizer's `response_schema` attribute will be used.\n         \"\"\"\n+        batched = (\n+            (isinstance(response, list) and not isinstance(response[0], int))\n+            or getattr(response, \"ndim\", 0) > 1  # For torch/numpy tensors\n+        )\n+\n         if schema is None:\n             if getattr(self, \"response_schema\", None) is None:\n                 raise AttributeError(\"This tokenizer does not have a `response_schema` for parsing chat responses!\")\n             schema = self.response_schema\n-        return recursive_parse(response, schema)\n+        if batched:\n+            if not (isinstance(response, list) and isinstance(response[0], str)):\n+                response = self.batch_decode(response)\n+            return [recursive_parse(single_response, schema) for single_response in response]\n+        else:\n+            if not isinstance(response, str):\n+                response = self.decode(response)\n+            return recursive_parse(response, schema)\n \n     @classmethod\n     def from_pretrained(\n@@ -3863,7 +3882,7 @@ def convert_tokens_to_string(self, tokens: list[str]) -> str:\n \n     def batch_decode(\n         self,\n-        sequences: Union[list[int], list[list[int]], np.ndarray, \"torch.Tensor\"],\n+        sequences: Union[list[int], list[list[int]], np.ndarray, torch.Tensor],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         **kwargs,\n@@ -3897,7 +3916,7 @@ def batch_decode(\n \n     def decode(\n         self,\n-        token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n+        token_ids: Union[int, list[int], np.ndarray, torch.Tensor],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         **kwargs,"
        },
        {
            "sha": "5557d20cb24a06cb0bd6ba24a0a2c591b3ca0749",
            "filename": "tests/utils/test_chat_parsing_utils.py",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/38df1e946dea636e3dc5875b858fc77bb5fccaec/tests%2Futils%2Ftest_chat_parsing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38df1e946dea636e3dc5875b858fc77bb5fccaec/tests%2Futils%2Ftest_chat_parsing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_chat_parsing_utils.py?ref=38df1e946dea636e3dc5875b858fc77bb5fccaec",
            "patch": "@@ -200,6 +200,40 @@ def test_tokenizer_method(self):\n         tokenizer_parsed_chat = tokenizer.parse_response(model_out)\n         self.assertEqual(tokenizer_parsed_chat, parsed_chat)\n \n+    def test_batched_inputs(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n+        model_out = '<|START_THINKING|>I should call a tool.<|END_THINKING|><|START_ACTION|>[\\n    {\"tool_call_id\": \"0\", \"tool_name\": \"simple_tool\", \"parameters\": {\"temperature_format\": \"Celsius\"}}\\n]<|END_ACTION|><|END_OF_TURN_TOKEN|>'\n+        tokenizer.response_schema = cohere_schema\n+        parsed_chat = tokenizer.parse_response(model_out)\n+        self.assertEqual(tokenizer.parse_response([model_out]), [parsed_chat])\n+        self.assertEqual(tokenizer.parse_response([model_out] * 2), [parsed_chat] * 2)\n+\n+    def test_token_id_inputs(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")  # Need an actual tokenizer to encode\n+        model_out = '<|START_THINKING|>I should call a tool.<|END_THINKING|><|START_ACTION|>[\\n    {\"tool_call_id\": \"0\", \"tool_name\": \"simple_tool\", \"parameters\": {\"temperature_format\": \"Celsius\"}}\\n]<|END_ACTION|><|END_OF_TURN_TOKEN|>'\n+        tokenizer.response_schema = cohere_schema\n+        parsed_chat = tokenizer.parse_response(model_out)\n+        tokenized_out = tokenizer(model_out).input_ids\n+        self.assertEqual(tokenizer.parse_response(tokenized_out), parsed_chat)\n+        self.assertEqual(tokenizer.parse_response([tokenized_out]), [parsed_chat])\n+        self.assertEqual(tokenizer.parse_response([tokenized_out] * 2), [parsed_chat] * 2)\n+\n+    def test_numpy_inputs(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")  # Need an actual tokenizer to encode\n+        model_out = '<|START_THINKING|>I should call a tool.<|END_THINKING|><|START_ACTION|>[\\n    {\"tool_call_id\": \"0\", \"tool_name\": \"simple_tool\", \"parameters\": {\"temperature_format\": \"Celsius\"}}\\n]<|END_ACTION|><|END_OF_TURN_TOKEN|>'\n+        tokenizer.response_schema = cohere_schema\n+        parsed_chat = tokenizer.parse_response(model_out)\n+        tokenized_out = tokenizer(model_out, return_tensors=\"np\").input_ids\n+        self.assertEqual(tokenizer.parse_response(tokenized_out), [parsed_chat])\n+\n+    def test_tensor_inputs(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")  # Need an actual tokenizer to encode\n+        model_out = '<|START_THINKING|>I should call a tool.<|END_THINKING|><|START_ACTION|>[\\n    {\"tool_call_id\": \"0\", \"tool_name\": \"simple_tool\", \"parameters\": {\"temperature_format\": \"Celsius\"}}\\n]<|END_ACTION|><|END_OF_TURN_TOKEN|>'\n+        tokenizer.response_schema = cohere_schema\n+        parsed_chat = tokenizer.parse_response(model_out)\n+        tokenized_out = tokenizer(model_out, return_tensors=\"pt\").input_ids\n+        self.assertEqual(tokenizer.parse_response(tokenized_out), [parsed_chat])\n+\n     def test_cohere_template(self):\n         model_out = '<|START_THINKING|>I should call a tool.<|END_THINKING|><|START_ACTION|>[\\n    {\"tool_call_id\": \"0\", \"tool_name\": \"simple_tool\", \"parameters\": {\"temperature_format\": \"Celsius\"}}\\n]<|END_ACTION|><|END_OF_TURN_TOKEN|>'\n         parsed_chat = recursive_parse(model_out, cohere_schema)"
        }
    ],
    "stats": {
        "total": 65,
        "additions": 59,
        "deletions": 6
    }
}