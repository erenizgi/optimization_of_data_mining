{
    "author": "ydshieh",
    "message": "Mark 2 tests as flaky for now (#37038)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "d13c390d0194eda30f787ec27be37515a029fd4d",
    "files": [
        {
            "sha": "e506d688098ac6625c3bcfdaa513e822542bcda5",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d13c390d0194eda30f787ec27be37515a029fd4d/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d13c390d0194eda30f787ec27be37515a029fd4d/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=d13c390d0194eda30f787ec27be37515a029fd4d",
            "patch": "@@ -29,6 +29,7 @@\n )\n from transformers.testing_utils import (\n     cleanup,\n+    is_flaky,\n     require_torch,\n     require_torch_sdpa,\n     slow,\n@@ -373,6 +374,11 @@ def test_contrastive_generate_low_memory(self):\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         pass\n \n+    @pytest.mark.generate\n+    @is_flaky(description=\"TODO: check why flaky\")\n+    def test_generate_methods_with_logits_to_keep(self):\n+        super().test_generate_methods_with_logits_to_keep()\n+\n     @unittest.skip(reason=\" FlashAttention only support fp16 and bf16 data type\")\n     def test_flash_attn_2_fp32_ln(self):\n         pass"
        },
        {
            "sha": "88023f6c1a850a0b9bc85578bc97550252610be5",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d13c390d0194eda30f787ec27be37515a029fd4d/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d13c390d0194eda30f787ec27be37515a029fd4d/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=d13c390d0194eda30f787ec27be37515a029fd4d",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import AutoTokenizer, ZambaConfig, is_torch_available\n from transformers.testing_utils import (\n+    is_flaky,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n@@ -327,6 +328,7 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n+    @is_flaky(description=\"TODO: ydshieh\")\n     def test_initialization(self):\n         r\"\"\"\n         Overriding the test_initialization test as the A_log and D params of the Mamba block are initialized differently"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 8,
        "deletions": 0
    }
}