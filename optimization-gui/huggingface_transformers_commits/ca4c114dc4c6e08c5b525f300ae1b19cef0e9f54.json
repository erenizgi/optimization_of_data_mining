{
    "author": "jiangyukunok",
    "message": "Add counters for dataset classes (#37636)\n\n* add counters for dataset classes\n\n* fix failed code style",
    "sha": "ca4c114dc4c6e08c5b525f300ae1b19cef0e9f54",
    "files": [
        {
            "sha": "7f8a266b44bcdd8a02ae9d71760640ab31b1068a",
            "filename": "examples/pytorch/text-classification/run_glue.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ca4c114dc4c6e08c5b525f300ae1b19cef0e9f54/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ca4c114dc4c6e08c5b525f300ae1b19cef0e9f54/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py?ref=ca4c114dc4c6e08c5b525f300ae1b19cef0e9f54",
            "patch": "@@ -19,6 +19,7 @@\n import os\n import random\n import sys\n+from collections import Counter\n from dataclasses import dataclass, field\n from typing import Optional\n \n@@ -467,13 +468,22 @@ def preprocess_function(examples):\n             load_from_cache_file=not data_args.overwrite_cache,\n             desc=\"Running tokenizer on dataset\",\n         )\n+\n+    def print_class_distribution(dataset, split_name):\n+        label_counts = Counter(dataset[\"label\"])\n+        total = sum(label_counts.values())\n+        logger.info(f\"Class distribution in {split_name} set:\")\n+        for label, count in label_counts.items():\n+            logger.info(f\"  Label {label}: {count} ({count / total:.2%})\")\n+\n     if training_args.do_train:\n         if \"train\" not in raw_datasets:\n             raise ValueError(\"--do_train requires a train dataset\")\n         train_dataset = raw_datasets[\"train\"]\n         if data_args.max_train_samples is not None:\n             max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n             train_dataset = train_dataset.select(range(max_train_samples))\n+        print_class_distribution(train_dataset, \"train\")\n \n     if training_args.do_eval:\n         if \"validation\" not in raw_datasets and \"validation_matched\" not in raw_datasets:\n@@ -482,6 +492,7 @@ def preprocess_function(examples):\n         if data_args.max_eval_samples is not None:\n             max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n             eval_dataset = eval_dataset.select(range(max_eval_samples))\n+        print_class_distribution(eval_dataset, \"validation\")\n \n     if training_args.do_predict or data_args.task_name is not None or data_args.test_file is not None:\n         if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n@@ -490,6 +501,7 @@ def preprocess_function(examples):\n         if data_args.max_predict_samples is not None:\n             max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n             predict_dataset = predict_dataset.select(range(max_predict_samples))\n+        print_class_distribution(predict_dataset, \"test\")\n \n     # Log a few random samples from the training set:\n     if training_args.do_train:"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 12,
        "deletions": 0
    }
}