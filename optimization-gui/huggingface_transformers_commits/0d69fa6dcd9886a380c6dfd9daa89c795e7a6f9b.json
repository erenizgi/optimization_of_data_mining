{
    "author": "zucchini-nlp",
    "message": "[qwen-omni] fix sliding window (#38525)\n\nfix",
    "sha": "0d69fa6dcd9886a380c6dfd9daa89c795e7a6f9b",
    "files": [
        {
            "sha": "3f76da5e3ebe3be5101ac9334830358735dec995",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d69fa6dcd9886a380c6dfd9daa89c795e7a6f9b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d69fa6dcd9886a380c6dfd9daa89c795e7a6f9b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=0d69fa6dcd9886a380c6dfd9daa89c795e7a6f9b",
            "patch": "@@ -658,6 +658,8 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         spatial_merge_size (`int`, *optional*, defaults to 2):\n             The size used for merging spatial dimensions.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n \n     Example:\n \n@@ -726,6 +728,7 @@ def __init__(\n         audio_end_token_id=151648,\n         initializer_range=0.02,\n         spatial_merge_size=2,\n+        layer_types=None,\n         **kwargs,\n     ):\n         self.audio_token_index = audio_token_index\n@@ -753,7 +756,7 @@ def __init__(\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window\n+        self.sliding_window = sliding_window if self.use_sliding_window else None\n         self.max_window_layers = max_window_layers\n \n         # for backward compatibility\n@@ -775,6 +778,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.spatial_merge_size = spatial_merge_size\n \n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if self.sliding_window is not None and i >= self.max_window_layers\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n "
        },
        {
            "sha": "46bd0f23209f0a730b33ea5bbf957332fba0353d",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d69fa6dcd9886a380c6dfd9daa89c795e7a6f9b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d69fa6dcd9886a380c6dfd9daa89c795e7a6f9b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=0d69fa6dcd9886a380c6dfd9daa89c795e7a6f9b",
            "patch": "@@ -697,6 +697,8 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         spatial_merge_size (`int`, *optional*, defaults to 2):\n             The size used for merging spatial dimensions.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n \n     Example:\n \n@@ -765,6 +767,7 @@ def __init__(\n         audio_end_token_id=151648,\n         initializer_range=0.02,\n         spatial_merge_size=2,\n+        layer_types=None,\n         **kwargs,\n     ):\n         self.audio_token_index = audio_token_index\n@@ -792,7 +795,7 @@ def __init__(\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window\n+        self.sliding_window = sliding_window if self.use_sliding_window else None\n         self.max_window_layers = max_window_layers\n \n         # for backward compatibility\n@@ -814,6 +817,16 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.spatial_merge_size = spatial_merge_size\n \n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if self.sliding_window is not None and i >= self.max_window_layers\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n "
        }
    ],
    "stats": {
        "total": 30,
        "additions": 28,
        "deletions": 2
    }
}