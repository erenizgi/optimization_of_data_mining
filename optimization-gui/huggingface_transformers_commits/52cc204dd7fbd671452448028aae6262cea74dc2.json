{
    "author": "zucchini-nlp",
    "message": "[blip-2] Fix dtype mismatch when keep in fp32  (#37068)\n\n* fix fp32 BLIP2\n\n* no need to reorder that\n\n* check for `Noneness` as well before casting dtype",
    "sha": "52cc204dd7fbd671452448028aae6262cea74dc2",
    "files": [
        {
            "sha": "de15c0d1ed60d02b09374816759a93e2287d660f",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 5,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/52cc204dd7fbd671452448028aae6262cea74dc2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52cc204dd7fbd671452448028aae6262cea74dc2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=52cc204dd7fbd671452448028aae6262cea74dc2",
            "patch": "@@ -1238,6 +1238,9 @@ def forward(\n                 embeddings += position_embeddings\n \n             if query_embeds is not None:\n+                # `query_embeds` are kept in fp32 when we use it with Qformer\n+                if query_embeds.dtype != embeddings.dtype:\n+                    query_embeds = query_embeds.to(embeddings.dtype)\n                 embeddings = torch.cat((query_embeds, embeddings), dim=1)\n         else:\n             embeddings = query_embeds\n@@ -1385,6 +1388,10 @@ def forward(\n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n         if encoder_hidden_states is not None:\n+            # Qformer and latent query tokens are kept in fp32. We cast `encoder_hidden_states` if not fp32 already\n+            if encoder_hidden_states.dtype != query_embeds.dtype:\n+                encoder_hidden_states = encoder_hidden_states.to(query_embeds.dtype)\n+\n             if isinstance(encoder_hidden_states, list):\n                 encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size()\n             else:\n@@ -1447,7 +1454,7 @@ def forward(\n class Blip2Model(Blip2PreTrainedModel):\n     config_class = Blip2Config\n     main_input_name = \"pixel_values\"\n-    _keep_in_fp32_modules = [\"query_tokens\"]\n+    _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -1728,6 +1735,10 @@ def forward(\n         )\n         query_output = query_outputs[0]\n \n+        # Qformer is kept in fp32, we downcast the output back if needed\n+        if query_output.dtype != image_embeds.dtype:\n+            query_output = query_output.to(image_embeds.dtype)\n+\n         # step 3: use the language model, conditioned on the query outputs and the prompt\n         language_model_inputs = self.language_projection(query_output)\n         language_model_attention_mask = torch.ones(\n@@ -1799,7 +1810,7 @@ def forward(\n )\n class Blip2TextModelWithProjection(Blip2PreTrainedModel):\n     supports_gradient_checkpointing = False\n-    _keep_in_fp32_modules = [\"query_tokens\"]\n+    _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -1898,7 +1909,7 @@ def forward(\n )\n class Blip2VisionModelWithProjection(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    _keep_in_fp32_modules = [\"query_tokens\"]\n+    _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -2019,7 +2030,7 @@ class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n-    _keep_in_fp32_modules = [\"query_tokens\"]\n+    _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -2192,6 +2203,10 @@ def forward(\n         )\n         query_output = query_outputs[0]\n \n+        # Qformer is kept in fp32, we downcast the output back if needed\n+        if query_output.dtype != image_embeds.dtype:\n+            query_output = query_output.to(image_embeds.dtype)\n+\n         # step 3: use the language model, conditioned on the query outputs and the prompt\n         language_model_inputs = self.language_projection(query_output)\n         language_model_attention_mask = torch.ones(\n@@ -2313,6 +2328,10 @@ def generate(\n         )\n         query_output = query_outputs.last_hidden_state\n \n+        # Qformer is kept in fp32, we downcast the output back if needed\n+        if query_output.dtype != image_embeds.dtype:\n+            query_output = query_output.to(image_embeds.dtype)\n+\n         language_model_inputs = self.language_projection(query_output)\n         language_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n@@ -2372,7 +2391,7 @@ def generate(\n )\n class Blip2ForImageTextRetrieval(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    _keep_in_fp32_modules = [\"query_tokens\"]\n+    _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 24,
        "deletions": 5
    }
}