{
    "author": "sheryc",
    "message": "Fix Llava-NeXT / Llava-NeXT Video / Llava-OneVision's token unpadding mismatch (#35779)\n\n* Fix Llava OneVision's token padding\r\n\r\n* Fix Llava next and Llava next video's token unpadding for consistency",
    "sha": "72d1a4cd53d90d5db384df948ccc293b3c1e3b9d",
    "files": [
        {
            "sha": "4de5fe63efce671335b66ff68c9d16992811f181",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d1a4cd53d90d5db384df948ccc293b3c1e3b9d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d1a4cd53d90d5db384df948ccc293b3c1e3b9d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=72d1a4cd53d90d5db384df948ccc293b3c1e3b9d",
            "patch": "@@ -200,11 +200,11 @@ def _get_unpadded_features(self, height, width, patches_height, patches_width, s\n         original_aspect_ratio = width / height\n         current_aspect_ratio = current_width / current_height\n         if original_aspect_ratio > current_aspect_ratio:\n-            new_height = (height * current_width) // width\n+            new_height = int(round(height * (current_width / width), 7))\n             padding = (current_height - new_height) // 2\n             current_height -= padding * 2\n         else:\n-            new_width = (width * current_height) // height\n+            new_width = int(round(width * (current_height / height), 7))\n             padding = (current_width - new_width) // 2\n             current_width -= padding * 2\n "
        },
        {
            "sha": "8a1294611b0d74f6cace682cb434e2ed9d458a1e",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d1a4cd53d90d5db384df948ccc293b3c1e3b9d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d1a4cd53d90d5db384df948ccc293b3c1e3b9d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=72d1a4cd53d90d5db384df948ccc293b3c1e3b9d",
            "patch": "@@ -253,11 +253,11 @@ def _get_unpadded_features(self, height, width, patches_height, patches_width, s\n         original_aspect_ratio = width / height\n         current_aspect_ratio = current_width / current_height\n         if original_aspect_ratio > current_aspect_ratio:\n-            new_height = (height * current_width) // width\n+            new_height = int(round(height * (current_width / width), 7))\n             padding = (current_height - new_height) // 2\n             current_height -= padding * 2\n         else:\n-            new_width = (width * current_height) // height\n+            new_width = int(round(width * (current_height / height), 7))\n             padding = (current_width - new_width) // 2\n             current_width -= padding * 2\n "
        },
        {
            "sha": "f4ca90f28c21db4bafe1e4901520c65b017c76fe",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d1a4cd53d90d5db384df948ccc293b3c1e3b9d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d1a4cd53d90d5db384df948ccc293b3c1e3b9d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=72d1a4cd53d90d5db384df948ccc293b3c1e3b9d",
            "patch": "@@ -225,6 +225,7 @@ def _get_number_of_features(self, orig_height: int, orig_width: int, height: int\n         num_image_tokens = unpadded_features + newline_features + base_features\n         return num_image_tokens\n \n+    # Adapted from transformers.models.llava_next.processing_llava_next.LlavaNextProcessor._get_unpadded_features\n     def _get_unpadded_features(self, height, width, patches_height, patches_width, scale_height, scale_width):\n         \"\"\"\n         Get number of features for a given image with height/width. LLaVA-NeXT is different from LLaVA\n@@ -237,11 +238,11 @@ def _get_unpadded_features(self, height, width, patches_height, patches_width, s\n         original_aspect_ratio = width / height\n         current_aspect_ratio = current_width / current_height\n         if original_aspect_ratio > current_aspect_ratio:\n-            new_height = int(height * (current_width / width))\n+            new_height = int(round(height * (current_width / width), 7))\n             padding = (current_height - new_height) // 2\n             current_height -= padding * 2\n         else:\n-            new_width = int(width * (current_height / height))\n+            new_width = int(round(width * (current_height / height), 7))\n             padding = (current_width - new_width) // 2\n             current_width -= padding * 2\n "
        }
    ],
    "stats": {
        "total": 13,
        "additions": 7,
        "deletions": 6
    }
}