{
    "author": "MekkCyber",
    "message": "[Quantization] fix fbgemm (#42561)\n\n* initial commit\n\n* passing tests\n\n* fix replace_linear\n\n* style\n\n* rm list\n\n* fix\n\n* style",
    "sha": "15b79ea808237691397be558bdebbef378b9fa35",
    "files": [
        {
            "sha": "1e581b1273743ee057a408f3f9b9e3b5972825e0",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 102,
            "deletions": 101,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/15b79ea808237691397be558bdebbef378b9fa35/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/15b79ea808237691397be558bdebbef378b9fa35/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=15b79ea808237691397be558bdebbef378b9fa35",
            "patch": "@@ -12,7 +12,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n+\n from ..activations import ACT2FN\n+from ..core_model_loading import ConversionOps\n+from ..quantizers.quantizers_utils import get_module_from_name, should_convert_module\n from ..utils import is_accelerate_available, is_fbgemm_gpu_available, is_torch_available, logging\n \n \n@@ -29,18 +33,75 @@\n logger = logging.get_logger(__name__)\n \n \n+class FbgemmFp8Quantize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, torch.Tensor | list[torch.Tensor]],\n+        model: Optional[torch.nn.Module] = None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        target_key, value = tuple(input_dict.items())[0]\n+        value = value[0]\n+\n+        from ..integrations import FbgemmFp8Llama4TextExperts\n+\n+        module, tensor_name = get_module_from_name(model, target_key)\n+\n+        if isinstance(module, FbgemmFp8Llama4TextExperts):\n+            if tensor_name == \"gate_up_proj\":\n+                # Process each expert separately\n+                # Transpose the second and third dimension\n+                transposed_param = value.transpose(1, 2)\n+\n+                # Reshape to 2D for quantization\n+                original_shape = transposed_param.shape\n+                flattened_param = transposed_param.reshape(-1, original_shape[-1])\n+\n+                # Quantize using per row instead of per column\n+                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n+\n+                # Reshape back to original dimensions\n+                new_value = new_value_flat.reshape(original_shape)\n+                new_value = new_value.transpose(1, 2)\n+                weight_scale = weight_scale_flat.reshape(original_shape[0], 1, original_shape[1])\n+            elif tensor_name == \"down_proj\":\n+                # Process each expert separately\n+                # Transpose the weights for proper quantization\n+                transposed_param = value.transpose(1, 2)\n+\n+                # Reshape to 2D for quantization\n+                original_shape = transposed_param.shape\n+                flattened_param = transposed_param.reshape(-1, original_shape[-1])\n+\n+                # Quantize using per column\n+                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n+\n+                # Reshape back to original dimensions\n+                new_value = new_value_flat.reshape(original_shape)\n+                new_value = new_value.transpose(1, 2)\n+                weight_scale = weight_scale_flat.reshape(original_shape[0], original_shape[1], 1)\n+        else:\n+            new_value, weight_scale = torch.ops.fbgemm.quantize_fp8_per_row(value)\n+            weight_scale = torch.nn.Parameter(weight_scale.view(weight_scale.shape[0], 1))\n+\n+        return {target_key: torch.nn.Parameter(new_value), f\"{target_key}_scale\": weight_scale}\n+\n+\n class FbgemmFp8Linear(torch.nn.Linear):\n-    def __init__(self, in_features, out_features, bias, weight_dtype=torch.float32):\n+    def __init__(self, in_features, out_features, bias, dtype=torch.float8_e4m3fn):\n         super().__init__(in_features, out_features, bias)\n         self.in_features = in_features\n         self.out_features = out_features\n \n-        self.weight = torch.nn.Parameter(torch.zeros((out_features, in_features), dtype=torch.float8_e4m3fn))\n-        self.weight_scale = torch.nn.Parameter(torch.zeros((out_features, 1), dtype=weight_dtype))\n+        self.weight = torch.nn.Parameter(torch.zeros((out_features, in_features), dtype=dtype))\n+        self.weight_scale = torch.nn.Parameter(torch.zeros((out_features, 1), dtype=torch.float32))\n         self.register_buffer(\"input_scale_ub\", torch.zeros([1], dtype=torch.float), persistent=False)\n \n         if bias:\n-            self.bias = torch.nn.Parameter(torch.zeros((self.out_features), dtype=weight_dtype))\n+            self.bias = torch.nn.Parameter(torch.zeros((self.out_features), dtype=torch.float32))\n         else:\n             self.bias = None\n \n@@ -154,90 +215,11 @@ def forward(self, hidden_states):\n         return next_states.view(-1, self.hidden_size)\n \n \n-def _replace_with_fbgemm_fp8_linear(\n-    model,\n-    modules_to_not_convert=None,\n-    current_key_name=None,\n-    quantization_config=None,\n-    has_been_replaced=False,\n-    pre_quantized=False,\n-    config=None,\n-    tp_plan=None,\n-):\n-    \"\"\"\n-    Private method that wraps the recursion for module replacement.\n-\n-    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n-    \"\"\"\n-\n-    import re\n-\n-    if current_key_name is None:\n-        current_key_name = []\n-\n-    for name, module in model.named_children():\n-        current_key_name.append(name)\n-\n-        if (isinstance(module, nn.Linear)) and name not in modules_to_not_convert:\n-            # Check if the current key is not in the `modules_to_not_convert`\n-            current_key_name_str = \".\".join(current_key_name)\n-            if not any(\n-                (key + \".\" in current_key_name_str) or (key == current_key_name_str) for key in modules_to_not_convert\n-            ):\n-                with init_empty_weights(include_buffers=True):\n-                    in_features = module.in_features\n-                    out_features = module.out_features\n-                    model._modules[name] = FbgemmFp8Linear(\n-                        in_features,\n-                        out_features,\n-                        module.bias is not None,\n-                    )\n-                    has_been_replaced = True\n-\n-                    # Force requires grad to False to avoid unexpected errors\n-                    model._modules[name].requires_grad_(False)\n-                # set non persistent buffer outside of init_empty_weights\n-                model._modules[name].input_scale_ub = torch.tensor(\n-                    [quantization_config.activation_scale_ub],\n-                    dtype=torch.float,\n-                )\n-        if module.__class__.__name__ == \"Llama4TextExperts\" and name not in modules_to_not_convert:\n-            current_key_name_str = \".\".join(current_key_name)\n-            if not any(\n-                (key + \".\" in current_key_name_str) or (key == current_key_name_str) for key in modules_to_not_convert\n-            ):\n-                with init_empty_weights(include_buffers=True):\n-                    tp_plan[re.sub(r\"\\d+\", \"*\", current_key_name_str + \".down_proj_scale\")] = None\n-                    model._modules[name] = FbgemmFp8Llama4TextExperts(\n-                        config.text_config,\n-                    )\n-                model._modules[name].input_scale_ub = torch.tensor(\n-                    [quantization_config.activation_scale_ub], dtype=torch.float\n-                )\n-\n-        if len(list(module.children())) > 0:\n-            _, has_been_replaced = _replace_with_fbgemm_fp8_linear(\n-                module,\n-                modules_to_not_convert,\n-                current_key_name,\n-                quantization_config,\n-                has_been_replaced=has_been_replaced,\n-                pre_quantized=pre_quantized,\n-                config=config,\n-                tp_plan=tp_plan,\n-            )\n-        # Remove the last key for recursion\n-        current_key_name.pop(-1)\n-    return model, has_been_replaced\n-\n-\n def replace_with_fbgemm_fp8_linear(\n     model,\n     modules_to_not_convert=None,\n-    current_key_name=None,\n     quantization_config=None,\n     pre_quantized=False,\n-    config=None,\n     tp_plan=None,\n ):\n     \"\"\"\n@@ -254,26 +236,45 @@ def replace_with_fbgemm_fp8_linear(\n         modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n             Names of the modules to not convert in `FP8Linear`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n-        current_key_name (`list[`str`]`, *optional*):\n-            An array to track the current key of the recursion. This is used to check whether the current key (part of\n-            it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n-            `disk`).\n     \"\"\"\n \n-    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n-\n-    if quantization_config.modules_to_not_convert is not None:\n-        modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n-    modules_to_not_convert = list(set(modules_to_not_convert))\n-    model, has_been_replaced = _replace_with_fbgemm_fp8_linear(\n-        model,\n-        modules_to_not_convert,\n-        current_key_name,\n-        quantization_config,\n-        pre_quantized=pre_quantized,\n-        config=config,\n-        tp_plan=tp_plan,\n-    )\n+    has_been_replaced = False\n+    module_kwargs = {} if pre_quantized else {\"dtype\": None}\n+\n+    for module_name, module in model.named_modules():\n+        if not should_convert_module(module_name, modules_to_not_convert):\n+            continue\n+\n+        new_module = None\n+        with init_empty_weights(include_buffers=True):\n+            if module.__class__.__name__ == \"Llama4TextExperts\":\n+                # TODO: make sure tp works later\n+                # if tp_plan is not None:\n+                #     tp_key = re.sub(r\"\\d+\", \"*\", f\"{module_name}.down_proj_scale\")\n+                #     tp_plan[tp_key] = None\n+                text_config = getattr(model.config, \"text_config\", model.config)\n+                new_module = FbgemmFp8Llama4TextExperts(text_config or model.config)\n+            elif isinstance(module, nn.Linear):\n+                new_module = FbgemmFp8Linear(\n+                    module.in_features,\n+                    module.out_features,\n+                    module.bias is not None,\n+                    **module_kwargs,\n+                )\n+                new_module.requires_grad_(False)\n+\n+        if new_module is None:\n+            continue\n+\n+        if hasattr(new_module, \"input_scale_ub\"):\n+            new_module.input_scale_ub = torch.tensor(\n+                [quantization_config.activation_scale_ub],\n+                dtype=torch.float,\n+            )\n+\n+        model.set_submodule(module_name, new_module)\n+        has_been_replaced = True\n+\n     if not has_been_replaced:\n         logger.warning(\n             \"You are loading your model using FP8 quantization but no linear modules were found in your model.\""
        },
        {
            "sha": "94fc34c1e70d369b3f7387b48042c180d50654c4",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/15b79ea808237691397be558bdebbef378b9fa35/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/15b79ea808237691397be558bdebbef378b9fa35/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=15b79ea808237691397be558bdebbef378b9fa35",
            "patch": "@@ -61,10 +61,9 @@ def get_keys_to_not_convert(model):\n         for name, module in model.named_modules()\n         if output_emb_module is not None and id(module) == id(output_emb_module)\n     }\n-    candidates = tied_keys | last_module_key | output_emb_keys\n+    modules_to_not_convert = tied_keys | last_module_key | output_emb_keys\n \n-    modules_to_not_convert = {name.replace(suffix, \"\") for name in candidates for suffix in [\".weight\", \".bias\"]}\n-    return modules_to_not_convert\n+    return list(modules_to_not_convert)\n \n \n class HfQuantizer(ABC):"
        },
        {
            "sha": "ae186d3cbdbfebff22001d6ed339af3f950d3b45",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/15b79ea808237691397be558bdebbef378b9fa35/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/15b79ea808237691397be558bdebbef378b9fa35/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=15b79ea808237691397be558bdebbef378b9fa35",
            "patch": "@@ -285,3 +285,8 @@ def is_serializable(self, safe_serialization=None):\n     @property\n     def is_trainable(self) -> bool:\n         return False\n+\n+    def get_quantize_ops(self):\n+        from ..integrations.fbgemm_fp8 import FbgemmFp8Quantize\n+\n+        return FbgemmFp8Quantize(self)"
        },
        {
            "sha": "ecd653292582972996a22d84a3e5a5c453907b4c",
            "filename": "tests/quantization/fbgemm_fp8/test_fbgemm_fp8.py",
            "status": "modified",
            "additions": 24,
            "deletions": 16,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/15b79ea808237691397be558bdebbef378b9fa35/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/15b79ea808237691397be558bdebbef378b9fa35/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py?ref=15b79ea808237691397be558bdebbef378b9fa35",
            "patch": "@@ -15,6 +15,7 @@\n import gc\n import tempfile\n import unittest\n+from typing import Any\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, FbgemmFp8Config, OPTForCausalLM\n from transformers.testing_utils import (\n@@ -71,7 +72,12 @@ class FbgemmFp8Test(unittest.TestCase):\n     input_text = \"What are we having for dinner?\"\n     max_new_tokens = 9\n \n-    EXPECTED_OUTPUT = \"What are we having for dinner?\\nI'm having a steak and a salad\"\n+    EXPECTED_OUTPUT = set[Any](\n+        [\n+            \"What are we having for dinner?\\nI'm having a steak and a salad\",\n+            \"What are we having for dinner? I donâ€™t know. What are we having\",\n+        ]\n+    )\n \n     device_map = \"cuda\"\n \n@@ -155,27 +161,29 @@ def test_quantized_model_conversion(self):\n             if isinstance(module, FbgemmFp8Linear):\n                 nb_fbgemm_linear += 1\n \n-        self.assertEqual(nb_linears - 1, nb_fbgemm_linear)\n+        self.assertEqual(nb_linears, nb_fbgemm_linear)\n \n         with init_empty_weights():\n             model = OPTForCausalLM(config)\n         quantization_config = FbgemmFp8Config(modules_to_not_convert=[\"fc1\"])\n-        model = replace_with_fbgemm_fp8_linear(model, quantization_config=quantization_config)\n+        model = replace_with_fbgemm_fp8_linear(\n+            model, modules_to_not_convert=[\"fc1\"], quantization_config=quantization_config\n+        )\n         nb_fbgemm_linear = 0\n         for module in model.modules():\n             if isinstance(module, FbgemmFp8Linear):\n                 nb_fbgemm_linear += 1\n \n-        self.assertEqual(nb_linears - 25, nb_fbgemm_linear)\n+        self.assertEqual(nb_linears - 24, nb_fbgemm_linear)\n \n     def test_quantized_model(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly\n         \"\"\"\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n-        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n+        self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n     def test_save_pretrained(self):\n         \"\"\"\n@@ -188,8 +196,8 @@ def test_save_pretrained(self):\n \n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n-            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n+            self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n     def test_change_loading_attributes(self):\n         \"\"\"\n@@ -208,8 +216,8 @@ def test_change_loading_attributes(self):\n \n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n-            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n+            self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n     @require_torch_multi_gpu\n     def test_quantized_model_multi_gpu(self):\n@@ -224,8 +232,8 @@ def test_quantized_model_multi_gpu(self):\n         )\n         self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n \n-        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n+        self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n     def test_quantized_model_offload(self):\n         \"\"\"\n@@ -250,8 +258,8 @@ def test_save_pretrained_offload(self):\n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n             quantized_model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=self.offload_device_map)\n-            output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+            output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n+            self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n     @require_torch_multi_gpu\n     def test_save_pretrained_multi_gpu(self):\n@@ -266,8 +274,8 @@ def test_save_pretrained_multi_gpu(self):\n \n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n-            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n-            self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n+            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n+            self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n \n @require_torch_gpu"
        }
    ],
    "stats": {
        "total": 253,
        "additions": 133,
        "deletions": 120
    }
}