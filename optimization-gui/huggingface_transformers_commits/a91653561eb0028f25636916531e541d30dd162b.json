{
    "author": "vasqu",
    "message": "[`Ernie 4.5`] Post merge adaptations (#39664)\n\n* ernie 4.5 fixes\n\n* Apply style fixes\n\n* fix\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "a91653561eb0028f25636916531e541d30dd162b",
    "files": [
        {
            "sha": "af24c0b8bf011d168bdba2b8799dd8bcd16ad531",
            "filename": "docs/source/en/model_doc/ernie4_5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -31,7 +31,7 @@ The Ernie 4.5 model was released in the [Ernie 4.5 Model Family](https://ernie.b\n This family of models contains multiple different architectures and model sizes. This model in specific targets the base text\n model without mixture of experts (moe) with 0.3B parameters in total. It uses the standard [Llama](./llama.md) at its core.\n \n-Other models from the family can be found at [Ernie 4.5 MoE](./ernie4_5_moe.md).\n+Other models from the family can be found at [Ernie 4.5 Moe](./ernie4_5_moe.md).\n \n <div class=\"flex justify-center\">\n     <img src=\"https://ernie.baidu.com/blog/posts/ernie4.5/overview.png\"/>"
        },
        {
            "sha": "8c3e75593cd5d3dfcc398446cc3c4f0711b968bf",
            "filename": "docs/source/en/model_doc/ernie4_5_moe.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -23,11 +23,11 @@ rendered properly in your Markdown viewer.\n     </div>\n </div>\n \n-# Ernie 4.5 MoE\n+# Ernie 4.5 Moe\n \n ## Overview\n \n-The Ernie 4.5 MoE model was released in the [Ernie 4.5 Model Family](https://ernie.baidu.com/blog/posts/ernie4.5/) release by baidu.\n+The Ernie 4.5 Moe model was released in the [Ernie 4.5 Model Family](https://ernie.baidu.com/blog/posts/ernie4.5/) release by baidu.\n This family of models contains multiple different architectures and model sizes. This model in specific targets the base text\n model with mixture of experts (moe) - one with 21B total, 3B active parameters and another one with 300B total, 47B active parameters.\n It uses the standard [Llama](./llama.md) at its core combined with a specialized MoE based on [Mixtral](./mixtral.md) with additional shared\n@@ -167,17 +167,17 @@ This model was contributed by [Anton Vlasjuk](https://huggingface.co/AntonV).\n The original code can be found [here](https://github.com/PaddlePaddle/ERNIE).\n \n \n-## Ernie4_5_MoEConfig\n+## Ernie4_5_MoeConfig\n \n-[[autodoc]] Ernie4_5_MoEConfig\n+[[autodoc]] Ernie4_5_MoeConfig\n \n-## Ernie4_5_MoEModel\n+## Ernie4_5_MoeModel\n \n-[[autodoc]] Ernie4_5_MoEModel\n+[[autodoc]] Ernie4_5_MoeModel\n     - forward\n \n-## Ernie4_5_MoEForCausalLM\n+## Ernie4_5_MoeForCausalLM\n \n-[[autodoc]] Ernie4_5_MoEForCausalLM\n+[[autodoc]] Ernie4_5_MoeForCausalLM\n     - forward\n     - generate"
        },
        {
            "sha": "75a328d3cc64f5cc4febbd8e64c13f22af09eb79",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -130,7 +130,7 @@\n         (\"eomt\", \"EomtConfig\"),\n         (\"ernie\", \"ErnieConfig\"),\n         (\"ernie4_5\", \"Ernie4_5Config\"),\n-        (\"ernie4_5_moe\", \"Ernie4_5_MoEConfig\"),\n+        (\"ernie4_5_moe\", \"Ernie4_5_MoeConfig\"),\n         (\"ernie_m\", \"ErnieMConfig\"),\n         (\"esm\", \"EsmConfig\"),\n         (\"falcon\", \"FalconConfig\"),"
        },
        {
            "sha": "0574346832f03a589feb2e7ff6e32656bce465e2",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -121,7 +121,7 @@\n         (\"encodec\", \"EncodecModel\"),\n         (\"ernie\", \"ErnieModel\"),\n         (\"ernie4_5\", \"Ernie4_5Model\"),\n-        (\"ernie4_5_moe\", \"Ernie4_5_MoEModel\"),\n+        (\"ernie4_5_moe\", \"Ernie4_5_MoeModel\"),\n         (\"ernie_m\", \"ErnieMModel\"),\n         (\"esm\", \"EsmModel\"),\n         (\"falcon\", \"FalconModel\"),\n@@ -597,7 +597,7 @@\n         (\"emu3\", \"Emu3ForCausalLM\"),\n         (\"ernie\", \"ErnieForCausalLM\"),\n         (\"ernie4_5\", \"Ernie4_5ForCausalLM\"),\n-        (\"ernie4_5_moe\", \"Ernie4_5_MoEForCausalLM\"),\n+        (\"ernie4_5_moe\", \"Ernie4_5_MoeForCausalLM\"),\n         (\"falcon\", \"FalconForCausalLM\"),\n         (\"falcon_h1\", \"FalconH1ForCausalLM\"),\n         (\"falcon_mamba\", \"FalconMambaForCausalLM\"),"
        },
        {
            "sha": "294ccfc638cf9683f15f94c3d2faee2a5f19cd44",
            "filename": "src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -21,9 +21,9 @@\n logger = logging.get_logger(__name__)\n \n \n-class Ernie4_5_MoEConfig(PretrainedConfig):\n+class Ernie4_5_MoeConfig(PretrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`Ernie4_5_MoEModel`]. It is used to instantiate a\n+    This is the configuration class to store the configuration of a [`Ernie4_5_MoeModel`]. It is used to instantiate a\n     Ernie 4.5 MoE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n     with the defaults will yield a similar configuration to that of [baidu/ERNIE-4.5-21B-A3B-PT](https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-PT).\n \n@@ -34,7 +34,7 @@ class Ernie4_5_MoEConfig(PretrainedConfig):\n     Args:\n         vocab_size (`int`, *optional*, defaults to 103424):\n             Vocabulary size of the Ernie 4.5 MoE model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`Ernie4_5_MoEModel`]\n+            `inputs_ids` passed when calling [`Ernie4_5_MoeModel`]\n         pad_token_id (`int`, *optional*, defaults to 0):\n             Padding token id.\n         bos_token_id (`int`, *optional*, defaults to 1):\n@@ -133,13 +133,13 @@ class Ernie4_5_MoEConfig(PretrainedConfig):\n             The aux loss factor for the total loss.\n \n     ```python\n-    >>> from transformers import Ernie4_5_MoEModel, Ernie4_5_MoEConfig\n+    >>> from transformers import Ernie4_5_MoeModel, Ernie4_5_MoEConfig\n \n     >>> # Initializing a Ernie4_5_MoE style configuration\n     >>> configuration = Ernie4_5_MoEConfig()\n \n     >>> # Initializing a model from the ERNIE-4.5-21B-A3B style configuration\n-    >>> model = Ernie4_5_MoEModel(configuration)\n+    >>> model = Ernie4_5_MoeModel(configuration)\n \n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n@@ -251,4 +251,4 @@ def __init__(\n         )\n \n \n-__all__ = [\"Ernie4_5_MoEConfig\"]\n+__all__ = [\"Ernie4_5_MoeConfig\"]"
        },
        {
            "sha": "d583c27321dc0b2f026466389ca9ee852c90c1b6",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 36,
            "deletions": 34,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -37,14 +37,14 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import OutputRecorder, check_model_inputs\n-from .configuration_ernie4_5_moe import Ernie4_5_MoEConfig\n+from .configuration_ernie4_5_moe import Ernie4_5_MoeConfig\n \n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n-class Ernie4_5_MoERMSNorm(nn.Module):\n+class Ernie4_5_MoeRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n-        Ernie4_5_MoERMSNorm is equivalent to T5LayerNorm\n+        Ernie4_5_MoeRMSNorm is equivalent to T5LayerNorm\n         \"\"\"\n         super().__init__()\n         self.weight = nn.Parameter(torch.ones(hidden_size))\n@@ -61,7 +61,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class Ernie4_5_MoEMLP(nn.Module):\n+class Ernie4_5_MoeMLP(nn.Module):\n     def __init__(self, config, intermediate_size=None):\n         super().__init__()\n         self.config = config\n@@ -78,8 +78,8 @@ def forward(self, x):\n         return down_proj\n \n \n-class Ernie4_5_MoERotaryEmbedding(nn.Module):\n-    def __init__(self, config: Ernie4_5_MoEConfig, device=None):\n+class Ernie4_5_MoeRotaryEmbedding(nn.Module):\n+    def __init__(self, config: Ernie4_5_MoeConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n@@ -194,10 +194,10 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n-class Ernie4_5_MoEAttention(nn.Module):\n+class Ernie4_5_MoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Ernie4_5_MoEConfig, layer_idx: int):\n+    def __init__(self, config: Ernie4_5_MoeConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n@@ -257,7 +257,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Ernie4_5_MoEStatics(nn.Module):\n+class Ernie4_5_MoeStatics(nn.Module):\n     \"\"\"\n     Stores MoE (Mixture of Experts) statistics\n         - Bias for the gating\n@@ -284,7 +284,7 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n-class Ernie4_5_MoESparseMoeBlock(nn.Module):\n+class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n     \"\"\"\n     This implementation is\n     strictly equivalent to standard MoE with full capacity (no\n@@ -305,19 +305,19 @@ def __init__(self, config):\n         self.top_k = config.moe_k\n \n         # correction bias (yes it seems to be a typo with statics <> statistics)\n-        self.moe_statics = Ernie4_5_MoEStatics(config)\n+        self.moe_statics = Ernie4_5_MoeStatics(config)\n \n         # gating\n         self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n         self.experts = nn.ModuleList(\n-            [Ernie4_5_MoEMLP(config, config.moe_intermediate_size) for _ in range(config.moe_num_experts)]\n+            [Ernie4_5_MoeMLP(config, config.moe_intermediate_size) for _ in range(config.moe_num_experts)]\n         )\n         self.norm_min = config.moe_norm_min\n \n         # (optional) shared experts for all forwards\n         self.shared_experts = None\n         if config.moe_num_shared_experts > 0:\n-            self.shared_experts = Ernie4_5_MoEMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n+            self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n \n     def forward(\n         self,\n@@ -379,24 +379,24 @@ def forward(\n         return final_hidden_states, router_logits\n \n \n-class Ernie4_5_MoEDecoderLayer(GradientCheckpointingLayer):\n+class Ernie4_5_MoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = Ernie4_5_MoEAttention(config, layer_idx)\n+        self.self_attn = Ernie4_5_MoeAttention(config, layer_idx)\n \n         if (\n             ((layer_idx + 1) % config.moe_layer_interval == 0)\n             and layer_idx >= config.moe_layer_start_index\n             and layer_idx <= config.moe_layer_end_index\n         ):\n-            self.mlp = Ernie4_5_MoESparseMoeBlock(config)\n+            self.mlp = Ernie4_5_MoeSparseMoeBlock(config)\n         else:\n-            self.mlp = Ernie4_5_MoEMLP(config)\n+            self.mlp = Ernie4_5_MoeMLP(config)\n \n-        self.input_layernorm = Ernie4_5_MoERMSNorm(config.hidden_size, config.rms_norm_eps)\n-        self.post_attention_layernorm = Ernie4_5_MoERMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.input_layernorm = Ernie4_5_MoeRMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.post_attention_layernorm = Ernie4_5_MoeRMSNorm(config.hidden_size, config.rms_norm_eps)\n \n     def forward(\n         self,\n@@ -461,43 +461,45 @@ def forward(\n \n \n @auto_docstring\n-class Ernie4_5_MoEPreTrainedModel(PreTrainedModel):\n-    config: Ernie4_5_MoEConfig\n+class Ernie4_5_MoePreTrainedModel(PreTrainedModel):\n+    config: Ernie4_5_MoeConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"Ernie4_5_MoEDecoderLayer\"]\n+    _no_split_modules = [\"Ernie4_5_MoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(Ernie4_5_MoESparseMoeBlock, index=1),\n-        \"hidden_states\": Ernie4_5_MoEDecoderLayer,\n-        \"attentions\": Ernie4_5_MoEAttention,\n+        \"router_logits\": OutputRecorder(Ernie4_5_MoeSparseMoeBlock, index=1),\n+        \"hidden_states\": Ernie4_5_MoeDecoderLayer,\n+        \"attentions\": Ernie4_5_MoeAttention,\n     }\n     _keep_in_fp32_modules_strict = [\"gate\", \"moe_statics\"]\n+    # Not supporting multi-token prediction (MTP) atm\n+    _keys_to_ignore_on_load_unexpected = [\"mtp\"]\n \n     def _init_weights(self, module):\n         super()._init_weights(module)\n-        if isinstance(module, Ernie4_5_MoEStatics):\n+        if isinstance(module, Ernie4_5_MoeStatics):\n             module.e_score_correction_bias.data.zero_()\n \n \n @auto_docstring\n-class Ernie4_5_MoEModel(Ernie4_5_MoEPreTrainedModel):\n-    def __init__(self, config: Ernie4_5_MoEConfig):\n+class Ernie4_5_MoeModel(Ernie4_5_MoePreTrainedModel):\n+    def __init__(self, config: Ernie4_5_MoeConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n \n         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n         self.layers = nn.ModuleList(\n-            [Ernie4_5_MoEDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+            [Ernie4_5_MoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.norm = Ernie4_5_MoERMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.rotary_emb = Ernie4_5_MoERotaryEmbedding(config=config)\n+        self.norm = Ernie4_5_MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Ernie4_5_MoeRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -650,14 +652,14 @@ def load_balancing_loss_func(\n \n \n @auto_docstring\n-class Ernie4_5_MoEForCausalLM(Ernie4_5_MoEPreTrainedModel, GenerationMixin):\n+class Ernie4_5_MoeForCausalLM(Ernie4_5_MoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)\n-        self.model = Ernie4_5_MoEModel(config)\n+        self.model = Ernie4_5_MoeModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=config.use_bias)\n \n@@ -745,4 +747,4 @@ def forward(\n         )\n \n \n-__all__ = [\"Ernie4_5_MoEForCausalLM\", \"Ernie4_5_MoEModel\", \"Ernie4_5_MoEPreTrainedModel\"]\n+__all__ = [\"Ernie4_5_MoeForCausalLM\", \"Ernie4_5_MoeModel\", \"Ernie4_5_MoePreTrainedModel\"]"
        },
        {
            "sha": "76fec0ff3bf08747b409f8e64a465ad11d9f7665",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 54,
            "deletions": 29,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -24,26 +24,25 @@\n from ...modeling_outputs import MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ..ernie4_5.modeling_ernie4_5 import Ernie4_5RotaryEmbedding, apply_rotary_pos_emb, rotate_half  # noqa: F401\n from ..llama.modeling_llama import LlamaAttention, LlamaRMSNorm\n from ..mixtral.modeling_mixtral import (\n     MixtralForCausalLM,\n-    MixtralModel,\n     MixtralPreTrainedModel,\n )\n from ..qwen3_moe.modeling_qwen3_moe import Qwen3MoeDecoderLayer, Qwen3MoeMLP\n-from .configuration_ernie4_5_moe import Ernie4_5_MoEConfig\n+from .configuration_ernie4_5_moe import Ernie4_5_MoeConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-class Ernie4_5_MoERMSNorm(LlamaRMSNorm):\n+class Ernie4_5_MoeRMSNorm(LlamaRMSNorm):\n     pass\n \n \n-class Ernie4_5_MoEMLP(Qwen3MoeMLP):\n+class Ernie4_5_MoeMLP(Qwen3MoeMLP):\n     def __init__(self, config, intermediate_size=None):\n         super().__init__(config, intermediate_size)\n \n@@ -52,12 +51,13 @@ def __init__(self, config, intermediate_size=None):\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n \n \n-class Ernie4_5_MoERotaryEmbedding(Ernie4_5RotaryEmbedding):\n-    pass\n+class Ernie4_5_MoeRotaryEmbedding(Ernie4_5RotaryEmbedding):\n+    def __init__(self, config: Ernie4_5_MoeConfig, device=None):\n+        super().__init__(config, device)\n \n \n-class Ernie4_5_MoEAttention(LlamaAttention):\n-    def __init__(self, config: Ernie4_5_MoEConfig, layer_idx: int):\n+class Ernie4_5_MoeAttention(LlamaAttention):\n+    def __init__(self, config: Ernie4_5_MoeConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n \n         self.attention_dropout = 0.0\n@@ -68,7 +68,7 @@ def __init__(self, config: Ernie4_5_MoEConfig, layer_idx: int):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n \n \n-class Ernie4_5_MoEStatics(nn.Module):\n+class Ernie4_5_MoeStatics(nn.Module):\n     \"\"\"\n     Stores MoE (Mixture of Experts) statistics\n         - Bias for the gating\n@@ -95,7 +95,7 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n-class Ernie4_5_MoESparseMoeBlock(nn.Module):\n+class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n     \"\"\"\n     This implementation is\n     strictly equivalent to standard MoE with full capacity (no\n@@ -116,19 +116,19 @@ def __init__(self, config):\n         self.top_k = config.moe_k\n \n         # correction bias (yes it seems to be a typo with statics <> statistics)\n-        self.moe_statics = Ernie4_5_MoEStatics(config)\n+        self.moe_statics = Ernie4_5_MoeStatics(config)\n \n         # gating\n         self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n         self.experts = nn.ModuleList(\n-            [Ernie4_5_MoEMLP(config, config.moe_intermediate_size) for _ in range(config.moe_num_experts)]\n+            [Ernie4_5_MoeMLP(config, config.moe_intermediate_size) for _ in range(config.moe_num_experts)]\n         )\n         self.norm_min = config.moe_norm_min\n \n         # (optional) shared experts for all forwards\n         self.shared_experts = None\n         if config.moe_num_shared_experts > 0:\n-            self.shared_experts = Ernie4_5_MoEMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n+            self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n \n     def forward(\n         self,\n@@ -190,38 +190,63 @@ def forward(\n         return final_hidden_states, router_logits\n \n \n-class Ernie4_5_MoEDecoderLayer(Qwen3MoeDecoderLayer, nn.Module):\n+class Ernie4_5_MoeDecoderLayer(Qwen3MoeDecoderLayer, nn.Module):\n     def __init__(self, config, layer_idx):\n         nn.Module().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = Ernie4_5_MoEAttention(config, layer_idx)\n+        self.self_attn = Ernie4_5_MoeAttention(config, layer_idx)\n \n         if (\n             ((layer_idx + 1) % config.moe_layer_interval == 0)\n             and layer_idx >= config.moe_layer_start_index\n             and layer_idx <= config.moe_layer_end_index\n         ):\n-            self.mlp = Ernie4_5_MoESparseMoeBlock(config)\n+            self.mlp = Ernie4_5_MoeSparseMoeBlock(config)\n         else:\n-            self.mlp = Ernie4_5_MoEMLP(config)\n+            self.mlp = Ernie4_5_MoeMLP(config)\n \n-        self.input_layernorm = Ernie4_5_MoERMSNorm(config.hidden_size, config.rms_norm_eps)\n-        self.post_attention_layernorm = Ernie4_5_MoERMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.input_layernorm = Ernie4_5_MoeRMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.post_attention_layernorm = Ernie4_5_MoeRMSNorm(config.hidden_size, config.rms_norm_eps)\n \n \n @auto_docstring\n-class Ernie4_5_MoEPreTrainedModel(MixtralPreTrainedModel):\n+class Ernie4_5_MoePreTrainedModel(MixtralPreTrainedModel):\n+    config: Ernie4_5_MoeConfig\n+    _no_split_modules = [\"Ernie4_5_MoeDecoderLayer\"]\n     _keep_in_fp32_modules_strict = [\"gate\", \"moe_statics\"]\n+    # Not supporting multi-token prediction (MTP) atm\n+    _keys_to_ignore_on_load_unexpected = [\"mtp\"]\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(Ernie4_5_MoeSparseMoeBlock, index=1),\n+        \"hidden_states\": Ernie4_5_MoeDecoderLayer,\n+        \"attentions\": Ernie4_5_MoeAttention,\n+    }\n \n     def _init_weights(self, module):\n         MixtralPreTrainedModel._init_weights(module)\n-        if isinstance(module, Ernie4_5_MoEStatics):\n+        if isinstance(module, Ernie4_5_MoeStatics):\n             module.e_score_correction_bias.data.zero_()\n \n \n @auto_docstring\n-class Ernie4_5_MoEModel(MixtralModel):\n+class Ernie4_5_MoeModel(Ernie4_5_MoePreTrainedModel):\n+    def __init__(self, config: Ernie4_5_MoeConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Ernie4_5_MoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Ernie4_5_MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Ernie4_5_MoeRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -287,10 +312,10 @@ def forward(\n \n \n @auto_docstring\n-class Ernie4_5_MoEForCausalLM(MixtralForCausalLM, Ernie4_5_MoEPreTrainedModel):\n+class Ernie4_5_MoeForCausalLM(MixtralForCausalLM, Ernie4_5_MoePreTrainedModel):\n     def __init__(self, config):\n-        Ernie4_5_MoEPreTrainedModel().__init__(config)\n-        self.model = Ernie4_5_MoEModel(config)\n+        Ernie4_5_MoePreTrainedModel().__init__(config)\n+        self.model = Ernie4_5_MoeModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=config.use_bias)\n \n@@ -314,7 +339,7 @@ def forward(self, **super_kwargs):\n \n \n __all__ = [\n-    \"Ernie4_5_MoEForCausalLM\",\n-    \"Ernie4_5_MoEModel\",\n-    \"Ernie4_5_MoEPreTrainedModel\",\n+    \"Ernie4_5_MoeForCausalLM\",\n+    \"Ernie4_5_MoeModel\",\n+    \"Ernie4_5_MoePreTrainedModel\",\n ]"
        },
        {
            "sha": "ea5a91804eeee784e57777cb5464c030cbbf56f8",
            "filename": "tests/models/ernie4_5/test_modeling_ernie4_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -102,7 +102,6 @@ def test_ernie4_5_0p3B(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"baidu/ERNIE-4.5-0.3B-PT\", revision=\"refs/pr/3\")\n         model = Ernie4_5ForCausalLM.from_pretrained(\n             \"baidu/ERNIE-4.5-0.3B-PT\",\n-            revision=\"refs/pr/3\",\n             device_map=\"auto\",\n             torch_dtype=torch.bfloat16,\n         )"
        },
        {
            "sha": "393f8ee5aafed92e6f2e9adc0edad8f7e57ed668",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -18,7 +18,7 @@\n \n import pytest\n \n-from transformers import Ernie4_5_MoEConfig, is_torch_available\n+from transformers import Ernie4_5_MoeConfig, is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n     is_flaky,\n@@ -38,33 +38,33 @@\n \n     from transformers import (\n         AutoTokenizer,\n-        Ernie4_5_MoEForCausalLM,\n-        Ernie4_5_MoEModel,\n+        Ernie4_5_MoeForCausalLM,\n+        Ernie4_5_MoeModel,\n     )\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n-class Ernie4_5_MoEModelTester(CausalLMModelTester):\n-    config_class = Ernie4_5_MoEConfig\n+class Ernie4_5_MoeModelTester(CausalLMModelTester):\n+    config_class = Ernie4_5_MoeConfig\n     if is_torch_available():\n-        base_model_class = Ernie4_5_MoEModel\n-        causal_lm_class = Ernie4_5_MoEForCausalLM\n+        base_model_class = Ernie4_5_MoeModel\n+        causal_lm_class = Ernie4_5_MoeForCausalLM\n \n \n @require_torch\n-class Ernie4_5_MoEModelTest(CausalLMModelTest, unittest.TestCase):\n+class Ernie4_5_MoeModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (\n-            Ernie4_5_MoEModel,\n-            Ernie4_5_MoEForCausalLM,\n+            Ernie4_5_MoeModel,\n+            Ernie4_5_MoeForCausalLM,\n         )\n         if is_torch_available()\n         else ()\n     )\n     pipeline_model_mapping = (\n         {\n-            \"feature-extraction\": Ernie4_5_MoEModel,\n-            \"text-generation\": Ernie4_5_MoEForCausalLM,\n+            \"feature-extraction\": Ernie4_5_MoeModel,\n+            \"text-generation\": Ernie4_5_MoeForCausalLM,\n         }\n         if is_torch_available()\n         else {}\n@@ -73,7 +73,7 @@ class Ernie4_5_MoEModelTest(CausalLMModelTest, unittest.TestCase):\n     test_headmasking = False\n     test_pruning = False\n     test_all_params_have_gradient = False\n-    model_tester_class = Ernie4_5_MoEModelTester\n+    model_tester_class = Ernie4_5_MoeModelTester\n \n     @require_flash_attn\n     @require_torch_gpu\n@@ -82,7 +82,7 @@ class Ernie4_5_MoEModelTest(CausalLMModelTest, unittest.TestCase):\n     @slow\n     def test_flash_attn_2_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(reason=\"Model does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -123,7 +123,7 @@ def test_load_balancing_loss(self):\n         config.output_router_logits = True\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(1).to(torch_device)\n-        model = Ernie4_5_MoEForCausalLM(config)\n+        model = Ernie4_5_MoeForCausalLM(config)\n         model.to(torch_device)\n         model.eval()\n         result = model(input_ids, attention_mask=attention_mask)\n@@ -153,7 +153,7 @@ def test_load_balancing_loss(self):\n @require_torch_multi_accelerator\n @require_torch_large_accelerator\n @require_torch\n-class Ernie4_5_MoEIntegrationTest(unittest.TestCase):\n+class Ernie4_5_MoeIntegrationTest(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.model = None\n@@ -169,9 +169,8 @@ def tearDown(self):\n     @classmethod\n     def get_model(cls):\n         if cls.model is None:\n-            cls.model = Ernie4_5_MoEForCausalLM.from_pretrained(\n+            cls.model = Ernie4_5_MoeForCausalLM.from_pretrained(\n                 \"baidu/ERNIE-4.5-21B-A3B-PT\",\n-                revision=\"refs/pr/11\",\n                 device_map=\"auto\",\n                 load_in_4bit=True,\n             )"
        },
        {
            "sha": "d8bd32847edaa6c481d67506369c7ea0cedca9d1",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a91653561eb0028f25636916531e541d30dd162b/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a91653561eb0028f25636916531e541d30dd162b/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=a91653561eb0028f25636916531e541d30dd162b",
            "patch": "@@ -33,7 +33,7 @@\n \n SPECIAL_CASES_TO_ALLOW = {\n     \"Ernie4_5Config\": [\"tie_word_embeddings\"],\n-    \"Ernie4_5_MoEConfig\": [\"tie_word_embeddings\"],\n+    \"Ernie4_5_MoeConfig\": [\"tie_word_embeddings\"],\n     \"Lfm2Config\": [\"full_attn_idxs\", \"tie_word_embeddings\"],\n     # used internally during generation to provide the custom logit processors with their necessary information\n     \"DiaConfig\": ["
        }
    ],
    "stats": {
        "total": 227,
        "additions": 126,
        "deletions": 101
    }
}