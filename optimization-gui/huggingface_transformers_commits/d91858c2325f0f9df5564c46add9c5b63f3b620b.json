{
    "author": "yao-matrix",
    "message": "enable 3 mpt test cases on XPU (#37546)\n\n* enable 3 mpt test cases on XPU\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "d91858c2325f0f9df5564c46add9c5b63f3b620b",
    "files": [
        {
            "sha": "458e458dd28726b40ec18aab60288503cfc063f2",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 25,
            "deletions": 5,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/d91858c2325f0f9df5564c46add9c5b63f3b620b/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d91858c2325f0f9df5564c46add9c5b63f3b620b/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=d91858c2325f0f9df5564c46add9c5b63f3b620b",
            "patch": "@@ -17,7 +17,14 @@\n import unittest\n \n from transformers import MptConfig, is_torch_available\n-from transformers.testing_utils import require_bitsandbytes, require_torch, require_torch_gpu, slow, torch_device\n+from transformers.testing_utils import (\n+    Expectations,\n+    require_bitsandbytes,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -424,7 +431,7 @@ def test_model_from_pretrained(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_bitsandbytes\n class MptIntegrationTests(unittest.TestCase):\n     def test_generation_8k(self):\n@@ -439,7 +446,7 @@ def test_generation_8k(self):\n         input_text = \"Hello\"\n         expected_output = \"Hello, I'm a new user of the forum. I have a question about the \\\"Solaris\"\n \n-        inputs = tokenizer(input_text, return_tensors=\"pt\")\n+        inputs = tokenizer(input_text, return_tensors=\"pt\").to(torch_device)\n         outputs = model.generate(**inputs, max_new_tokens=20)\n \n         decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n@@ -455,9 +462,22 @@ def test_generation(self):\n         )\n \n         input_text = \"Hello\"\n-        expected_output = \"Hello and welcome to the first episode of the new podcast, The Frugal Feminist.\\n\"\n+        expected_outputs = Expectations(\n+            {\n+                (\n+                    \"xpu\",\n+                    3,\n+                ): \"Hello and welcome to the first ever episode of the new and improved, and hopefully improved, podcast.\\n\",\n+                (\"cuda\", 7): \"Hello and welcome to the first episode of the new podcast, The Frugal Feminist.\\n\",\n+                (\n+                    \"cuda\",\n+                    8,\n+                ): \"Hello and welcome to the first day of the new release countdown for the month of May!\\nToday\",\n+            }\n+        )\n+        expected_output = expected_outputs.get_expectation()\n \n-        inputs = tokenizer(input_text, return_tensors=\"pt\")\n+        inputs = tokenizer(input_text, return_tensors=\"pt\").to(torch_device)\n         outputs = model.generate(**inputs, max_new_tokens=20)\n \n         decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 25,
        "deletions": 5
    }
}