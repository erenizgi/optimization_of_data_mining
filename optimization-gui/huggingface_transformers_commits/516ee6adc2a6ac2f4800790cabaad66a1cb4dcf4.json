{
    "author": "sergiopaniego",
    "message": "Fix incomplete sentence in `Zero-shot object detection` documentation (#33430)\n\nRephrase sentence in zero-shot object detection docs",
    "sha": "516ee6adc2a6ac2f4800790cabaad66a1cb4dcf4",
    "files": [
        {
            "sha": "5ac4706bffea8c5f3aa0a328285561e094fa3ac3",
            "filename": "docs/source/en/tasks/zero_shot_object_detection.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/516ee6adc2a6ac2f4800790cabaad66a1cb4dcf4/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/516ee6adc2a6ac2f4800790cabaad66a1cb4dcf4/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md?ref=516ee6adc2a6ac2f4800790cabaad66a1cb4dcf4",
            "patch": "@@ -26,8 +26,8 @@ is an open-vocabulary object detector. It means that it can detect objects in im\n the need to fine-tune the model on labeled datasets.\n \n OWL-ViT leverages multi-modal representations to perform open-vocabulary detection. It combines [CLIP](../model_doc/clip) with\n-lightweight object classification and localization heads. Open-vocabulary detection is achieved by embedding free-text queries with the text encoder of CLIP and using them as input to the object classification and localization heads.\n-associate images and their corresponding textual descriptions, and ViT processes image patches as inputs. The authors\n+lightweight object classification and localization heads. Open-vocabulary detection is achieved by embedding free-text queries with the text encoder of CLIP and using them as input to the object classification and localization heads,\n+which associate images with their corresponding textual descriptions, while ViT processes image patches as inputs. The authors\n of OWL-ViT first trained CLIP from scratch and then fine-tuned OWL-ViT end to end on standard object detection datasets using\n a bipartite matching loss.\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}