{
    "author": "MekkCyber",
    "message": "[kernels] Final kernel removal ðŸ¥³  (#41664)\n\n* fix bitnet\n\n* finallyyy\n\n* rm the kernel folder !\n\n* fix revision\n\n* fix\n\n* fix\n\n* revert",
    "sha": "de055d6db06d906cdd82030ef1e6776a2be3d6b5",
    "files": [
        {
            "sha": "89476b6f4e33b76a5ab705cf5b18cee05e7ad469",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/de055d6db06d906cdd82030ef1e6776a2be3d6b5/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de055d6db06d906cdd82030ef1e6776a2be3d6b5/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=de055d6db06d906cdd82030ef1e6776a2be3d6b5",
            "patch": "@@ -253,6 +253,8 @@ def register_kernel_mapping_transformers(*args, **kwargs):\n \n _HUB_KERNEL_MAPPING: dict[str, dict[str, str]] = {\n     \"causal-conv1d\": {\"repo_id\": \"kernels-community/causal-conv1d\"},\n+    \"mamba-ssm\": {\"repo_id\": \"kernels-community/mamba-ssm\", \"revision\": \"v0.0.4\"},\n+    \"falcon_mamba-ssm\": {\"repo_id\": \"kernels-community/mamba-ssm\", \"revision\": \"v0.0.4\"},\n }\n \n _KERNEL_MODULE_MAPPING: dict[str, ModuleType | None] = {}\n@@ -336,8 +338,9 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n \n         try:\n             repo_id = _HUB_KERNEL_MAPPING[kernel_name][\"repo_id\"]\n+            revision = _HUB_KERNEL_MAPPING[kernel_name].get(\"revision\", None)\n             version = _HUB_KERNEL_MAPPING[kernel_name].get(\"version\", None)\n-            kernel = get_kernel(repo_id, version=version)\n+            kernel = get_kernel(repo_id, revision=revision, version=version)\n             mapping[kernel_name] = kernel\n         except FileNotFoundError:\n             mapping[kernel_name] = None"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "src/transformers/kernels/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/src%2Ftransformers%2Fkernels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/src%2Ftransformers%2Fkernels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2F__init__.py?ref=eaa3d4dd35d0ef856ee976cb1d2018e816e439c4"
        },
        {
            "sha": "da88e3394f653369a7443245c67dcbe57f2ed23e",
            "filename": "src/transformers/kernels/falcon_mamba/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2F__init__.py?ref=eaa3d4dd35d0ef856ee976cb1d2018e816e439c4",
            "patch": "@@ -1,15 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 Tri Dao, Albert Gu, Technological Innovation Institute and HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from .selective_scan_with_ln_interface import mamba_inner_fn"
        },
        {
            "sha": "7e3474993c1f464f4a14e5efbef1f7e3cdd621d4",
            "filename": "src/transformers/kernels/falcon_mamba/selective_scan_with_ln_interface.py",
            "status": "removed",
            "additions": 0,
            "deletions": 529,
            "changes": 529,
            "blob_url": "https://github.com/huggingface/transformers/blob/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2Fselective_scan_with_ln_interface.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eaa3d4dd35d0ef856ee976cb1d2018e816e439c4/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2Fselective_scan_with_ln_interface.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Ffalcon_mamba%2Fselective_scan_with_ln_interface.py?ref=eaa3d4dd35d0ef856ee976cb1d2018e816e439c4",
            "patch": "@@ -1,529 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 Tri Dao, Albert Gu, Technological Innovation Institute and HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# Original code from: https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py\n-\n-import torch\n-import torch.nn.functional as F\n-from einops import rearrange, repeat\n-from torch.cuda.amp import custom_bwd, custom_fwd\n-\n-\n-try:\n-    import causal_conv1d_cuda\n-except ImportError:\n-    causal_conv1d_cuda = None\n-\n-import mamba_ssm\n-import selective_scan_cuda\n-\n-\n-# For BC for old mamba-ssm versions: https://github.com/huggingface/transformers/pull/33195#discussion_r1736401127\n-if hasattr(mamba_ssm.ops.triton, \"layernorm\"):\n-    from mamba_ssm.ops.triton.layernorm import _layer_norm_fwd\n-else:\n-    from mamba_ssm.ops.triton.layer_norm import _layer_norm_fwd\n-\n-\n-class SelectiveScanFn(torch.autograd.Function):\n-    @staticmethod\n-    def forward(\n-        ctx, u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False, return_last_state=False\n-    ):\n-        if u.stride(-1) != 1:\n-            u = u.contiguous()\n-        if delta.stride(-1) != 1:\n-            delta = delta.contiguous()\n-        if D is not None:\n-            D = D.contiguous()\n-        if B.stride(-1) != 1:\n-            B = B.contiguous()\n-        if C.stride(-1) != 1:\n-            C = C.contiguous()\n-        if z is not None and z.stride(-1) != 1:\n-            z = z.contiguous()\n-        if B.dim() == 3:\n-            B = rearrange(B, \"b dstate l -> b 1 dstate l\")\n-            ctx.squeeze_B = True\n-        if C.dim() == 3:\n-            C = rearrange(C, \"b dstate l -> b 1 dstate l\")\n-            ctx.squeeze_C = True\n-        out, x, *rest = selective_scan_cuda.fwd(u, delta, A, B, C, D, z, delta_bias, delta_softplus)\n-        ctx.delta_softplus = delta_softplus\n-        ctx.has_z = z is not None\n-        last_state = x[:, :, -1, 1::2]  # (batch, dim, dstate)\n-        if not ctx.has_z:\n-            ctx.save_for_backward(u, delta, A, B, C, D, delta_bias, x)\n-            return out if not return_last_state else (out, last_state)\n-        else:\n-            ctx.save_for_backward(u, delta, A, B, C, D, z, delta_bias, x, out)\n-            out_z = rest[0]\n-            return out_z if not return_last_state else (out_z, last_state)\n-\n-    @staticmethod\n-    def backward(ctx, dout, *args):\n-        if not ctx.has_z:\n-            u, delta, A, B, C, D, delta_bias, x = ctx.saved_tensors\n-            z = None\n-            out = None\n-        else:\n-            u, delta, A, B, C, D, z, delta_bias, x, out = ctx.saved_tensors\n-        if dout.stride(-1) != 1:\n-            dout = dout.contiguous()\n-        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n-        # backward of selective_scan_cuda with the backward of chunk).\n-        # Here we just pass in None and dz will be allocated in the C++ code.\n-        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(\n-            u,\n-            delta,\n-            A,\n-            B,\n-            C,\n-            D,\n-            z,\n-            delta_bias,\n-            dout,\n-            x,\n-            out,\n-            None,\n-            ctx.delta_softplus,\n-            False,  # option to recompute out_z, not used here\n-        )\n-        dz = rest[0] if ctx.has_z else None\n-        dB = dB.squeeze(1) if getattr(ctx, \"squeeze_B\", False) else dB\n-        dC = dC.squeeze(1) if getattr(ctx, \"squeeze_C\", False) else dC\n-        return (\n-            du,\n-            ddelta,\n-            dA,\n-            dB,\n-            dC,\n-            dD if D is not None else None,\n-            dz,\n-            ddelta_bias if delta_bias is not None else None,\n-            None,\n-            None,\n-        )\n-\n-\n-def rms_norm_forward(\n-    x,\n-    weight,\n-    bias,\n-    eps=1e-6,\n-    is_rms_norm=True,\n-):\n-    # x (b l) d\n-    if x.stride(-1) != 1:\n-        x = x.contiguous()\n-    weight = weight.contiguous()\n-    if bias is not None:\n-        bias = bias.contiguous()\n-    y = _layer_norm_fwd(x, weight, bias, eps, None, residual_dtype=None, is_rms_norm=is_rms_norm)[0]\n-    # y (b l) d\n-    return y\n-\n-\n-def selective_scan_fn(\n-    u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False, return_last_state=False\n-):\n-    \"\"\"if return_last_state is True, returns (out, last_state)\n-    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is\n-    not considered in the backward pass.\n-    \"\"\"\n-    return SelectiveScanFn.apply(u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state)\n-\n-\n-def selective_scan_ref(\n-    u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False, return_last_state=False\n-):\n-    \"\"\"\n-    u: r(B D L)\n-    delta: r(B D L)\n-    A: c(D N) or r(D N)\n-    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n-    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n-    D: r(D)\n-    z: r(B D L)\n-    delta_bias: r(D), fp32\n-\n-    out: r(B D L)\n-    last_state (optional): r(B D dstate) or c(B D dstate)\n-    \"\"\"\n-    dtype_in = u.dtype\n-    u = u.float()\n-    delta = delta.float()\n-    if delta_bias is not None:\n-        delta = delta + delta_bias[..., None].float()\n-    if delta_softplus:\n-        delta = F.softplus(delta)\n-    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n-    is_variable_B = B.dim() >= 3\n-    is_variable_C = C.dim() >= 3\n-    if A.is_complex():\n-        if is_variable_B:\n-            B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n-        if is_variable_C:\n-            C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n-    else:\n-        B = B.float()\n-        C = C.float()\n-    x = A.new_zeros((batch, dim, dstate))\n-    ys = []\n-    deltaA = torch.exp(torch.einsum(\"bdl,dn->bdln\", delta, A))\n-    if not is_variable_B:\n-        deltaB_u = torch.einsum(\"bdl,dn,bdl->bdln\", delta, B, u)\n-    else:\n-        if B.dim() == 3:\n-            deltaB_u = torch.einsum(\"bdl,bnl,bdl->bdln\", delta, B, u)\n-        else:\n-            B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n-            deltaB_u = torch.einsum(\"bdl,bdnl,bdl->bdln\", delta, B, u)\n-    if is_variable_C and C.dim() == 4:\n-        C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n-    last_state = None\n-    for i in range(u.shape[2]):\n-        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n-        if not is_variable_C:\n-            y = torch.einsum(\"bdn,dn->bd\", x, C)\n-        else:\n-            if C.dim() == 3:\n-                y = torch.einsum(\"bdn,bn->bd\", x, C[:, :, i])\n-            else:\n-                y = torch.einsum(\"bdn,bdn->bd\", x, C[:, :, :, i])\n-        if i == u.shape[2] - 1:\n-            last_state = x\n-        if y.is_complex():\n-            y = y.real * 2\n-        ys.append(y)\n-    y = torch.stack(ys, dim=2)  # (batch dim L)\n-    out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n-    if z is not None:\n-        out = out * F.silu(z)\n-    out = out.to(dtype=dtype_in)\n-    return out if not return_last_state else (out, last_state)\n-\n-\n-class MambaInnerFn(torch.autograd.Function):\n-    @staticmethod\n-    @custom_fwd\n-    def forward(\n-        ctx,\n-        xz,\n-        conv1d_weight,\n-        conv1d_bias,\n-        x_proj_weight,\n-        delta_proj_weight,\n-        out_proj_weight,\n-        out_proj_bias,\n-        A,\n-        B=None,\n-        C=None,\n-        D=None,\n-        delta_bias=None,\n-        B_proj_bias=None,\n-        C_proj_bias=None,\n-        delta_softplus=True,\n-        checkpoint_lvl=1,\n-        b_rms_weight=None,\n-        c_rms_weight=None,\n-        dt_rms_weight=None,\n-        b_c_dt_rms_eps=1e-6,\n-    ):\n-        \"\"\"\n-        xz: (batch, dim, seqlen)\n-        \"\"\"\n-        assert causal_conv1d_cuda is not None, \"causal_conv1d_cuda is not available. Please install causal-conv1d.\"\n-        assert checkpoint_lvl in [0, 1]\n-        L = xz.shape[-1]\n-        delta_rank = delta_proj_weight.shape[1]\n-        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n-        if torch.is_autocast_enabled():\n-            # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n-            target_dtype = (\n-                torch.get_autocast_dtype(\"cuda\")\n-                if hasattr(torch, \"get_autocast_dtype\")\n-                else torch.get_autocast_gpu_dtype()\n-            )\n-            x_proj_weight = x_proj_weight.to(dtype=target_dtype)\n-            delta_proj_weight = delta_proj_weight.to(dtype=target_dtype)\n-            out_proj_weight = out_proj_weight.to(dtype=target_dtype)\n-            out_proj_bias = out_proj_bias.to(dtype=target_dtype) if out_proj_bias is not None else None\n-        if xz.stride(-1) != 1:\n-            xz = xz.contiguous()\n-        conv1d_weight = rearrange(conv1d_weight, \"d 1 w -> d w\")\n-        x, z = xz.chunk(2, dim=1)\n-        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None\n-        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, None, None, None, True)\n-        # We're being very careful here about the layout, to avoid extra transposes.\n-        # We want delta to have d as the slowest moving dimension\n-        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n-        x_dbl = F.linear(rearrange(conv1d_out, \"b d l -> (b l) d\"), x_proj_weight)  # (bl d)\n-        delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l=L)\n-        ctx.is_variable_B = B is None\n-        ctx.is_variable_C = C is None\n-        ctx.B_proj_bias_is_None = B_proj_bias is None\n-        ctx.C_proj_bias_is_None = C_proj_bias is None\n-        if B is None:  # variable B\n-            B = x_dbl[:, delta_rank : delta_rank + d_state]  # (bl dstate)\n-            if B_proj_bias is not None:\n-                B = B + B_proj_bias.to(dtype=B.dtype)\n-            if not A.is_complex():\n-                # B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n-                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n-            else:\n-                B = rearrange(B, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n-        else:\n-            if B.stride(-1) != 1:\n-                B = B.contiguous()\n-        if C is None:  # variable C\n-            C = x_dbl[:, -d_state:]  # (bl dstate)\n-            if C_proj_bias is not None:\n-                C = C + C_proj_bias.to(dtype=C.dtype)\n-            if not A.is_complex():\n-                # C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n-                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n-            else:\n-                C = rearrange(C, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n-        else:\n-            if C.stride(-1) != 1:\n-                C = C.contiguous()\n-        if D is not None:\n-            D = D.contiguous()\n-\n-        if b_rms_weight is not None:\n-            B = rearrange(B, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n-            B = rms_norm_forward(B, b_rms_weight, bias=None, eps=b_c_dt_rms_eps)\n-            B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n-        if c_rms_weight is not None:\n-            C = rearrange(C, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n-            C = rms_norm_forward(C, c_rms_weight, bias=None, eps=b_c_dt_rms_eps)\n-            C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n-        if dt_rms_weight is not None:\n-            delta = rearrange(delta, \"b d l -> (b l) d\", l=L).contiguous()\n-            delta = rms_norm_forward(delta, dt_rms_weight, bias=None, eps=b_c_dt_rms_eps)\n-            delta = rearrange(delta, \"(b l) d -> b d l\", l=L).contiguous()\n-\n-        out, scan_intermediates, out_z = selective_scan_cuda.fwd(\n-            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus\n-        )\n-        ctx.delta_softplus = delta_softplus\n-        ctx.out_proj_bias_is_None = out_proj_bias is None\n-        ctx.checkpoint_lvl = checkpoint_lvl\n-        ctx.b_rms_weight = b_rms_weight\n-        ctx.c_rms_weight = c_rms_weight\n-        ctx.dt_rms_weight = dt_rms_weight\n-        ctx.b_c_dt_rms_eps = b_c_dt_rms_eps\n-        if checkpoint_lvl >= 1:  # Will recompute conv1d_out and delta in the backward pass\n-            conv1d_out, delta = None, None\n-        ctx.save_for_backward(\n-            xz,\n-            conv1d_weight,\n-            conv1d_bias,\n-            x_dbl,\n-            x_proj_weight,\n-            delta_proj_weight,\n-            out_proj_weight,\n-            conv1d_out,\n-            delta,\n-            A,\n-            B,\n-            C,\n-            D,\n-            delta_bias,\n-            scan_intermediates,\n-            b_rms_weight,\n-            c_rms_weight,\n-            dt_rms_weight,\n-            out,\n-        )\n-        return F.linear(rearrange(out_z, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n-\n-    @staticmethod\n-    @custom_bwd\n-    def backward(ctx, dout):\n-        # dout: (batch, seqlen, dim)\n-        assert causal_conv1d_cuda is not None, \"causal_conv1d_cuda is not available. Please install causal-conv1d.\"\n-        (\n-            xz,\n-            conv1d_weight,\n-            conv1d_bias,\n-            x_dbl,\n-            x_proj_weight,\n-            delta_proj_weight,\n-            out_proj_weight,\n-            conv1d_out,\n-            delta,\n-            A,\n-            B,\n-            C,\n-            D,\n-            delta_bias,\n-            scan_intermediates,\n-            b_rms_weight,\n-            c_rms_weight,\n-            dt_rms_weight,\n-            out,\n-        ) = ctx.saved_tensors\n-        L = xz.shape[-1]\n-        delta_rank = delta_proj_weight.shape[1]\n-        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n-        x, z = xz.chunk(2, dim=1)\n-        if dout.stride(-1) != 1:\n-            dout = dout.contiguous()\n-        if ctx.checkpoint_lvl == 1:\n-            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, None, None, None, True)\n-            delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l=L)\n-            if dt_rms_weight is not None:\n-                delta = rearrange(delta, \"b d l -> (b l) d\", l=L).contiguous()\n-                delta = rms_norm_forward(delta, ctx.dt_rms_weight, None, ctx.b_c_dt_rms_eps)\n-                delta = rearrange(delta, \"(b l) d -> b d l\", l=L).contiguous()\n-            if b_rms_weight is not None:\n-                # Recompute & RMSNorm B\n-                B = rearrange(B, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n-                B = rms_norm_forward(B, ctx.b_rms_weight, None, ctx.b_c_dt_rms_eps)\n-                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n-            if c_rms_weight is not None:\n-                # Recompute & RMSNorm C\n-                C = rearrange(C, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n-                C = rms_norm_forward(C, ctx.c_rms_weight, None, ctx.b_c_dt_rms_eps)\n-                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n-\n-        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n-        # backward of selective_scan_cuda with the backward of chunk).\n-        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)\n-        dx, dz = dxz.chunk(2, dim=1)\n-        dout = rearrange(dout, \"b l e -> e (b l)\")\n-        dout_y = rearrange(out_proj_weight.t() @ dout, \"d (b l) -> b d l\", l=L)\n-        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = selective_scan_cuda.bwd(\n-            conv1d_out,\n-            delta,\n-            A,\n-            B,\n-            C,\n-            D,\n-            z,\n-            delta_bias,\n-            dout_y,\n-            scan_intermediates,\n-            out,\n-            dz,\n-            ctx.delta_softplus,\n-            True,  # option to recompute out_z\n-        )\n-        dout_proj_weight = torch.einsum(\"eB,dB->ed\", dout, rearrange(out_z, \"b d l -> d (b l)\"))\n-        dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None\n-        dD = dD if D is not None else None\n-        dx_dbl = torch.empty_like(x_dbl)\n-        dB_proj_bias = None\n-        if ctx.is_variable_B:\n-            if not A.is_complex():\n-                dB = rearrange(dB, \"b 1 dstate l -> (b l) dstate\").contiguous()\n-            else:\n-                dB = rearrange(dB, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n-            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None\n-            dx_dbl[:, delta_rank : delta_rank + d_state] = dB  # (bl d)\n-            dB = None\n-        dC_proj_bias = None\n-        if ctx.is_variable_C:\n-            if not A.is_complex():\n-                dC = rearrange(dC, \"b 1 dstate l -> (b l) dstate\").contiguous()\n-            else:\n-                dC = rearrange(dC, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n-            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None\n-            dx_dbl[:, -d_state:] = dC  # (bl d)\n-            dC = None\n-        ddelta = rearrange(ddelta, \"b d l -> d (b l)\")\n-        ddelta_proj_weight = torch.einsum(\"dB,Br->dr\", ddelta, x_dbl[:, :delta_rank])\n-        dx_dbl[:, :delta_rank] = torch.einsum(\"dB,dr->Br\", ddelta, delta_proj_weight)\n-        dconv1d_out = rearrange(dconv1d_out, \"b d l -> d (b l)\")\n-        dx_proj_weight = torch.einsum(\"Br,Bd->rd\", dx_dbl, rearrange(conv1d_out, \"b d l -> (b l) d\"))\n-        dconv1d_out = torch.addmm(dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out)\n-        dconv1d_out = rearrange(dconv1d_out, \"d (b l) -> b d l\", b=x.shape[0], l=x.shape[-1])\n-        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the\n-        # backward of conv1d with the backward of chunk).\n-        dx, dconv1d_weight, dconv1d_bias, *_ = causal_conv1d_cuda.causal_conv1d_bwd(\n-            x, conv1d_weight, conv1d_bias, dconv1d_out, None, None, None, dx, False, True\n-        )\n-        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None\n-        dconv1d_weight = rearrange(dconv1d_weight, \"d w -> d 1 w\")\n-        return (\n-            dxz,\n-            dconv1d_weight,\n-            dconv1d_bias,\n-            dx_proj_weight,\n-            ddelta_proj_weight,\n-            dout_proj_weight,\n-            dout_proj_bias,\n-            dA,\n-            dB,\n-            dC,\n-            dD,\n-            ddelta_bias if delta_bias is not None else None,\n-            # 6-None are delta_softplus, checkpoint_lvl, b_rms_weight, c_rms_weight, dt_rms_weight, b_c_dt_rms_eps\n-            dB_proj_bias,\n-            dC_proj_bias,\n-            None,\n-            None,\n-            None,\n-            None,\n-            None,\n-            None,\n-        )\n-\n-\n-def mamba_inner_fn(\n-    xz,\n-    conv1d_weight,\n-    conv1d_bias,\n-    x_proj_weight,\n-    delta_proj_weight,\n-    out_proj_weight,\n-    out_proj_bias,\n-    A,\n-    B=None,\n-    C=None,\n-    D=None,\n-    delta_bias=None,\n-    B_proj_bias=None,\n-    C_proj_bias=None,\n-    delta_softplus=True,\n-    checkpoint_lvl=1,\n-    b_rms_weight=None,\n-    c_rms_weight=None,\n-    dt_rms_weight=None,\n-    b_c_dt_rms_eps=1e-6,\n-):\n-    return MambaInnerFn.apply(\n-        xz,\n-        conv1d_weight,\n-        conv1d_bias,\n-        x_proj_weight,\n-        delta_proj_weight,\n-        out_proj_weight,\n-        out_proj_bias,\n-        A,\n-        B,\n-        C,\n-        D,\n-        delta_bias,\n-        B_proj_bias,\n-        C_proj_bias,\n-        delta_softplus,\n-        checkpoint_lvl,\n-        b_rms_weight,\n-        c_rms_weight,\n-        dt_rms_weight,\n-        b_c_dt_rms_eps,\n-    )"
        },
        {
            "sha": "83d9a9ad873449691f442d812a12653e81c6825e",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 24,
            "deletions": 35,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/de055d6db06d906cdd82030ef1e6776a2be3d6b5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de055d6db06d906cdd82030ef1e6776a2be3d6b5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=de055d6db06d906cdd82030ef1e6776a2be3d6b5",
            "patch": "@@ -35,11 +35,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.import_utils import (\n-    is_mamba_ssm_available,\n-    is_mambapy_available,\n-    is_torchdynamo_compiling,\n-)\n+from ...utils.import_utils import is_mambapy_available, is_torchdynamo_compiling\n from .configuration_falcon_mamba import FalconMambaConfig\n \n \n@@ -48,14 +44,6 @@\n else:\n     pscan = None\n \n-if is_mamba_ssm_available():\n-    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n-    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n-\n-    from ...kernels.falcon_mamba import mamba_inner_fn\n-else:\n-    selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n-\n \n logger = logging.get_logger(__name__)\n \n@@ -231,7 +219,27 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n         self.use_bias = config.use_bias\n \n+        global causal_conv1d, causal_conv1d_update, causal_conv1d_fn\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n+        global falcon_mamba_ssm, selective_state_update, selective_scan_fn, falcon_mamba_inner_fn\n+        falcon_mamba_ssm = lazy_load_kernel(\"falcon_mamba-ssm\")\n+        selective_state_update, selective_scan_fn, falcon_mamba_inner_fn = (\n+            (\n+                falcon_mamba_ssm.selective_state_update,\n+                falcon_mamba_ssm.selective_scan_fn,\n+                falcon_mamba_ssm.falcon_mamba_inner_fn,\n+            )\n+            if falcon_mamba_ssm is not None\n+            else (None, None, None)\n+        )\n+\n         self.warn_slow_implementation()\n+\n         # Triton expects to pass RMS weights even if they are non learnable, thus we need to create these weights here\n         self.register_buffer(\n             \"b_c_rms\", torch.nn.Parameter(torch.ones(self.ssm_state_size), requires_grad=False), persistent=False\n@@ -242,14 +250,8 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.rms_eps = config.mixer_rms_eps\n \n     def warn_slow_implementation(self):\n-        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n-        causal_conv1d_update, causal_conv1d_fn = (\n-            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n-            if causal_conv1d is not None\n-            else (None, None)\n-        )\n         is_fast_path_available = all(\n-            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, falcon_mamba_inner_fn)\n         )\n         if not is_fast_path_available:\n             if self.use_falcon_mambapy:\n@@ -279,9 +281,8 @@ def cuda_kernels_forward(\n     ):\n         # 1. Gated MLP's linear projection\n         projected_states = self.in_proj(hidden_states).transpose(1, 2)\n-\n         if self.training and cache_params is None:  # Doesn't support outputting the states -> used for training\n-            contextualized_states = mamba_inner_fn(\n+            contextualized_states = falcon_mamba_inner_fn(\n                 projected_states,\n                 self.conv1d.weight,\n                 self.conv1d.bias if self.use_conv_bias else None,\n@@ -302,12 +303,6 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n-            causal_conv1d_update, causal_conv1d_fn = (\n-                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n-                if causal_conv1d is not None\n-                else (None, None)\n-            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -502,14 +497,8 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n-        causal_conv1d_update, causal_conv1d_fn = (\n-            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n-            if causal_conv1d is not None\n-            else (None, None)\n-        )\n         is_fast_path_available = all(\n-            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, falcon_mamba_inner_fn)\n         )\n         if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not is_torchdynamo_compiling():\n             return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)"
        },
        {
            "sha": "ce55cf6bf2a49c11291baaecedd39c5ac3d998de",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 12,
            "deletions": 31,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/de055d6db06d906cdd82030ef1e6776a2be3d6b5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de055d6db06d906cdd82030ef1e6776a2be3d6b5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=de055d6db06d906cdd82030ef1e6776a2be3d6b5",
            "patch": "@@ -19,9 +19,8 @@\n import torch\n from torch import nn\n \n-from ...integrations.hub_kernels import lazy_load_kernel\n from ...utils import auto_docstring, logging\n-from ...utils.import_utils import is_mamba_ssm_available, is_mambapy_available, is_torchdynamo_compiling\n+from ...utils.import_utils import is_mambapy_available, is_torchdynamo_compiling\n from ..mamba.configuration_mamba import MambaConfig\n from ..mamba.modeling_mamba import (\n     MambaBlock,\n@@ -43,13 +42,13 @@\n else:\n     pscan = None\n \n-if is_mamba_ssm_available():\n-    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n-    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n-\n-    from ...kernels.falcon_mamba import mamba_inner_fn\n-else:\n-    selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n+selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, falcon_mamba_inner_fn = (\n+    None,\n+    None,\n+    None,\n+    None,\n+    None,\n+)\n \n \n class FalconMambaConfig(MambaConfig):\n@@ -251,14 +250,8 @@ def rms_forward(hidden_states, variance_epsilon=1e-6):\n \n class FalconMambaMixer(MambaMixer):\n     def warn_slow_implementation(self):\n-        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n-        causal_conv1d_update, causal_conv1d_fn = (\n-            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n-            if causal_conv1d is not None\n-            else (None, None)\n-        )\n         is_fast_path_available = all(\n-            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, falcon_mamba_inner_fn)\n         )\n         if not is_fast_path_available:\n             if self.use_falcon_mambapy:\n@@ -281,6 +274,7 @@ def warn_slow_implementation(self):\n \n     def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n+\n         # Triton expects to pass RMS weights even if they are non learnable, thus we need to create these weights here\n         self.register_buffer(\n             \"b_c_rms\", torch.nn.Parameter(torch.ones(self.ssm_state_size), requires_grad=False), persistent=False\n@@ -299,9 +293,8 @@ def cuda_kernels_forward(\n     ):\n         # 1. Gated MLP's linear projection\n         projected_states = self.in_proj(hidden_states).transpose(1, 2)\n-\n         if self.training and cache_params is None:  # Doesn't support outputting the states -> used for training\n-            contextualized_states = mamba_inner_fn(\n+            contextualized_states = falcon_mamba_inner_fn(\n                 projected_states,\n                 self.conv1d.weight,\n                 self.conv1d.bias if self.use_conv_bias else None,\n@@ -322,12 +315,6 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n-            causal_conv1d_update, causal_conv1d_fn = (\n-                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n-                if causal_conv1d is not None\n-                else (None, None)\n-            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -521,14 +508,8 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n-        causal_conv1d_update, causal_conv1d_fn = (\n-            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n-            if causal_conv1d is not None\n-            else (None, None)\n-        )\n         is_fast_path_available = all(\n-            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, falcon_mamba_inner_fn)\n         )\n         if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not is_torchdynamo_compiling():\n             return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)"
        },
        {
            "sha": "f89d268658d5d937f8c995dd98a675383766f3c7",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 22,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/de055d6db06d906cdd82030ef1e6776a2be3d6b5/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de055d6db06d906cdd82030ef1e6776a2be3d6b5/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=de055d6db06d906cdd82030ef1e6776a2be3d6b5",
            "patch": "@@ -34,7 +34,7 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.import_utils import is_mamba_ssm_available, is_mambapy_available, is_torchdynamo_compiling\n+from ...utils.import_utils import is_mambapy_available, is_torchdynamo_compiling\n from .configuration_mamba import MambaConfig\n \n \n@@ -45,12 +45,6 @@\n else:\n     pscan = None\n \n-if is_mamba_ssm_available():\n-    from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n-    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n-else:\n-    selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n-\n \n class MambaCache:\n     \"\"\"\n@@ -204,15 +198,24 @@ def __init__(self, config: MambaConfig, layer_idx: int):\n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n         self.use_bias = config.use_bias\n \n-        self.warn_slow_implementation()\n-\n-    def warn_slow_implementation(self):\n+        global causal_conv1d, causal_conv1d_update, causal_conv1d_fn\n         causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n         causal_conv1d_update, causal_conv1d_fn = (\n             (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n             if causal_conv1d is not None\n             else (None, None)\n         )\n+        global mamba_ssm, selective_state_update, selective_scan_fn, mamba_inner_fn\n+        mamba_ssm = lazy_load_kernel(\"mamba-ssm\")\n+        selective_state_update, selective_scan_fn, mamba_inner_fn = (\n+            (mamba_ssm.selective_state_update, mamba_ssm.selective_scan_fn, mamba_ssm.mamba_inner_fn)\n+            if mamba_ssm is not None\n+            else (None, None, None)\n+        )\n+\n+        self.warn_slow_implementation()\n+\n+    def warn_slow_implementation(self):\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -263,12 +266,6 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n-            causal_conv1d_update, causal_conv1d_fn = (\n-                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n-                if causal_conv1d is not None\n-                else (None, None)\n-            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -432,12 +429,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n-        causal_conv1d_update, causal_conv1d_fn = (\n-            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n-            if causal_conv1d is not None\n-            else (None, None)\n-        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
        }
    ],
    "stats": {
        "total": 686,
        "additions": 53,
        "deletions": 633
    }
}