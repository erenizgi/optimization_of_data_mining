{
    "author": "zucchini-nlp",
    "message": "LLaVA: latency issues (#34460)\n\n* fix llavas\r\n\r\n* code style\r\n\r\n* green ci",
    "sha": "fe76b603702c7ae7ee4acafd1bc8a7ed80d61950",
    "files": [
        {
            "sha": "a0079f1787a2e9df5364eacf36f1286874d0da32",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 60,
            "deletions": 67,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=fe76b603702c7ae7ee4acafd1bc8a7ed80d61950",
            "patch": "@@ -472,76 +472,75 @@ def forward(\n                 (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n             ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\n \n+        image_features = None\n         if pixel_values is not None:\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values,\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n \n-            if legacy_processing:\n-                logger.warning_once(\n-                    \"Expanding inputs for image tokens in LLaVa should be done in processing. \"\n-                    \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                    \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+        if legacy_processing:\n+            logger.warning_once(\n+                \"Expanding inputs for image tokens in LLaVa should be done in processing. \"\n+                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n+                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+            )\n+            # prefill stage vs decoding stage (legacy behavior copied)\n+            if input_ids.shape[1] != 1:\n+                inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\n+                    image_features, inputs_embeds, input_ids, attention_mask, labels\n                 )\n-                # prefill stage vs decoding stage (legacy behavior copied)\n-                if input_ids.shape[1] != 1:\n-                    inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\n-                        image_features, inputs_embeds, input_ids, attention_mask, labels\n-                    )\n-                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-                else:\n-                    # Retrieve the first layer to inspect the logits and mask out the hidden states\n-                    # that are set to 0\n-                    first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-\n-                    # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                    batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-\n-                    # Get the target length\n-                    target_length = input_ids.shape[1]\n-                    past_length = first_layer_past_key_value.shape[-1]\n-\n-                    extended_attention_mask = torch.ones(\n-                        (attention_mask.shape[0], past_length),\n-                        dtype=attention_mask.dtype,\n-                        device=attention_mask.device,\n-                    )\n-\n-                    # Filter out only the tokens that can be un-attended, this can happen\n-                    # if one uses Llava + Fused modules where the cache on the\n-                    # first iteration is already big enough, or if one passes custom cache\n-                    valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                    new_batch_index = batch_index[valid_indices]\n-                    new_non_attended_tokens = non_attended_tokens[valid_indices]\n-\n-                    # Zero-out the places where we don't need to attend\n-                    extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-\n-                    attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                    position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[\n-                        -target_length:\n-                    ]\n-\n-            # TODO: @raushan retain only the new behavior after v4.47\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n             else:\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n-                n_image_features = image_features.shape[1]\n-                if n_image_tokens != n_image_features:\n-                    raise ValueError(\n-                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                    )\n-                special_image_mask = (\n-                    (input_ids == self.config.image_token_index)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+                # Retrieve the first layer to inspect the logits and mask out the hidden states\n+                # that are set to 0\n+                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n+\n+                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n+                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n+\n+                # Get the target length\n+                target_length = input_ids.shape[1]\n+                past_length = first_layer_past_key_value.shape[-1]\n+\n+                extended_attention_mask = torch.ones(\n+                    (attention_mask.shape[0], past_length),\n+                    dtype=attention_mask.dtype,\n+                    device=attention_mask.device,\n                 )\n-                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+                # Filter out only the tokens that can be un-attended, this can happen\n+                # if one uses Llava + Fused modules where the cache on the\n+                # first iteration is already big enough, or if one passes custom cache\n+                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n+                new_batch_index = batch_index[valid_indices]\n+                new_non_attended_tokens = non_attended_tokens[valid_indices]\n+\n+                # Zero-out the places where we don't need to attend\n+                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n+\n+                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n+                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n+\n+        # TODO: @raushan retain only the new behavior after v4.47\n+        elif image_features is not None:\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n+            n_image_features = image_features.shape[1]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            special_image_mask = (\n+                (input_ids == self.config.image_token_index)\n+                .unsqueeze(-1)\n+                .expand_as(inputs_embeds)\n+                .to(inputs_embeds.device)\n+            )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,\n@@ -602,12 +601,6 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        # Trigger the new behavior if we have more than image embeddings seq length tokens for images\n-        legacy_processing = (\n-            input_ids is not None\n-            and (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n-        )\n-\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n@@ -618,7 +611,7 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        if legacy_processing or cache_position[0] == 0:\n+        if cache_position[0] == 0:\n             # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n             # Otherwise we need pixel values to be passed to model\n             model_inputs[\"pixel_values\"] = pixel_values"
        },
        {
            "sha": "5a49337b2b5d961bc902f7649d4b9e577c6f7ce4",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 65,
            "deletions": 70,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=fe76b603702c7ae7ee4acafd1bc8a7ed80d61950",
            "patch": "@@ -846,6 +846,7 @@ def forward(\n                 (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n             ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\n \n+        image_features = None\n         if pixel_values is not None and pixel_values.size(0) > 0:\n             image_features = self.get_image_features(\n                 pixel_values,\n@@ -861,74 +862,73 @@ def forward(\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n                 image_newline=self.image_newline,\n             )\n-            if legacy_processing:\n-                logger.warning_once(\n-                    \"Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. \"\n-                    \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                    \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+\n+        if legacy_processing:\n+            logger.warning_once(\n+                \"Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. \"\n+                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n+                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+            )\n+            if input_ids.shape[1] != 1:\n+                inputs_embeds = inputs_embeds.to(image_features.dtype)\n+                inputs_embeds, attention_mask, position_ids, labels, _ = self._merge_input_ids_with_image_features(\n+                    image_features,\n+                    feature_lens,\n+                    inputs_embeds,\n+                    input_ids,\n+                    attention_mask,\n+                    position_ids,\n+                    labels=labels,\n                 )\n-                if input_ids.shape[1] != 1:\n-                    inputs_embeds = inputs_embeds.to(image_features.dtype)\n-                    inputs_embeds, attention_mask, position_ids, labels, _ = self._merge_input_ids_with_image_features(\n-                        image_features,\n-                        feature_lens,\n-                        inputs_embeds,\n-                        input_ids,\n-                        attention_mask,\n-                        position_ids,\n-                        labels=labels,\n-                    )\n-                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-                else:\n-                    # Retrieve the first layer to inspect the logits and mask out the hidden states\n-                    # that are set to 0\n-                    first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-\n-                    # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                    batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-\n-                    # Get the target length\n-                    target_length = input_ids.shape[1]\n-                    past_length = first_layer_past_key_value.shape[-1]\n-\n-                    extended_attention_mask = torch.ones(\n-                        (attention_mask.shape[0], past_length),\n-                        dtype=attention_mask.dtype,\n-                        device=attention_mask.device,\n-                    )\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n+            else:\n+                # Retrieve the first layer to inspect the logits and mask out the hidden states\n+                # that are set to 0\n+                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n \n-                    # Filter out only the tokens that can be un-attended, this can happen\n-                    # if one uses Llava + Fused modules where the cache on the\n-                    # first iteration is already big enough, or if one passes custom cache\n-                    valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                    new_batch_index = batch_index[valid_indices]\n-                    new_non_attended_tokens = non_attended_tokens[valid_indices]\n-\n-                    # Zero-out the places where we don't need to attend\n-                    extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-                    attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                    position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[\n-                        -target_length:\n-                    ]\n+                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n+                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n \n-            # TODO: @raushan retain only the new behavior after v4.47\n-            else:\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-                n_image_features = image_features.shape[0]\n-                if n_image_tokens != n_image_features:\n-                    raise ValueError(\n-                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                    )\n-                special_image_mask = (\n-                    (input_ids == self.config.image_token_index)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+                # Get the target length\n+                target_length = input_ids.shape[1]\n+                past_length = first_layer_past_key_value.shape[-1]\n+\n+                extended_attention_mask = torch.ones(\n+                    (attention_mask.shape[0], past_length),\n+                    dtype=attention_mask.dtype,\n+                    device=attention_mask.device,\n+                )\n+\n+                # Filter out only the tokens that can be un-attended, this can happen\n+                # if one uses Llava + Fused modules where the cache on the\n+                # first iteration is already big enough, or if one passes custom cache\n+                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n+                new_batch_index = batch_index[valid_indices]\n+                new_non_attended_tokens = non_attended_tokens[valid_indices]\n+\n+                # Zero-out the places where we don't need to attend\n+                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n+                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n+                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n+\n+        # TODO: @raushan retain only the new behavior after v4.47\n+        elif image_features is not None:\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n+            n_image_features = image_features.shape[0]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+            special_image_mask = (\n+                (input_ids == self.config.image_token_index)\n+                .unsqueeze(-1)\n+                .expand_as(inputs_embeds)\n+                .to(inputs_embeds.device)\n+            )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,\n@@ -990,11 +990,6 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        legacy_processing = (\n-            input_ids is not None\n-            and (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n-        )\n-\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n@@ -1007,7 +1002,7 @@ def prepare_inputs_for_generation(\n \n         # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n         # Otherwise we need pixel values to be passed to model\n-        if legacy_processing or cache_position[0] == 0:\n+        if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"image_sizes\"] = image_sizes\n "
        },
        {
            "sha": "44b372535d70bdf1c597eb94f0d4d9aba7ac720b",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=fe76b603702c7ae7ee4acafd1bc8a7ed80d61950",
            "patch": "@@ -1110,17 +1110,6 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- extra custom processing\n \n-        if input_ids is not None:\n-            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n-                1\n-            ).max() < self.config.image_seq_length\n-            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n-                1\n-            ).max() < self.config.video_seq_length\n-            legacy_processing = (img_token_not_enough and pixel_values is not None) or (\n-                video_token_not_enough and pixel_values_videos is not None\n-            )\n-\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n@@ -1133,7 +1122,7 @@ def prepare_inputs_for_generation(\n \n         # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n         # Otherwise we need pixel values to be passed to model\n-        if legacy_processing or cache_position[0] == 0:\n+        if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n             model_inputs[\"image_sizes\"] = image_sizes"
        },
        {
            "sha": "e9974e920493ff650e957c80c4b7b09c0ac285b1",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=fe76b603702c7ae7ee4acafd1bc8a7ed80d61950",
            "patch": "@@ -623,17 +623,6 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- extra custom processing\n \n-        if input_ids is not None:\n-            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n-                1\n-            ).max() < self.config.image_seq_length\n-            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n-                1\n-            ).max() < self.config.video_seq_length\n-            legacy_processing = (img_token_not_enough and pixel_values is not None) or (\n-                video_token_not_enough and pixel_values_videos is not None\n-            )\n-\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n@@ -646,7 +635,7 @@ def prepare_inputs_for_generation(\n \n         # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n         # Otherwise we need pixel values to be passed to model\n-        if legacy_processing or cache_position[0] == 0:\n+        if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n             model_inputs[\"image_sizes\"] = image_sizes"
        },
        {
            "sha": "30f82e45056c77cc873df5ced52ff31e000266ac",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=fe76b603702c7ae7ee4acafd1bc8a7ed80d61950",
            "patch": "@@ -720,17 +720,6 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        if input_ids is not None:\n-            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n-                1\n-            ).max() < self.config.image_seq_length\n-            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n-                1\n-            ).max() < self.config.video_seq_length\n-            legacy_processing = (img_token_not_enough and pixel_values_images is not None) or (\n-                video_token_not_enough and pixel_values_videos is not None\n-            )\n-\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n@@ -741,7 +730,7 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        if legacy_processing or cache_position[0] == 0:\n+        if cache_position[0] == 0:\n             # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n             # Otherwise we need pixel values to be passed to model\n             model_inputs[\"pixel_values_images\"] = pixel_values_images"
        },
        {
            "sha": "c9db6e261c6a72fb84cde6f3b5e684e97c82d722",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 58,
            "deletions": 65,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe76b603702c7ae7ee4acafd1bc8a7ed80d61950/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=fe76b603702c7ae7ee4acafd1bc8a7ed80d61950",
            "patch": "@@ -466,72 +466,71 @@ def forward(\n                 (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n             ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\n \n+        image_features = None\n         if pixel_values is not None:\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n             )\n \n-            if legacy_processing:\n-                logger.warning_once(\n-                    \"Expanding inputs for image tokens in VipLLaVa should be done in processing. \"\n-                    \"Please add `patch_size` and `vision_feature_select_strategy` to the model's image processing config. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+        if legacy_processing:\n+            logger.warning_once(\n+                \"Expanding inputs for image tokens in VipLLaVa should be done in processing. \"\n+                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's image processing config. \"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+            )\n+            # prefill stage vs decoding stage (legacy behavior copied)\n+            if input_ids.shape[1] != 1:\n+                inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\n+                    image_features, inputs_embeds, input_ids, attention_mask, labels\n                 )\n-                # prefill stage vs decoding stage (legacy behavior copied)\n-                if input_ids.shape[1] != 1:\n-                    inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\n-                        image_features, inputs_embeds, input_ids, attention_mask, labels\n-                    )\n-                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-                else:\n-                    # Retrieve the first layer to inspect the logits and mask out the hidden states\n-                    # that are set to 0\n-                    first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-\n-                    # Sum all dimensions of head_dim (-1) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                    batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-\n-                    target_length = input_ids.shape[1]\n-                    past_length = first_layer_past_key_value.shape[-1]\n-\n-                    extended_attention_mask = torch.ones(\n-                        (attention_mask.shape[0], past_length),\n-                        dtype=attention_mask.dtype,\n-                        device=attention_mask.device,\n-                    )\n-\n-                    # Filter out only the tokens that can be un-attended, this can happen\n-                    # in the case one uses Llava + Fused modules where the cache on the\n-                    # first iteration is already big enough, or if one passes custom cache\n-                    valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                    new_batch_index = batch_index[valid_indices]\n-                    new_non_attended_tokens = non_attended_tokens[valid_indices]\n-\n-                    # Zero-out the places where we don't need to attend\n-                    extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-\n-                    attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                    position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[\n-                        -target_length:\n-                    ]\n-\n-            # TODO: @raushan retain only the new behavior after v4.47\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n             else:\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n-                n_image_features = image_features.shape[1]\n-                if n_image_tokens != n_image_features:\n-                    raise ValueError(\n-                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                    )\n-                special_image_mask = (\n-                    (input_ids == self.config.image_token_index)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+                # Retrieve the first layer to inspect the logits and mask out the hidden states\n+                # that are set to 0\n+                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n+\n+                # Sum all dimensions of head_dim (-1) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n+                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n+\n+                target_length = input_ids.shape[1]\n+                past_length = first_layer_past_key_value.shape[-1]\n+\n+                extended_attention_mask = torch.ones(\n+                    (attention_mask.shape[0], past_length),\n+                    dtype=attention_mask.dtype,\n+                    device=attention_mask.device,\n                 )\n-                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+                # Filter out only the tokens that can be un-attended, this can happen\n+                # in the case one uses Llava + Fused modules where the cache on the\n+                # first iteration is already big enough, or if one passes custom cache\n+                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n+                new_batch_index = batch_index[valid_indices]\n+                new_non_attended_tokens = non_attended_tokens[valid_indices]\n+\n+                # Zero-out the places where we don't need to attend\n+                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n+\n+                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n+                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n+\n+        # TODO: @raushan retain only the new behavior after v4.47\n+        elif image_features is not None:\n+            n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n+            n_image_features = image_features.shape[1]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            special_image_mask = (\n+                (input_ids == self.config.image_token_index)\n+                .unsqueeze(-1)\n+                .expand_as(inputs_embeds)\n+                .to(inputs_embeds.device)\n+            )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,\n@@ -590,12 +589,6 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        # Trigger the new behavior if we have more than image embeddings seq length tokens for images\n-        legacy_processing = (\n-            input_ids is not None\n-            and (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n-        )\n-\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n@@ -606,7 +599,7 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        if legacy_processing or cache_position[0] == 0:\n+        if cache_position[0] == 0:\n             # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n             # Otherwise we need pixel values to be passed to model\n             model_inputs[\"pixel_values\"] = pixel_values"
        }
    ],
    "stats": {
        "total": 424,
        "additions": 186,
        "deletions": 238
    }
}